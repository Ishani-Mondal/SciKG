{"title": [{"text": "Modeling Missing Data in Distant Supervision for Information Extraction", "labels": [], "entities": [{"text": "Modeling Missing Data in Distant Supervision", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.7420627375443777}, {"text": "Information Extraction", "start_pos": 49, "end_pos": 71, "type": "TASK", "confidence": 0.7181183248758316}]}], "abstractContent": [{"text": "Distant supervision algorithms learn information extraction models given only large readily available databases and text collections.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 37, "end_pos": 59, "type": "TASK", "confidence": 0.7940858006477356}]}, {"text": "Most previous work has used heuristics for generating labeled data, for example assuming that facts not contained in the database are not mentioned in the text, and facts in the database must be mentioned at least once.", "labels": [], "entities": []}, {"text": "In this paper, we propose anew latent-variable approach that models missing data.", "labels": [], "entities": []}, {"text": "This provides a natural way to incorporate side information , for instance modeling the intuition that text will often mention rare entities which are likely to be missing in the database.", "labels": [], "entities": []}, {"text": "Despite the added complexity introduced by reasoning about missing data, we demonstrate that a carefully designed local search approach to inference is very accurate and scales to large datasets.", "labels": [], "entities": []}, {"text": "Experiments demonstrate improved performance for binary and unary relation extraction when compared to learning with heuristic labels, including on average a 27% increase in area under the precision recall curve in the binary case.", "labels": [], "entities": [{"text": "unary relation extraction", "start_pos": 60, "end_pos": 85, "type": "TASK", "confidence": 0.688806414604187}, {"text": "precision recall curve", "start_pos": 189, "end_pos": 211, "type": "METRIC", "confidence": 0.8382353583971659}]}], "introductionContent": [{"text": "This paper addresses the issue of missing data) in the context of distant supervision.", "labels": [], "entities": []}, {"text": "The goal of distant supervision is to learn to process unstructured data, for instance to extract binary or unary relations from text (), using a large database of propositions as a  distant source of supervision.", "labels": [], "entities": []}, {"text": "In the case of binary relations, the intuition is that any sentence which mentions a pair of entities (e 1 and e 2 ) that participate in a relation, r, is likely to express the proposition r(e 1 , e 2 ), so we can treat it as a positive training example of r. presents an example of this process.", "labels": [], "entities": []}, {"text": "One question which has received little attention in previous work is how to handle the situation where information is missing, either from the text corpus, or the database.", "labels": [], "entities": []}, {"text": "As an example, suppose the pair of entities (John P. McNamara, Washington State University) is absent from the EMPLOYER relation.", "labels": [], "entities": [{"text": "McNamara, Washington State University)", "start_pos": 53, "end_pos": 91, "type": "DATASET", "confidence": 0.6034245093663534}]}, {"text": "In this case, the sentence in (and others which mention the entity pair) is effectively treated as a negative example of the relation.", "labels": [], "entities": []}, {"text": "This is an issue of practical concern, as most databases of interest are highly incomplete -this is the reason we need to extend them by extracting information from text in the first place.", "labels": [], "entities": []}, {"text": "We need to be cautious in how we handle missing data in distant supervision, because this is a case where data is not missing at random (NMAR).", "labels": [], "entities": []}, {"text": "Whether a proposition is observed or missing in the text or database depends heavily on its truth value: given that it is true we have some chance to observe it, however we do not observe those which are false.", "labels": [], "entities": []}, {"text": "To address this challenge, we propose a joint model of extraction from text and the process by which propositions are observed or missing in both the database and text.", "labels": [], "entities": []}, {"text": "Our approach provides a natural way to incorporate side information in the form of a missing data model.", "labels": [], "entities": []}, {"text": "For instance, popular entities such as Barack Obama already have good coverage in Freebase, so new extractions are more likely to be errors than those involving rare entities with poor coverage.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 82, "end_pos": 90, "type": "DATASET", "confidence": 0.9597035050392151}]}, {"text": "Our approach to missing data is general and can be combined with various IE solutions.", "labels": [], "entities": [{"text": "IE", "start_pos": 73, "end_pos": 75, "type": "TASK", "confidence": 0.9541730880737305}]}, {"text": "As a proof of concept, we extend MultiR (), a recent model for distantly supervised information extraction, to explicitly model missing data.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 84, "end_pos": 106, "type": "TASK", "confidence": 0.727588340640068}]}, {"text": "These extensions complicate the MAP inference problem which is used as a subroutine in learning.", "labels": [], "entities": [{"text": "MAP inference", "start_pos": 32, "end_pos": 45, "type": "TASK", "confidence": 0.9128265976905823}]}, {"text": "This motivated us to explore a variety of approaches to inference in the joint extraction and missing data model.", "labels": [], "entities": [{"text": "joint extraction", "start_pos": 73, "end_pos": 89, "type": "TASK", "confidence": 0.7305978834629059}]}, {"text": "We explore both exact inference based on A* search and efficient approximate inference using local search.", "labels": [], "entities": []}, {"text": "Our experiments demonstrate that with a carefully designed set of search operators, local search produces optimal solutions inmost cases.", "labels": [], "entities": []}, {"text": "Experimental results demonstrate large performance gains over the heuristic labeling strategy on both binary relation extraction and weakly supervised named entity categorization.", "labels": [], "entities": [{"text": "binary relation extraction", "start_pos": 102, "end_pos": 128, "type": "TASK", "confidence": 0.6265162428220113}]}, {"text": "For example our model obtains a 27% increase in area under the precision recall curve on the sentence-level relation extraction task.", "labels": [], "entities": [{"text": "area", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.9570780396461487}, {"text": "precision", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9979396462440491}, {"text": "recall", "start_pos": 73, "end_pos": 79, "type": "METRIC", "confidence": 0.6782235503196716}, {"text": "sentence-level relation extraction task", "start_pos": 93, "end_pos": 132, "type": "TASK", "confidence": 0.7009900957345963}]}], "datasetContent": [{"text": "In Section 5, we presented a scalable approach to inference in the DNMAR model which almost always finds an optimal solution.", "labels": [], "entities": [{"text": "DNMAR model", "start_pos": 67, "end_pos": 78, "type": "DATASET", "confidence": 0.9275747537612915}]}, {"text": "Of course the real question is: does modeling missing data improve performance at extracting information from text?", "labels": [], "entities": []}, {"text": "In this section we present experimental results showing large improvements in both precision and recall on two distantly supervised learning tasks: binary relation extraction and named entity categorization.", "labels": [], "entities": [{"text": "precision", "start_pos": 83, "end_pos": 92, "type": "METRIC", "confidence": 0.9992265701293945}, {"text": "recall", "start_pos": 97, "end_pos": 103, "type": "METRIC", "confidence": 0.9988355040550232}, {"text": "binary relation extraction", "start_pos": 148, "end_pos": 174, "type": "TASK", "confidence": 0.6274433533350626}]}, {"text": "To demonstrate the effect of modeling missing data in the distantly supervised named entity categorization task, we adapt the MultiR and DNMAR models to the Twitter named entity categorization dataset which was presented by.", "labels": [], "entities": []}, {"text": "The models described so far are applied unchanged: rather than modeling a set of relations in Freebase between a pair of entities, e 1 and e 2 , we now model a set of possible Freebase categories associated with a single entity e.", "labels": [], "entities": []}, {"text": "This is a natural extension of distant supervision from binary to unary relations.", "labels": [], "entities": []}, {"text": "The unlabeled data and features described by are used for training the model, and their manually annotated Twitter named entity dataset is used for evaluation.", "labels": [], "entities": []}], "tableCaptions": []}