{"title": [{"text": "An HDP Model for Inducing Combinatory Categorial Grammars", "labels": [], "entities": [{"text": "Inducing Combinatory Categorial Grammars", "start_pos": 17, "end_pos": 57, "type": "TASK", "confidence": 0.6708150058984756}]}], "abstractContent": [{"text": "We introduce a novel nonparametric Bayesian model for the induction of Combinatory Categorial Grammars from POS-tagged text.", "labels": [], "entities": [{"text": "induction of Combinatory Categorial Grammars from POS-tagged text", "start_pos": 58, "end_pos": 123, "type": "TASK", "confidence": 0.7581329867243767}]}, {"text": "It achieves state of the art performance on a number of languages, and induces linguistically plausible lexicons.", "labels": [], "entities": []}], "introductionContent": [{"text": "What grammatical representation is appropriate for unsupervised grammar induction?", "labels": [], "entities": [{"text": "unsupervised grammar induction", "start_pos": 51, "end_pos": 81, "type": "TASK", "confidence": 0.7207942605018616}]}, {"text": "Initial attempts with context-free grammars (CFGs) were not very successful.", "labels": [], "entities": [{"text": "context-free grammars (CFGs)", "start_pos": 22, "end_pos": 50, "type": "TASK", "confidence": 0.6539472341537476}]}, {"text": "One reason maybe that CFGs require the specification of a finite inventory of nonterminal categories and rewrite rules, but unless one adopts linguistic principles such as X-bar theory, these nonterminals are essentially arbitrary labels that can be combined in arbitrary ways.", "labels": [], "entities": []}, {"text": "While further CFG-based approaches have been proposed), most recent work has followed in developing models for the induction of projective dependency grammars.", "labels": [], "entities": [{"text": "induction of projective dependency grammars", "start_pos": 115, "end_pos": 158, "type": "TASK", "confidence": 0.7649661540985108}]}, {"text": "It has been shown that more sophisticated probability models and learning regimes (, as well as the incorporation of prior linguistic knowledge can lead to significant improvement over Klein and Manning's baseline model.", "labels": [], "entities": []}, {"text": "The use of dependency grammars circumvents the question of how to obtain an appropriate inventory of categories, since dependency parses are simply defined by unlabeled edges between the lexical items in the sentence.", "labels": [], "entities": []}, {"text": "But dependency grammars make it also difficult to capture non-local structures, and show that it maybe advantageous to reformulate the underlying dependency grammar in terms of a tree-substitution grammar (TSG) which pairs words with treelets that specify the number of left and right dependents they have.", "labels": [], "entities": []}, {"text": "In this paper, we explore yet another option: instead of dependency grammars, we use Combinatory Categorial Grammar), a linguistically expressive formalism that pairs lexical items with rich categories that capture all language-specific information.", "labels": [], "entities": []}, {"text": "This may seem a puzzling choice, since CCG requires a significantly larger inventory of categories than is commonly assumed for CFGs.", "labels": [], "entities": []}, {"text": "However, unlike CFG nonterminals, CCG categories are not arbitrary symbols: they encode, and are determined by, the basic word order of the language and the number of arguments each word takes.", "labels": [], "entities": []}, {"text": "CCG is very similar to TSG in that it also pairs lexical items with rich items that capture all language-specific information.", "labels": [], "entities": []}, {"text": "Like TSG and projective dependency grammars, we restrict ourselves to a weakly contextfree fragment of CCG.", "labels": [], "entities": []}, {"text": "But while TSG does not distinguish between argument and modifier dependencies, CCG makes an explicit distinction between the two.", "labels": [], "entities": [{"text": "TSG", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.5661295652389526}]}, {"text": "And while the elementary trees of's TSG and their internal nodel labels have no obvious linguistic interpretation, the syntactic behavior of any CCG constituent can be directly inferred from its category.", "labels": [], "entities": []}, {"text": "To see whether the algorithm has identified the basic syntactic properties of the language, it is hence sufficient to inspect the induced lexicon.", "labels": [], "entities": []}, {"text": "Conversely, show that knowledge of these basic syntactic properties makes it very easy to create a language-specific lexicon for accurate unsupervised CCG parsing.", "labels": [], "entities": [{"text": "CCG parsing", "start_pos": 151, "end_pos": 162, "type": "TASK", "confidence": 0.6194407194852829}]}, {"text": "We have recently proposed an algorithm for inducing CCGs) that has been shown to be competitive with other approaches even when paired with a very simple probability model.", "labels": [], "entities": []}, {"text": "In this paper, we pair this induction algorithm with a novel nonparametric Bayesian model that is based on a different factorization of CCG derivations, and show that it outperforms our original model and many other approaches on a large number of languages.", "labels": [], "entities": []}, {"text": "Our results indicate that the use of CCG yields grammars that are significantly more robust when dealing with longer sentences than most dependency grammar-based approaches.", "labels": [], "entities": []}], "datasetContent": [{"text": "As is standard for this task, we evaluate our systems against a number of different dependency treebanks, and measure performance in terms of the accuracy of directed dependencies (i.e. the percentage of words in the test corpus that are correctly attached).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 146, "end_pos": 154, "type": "METRIC", "confidence": 0.9963493347167969}]}, {"text": "We use the data from the PASCAL challenge for grammar induction, the data from the CoNLL-X shared task () and Goldberg (2011)'s Hebrew corpus.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.8963892757892609}]}, {"text": "Converting CCG derivations into dependencies is mostly straightforward, since the CCG derivation identifies the root word of each sentence, and headargument and head-modifier dependencies are easily read off of CCG derivations, since the lexicon defines them explicitly.", "labels": [], "entities": []}, {"text": "Unlike dependency grammar, CCG is designed to recover non-local dependencies that arise in control and binding constructions as well as in wh-extraction and non-standard coordination, but since this requires re-entrancies, or coindexation of arguments, within the lexical categories that trigger these constructions, our current system returns only local dependencies.", "labels": [], "entities": []}, {"text": "But since dependency grammars also captures only local dependencies, this has no negative influence on our current evaluation.", "labels": [], "entities": []}, {"text": "However, a direct comparison between dependency treebanks and dependencies produced by CCG is more difficult, since dependency grammars allow considerable freedom in how to analyze specific constructions such as verb clusters (which verb is the head?) prepositional phrases and particles (is the head the noun or the preposition/particle?), subordinating conjunctions (is the conjunction a dependent of the head of the main clause and the head of the embedded clause a dependent of the conjunction, or vice versa?) and this is reflected in the fact that the treebanks we consider often apply different conventions for these cases.", "labels": [], "entities": []}, {"text": "Although remedying this issue is beyond the scope of this work, these discrepancies very much hint at the need fora better mechanism to evaluate linguistically equivalent structures or treebank standardization.", "labels": [], "entities": []}, {"text": "The most problematic construction is coordination.", "labels": [], "entities": []}, {"text": "In standard CCG-to-dependency schemes, both conjuncts are independent, and the conjunction itself is not attached to the dependency graph, whereas dependency grammars have to stipulate that either one of the conjuncts or the conjunction itself is the head, with multiple possibilities of where the remaining constituents attach.", "labels": [], "entities": []}, {"text": "In addition to the standard CCG scheme, we have identified five main styles of conjunction in our data), although several corpora distinguish multiple types of coordinating conjunctions which use different styles (not all shown here).", "labels": [], "entities": []}, {"text": "Since our system has explicit rules for coordination, we transform its output into the desired target representation that is specific to each language.", "labels": [], "entities": []}, {"text": "We evaluate our system on 13 different languages.", "labels": [], "entities": []}, {"text": "In each case, we follow the test and training regimes that were used to obtain previously published results in order to allow a direct comparison.", "labels": [], "entities": []}, {"text": "We compare our system to the results presented at the PAS-CAL Challenge on Grammar Induction (Gelling et al., 2012) 6 , as well as to and) into dependencies.", "labels": [], "entities": [{"text": "PAS-CAL Challenge on Grammar Induction (Gelling et al., 2012)", "start_pos": 54, "end_pos": 115, "type": "TASK", "confidence": 0.6755863502621651}]}, {"text": "Finally, when training the MLE version of our model we use a simple smoothing scheme which defines a small rule probability (e \u221215 ) to prevent any rule used during training from going to zero.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: A comparison of the basic Argument model (MLE) and four hyper-parameter settings of the HDP- CCG against two syntactic formalisms that participated in the PASCAL Challenge (Gelling et al., 2012),  BH (Bisk and Hockenmaier, 2012a) and BC (", "labels": [], "entities": [{"text": "BH", "start_pos": 207, "end_pos": 209, "type": "METRIC", "confidence": 0.9843387007713318}, {"text": "BC", "start_pos": 244, "end_pos": 246, "type": "METRIC", "confidence": 0.9490890502929688}]}, {"text": " Table 2: A comparison of our system with", "labels": [], "entities": []}, {"text": " Table 3: A comparison of our system with Gillenwa- ter et al.", "labels": [], "entities": [{"text": "Gillenwa- ter et al", "start_pos": 42, "end_pos": 61, "type": "DATASET", "confidence": 0.9146561741828918}]}]}