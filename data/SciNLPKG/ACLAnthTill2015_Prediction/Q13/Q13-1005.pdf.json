{"title": [{"text": "Weakly Supervised Learning of Semantic Parsers for Mapping Instructions to Actions", "labels": [], "entities": [{"text": "Mapping Instructions to Actions", "start_pos": 51, "end_pos": 82, "type": "TASK", "confidence": 0.9150944948196411}]}], "abstractContent": [{"text": "The context in which language is used provides a strong signal for learning to recover its meaning.", "labels": [], "entities": []}, {"text": "In this paper, we show it can be used within a grounded CCG semantic parsing approach that learns a joint model of meaning and context for interpreting and executing natural language instructions, using various types of weak supervision.", "labels": [], "entities": [{"text": "CCG semantic parsing", "start_pos": 56, "end_pos": 76, "type": "TASK", "confidence": 0.5522573490937551}]}, {"text": "The joint nature provides crucial benefits by allowing situated cues, such as the set of visible objects, to directly influence learning.", "labels": [], "entities": []}, {"text": "It also enables algorithms that learn while executing instructions, for example by trying to replicate human actions.", "labels": [], "entities": []}, {"text": "Experiments on a benchmark naviga-tional dataset demonstrate strong performance under differing forms of supervision, including correctly executing 60% more instruction sets relative to the previous state of the art.", "labels": [], "entities": []}], "introductionContent": [{"text": "The context in which natural language is used provides a strong signal to reason about its meaning.", "labels": [], "entities": []}, {"text": "However, using such a signal to automatically learn to understand unrestricted natural language remains a challenging, unsolved problem.", "labels": [], "entities": []}, {"text": "For example, consider the instructions in.", "labels": [], "entities": []}, {"text": "Correct interpretation requires us to solve many subproblems, such as resolving all referring expressions to specific objects in the environment (including, \"the corner\" or \"the third intersection\"), disambiguating word sense based on context (e.g., \"the chair\" could refer to a chair or sofa), and finding executable action sequences that satisfy stated constraints (such as \"twice\" or \"to face the blue hall\").", "labels": [], "entities": []}, {"text": "Figure 1: A sample navigation instruction set, paired with lambda-calculus meaning representations.", "labels": [], "entities": []}, {"text": "We must also understand implicit requests, for example from the phrase \"at the corner,\" that describe goals to be achieved without specifying the specific steps.", "labels": [], "entities": []}, {"text": "Finally, to do all of this robustly without prohibitive engineering effort, we need grounded learning approaches that jointly reason about meaning and context to learn directly from their interplay, with as little human intervention as possible.", "labels": [], "entities": []}, {"text": "Although many of these challenges have been studied separately, as we will review in Section 3, this paper represents, to the best of our knowledge, the first attempt at a comprehensive model that addresses them all.", "labels": [], "entities": []}, {"text": "Our approach induces a weighted Combinatory Categorial Grammar (CCG), including both the parameters of the linear model and a CCG lexicon.", "labels": [], "entities": []}, {"text": "To model complex instructional language, we introduce anew semantic modeling approach that can represent a number of key linguistic constructs that are common in spatial and instructional language.", "labels": [], "entities": []}, {"text": "To learn from indirect supervision, we define the notion of a validation function, for example that tests the state of the agent after interpreting an instruction.", "labels": [], "entities": []}, {"text": "We then show how this function can be used to drive online learning.", "labels": [], "entities": []}, {"text": "For that purpose, we adapt the loss-sensitive Perceptron algorithm) to use a validation function and coarse-to-fine inference for lexical induction.", "labels": [], "entities": [{"text": "lexical induction", "start_pos": 130, "end_pos": 147, "type": "TASK", "confidence": 0.7129241526126862}]}, {"text": "The joint nature of this approach provides crucial benefits in that it allows situated cues, such as the set of visible objects, to directly influence parsing and learning.", "labels": [], "entities": [{"text": "parsing", "start_pos": 151, "end_pos": 158, "type": "TASK", "confidence": 0.965038537979126}]}, {"text": "It also enables the model to be learned while executing instructions, for example by trying to replicate actions taken by humans.", "labels": [], "entities": []}, {"text": "In particular, we show that, given only a small seed lexicon and a task-specific executor, we can induce high quality models for interpreting complex instructions.", "labels": [], "entities": [{"text": "interpreting complex instructions", "start_pos": 129, "end_pos": 162, "type": "TASK", "confidence": 0.8979387482007345}]}, {"text": "We evaluate the method on a benchmark navigational instructions dataset (.", "labels": [], "entities": []}, {"text": "Our joint approach successfully completes 60% more instruction sets relative to the previous state of the art.", "labels": [], "entities": []}, {"text": "We also report experiments that vary supervision type, finding that observing the final position of an instruction execution is nearly as informative as observing the entire path.", "labels": [], "entities": []}, {"text": "Finally, we present improved results on anew version of the corpus, which we filtered to include only executable instructions paired with correct traces.", "labels": [], "entities": []}], "datasetContent": [{"text": "Data For evaluation, we use the navigation task from, which includes three environments and the SAIL corpus of instructions and follower traces.", "labels": [], "entities": [{"text": "SAIL corpus", "start_pos": 96, "end_pos": 107, "type": "DATASET", "confidence": 0.8410393297672272}]}, {"text": "Chen and Mooney (2011) segmented the data, aligned traces to instructions, and merged traces created by different subjects.", "labels": [], "entities": []}, {"text": "The corpus includes raw sentences, without any form of linguistic annotation.", "labels": [], "entities": []}, {"text": "The original collection process () created many uninterpretable instructions and incorrect traces.", "labels": [], "entities": []}, {"text": "To focus on the learning and interpretation tasks, we also created anew dataset that includes only accurate instructions labeled with a single, correct execution trace.", "labels": [], "entities": []}, {"text": "From this oracle corpus, we randomly sampled 164 instruction sequences (816 sentences) for evaluation, leaving 337 (1863 sentences) for training.", "labels": [], "entities": []}, {"text": "This simple effort will allow us to measure the effects of noise on the learning approach and provides a resource for building more accurate algorithms.: Cross-validation development accuracy and standard deviation on the oracle corpus. tries.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 183, "end_pos": 191, "type": "METRIC", "confidence": 0.9947156310081482}]}, {"text": "The sequences were randomly selected from the training set, so as to include two sequences for each participant in the original experiment. and 4 include a sample of our seed lexicon.", "labels": [], "entities": []}, {"text": "Initialization and Parameters We set the weight of each template indicator feature to the number of times it is used in the seed lexicon and each repetition feature to -10.", "labels": [], "entities": [{"text": "Parameters", "start_pos": 19, "end_pos": 29, "type": "METRIC", "confidence": 0.9761062264442444}]}, {"text": "Learning parameters were tuned using cross-validation on the training set: the margin \u03b4 is set to 1, the GEN LEX margin \u03b4 L is set to 2, we use 6 iterations (8 for experiments on SAIL) and take the 250 top parses during lexical generation (step 1,).", "labels": [], "entities": [{"text": "GEN LEX margin \u03b4 L", "start_pos": 105, "end_pos": 123, "type": "METRIC", "confidence": 0.7985026359558105}]}, {"text": "For parameter update (step 2,) we use a parser with abeam of 100.", "labels": [], "entities": []}, {"text": "GEN LEX generates lexical entries for token sequences up to length 4.", "labels": [], "entities": [{"text": "GEN LEX", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.7256379425525665}]}, {"text": "k s , the instruction sequence execution beam, is set to 10.", "labels": [], "entities": []}, {"text": "Finally, k I is set to 2, allowing up to two implicit action sequences per explicit one.", "labels": [], "entities": []}, {"text": "Evaluation Metrics To evaluate single instructions x, we compare the agent's end state to a labeled state s , as described in Section 2.", "labels": [], "entities": []}, {"text": "We use a similar method to evaluate the execution of instruction sequences x, but disregard the orientation, since end goals in are defined without orientation.", "labels": [], "entities": []}, {"text": "When evaluating logical forms we measure exact match accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.6183082461357117}]}], "tableCaptions": [{"text": " Table 1: Corpora statistics (lower-cased data).", "labels": [], "entities": []}, {"text": " Table 3: Cross-validation accuracy and standard de- viation for the SAIL corpus.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9909316301345825}, {"text": "standard de- viation", "start_pos": 40, "end_pos": 60, "type": "METRIC", "confidence": 0.7899219840764999}, {"text": "SAIL corpus", "start_pos": 69, "end_pos": 80, "type": "DATASET", "confidence": 0.8662945032119751}]}]}