{"title": [{"text": "Dijkstra-WSA: A Graph-Based Approach to Word Sense Alignment", "labels": [], "entities": [{"text": "Word Sense Alignment", "start_pos": 40, "end_pos": 60, "type": "TASK", "confidence": 0.7226915955543518}]}], "abstractContent": [{"text": "In this paper, we present Dijkstra-WSA, a novel graph-based algorithm for word sense alignment.", "labels": [], "entities": [{"text": "word sense alignment", "start_pos": 74, "end_pos": 94, "type": "TASK", "confidence": 0.808709720770518}]}, {"text": "We evaluate it on four different pairs of lexical-semantic resources with different characteristics (WordNet-OmegaWiki, WordNet-Wiktionary, GermaNet-Wiktionary and WordNet-Wikipedia) and show that it achieves competitive performance on 3 out of 4 datasets.", "labels": [], "entities": [{"text": "WordNet-OmegaWiki", "start_pos": 101, "end_pos": 118, "type": "DATASET", "confidence": 0.9452055096626282}, {"text": "WordNet-Wiktionary", "start_pos": 120, "end_pos": 138, "type": "DATASET", "confidence": 0.9060415029525757}]}, {"text": "Dijkstra-WSA outperforms the state of the art on every dataset if it is combined with a back-off based on gloss similarity.", "labels": [], "entities": []}, {"text": "We also demonstrate that Dijkstra-WSA is not only flexibly applicable to different resources but also highly parameterizable to optimize for precision or recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 141, "end_pos": 150, "type": "METRIC", "confidence": 0.9989641904830933}, {"text": "recall", "start_pos": 154, "end_pos": 160, "type": "METRIC", "confidence": 0.9891490340232849}]}], "introductionContent": [{"text": "Lexical-semantic resources (LSRs) area cornerstone for many Natural Language Processing (NLP) applications such as word sense disambiguation (WSD) and information extraction.", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 115, "end_pos": 146, "type": "TASK", "confidence": 0.8052818328142166}, {"text": "information extraction", "start_pos": 151, "end_pos": 173, "type": "TASK", "confidence": 0.8587307035923004}]}, {"text": "However, the growing demand for large-scale resources in different languages is hard to meet.", "labels": [], "entities": []}, {"text": "The Princeton WordNet (WN)) is widely used for English, but for most languages corresponding resources are considerably smaller or missing.", "labels": [], "entities": [{"text": "Princeton WordNet (WN))", "start_pos": 4, "end_pos": 27, "type": "DATASET", "confidence": 0.9507099986076355}]}, {"text": "Collaboratively constructed resources like Wiktionary (WKT) and OmegaWiki (OW) provide a viable option for such cases and seem especially suitable for smaller languages), but there are still considerable gaps in coverage which need to be filled.", "labels": [], "entities": []}, {"text": "A related problem is that there usually does not exist a single resource which works best for all purposes, as different LSRs cover different words, senses and information types.", "labels": [], "entities": []}, {"text": "These considerations have sparked increasing research efforts in the area of word sense alignment (WSA).", "labels": [], "entities": [{"text": "word sense alignment (WSA)", "start_pos": 77, "end_pos": 103, "type": "TASK", "confidence": 0.8131459305683771}]}, {"text": "It has been shown that aligned resources can indeed lead to better performance than using the resources individually.", "labels": [], "entities": []}, {"text": "Examples include semantic parsing using FrameNet (FN), WN, and VerbNet (VN)), word sense disambiguation using an alignment of WN and Wikipedia (WP)) and semantic role labeling using a combination of PropBank, VN and FN in the SemLink project.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 17, "end_pos": 33, "type": "TASK", "confidence": 0.7180963158607483}, {"text": "word sense disambiguation", "start_pos": 78, "end_pos": 103, "type": "TASK", "confidence": 0.7401992678642273}, {"text": "semantic role labeling", "start_pos": 153, "end_pos": 175, "type": "TASK", "confidence": 0.6775071918964386}]}, {"text": "Some of these approaches to WSA either rely heavily on manual labor (e.g.) or on information which is only present in few resources such as the most frequent sense (MFS).", "labels": [], "entities": [{"text": "WSA", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.9916535019874573}]}, {"text": "This makes it difficult to apply them to a larger set of resources.", "labels": [], "entities": []}, {"text": "In earlier work, we presented the large-scale resource UBY . It contains nine resources in two languages which are mapped to a uniform representation using the LMF standard ).", "labels": [], "entities": []}, {"text": "They are thus structurally interoperable.", "labels": [], "entities": []}, {"text": "UBY contains pairwise sense alignments between a subset of these resources, and this work also presented a framework for creating alignments based on the similarity of glosses ().", "labels": [], "entities": [{"text": "UBY", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8841089010238647}]}, {"text": "However, it is not clear to what extent this approach can be applied to resources which lack this kind of information (see Section 3).", "labels": [], "entities": []}, {"text": "In summary, aligning senses is a key requirement for semantic interoperability of LSRs to increase the coverage and effectiveness in NLP tasks.", "labels": [], "entities": []}, {"text": "Still, existing efforts are mostly focused on specific types of resources (most often requiring glosses) or application scenarios.", "labels": [], "entities": []}, {"text": "In this paper, we propose an approach to alleviate this and present Dijkstra-WSA, a novel, robust algorithm for word sense alignment which is applicable to a wide variety of resource pairs and languages.", "labels": [], "entities": [{"text": "word sense alignment", "start_pos": 112, "end_pos": 132, "type": "TASK", "confidence": 0.7485067844390869}]}, {"text": "For the first time, we apply a graph-based algorithm which works on full graph representations of both resources to word sense alignment.", "labels": [], "entities": [{"text": "word sense alignment", "start_pos": 116, "end_pos": 136, "type": "TASK", "confidence": 0.7907885511716207}]}, {"text": "This enables us to take a more abstract perspective and reduce the problem of identifying equivalent senses to the problem of matching nodes in these graphs.", "labels": [], "entities": []}, {"text": "Also for the first time, we comparatively evaluate a WSA algorithm on a variety of different datasets with different characteristics.", "labels": [], "entities": [{"text": "WSA", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.9713080525398254}]}, {"text": "The key properties of Dijkstra-WSA are: Robustness The entities within the LSRs which are to be aligned (usually senses or synsets) are modeled as nodes in the graph.", "labels": [], "entities": []}, {"text": "These nodes are connected by an edge if they are semantically related.", "labels": [], "entities": []}, {"text": "While, for instance, semantic relations lend themselves very well to deriving edges, different possibilities for graph construction are equally valid as the algorithm is agnostic to the origin of the edges.", "labels": [], "entities": [{"text": "graph construction", "start_pos": 113, "end_pos": 131, "type": "TASK", "confidence": 0.7608464062213898}]}, {"text": "Language-independence No external resources such as corpora or other dictionaries are needed; the graph construction and alignment only rely on the information from the considered LSRs.", "labels": [], "entities": [{"text": "graph construction", "start_pos": 98, "end_pos": 116, "type": "TASK", "confidence": 0.7618278861045837}]}, {"text": "Flexibility The graph construction as well as the actual alignment are highly parameterizable to accommodate different requirements regarding precision or recall.", "labels": [], "entities": [{"text": "Flexibility", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9381535649299622}, {"text": "precision", "start_pos": 142, "end_pos": 151, "type": "METRIC", "confidence": 0.9987425208091736}, {"text": "recall", "start_pos": 155, "end_pos": 161, "type": "METRIC", "confidence": 0.9665996432304382}]}, {"text": "The rest of this paper is structured as follows: In Section 2 we give a precise problem description and introduce the resources covered in our experiments, in Section 3 we discuss some related work, while our graph-based algorithm Dijkstra-WSA is presented in Section 4.", "labels": [], "entities": []}, {"text": "We describe an evaluation on four datasets with different properties, including an error analysis, in Section 5 and conclude in Section 6, pointing out directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the evaluation of Dijkstra-WSA, we align four pairs of LSRs used in previous work, namely WN-OW ( ), WN-WKT (Meyer and Gurevych, 2011), GN-WKT ( and WN-WP ().", "labels": [], "entities": []}, {"text": "Our goal is to cover resources with different characteristics: Expert-built (WN, GN) and collaboratively constructed LSRs (WP, WKT, OW), resources in different languages (English and German) and also resources with few sense descriptions (GN) or semantic relations (WKT).", "labels": [], "entities": []}, {"text": "We contrastively discuss the results of the Dijkstra-WSA algorithm on these different datasets and relate the results to the properties of the LSRs involved.", "labels": [], "entities": []}, {"text": "Moreover, using existing datasets ensures comparability to previous work which discusses only one dataset at a time.", "labels": [], "entities": []}, {"text": "WordNet (WN)) is a lexical resource for the English language created at Princeton University.", "labels": [], "entities": [{"text": "WordNet (WN))", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.9077803045511246}]}, {"text": "The resource is organized in sets of synonymous words (synsets) which are represented by glosses (sometimes accompanied by example sentences) and organized in a hierarchy.", "labels": [], "entities": []}, {"text": "The latest version 3.0 contains 117,659 synsets.", "labels": [], "entities": []}, {"text": "Wikipedia (WP) is a freely available, multilingual online encyclopedia.", "labels": [], "entities": [{"text": "Wikipedia (WP)", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.8774859011173248}]}, {"text": "WP can be edited by every Web user, which causes rapid growth: By February 2013 the English WP contained over 4,000,000 article pages.", "labels": [], "entities": []}, {"text": "Each article usually describes a distinct concept, and articles are connected by hyperlinks within the article texts.", "labels": [], "entities": []}, {"text": "Wiktionary (WKT) is the dictionary pendant to WP.", "labels": [], "entities": [{"text": "Wiktionary (WKT)", "start_pos": 0, "end_pos": 16, "type": "DATASET", "confidence": 0.656392365694046}]}, {"text": "By February 2013 the English WKT contained over 3,200,000 article pages, while the German edition contained over 200,000 ones.", "labels": [], "entities": [{"text": "English WKT", "start_pos": 21, "end_pos": 32, "type": "DATASET", "confidence": 0.6282534003257751}]}, {"text": "For each word, multiple senses can be encoded.", "labels": [], "entities": []}, {"text": "Similar to WN, they are represented by a gloss and usage examples.", "labels": [], "entities": []}, {"text": "There also exist hyperlinks to synonyms, hypernyms, meronyms etc.", "labels": [], "entities": []}, {"text": "The targets of these relations are not senses, however, but merely lexemes (i.e. the relations are not disambiguated).", "labels": [], "entities": []}, {"text": "OmegaWiki (OW) is a freely editable online dictionary like WKT.", "labels": [], "entities": [{"text": "OmegaWiki (OW)", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.8907773047685623}, {"text": "WKT", "start_pos": 59, "end_pos": 62, "type": "DATASET", "confidence": 0.9829137921333313}]}, {"text": "However, there do not exist distinct language editions as OW is organized in language-independent concepts (\"Defined Meanings\") to which lexicalizations in various languages are attached.", "labels": [], "entities": []}, {"text": "These can be considered as multilingual synsets, and they are interconnected by unambiguous relations just like WN.", "labels": [], "entities": []}, {"text": "As of February 2013, OW contains over 46,000 of these concepts and lexicalizations in over 400 languages.", "labels": [], "entities": []}, {"text": "GermaNet (GN) is the German counterpart to WN.", "labels": [], "entities": [{"text": "GermaNet (GN)", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.8236046880483627}]}, {"text": "It is also organized in synsets (around 70,000 in the latest version 7.0) which are connected via semantic relations.", "labels": [], "entities": []}, {"text": "WN 3.0-English OW The previous alignment between these two resources reported in  is based on the German OW (database dump from 2010/01/03) and WN 3.0 and utilizes gloss similarities using machine translation as an intermediate component.", "labels": [], "entities": [{"text": "WN 3.0-English OW", "start_pos": 0, "end_pos": 17, "type": "DATASET", "confidence": 0.8582970897356669}, {"text": "German OW (database dump from 2010/01/03)", "start_pos": 98, "end_pos": 139, "type": "DATASET", "confidence": 0.9432530403137207}]}, {"text": "This does not pose a problem since for each synset in the German part of OW, there is a translation in the English part.", "labels": [], "entities": [{"text": "OW", "start_pos": 73, "end_pos": 75, "type": "DATASET", "confidence": 0.7059178948402405}]}, {"text": "This makes the German-English gold standard directly usable for our purposes.", "labels": [], "entities": [{"text": "German-English gold standard", "start_pos": 15, "end_pos": 43, "type": "DATASET", "confidence": 0.9011454582214355}]}, {"text": "presents the details about this as well as the other evaluation datasets, including the observed inter-rater agreement A 0 (where available) which can be considered as an upper bound for automatic alignment accuracy and the degree of polysemy (i.e. the number of possible alignment targets per sense) which is a hint towards the difficulty of the alignment task.", "labels": [], "entities": [{"text": "inter-rater agreement A 0", "start_pos": 97, "end_pos": 122, "type": "METRIC", "confidence": 0.804215595126152}, {"text": "accuracy", "start_pos": 207, "end_pos": 215, "type": "METRIC", "confidence": 0.874537467956543}]}, {"text": "WN 3.0-English WKT We use the gold standard dataset from Meyer and Gurevych (2011) without any modification, thus for comparability to this work, we use the same WKT dump version (from 2010/02/01) which contains around 421,000 senses.", "labels": [], "entities": [{"text": "WN 3.0-English WKT", "start_pos": 0, "end_pos": 18, "type": "DATASET", "confidence": 0.8561676144599915}, {"text": "WKT dump version (from 2010/02/01", "start_pos": 162, "end_pos": 195, "type": "DATASET", "confidence": 0.8931734740734101}]}, {"text": "GN 7.0-German WKT Henrich et al.", "labels": [], "entities": [{"text": "GN 7.0-German WKT Henrich et al.", "start_pos": 0, "end_pos": 32, "type": "DATASET", "confidence": 0.9592663049697876}]}, {"text": "(2011) aligned the German WKT (dump from 2011/04/02, 72,000 senses) and GN 7.0.", "labels": [], "entities": [{"text": "German WKT (dump from 2011/04/02", "start_pos": 19, "end_pos": 51, "type": "DATASET", "confidence": 0.8621688485145569}, {"text": "GN 7.0", "start_pos": 72, "end_pos": 78, "type": "DATASET", "confidence": 0.9031360149383545}]}, {"text": "This is the only existing alignment between these two resources so far, and we use their freely available dataset 3 to test Dijkstra-WSA on a language other than English.", "labels": [], "entities": []}, {"text": "As this alignment is fairly large (see), we created a random sample as a gold standard to keep the computation time at bay.", "labels": [], "entities": []}, {"text": "However, the datasets are still similar enough to allow direct comparison of the results.", "labels": [], "entities": []}, {"text": "Note that no inter-annotator agreement is available for this study.", "labels": [], "entities": []}, {"text": "WN 3.0-English WP We use the gold standard from.", "labels": [], "entities": [{"text": "WN 3.0-English WP", "start_pos": 0, "end_pos": 17, "type": "DATASET", "confidence": 0.9018239577611288}]}, {"text": "For comparability, we use the same Wikipeda dump version (from 2009/08/22) with around 2,921,000 articles.", "labels": [], "entities": [{"text": "Wikipeda dump version (from 2009/08/22)", "start_pos": 35, "end_pos": 74, "type": "DATASET", "confidence": 0.8862689245830883}]}, {"text": "WN-OW When using only semantic relations (SR), we achieved an F-Measure of 0.53 which is comparable with the 0.54 from . Notably, our approach has a high precision, while the recall is considerably worse due to the relative sparsity of the resulting OW resource graph.", "labels": [], "entities": [{"text": "F-Measure", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.9991303086280823}, {"text": "precision", "start_pos": 154, "end_pos": 163, "type": "METRIC", "confidence": 0.9988698363304138}, {"text": "recall", "start_pos": 175, "end_pos": 181, "type": "METRIC", "confidence": 0.9995887875556946}]}, {"text": "When adding more edges to the graph by linking monosemous lexemes (SR+LM), we can drastically improve the recall, leading to an overall F-Measure of 0.62, which is a significant improvement over our previous results ).", "labels": [], "entities": [{"text": "recall", "start_pos": 106, "end_pos": 112, "type": "METRIC", "confidence": 0.9996660947799683}, {"text": "F-Measure", "start_pos": 136, "end_pos": 145, "type": "METRIC", "confidence": 0.9993534684181213}]}, {"text": "Using monosemous links only (LM), the result of 0.58 still outperforms  due to the higher precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 90, "end_pos": 99, "type": "METRIC", "confidence": 0.9969266057014465}]}, {"text": "Building a graph from glosses alone is thus a viable approach if no or only few semantic relations are available.", "labels": [], "entities": []}, {"text": "Regarding the path lengths, \u03bb = 10 works best when semantic relations are included in the graph, while for the LM configuration shorter paths (\u03bb \u2264 5) were more appropriate.", "labels": [], "entities": []}, {"text": "The intuition behind this is that for semantic relations, unlike monosemous links, even longer paths still express a high degree of semantic relatedness.", "labels": [], "entities": []}, {"text": "Also, when semantic relations are involved allowing multiple alignments increases the overall results (which is inline with the relatively high number of 1:n alignments in the gold standard), while this is not the case for the LM configuration; here, the edges again do not sufficiently express relatedness.", "labels": [], "entities": []}, {"text": "Using the hybrid approach (+SB), we can increase the F-Measure up to 0.65 if semantic relations and monosemous linking are combined (SR+LM) and the parameters are tuned for high precision (\u03bb \u2264 3, 1:1 alignments).", "labels": [], "entities": [{"text": "F-Measure", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.9827213883399963}, {"text": "SR+LM)", "start_pos": 133, "end_pos": 139, "type": "METRIC", "confidence": 0.8772128075361252}, {"text": "precision", "start_pos": 178, "end_pos": 187, "type": "METRIC", "confidence": 0.9893546104431152}]}, {"text": "This is significantly better than Dijkstra-WSA alone in any configuration.", "labels": [], "entities": []}, {"text": "In this scenario, we also observe the best overall recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9964699745178223}]}, {"text": "WN-WKT Experiments using only the semantic relations (SR) yield a very low recall -the small number of sense relations with monosemous targets in WKT leaves the graph very sparse.", "labels": [], "entities": [{"text": "WN-WKT", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.7499597668647766}, {"text": "recall", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.9995697140693665}]}, {"text": "Nevertheless, the alignment targets which Dijkstra-WSA finds are mostly correct, with a precision greater than 0.95 even when allowing 1:n alignments.", "labels": [], "entities": [{"text": "precision", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.9971998929977417}]}, {"text": "Using only monosemous links (LM) improves the recall considerably, but unlike the WN-OW alignment, it stays fairly low.", "labels": [], "entities": [{"text": "recall", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.9997014403343201}]}, {"text": "Consequently, even when using semantic relations and monosemous links in conjunction (SR+LM), the recall can only be increased slightly, leading to an overall F-Measure of 0.39.", "labels": [], "entities": [{"text": "recall", "start_pos": 98, "end_pos": 104, "type": "METRIC", "confidence": 0.9997228980064392}, {"text": "F-Measure", "start_pos": 159, "end_pos": 168, "type": "METRIC", "confidence": 0.9994202852249146}]}, {"text": "As mentioned above, this is due to the WKT glosses.", "labels": [], "entities": [{"text": "WKT glosses", "start_pos": 39, "end_pos": 50, "type": "DATASET", "confidence": 0.942479819059372}]}, {"text": "In many cases, they are very short, often consisting of only 3-5 words, many of which are polysemous.", "labels": [], "entities": []}, {"text": "This leads to many isolated nodes in the graph with no or only very few connecting edges.", "labels": [], "entities": []}, {"text": "The ideal, rather short path length \u03bb of 2-3 stems from the relatively high polysemy of the gold standard (see).", "labels": [], "entities": []}, {"text": "We experimented with \u03bb \u2265 4, achieving reasonable recall, but in this case the precision was so low that this configuration, in conclusion, does not increase the F-Measure.", "labels": [], "entities": [{"text": "recall", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.9992621541023254}, {"text": "precision", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.9993501305580139}, {"text": "F-Measure", "start_pos": 161, "end_pos": 170, "type": "METRIC", "confidence": 0.995474636554718}]}, {"text": "However, 1:n alignments work well with these short paths as the correct alignments are mostly in the close vicinity of a sense, hence we achieve an increase in recall in this case without too much loss of precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 160, "end_pos": 166, "type": "METRIC", "confidence": 0.9992258548736572}, {"text": "precision", "start_pos": 205, "end_pos": 214, "type": "METRIC", "confidence": 0.9969671368598938}]}, {"text": "For the hybrid approach, we achieve an F-Measure of 0.69 when using all edges (SR+LM+SB), setting the path length to 2, and also allowing 1:n alignments.", "labels": [], "entities": [{"text": "F-Measure", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9984846711158752}, {"text": "SR+LM+SB)", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.8659904599189758}]}, {"text": "This is a statistically significant improvement over which again confirms the effectiveness of the hybrid approach.", "labels": [], "entities": []}, {"text": "GN-WKT As stated above, we used the SR+LM configuration for GN in every case.", "labels": [], "entities": [{"text": "GN-WKT", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9620013236999512}]}, {"text": "For the German WKT, the much greater number of relations compared to its English counterpart is directly reflected in the results, as using the semantic relations only (SR) not only yields the best precision of 0.94 but also a good recall of 0.65.", "labels": [], "entities": [{"text": "German WKT", "start_pos": 8, "end_pos": 18, "type": "DATASET", "confidence": 0.7120019793510437}, {"text": "precision", "start_pos": 198, "end_pos": 207, "type": "METRIC", "confidence": 0.9965085387229919}, {"text": "recall", "start_pos": 232, "end_pos": 238, "type": "METRIC", "confidence": 0.9994058609008789}]}, {"text": "Using the semantic relations together with monosemous links (SR+LM) yields the F-Measure of 0.83, which is on par with the similarity-based (SB) approach.", "labels": [], "entities": [{"text": "F-Measure", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.9980746507644653}]}, {"text": "In the hybrid configuration, we can increase the performance to an F-Measure of up to 0.87 (SR+LM+SB), significantly outperforming all graph-based and similarity-based configurations.", "labels": [], "entities": [{"text": "F-Measure", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.9984666109085083}, {"text": "SR+LM+SB)", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.9437539577484131}]}, {"text": "In general, results for this pair of LSRs are higher in comparison with the others.", "labels": [], "entities": []}, {"text": "We attribute this to the fact that the German WKT and GN both are densely linked with semantic relations which is especially beneficial for the recall of Dijkstra-WSA.", "labels": [], "entities": [{"text": "German WKT", "start_pos": 39, "end_pos": 49, "type": "DATASET", "confidence": 0.7690706551074982}, {"text": "GN", "start_pos": 54, "end_pos": 56, "type": "DATASET", "confidence": 0.6209489703178406}, {"text": "recall", "start_pos": 144, "end_pos": 150, "type": "METRIC", "confidence": 0.6916013360023499}]}, {"text": "This is also reflected in the ideal \u03bb of 10-12: Many high-confidence edges allow long paths which still express a considerable degree of relatedness.", "labels": [], "entities": []}, {"text": "However, while the results for 1:n alignments are already good, restricting oneself to 1:1 alignments gives the best overall results as the precision can then be pushed towards 0.90 without hurting recall too much.", "labels": [], "entities": [{"text": "precision", "start_pos": 140, "end_pos": 149, "type": "METRIC", "confidence": 0.9983125925064087}, {"text": "recall", "start_pos": 198, "end_pos": 204, "type": "METRIC", "confidence": 0.9962624907493591}]}, {"text": "An important factor in this respect is that the GN-WKT dataset has a relatively low degree of polysemy (compared to WN-WKT) and only few 1:n alignments (compared to WN-OW), two facts which make the task significantly easier.", "labels": [], "entities": [{"text": "GN-WKT dataset", "start_pos": 48, "end_pos": 62, "type": "DATASET", "confidence": 0.9362123012542725}]}, {"text": "WN-WP The SR configuration (WN relations + WP category/first paragraph links) yields the best precision (0.82), even outperforming the SB approach, and an F-Measure of 0.71.", "labels": [], "entities": [{"text": "precision", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.9988345503807068}, {"text": "F-Measure", "start_pos": 155, "end_pos": 164, "type": "METRIC", "confidence": 0.9938795566558838}]}, {"text": "This again shows that using an appropriate parametrization (\u03bb \u2264 4 in this case) Dijkstra-WSA can detect alignments with high confidence.", "labels": [], "entities": []}, {"text": "The relatively low recall of 0.63 could be increased by allowing longer paths, however, as hyperlinks do not express relatedness as reliably as semantic relations, this introduces many false positives and thus hurts precision considerably.", "labels": [], "entities": [{"text": "recall", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.9991582632064819}, {"text": "precision", "start_pos": 216, "end_pos": 225, "type": "METRIC", "confidence": 0.9988117218017578}]}, {"text": "This issue of \"misleading\" WP links becomes even more prominent when the links from the full articles are used as edges (LM); while the increase in recall is relatively small the precision drops substantially.", "labels": [], "entities": [{"text": "edges (LM)", "start_pos": 114, "end_pos": 124, "type": "METRIC", "confidence": 0.7858940660953522}, {"text": "recall", "start_pos": 148, "end_pos": 154, "type": "METRIC", "confidence": 0.9985381364822388}, {"text": "precision", "start_pos": 179, "end_pos": 188, "type": "METRIC", "confidence": 0.9994357228279114}]}, {"text": "However, using all possible links (SR+LM) allows us to balance out precision and recall to some extent, while yielding the same F-Measure as the SR configuration.", "labels": [], "entities": [{"text": "precision", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.9993810653686523}, {"text": "recall", "start_pos": 81, "end_pos": 87, "type": "METRIC", "confidence": 0.9989356398582458}, {"text": "F-Measure", "start_pos": 128, "end_pos": 137, "type": "METRIC", "confidence": 0.9921642541885376}]}, {"text": "Note that 1:1 alignments were enforced in any case, as the high polysemy of the dataset in conjunction with the dense WP link structure rendered 1:n alignments very imprecise.", "labels": [], "entities": []}, {"text": "Using the hybrid approach, we can increase the FMeasure up to 0.81 (SR+SB), outperforming the results reported in by a significant margin.", "labels": [], "entities": [{"text": "FMeasure", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9985442161560059}, {"text": "SR+SB)", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.9684583842754364}]}, {"text": "The F-Measure for LM+SB is slightly worse due to the lower precision.", "labels": [], "entities": [{"text": "F-Measure", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9991788268089294}, {"text": "LM+SB", "start_pos": 18, "end_pos": 23, "type": "TASK", "confidence": 0.3897046049435933}, {"text": "precision", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9975466132164001}]}, {"text": "Combining all edges (SR+LM+SB) does not influence the results anymore, but in any case the hybrid configuration achieves the best overall recall (0.87).", "labels": [], "entities": [{"text": "SR+LM+SB)", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.8500527441501617}, {"text": "recall", "start_pos": 138, "end_pos": 144, "type": "METRIC", "confidence": 0.9995249509811401}]}, {"text": "In conclusion, our experiments on all four datasets consistently demonstrate that combining Dijkstra-WSA with a similarity-based approach as a back-off yields the strongest performance.", "labels": [], "entities": []}, {"text": "The results of these best alignments will be made freely available to the research community on our website (http://www.ukp.tu-darmstadt.de).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Pseudocode of the Dijkstra-WSA algorithm.", "labels": [], "entities": [{"text": "Pseudocode", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9372209310531616}]}, {"text": " Table 3: Characteristics of the gold standards used in the evaluation. A 0 is the observed inter-rater agreement which  can be considered as an upper bound for alignment accuracy. The degree of polysemy (i.e. the number of possible  alignment targets per sense) hints towards the difficulty of the alignment task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 171, "end_pos": 179, "type": "METRIC", "confidence": 0.944574773311615}]}, {"text": " Table 4: This table describes what percentage of nodes  remains isolated (i.", "labels": [], "entities": []}, {"text": " Table 5: Alignment results for all datasets and configurations: Using semantic relations (SR), monosemous links  (LM) or both (SR+LM). The similarity-based (SB) baselines, also used as a back-off for the hybrid approaches (+SB),  were created using the approach reported in", "labels": [], "entities": []}]}