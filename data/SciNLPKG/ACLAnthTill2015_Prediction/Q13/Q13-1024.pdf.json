{"title": [{"text": "Large-scale Word Alignment Using Soft Dependency Cohesion Constraints", "labels": [], "entities": [{"text": "Large-scale Word Alignment", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.5748598476250967}]}], "abstractContent": [{"text": "Dependency cohesion refers to the observation that phrases dominated by disjoint dependency subtrees in the source language generally do not overlap in the target language.", "labels": [], "entities": []}, {"text": "It has been verified to be a useful constraint for word alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 51, "end_pos": 65, "type": "TASK", "confidence": 0.8272276520729065}]}, {"text": "However, previous work either treats this as a hard constraint or uses it as a feature in discriminative models, which is ineffective for large-scale tasks.", "labels": [], "entities": []}, {"text": "In this paper, we take dependency cohesion as a soft constraint, and integrate it into a generative model for large-scale word alignment experiments.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 122, "end_pos": 136, "type": "TASK", "confidence": 0.7258802056312561}]}, {"text": "We also propose an approximate EM algorithm and a Gibbs sampling algorithm to estimate model parameters in an unsupervised manner.", "labels": [], "entities": []}, {"text": "Experiments on large-scale Chinese-English translation tasks demonstrate that our model achieves improvements in both alignment quality and translation quality.", "labels": [], "entities": [{"text": "Chinese-English translation tasks", "start_pos": 27, "end_pos": 60, "type": "TASK", "confidence": 0.7380073070526123}]}], "introductionContent": [{"text": "Word alignment is the task of identifying word correspondences between parallel sentence pairs.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7311021089553833}, {"text": "identifying word correspondences between parallel sentence pairs", "start_pos": 30, "end_pos": 94, "type": "TASK", "confidence": 0.7296582460403442}]}, {"text": "Word alignment has become a vital component of statistical machine translation (SMT) systems, since it is required by almost all state-of-the-art SMT systems for the purpose of extracting phrase tables or even syntactic transformation rules ().", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7322399020195007}, {"text": "statistical machine translation (SMT)", "start_pos": 47, "end_pos": 84, "type": "TASK", "confidence": 0.7880467474460602}, {"text": "SMT", "start_pos": 146, "end_pos": 149, "type": "TASK", "confidence": 0.9795108437538147}]}, {"text": "During the past two decades, generative word alignment models such as the IBM Models () and the HMM model () have been widely used, primarily because they are trained on bilingual sentences in an unsupervised manner and the implementation is freely available in the GIZA++ toolkit.", "labels": [], "entities": [{"text": "generative word alignment", "start_pos": 29, "end_pos": 54, "type": "TASK", "confidence": 0.902144173781077}, {"text": "IBM Models", "start_pos": 74, "end_pos": 84, "type": "DATASET", "confidence": 0.9049578607082367}]}, {"text": "However, the word alignment quality of generative models is still far from satisfactory for SMT systems.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 13, "end_pos": 27, "type": "TASK", "confidence": 0.6997883021831512}, {"text": "SMT", "start_pos": 92, "end_pos": 95, "type": "TASK", "confidence": 0.9950893521308899}]}, {"text": "In recent years, discriminative alignment models incorporating linguistically motivated features have become increasingly popular).", "labels": [], "entities": []}, {"text": "These models are usually trained with manually annotated parallel data.", "labels": [], "entities": []}, {"text": "However, when moving to anew language pair, large amount of hand-aligned data are usually unavailable and expensive to create.", "labels": [], "entities": []}, {"text": "A more practical way to improve large-scale word alignment quality is to introduce syntactic knowledge into a generative model and train the model in an unsupervised manner;.", "labels": [], "entities": [{"text": "word alignment quality", "start_pos": 44, "end_pos": 66, "type": "TASK", "confidence": 0.7958752115567526}]}, {"text": "In this paper, we take dependency cohesion) into account, which assumes phrases dominated by disjoint dependency subtrees tend not to overlap after translation.", "labels": [], "entities": []}, {"text": "Instead of treating dependency cohesion as a hard constraint (  or using it as a feature in discriminative models), we treat dependency cohesion as a distortion constraint, and integrate it into a modified HMM word alignment model to softly influence the probabilities of alignment candidates.", "labels": [], "entities": [{"text": "HMM word alignment", "start_pos": 206, "end_pos": 224, "type": "TASK", "confidence": 0.5933944682280222}]}, {"text": "We also propose an approximate EM algorithm and an explicit Gibbs sampling algorithm to train the model in an unsupervised manner.", "labels": [], "entities": []}, {"text": "Experiments on a large-scale Chinese-English translation task demonstrate that our model achieves improvements in both word alignment quality and machine translation quality.", "labels": [], "entities": [{"text": "Chinese-English translation task", "start_pos": 29, "end_pos": 61, "type": "TASK", "confidence": 0.7773543000221252}, {"text": "word alignment", "start_pos": 119, "end_pos": 133, "type": "TASK", "confidence": 0.7727276980876923}, {"text": "machine translation", "start_pos": 146, "end_pos": 165, "type": "TASK", "confidence": 0.6558287590742111}]}, {"text": "The remainder of this paper is organized as follows: Section 2 introduces dependency cohesion constraint for word alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 109, "end_pos": 123, "type": "TASK", "confidence": 0.8025860786437988}]}, {"text": "Section 3 presents our generative model for word alignment using dependency cohesion constraint.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 44, "end_pos": 58, "type": "TASK", "confidence": 0.7922840714454651}]}, {"text": "Section 4 describes algorithms for parameter estimation.", "labels": [], "entities": [{"text": "parameter estimation", "start_pos": 35, "end_pos": 55, "type": "TASK", "confidence": 0.6680146753787994}]}, {"text": "We discuss and analyze the experiments in Section 5.", "labels": [], "entities": []}, {"text": "Section 6 gives the related work.", "labels": [], "entities": []}, {"text": "Finally, we conclude this paper and mention future work in Section 7.", "labels": [], "entities": []}, {"text": "showed that dependency cohesion is generally maintained between English and French.", "labels": [], "entities": []}, {"text": "To test how well this assumption holds between Chinese and English, we measure the dependency cohesion between the two languages with a manually annotated bilingual Chinese-English data set of 502 sentence pairs 1 . We use the headmodifier cohesion percentage (HCP) and the modifier-modifier cohesion percentage (MCP) to measure the degree of cohesion in the corpus.", "labels": [], "entities": [{"text": "headmodifier cohesion percentage (HCP)", "start_pos": 227, "end_pos": 265, "type": "METRIC", "confidence": 0.6995708048343658}, {"text": "modifier-modifier cohesion percentage (MCP)", "start_pos": 274, "end_pos": 317, "type": "METRIC", "confidence": 0.7305635015169779}]}, {"text": "HCP (or MCP) is used for measuring how many headmodifier (or modifier-modifier) pairs are actually cohesive.", "labels": [], "entities": []}, {"text": "lists the relative percentages in both Chinese-to-English (ch-en, using Chinese side dependency trees) and English-to-Chinese (en-ch, using English side dependency trees) directions.", "labels": [], "entities": []}, {"text": "As we see from, dependency cohesion is The data set is the development set used in Section 5.", "labels": [], "entities": []}, {"text": "Figure 1: A Chinese-English sentence pair including the word alignments and the Chinese side dependency tree.", "labels": [], "entities": []}, {"text": "The Chinese and English words are listed horizontally and vertically, respectively.", "labels": [], "entities": []}, {"text": "The black grids are gold-standard alignments.", "labels": [], "entities": []}, {"text": "For the Chinese word \"\u6709/have\", we give two alignment positions, where \"R\" is the correct alignment and \"W\" is the incorrect alignment.", "labels": [], "entities": []}, {"text": "generally maintained between Chinese and English.", "labels": [], "entities": []}, {"text": "So dependency cohesion would be helpful for word alignment between Chinese and English.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 44, "end_pos": 58, "type": "TASK", "confidence": 0.7946042716503143}]}, {"text": "However, there are still a number of crossings.", "labels": [], "entities": []}, {"text": "If we restrict alignment space with a hard cohesion constraint, the correct alignments that result in crossings will be ruled out directly.", "labels": [], "entities": []}, {"text": "In the next section, we describe an approach to integrating dependency cohesion constraint into a generative model to softly influence the probabilities of alignment candidates.", "labels": [], "entities": []}, {"text": "We show that our new approach addresses the shortcomings of using dependency cohesion as a hard constraint.", "labels": [], "entities": []}], "datasetContent": [{"text": "We performed a series of experiments to evaluate our model.", "labels": [], "entities": []}, {"text": "All the experiments are conducted on the Chinese-English language pair.", "labels": [], "entities": []}, {"text": "We employ two training sets: FBIS and LARGE.", "labels": [], "entities": [{"text": "FBIS", "start_pos": 29, "end_pos": 33, "type": "DATASET", "confidence": 0.5479019284248352}, {"text": "LARGE", "start_pos": 38, "end_pos": 43, "type": "METRIC", "confidence": 0.6391946077346802}]}, {"text": "The size and source corpus of these training sets are listed in.", "labels": [], "entities": []}, {"text": "We will use the smaller training set FBIS to evaluate the characters of our model and use the LARGE training set to evaluate whether our model is adaptable for large-scale task.", "labels": [], "entities": [{"text": "FBIS", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.5578646063804626}, {"text": "LARGE training set", "start_pos": 94, "end_pos": 112, "type": "DATASET", "confidence": 0.7984073162078857}]}, {"text": "For word alignment quality evaluation, we take the handaligned data sets from SSMT2007 2 , which contains 505 sentence pairs in the testing set and 502 sentence pairs in the development set.", "labels": [], "entities": [{"text": "word alignment quality evaluation", "start_pos": 4, "end_pos": 37, "type": "TASK", "confidence": 0.9019516408443451}, {"text": "SSMT2007 2", "start_pos": 78, "end_pos": 88, "type": "DATASET", "confidence": 0.7334100604057312}]}, {"text": "Following, we evaluate word alignment quality with the alignment error rate (AER), where lower AER is better.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 23, "end_pos": 37, "type": "TASK", "confidence": 0.7371701300144196}, {"text": "alignment error rate (AER)", "start_pos": 55, "end_pos": 81, "type": "METRIC", "confidence": 0.9114982982476553}, {"text": "AER", "start_pos": 95, "end_pos": 98, "type": "METRIC", "confidence": 0.9987484216690063}]}, {"text": "Because our model takes dependency trees as input, we parse both sides of the two training sets, the development set and the testing set with Berkeley parser (), and then convert the generated phrase trees into dependency trees according to.", "labels": [], "entities": []}, {"text": "Our model is an asymmetric model, so we perform word alignment in both forward (Chinese\uf0e0English) and reverse (English\uf0e0Chinese) directions.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 48, "end_pos": 62, "type": "TASK", "confidence": 0.7634517550468445}]}], "tableCaptions": [{"text": " Table 4: AERs on the testing set (trained on the  FBIS data set).", "labels": [], "entities": [{"text": "AERs", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9981483221054077}, {"text": "FBIS data set", "start_pos": 51, "end_pos": 64, "type": "DATASET", "confidence": 0.9676043192545573}]}, {"text": " Table 3: AERs on the development set (trained  on the FBIS data set).", "labels": [], "entities": [{"text": "AERs", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9975531697273254}, {"text": "FBIS data set", "start_pos": 55, "end_pos": 68, "type": "DATASET", "confidence": 0.9617142081260681}]}, {"text": " Table 5: HCPs and MCPs on the development  set.", "labels": [], "entities": []}, {"text": " Table 7: BLEU scores, where * indicates  significantly better than IBM4 (p<0.05).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9985742568969727}, {"text": "IBM4", "start_pos": 68, "end_pos": 72, "type": "DATASET", "confidence": 0.8118407130241394}]}, {"text": " Table 6: AERs on the testing set (trained on the  LARGE data set).", "labels": [], "entities": [{"text": "AERs", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9984690546989441}, {"text": "LARGE data set", "start_pos": 51, "end_pos": 65, "type": "DATASET", "confidence": 0.9629928668340048}]}]}