{"title": [{"text": "Dynamically Shaping the Reordering Search Space of Phrase-Based Statistical Machine Translation", "labels": [], "entities": [{"text": "Phrase-Based Statistical Machine Translation", "start_pos": 51, "end_pos": 95, "type": "TASK", "confidence": 0.6110173687338829}]}], "abstractContent": [{"text": "Defining the reordering search space is a crucial issue in phrase-based SMT between distant languages.", "labels": [], "entities": [{"text": "SMT between distant languages", "start_pos": 72, "end_pos": 101, "type": "TASK", "confidence": 0.8197654783725739}]}, {"text": "In fact, the optimal trade-off between accuracy and complexity of decoding is nowadays reached by harshly limiting the input permutation space.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9990725517272949}]}, {"text": "We propose a method to dynamically shape such space and, thus, capture long-range word movements without hurting translation quality nor decoding time.", "labels": [], "entities": []}, {"text": "The space defined by loose reordering constraints is dynamically pruned through a binary classifier that predicts whether a given input word should be translated right after another.", "labels": [], "entities": []}, {"text": "The integration of this model into a phrase-based decoder improves a strong Arabic-English baseline already including state-of-the-art early distortion cost (Moore and Quirk, 2007) and hierarchical phrase orientation models (Galley and Manning, 2008).", "labels": [], "entities": [{"text": "phrase orientation", "start_pos": 198, "end_pos": 216, "type": "TASK", "confidence": 0.7058331370353699}]}, {"text": "Significant improvements in the reordering of verbs are achieved by a system that is notably faster than the baseline, while BLEU and METEOR remain stable, or even increase, at a very high distortion limit.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 125, "end_pos": 129, "type": "METRIC", "confidence": 0.9987744688987732}, {"text": "METEOR", "start_pos": 134, "end_pos": 140, "type": "METRIC", "confidence": 0.971863865852356}]}], "introductionContent": [{"text": "Word order differences are among the most important factors determining the performance of statistical machine translation (SMT) on a given language pair (.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 91, "end_pos": 128, "type": "TASK", "confidence": 0.7635039736827215}]}, {"text": "This is particularly true in the framework of phrase-based SMT (PSMT) (), an approach that remains highly competitive despite the recent advances of the tree-based approaches.", "labels": [], "entities": [{"text": "phrase-based SMT (PSMT)", "start_pos": 46, "end_pos": 69, "type": "TASK", "confidence": 0.6803728878498078}]}, {"text": "During the PSMT decoding process, the output sentence is built from left to right, while the input sentence positions can be covered in different orders.", "labels": [], "entities": [{"text": "PSMT decoding", "start_pos": 11, "end_pos": 24, "type": "TASK", "confidence": 0.947725772857666}]}, {"text": "Thus, reordering in PSMT can be viewed as the problem of choosing the input permutation that leads to the highest-scoring output sentence.", "labels": [], "entities": []}, {"text": "Due to efficiency reasons, however, the input permutation space cannot be fully explored, and is therefore limited with hard reordering constraints.", "labels": [], "entities": []}, {"text": "Although many solutions have been proposed to explicitly model word reordering during decoding, PSMT still largely fails to handle long-range word movements in language pairs with different syntactic structures . We believe this is mostly not due to deficiencies of the existing reordering models, but rather to a very coarse definition of the reordering search space.", "labels": [], "entities": [{"text": "PSMT", "start_pos": 96, "end_pos": 100, "type": "TASK", "confidence": 0.9256641268730164}]}, {"text": "Indeed, the existing reordering constraints are rather simple and typically based on word-to-word distances.", "labels": [], "entities": []}, {"text": "Moreover, they are uniform throughout the input sentence and insensitive to the actual words being translated.", "labels": [], "entities": []}, {"text": "Relaxing this kind of constraints means dramatically increasing the size of the search space and making the reordering model's task extremely complex.", "labels": [], "entities": []}, {"text": "As a result, even in language pairs where long reordering is regularly observed, PSMT quality degrades when long word movements are allowed to the decoder.", "labels": [], "entities": []}, {"text": "We address this problem by training a binary classifier to predict whether a given input position should be translated right after another, given the words at those positions and their contexts.", "labels": [], "entities": []}, {"text": "When this model is integrated into the decoder, its predic-tions can be used not only as an additional feature function, but also as an early indication of whether or not a given reordering path should be further explored.", "labels": [], "entities": []}, {"text": "More specifically, at each hypothesis expansion, we consider the set of input positions that are reachable according to the usual reordering constraints, and prune it based only on the reordering model score.", "labels": [], "entities": []}, {"text": "Then, the hypothesis can be expanded normally by covering the non-pruned positions.", "labels": [], "entities": []}, {"text": "This technique makes it possible to dynamically shape the search space while decoding with a very high distortion limit, which can improve translation quality and efficiency at the same time.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "After an overview of the relevant literature, we describe in detail our word reordering model.", "labels": [], "entities": [{"text": "word reordering", "start_pos": 72, "end_pos": 87, "type": "TASK", "confidence": 0.6898970901966095}]}, {"text": "In the following section, we introduce early pruning of reordering steps as away to dynamically shape the input permutation space.", "labels": [], "entities": []}, {"text": "Finally, we present an empirical analysis of our approach, including intrinsic evaluation of the model and SMT experiments on a well-known Arabic-English news translation task.", "labels": [], "entities": [{"text": "SMT", "start_pos": 107, "end_pos": 110, "type": "TASK", "confidence": 0.9810611009597778}, {"text": "Arabic-English news translation task", "start_pos": 139, "end_pos": 175, "type": "TASK", "confidence": 0.7229531928896904}]}], "datasetContent": [{"text": "We test our approach on an Arabic-English news translation task where sentences are typically long and complex.", "labels": [], "entities": [{"text": "Arabic-English news translation task", "start_pos": 27, "end_pos": 63, "type": "TASK", "confidence": 0.7393466904759407}]}, {"text": "In this language pair, long reordering errors mostly concern verbs, as all of SubjectVerb-Object (SVO), VSO and, more rarerly, VOS As SMT training data, we use all the in-domain parallel data provided for the NIST-MT09 evaluation fora total of 986K sentence pairs (31M English words).", "labels": [], "entities": [{"text": "SMT training", "start_pos": 134, "end_pos": 146, "type": "TASK", "confidence": 0.9091807901859283}]}, {"text": "The target LM used to run the main series of experiments is trained on the English side of all available NIST-MT09 parallel data, UN included (147M words).", "labels": [], "entities": [{"text": "NIST-MT09 parallel data", "start_pos": 105, "end_pos": 128, "type": "DATASET", "confidence": 0.8782655000686646}]}, {"text": "In the large-scale experiments, the LM training data also include the sections of the English Gigaword that best fit to the development data in terms of perplexity: namely, the Agence FrancePresse, Xinhua News Agency and Associated Press Worldstream sections (2130M words in total).", "labels": [], "entities": [{"text": "English Gigaword", "start_pos": 86, "end_pos": 102, "type": "DATASET", "confidence": 0.9000181257724762}, {"text": "Agence FrancePresse, Xinhua News Agency", "start_pos": 177, "end_pos": 216, "type": "DATASET", "confidence": 0.7385388563076655}, {"text": "Associated Press Worldstream", "start_pos": 221, "end_pos": 249, "type": "DATASET", "confidence": 0.6951160132884979}]}, {"text": "For development and test, we use the newswire sections of the NIST benchmarks: dev06-nw, eval08-nw, eval09-nw consisting of 1033, 813, 586 sentences respectively.", "labels": [], "entities": [{"text": "NIST benchmarks", "start_pos": 62, "end_pos": 77, "type": "DATASET", "confidence": 0.970114678144455}]}, {"text": "Each set includes 4 reference translations and the average sentence length is 33 words.", "labels": [], "entities": []}, {"text": "To focus the evaluation on problematic reordering, we also consider a subset of eval09-nw containing only sentences where the Arabic main verb is placed before the subject (vs-09: 299 sent.).", "labels": [], "entities": []}, {"text": "8 As pre-processing, we apply standard tokenization to the English data, while the Arabic data is segmented with AMIRA () according to the ATB scheme 9 . The same tool also produces POS tagging and shallow syntax annotation.", "labels": [], "entities": [{"text": "AMIRA", "start_pos": 113, "end_pos": 118, "type": "METRIC", "confidence": 0.9685983657836914}, {"text": "ATB scheme 9", "start_pos": 139, "end_pos": 151, "type": "DATASET", "confidence": 0.859479566415151}, {"text": "POS tagging", "start_pos": 182, "end_pos": 193, "type": "TASK", "confidence": 0.7711218595504761}]}, {"text": "Before proceeding to the SMT experiments, we evaluate the performance of the WaW reordering model in isolation.", "labels": [], "entities": [{"text": "SMT", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9924677014350891}]}, {"text": "All the tested configurations are trained with the freely available MegaM Toolkit , implementing the conjugate gradient method, in maximum 100 iterations.", "labels": [], "entities": [{"text": "MegaM Toolkit", "start_pos": 68, "end_pos": 81, "type": "DATASET", "confidence": 0.9450577199459076}]}, {"text": "Training samples are generated within a sampling window of width \u03b4=10, from a subset (30K sentences) of the parallel data described above, resulting in 8M training word pairs . Test samples are generated from TIDES-MT04 (1324 sentences, 370K samples with \u03b4=10), one of the corpora included in our SMT training data.", "labels": [], "entities": [{"text": "TIDES-MT04", "start_pos": 209, "end_pos": 219, "type": "DATASET", "confidence": 0.8703961372375488}, {"text": "SMT training", "start_pos": 297, "end_pos": 309, "type": "TASK", "confidence": 0.9366287887096405}]}, {"text": "Features with less than 20 occurrences are ignored.", "labels": [], "entities": []}, {"text": "presents precision, recall, and F-score achieved by different feature subsets, where W stands for word-based, P for POS-based and C for chunk-based feature templates.", "labels": [], "entities": [{"text": "precision", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.9995777010917664}, {"text": "recall", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.999325156211853}, {"text": "F-score", "start_pos": 32, "end_pos": 39, "type": "METRIC", "confidence": 0.9994534850120544}]}, {"text": "We can see that all feature types contribute to improve the classifier's performance.", "labels": [], "entities": []}, {"text": "The word-based model achieves the highest precision but a very low recall, while the POS-based has much more balanced scores.", "labels": [], "entities": [{"text": "precision", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.9992051720619202}, {"text": "recall", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.9994489550590515}]}, {"text": "A better performance overall is obtained by combining word-, POS-and mixed word-POS-based features (62.6% F-score).", "labels": [], "entities": [{"text": "F-score", "start_pos": 106, "end_pos": 113, "type": "METRIC", "confidence": 0.9985729455947876}]}, {"text": "Finally, the addition of chunk-based features yields a further improvement of about 1 point, reaching 63.8% F-score.", "labels": [], "entities": [{"text": "F-score", "start_pos": 108, "end_pos": 115, "type": "METRIC", "confidence": 0.9987553358078003}]}, {"text": "Given these results, we decide to use the W,P,C model for the rest of the evaluation.", "labels": [], "entities": []}, {"text": "A more important aspect to evaluate for our application is how well our model's scores can rank atypical set of reordering options.", "labels": [], "entities": []}, {"text": "In fact, the WaW model is not meant to be used as http://www.cs.utah.edu/\u02dchal/megam/ ().", "labels": [], "entities": []}, {"text": "11 This is the maximum number of samples manageable by MegaM.", "labels": [], "entities": [{"text": "MegaM", "start_pos": 55, "end_pos": 60, "type": "DATASET", "confidence": 0.9678524136543274}]}, {"text": "However, even scaling from 4M to 8M was only slightly helpful in our experiments.", "labels": [], "entities": []}, {"text": "In the future we plan to test other learning approaches that scale better to large data sets.", "labels": [], "entities": []}, {"text": "a stand-alone classifier, but as one of several SMT feature functions.", "labels": [], "entities": [{"text": "SMT", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.9856840372085571}]}, {"text": "Moreover, for early reordering pruning to be effective, it is especially important that the correct reordering option be ranked in the top n among those available at the time of a given hypothesis expansion.", "labels": [], "entities": []}, {"text": "In order to measure this, we simulate the decoding process by traversing the source words in target order and, for each of them, we examine the ranking of all words that maybe translated next (i. e. the uncovered positions within a given DL).", "labels": [], "entities": []}, {"text": "We check how often the correct jump was ranked first (Top-1) or at most third (Top-3).", "labels": [], "entities": []}, {"text": "We also compute the latter score on long reorderings only (Top-3-long): i. e. backward jumps with distortion D>7 and forward jumps with D>6.", "labels": [], "entities": []}, {"text": "In, results are compared with the ranking produced by standard distortion, which always favors shorter jumps.", "labels": [], "entities": []}, {"text": "Two conditions are considered: DL=10 corresponding to the sampling window \u03b4 used to produce the training data, and DL=18 that is the maximum distortion of jumps that will be considered in our early-pruning SMT experiment.", "labels": [], "entities": [{"text": "DL", "start_pos": 31, "end_pos": 33, "type": "METRIC", "confidence": 0.9913866519927979}, {"text": "DL", "start_pos": 115, "end_pos": 117, "type": "METRIC", "confidence": 0.9914132356643677}, {"text": "SMT", "start_pos": 206, "end_pos": 209, "type": "TASK", "confidence": 0.9690662026405334}]}, {"text": "We can see that, in terms of overall accuracies, the WaW reordering model outperforms standard distortion by a large margin (about 10% absolute).", "labels": [], "entities": []}, {"text": "This is an important result, considering that the jump length, strongly correlating with the jump likelihood, is not directly known to our model.", "labels": [], "entities": []}, {"text": "As regards the DL, the higher limit naturally results in a lower DL-error rate (percentage of correct jumps beyond DL): namely 0.8% instead of 2.4%.", "labels": [], "entities": [{"text": "DL-error rate", "start_pos": 65, "end_pos": 78, "type": "METRIC", "confidence": 0.8783577084541321}]}, {"text": "However, jump prediction becomes much harder: Top-3 accuracy of long jumps by distortion drops from 50.7% to 18.9% (backward) and from 66.0% to 52.3% (forward).", "labels": [], "entities": [{"text": "jump prediction", "start_pos": 9, "end_pos": 24, "type": "TASK", "confidence": 0.9723261296749115}, {"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9489286541938782}]}, {"text": "Our model is remarkably robust to this effect on backward jumps, where it achieves 68.0% accu-racy.", "labels": [], "entities": [{"text": "accu-racy", "start_pos": 89, "end_pos": 98, "type": "METRIC", "confidence": 0.9896999597549438}]}, {"text": "Due to the syntactic characteristics of Arabic and English, the typical long reordering pattern consists in (i) skipping a clause-initial Arabic verb, (ii) covering along subject, then finally (iii) jumping back to translate the verb and (iv) jumping forward to continue translating the rest of the sentence (see for an example).", "labels": [], "entities": []}, {"text": "12 Deciding when to jump back to cover the verb (iii) is the hardest part of this process, and that is precisely where our model seems more helpful, while distortion always prefers to proceed monotonically achieving a very low accuracy of 18.9%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 227, "end_pos": 235, "type": "METRIC", "confidence": 0.9970971345901489}]}, {"text": "In the case of long forward jumps (iv), instead, distortion is advantaged as the correct choice typically corresponds to translating the first uncovered position, that is the shortest jump available from the last translated word.", "labels": [], "entities": [{"text": "distortion", "start_pos": 49, "end_pos": 59, "type": "METRIC", "confidence": 0.9600139856338501}]}, {"text": "Even here, our model achieves an accuracy of 51.8%, only slightly lower than that of distortion (52.3%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.999616265296936}, {"text": "distortion", "start_pos": 85, "end_pos": 95, "type": "METRIC", "confidence": 0.9960867166519165}]}, {"text": "Our SMT systems are built with the Moses toolkit, while word alignment is produced by the Berkeley Aligner ().", "labels": [], "entities": [{"text": "SMT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9877341389656067}, {"text": "word alignment", "start_pos": 56, "end_pos": 70, "type": "TASK", "confidence": 0.8134252429008484}]}, {"text": "The baseline decoder includes a phrase translation model, a lexicalized reordering model, a 6-gram target language model, distortion cost, word and phrase penalties.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.816682368516922}]}, {"text": "More specifically, the baseline reordering model is a hierarchical phrase orientation model) trained on all the available parallel data.", "labels": [], "entities": []}, {"text": "This variant was shown to outperform the default wordbased on an Arabic-English task.", "labels": [], "entities": []}, {"text": "To make our baseline even more competitive, we apply early distortion cost, as proposed by.", "labels": [], "entities": [{"text": "early distortion cost", "start_pos": 53, "end_pos": 74, "type": "METRIC", "confidence": 0.9472175240516663}]}, {"text": "This function has the same value as the standard one over a complete translation hypothesis, but it anticipates the gradual accumulation of the cost, making hypotheses of the same length more comparable to one another.", "labels": [], "entities": []}, {"text": "Note that this option has no ef-fect on the distortion limit, but only on the distortion cost feature function.", "labels": [], "entities": [{"text": "distortion", "start_pos": 44, "end_pos": 54, "type": "METRIC", "confidence": 0.9727630019187927}]}, {"text": "As proposed by, statistically improbable phrase pairs are removed from the translation model.", "labels": [], "entities": []}, {"text": "The language models are estimated by the IRSTLM toolkit) with modified Kneser-Ney smoothing.", "labels": [], "entities": [{"text": "IRSTLM toolkit", "start_pos": 41, "end_pos": 55, "type": "DATASET", "confidence": 0.9055880904197693}]}, {"text": "Feature weights are optimized by minimum BLEU-error training) on dev06-nw.", "labels": [], "entities": [{"text": "BLEU-error", "start_pos": 41, "end_pos": 51, "type": "METRIC", "confidence": 0.9986288547515869}]}, {"text": "To reduce the effects of the optimizer instability, we tune each configuration four times and use the average of the resulting weight vectors to translate the test sets, as suggested by.", "labels": [], "entities": []}, {"text": "Finally, eval08-nw is used to select the early pruning parameters for the last experiment, while eval09-nw is always reserved as blind test.", "labels": [], "entities": []}, {"text": "We evaluate global translation quality with BLEU () and METEOR (Banerjee and).", "labels": [], "entities": [{"text": "global translation", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.6332281827926636}, {"text": "BLEU", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.9989996552467346}, {"text": "METEOR", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.9947918057441711}]}, {"text": "These metrics, though, are only indirectly sensitive to word order, and especially unlikely to capture improvements at the level of longrange reordering.", "labels": [], "entities": []}, {"text": "For this reason, we also compute the Kendall Reordering Score or) which is a positive score based on the Kendall's Tau distance between the source-output permutation \u03c0 and the source-reference permutations \u03c3: where BP is a sentence-level brevity penalty, similar to that of BLEU.", "labels": [], "entities": [{"text": "Reordering Score", "start_pos": 45, "end_pos": 61, "type": "METRIC", "confidence": 0.8595313131809235}, {"text": "BP", "start_pos": 215, "end_pos": 217, "type": "METRIC", "confidence": 0.9875291585922241}, {"text": "BLEU", "start_pos": 274, "end_pos": 278, "type": "METRIC", "confidence": 0.9386113286018372}]}, {"text": "The KRS is robust to lexical choice because it performs no comparison between output and reference words, but only between the positions of their translations.", "labels": [], "entities": []}, {"text": "Besides, it was shown to correlate strongly with human judgements of fluency.", "labels": [], "entities": []}, {"text": "Our work specifically addresses long-range reordering phenomena in language pairs where these are quite rare, although crucial for preserving the source text meaning.", "labels": [], "entities": []}, {"text": "Hence, an improvement at this level may not be detected by the general-purpose metrics.", "labels": [], "entities": []}, {"text": "We then develop a KRS variant that is only sensitive to the positioning of specific input words.", "labels": [], "entities": []}, {"text": "Assuming that each input word f i is assigned a weight \u03bb i , the formula above is modified as follows: A similar element-weighted version of Kendall Tau was proposed by to evaluate document rankings in information retrieval.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 202, "end_pos": 223, "type": "TASK", "confidence": 0.7757830321788788}]}, {"text": "Because long reordering errors in Arabic-English mostly affect verbs, we set the weights to 1 for verbs and 0 for all other words to only capture verb reordering errors, and call the resulting metric KRS-V.", "labels": [], "entities": []}, {"text": "The source-reference word alignments needed to compute the reordering scores are generated by the Berkeley Aligner previously trained on the training data.", "labels": [], "entities": []}, {"text": "Source-output word alignments are instead obtained from the decoder's trace.", "labels": [], "entities": [{"text": "Source-output word alignments", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.5849623878796896}]}], "tableCaptions": [{"text": " Table 3: Classification accuracy of the WaW reordering  model on TIDES-MT04, using different feature subsets.  The template numbers refer to the rows of Table 1.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9713007807731628}, {"text": "TIDES-MT04", "start_pos": 66, "end_pos": 76, "type": "DATASET", "confidence": 0.8931399583816528}]}, {"text": " Table 4: Word-to-word jump ranking accuracy (%) of  standard distortion and WaW reordering model, in dif- ferent DL conditions. DL-err is the percentage of correct  jumps beyond DL. The test set consists of 40K reordering  decisions: one for each source word in TIDES-MT04.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9604875445365906}, {"text": "TIDES-MT04", "start_pos": 263, "end_pos": 273, "type": "DATASET", "confidence": 0.8910107612609863}]}, {"text": " Table 5: Effects of WaW reordering modeling and early reordering pruning on translation quality, measured with  % BLEU, METEOR, and Kendall Reordering Score: regular (KRS) and verb-specific (KRS-V). Statistically significant  differences with respect to the baseline [B] are marked with \ud97b\udf59\ud97b\udf59 at the p \u2264 .05 level and \ud97b\udf59\ud97b\udf59 at the p \u2264 .10 level.  Decoding time is measured in milliseconds per input word.", "labels": [], "entities": [{"text": "WaW reordering modeling", "start_pos": 21, "end_pos": 44, "type": "TASK", "confidence": 0.8459211985270182}, {"text": "BLEU", "start_pos": 115, "end_pos": 119, "type": "METRIC", "confidence": 0.9991831183433533}, {"text": "METEOR", "start_pos": 121, "end_pos": 127, "type": "METRIC", "confidence": 0.9960528612136841}, {"text": "Kendall Reordering Score", "start_pos": 133, "end_pos": 157, "type": "METRIC", "confidence": 0.9166368047396342}]}]}