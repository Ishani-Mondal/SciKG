{"title": [{"text": "Incremental Tree Substitution Grammar for Parsing and Sentence Prediction", "labels": [], "entities": [{"text": "Parsing and Sentence Prediction", "start_pos": 42, "end_pos": 73, "type": "TASK", "confidence": 0.82646344602108}]}], "abstractContent": [{"text": "In this paper, we present the first incremental parser for Tree Substitution Grammar (TSG).", "labels": [], "entities": [{"text": "Tree Substitution Grammar (TSG)", "start_pos": 59, "end_pos": 90, "type": "TASK", "confidence": 0.8282765944798788}]}, {"text": "A TSG allows arbitrarily large syntactic fragments to be combined into complete trees; we show how constraints (including lexical-ization) can be imposed on the shape of the TSG fragments to enable incremental processing.", "labels": [], "entities": []}, {"text": "We propose an efficient Earley-based algorithm for incremental TSG parsing and report an F-score competitive with other incre-mental parsers.", "labels": [], "entities": [{"text": "TSG parsing", "start_pos": 63, "end_pos": 74, "type": "TASK", "confidence": 0.8557417690753937}, {"text": "F-score", "start_pos": 89, "end_pos": 96, "type": "METRIC", "confidence": 0.9960057139396667}]}, {"text": "In addition to whole-sentence F-score, we also evaluate the partial trees that the parser constructs for sentence prefixes; partial trees play an important role in incre-mental interpretation, language modeling, and psycholinguistics.", "labels": [], "entities": [{"text": "F-score", "start_pos": 30, "end_pos": 37, "type": "METRIC", "confidence": 0.8620383143424988}, {"text": "incre-mental interpretation", "start_pos": 164, "end_pos": 191, "type": "TASK", "confidence": 0.7509279847145081}, {"text": "language modeling", "start_pos": 193, "end_pos": 210, "type": "TASK", "confidence": 0.7430106699466705}]}, {"text": "Unlike existing parsers, our incremental TSG parser can generate partial trees that include predictions about the up-coming words in a sentence.", "labels": [], "entities": []}, {"text": "We show that it outperforms an n-gram model in predicting more than one upcoming word.", "labels": [], "entities": []}], "introductionContent": [{"text": "When humans listen to speech, the input becomes available gradually as the speech signal unfolds.", "labels": [], "entities": []}, {"text": "Reading happens in a similarly gradual manner when the eyes scan a text.", "labels": [], "entities": [{"text": "Reading", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.9332865476608276}]}, {"text": "There is good evidence that the human language processor is adapted to this and works incrementally, i.e., computes an interpretation for an incoming sentence on a word-by-word basis ().", "labels": [], "entities": []}, {"text": "Also language processing systems often deal with speech as it is spoken, or text as it is being typed.", "labels": [], "entities": []}, {"text": "A dialogue system should start interpreting a sentence while it is being spoken, and a question answering system should start retrieving answers before the user has finished typing the question.", "labels": [], "entities": [{"text": "question answering", "start_pos": 87, "end_pos": 105, "type": "TASK", "confidence": 0.7476814389228821}]}, {"text": "Incremental processing is therefore essential both for realistic models of human language processing and for NLP applications that react to user input in real time.", "labels": [], "entities": [{"text": "Incremental processing", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.856767475605011}]}, {"text": "In response to this, a number of incremental parsers have been developed, which use context-free grammar, dependency grammar, or treeadjoining grammar).", "labels": [], "entities": []}, {"text": "Typical applications of incremental parsers include speech recognition), machine translation (), reading time modeling, or dialogue systems ().", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.757989913225174}, {"text": "machine translation", "start_pos": 73, "end_pos": 92, "type": "TASK", "confidence": 0.806168407201767}, {"text": "reading time modeling", "start_pos": 97, "end_pos": 118, "type": "TASK", "confidence": 0.7716184258460999}]}, {"text": "Another potential use of incremental parsers is sentence prediction, i.e., the task of predicting upcoming words in a sentence given a prefix.", "labels": [], "entities": [{"text": "sentence prediction", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.7623828649520874}, {"text": "predicting upcoming words in a sentence given a prefix", "start_pos": 87, "end_pos": 141, "type": "TASK", "confidence": 0.8134580784373813}]}, {"text": "However, so far only n-gram models and classifiers have been used for this task.", "labels": [], "entities": []}, {"text": "In this paper, we present an incremental parser for Tree Substitution Grammar (TSG).", "labels": [], "entities": [{"text": "Tree Substitution Grammar (TSG)", "start_pos": 52, "end_pos": 83, "type": "TASK", "confidence": 0.8202333648999532}]}, {"text": "A TSG contains a set of arbitrarily large tree fragments, which can be combined into new syntax trees by means of a substitution operation.", "labels": [], "entities": []}, {"text": "An extensive tradition of parsing with TSG (also referred to as data-oriented parsing) exists, but none of the existing TSG parsers are incremental.", "labels": [], "entities": [{"text": "parsing", "start_pos": 26, "end_pos": 33, "type": "TASK", "confidence": 0.9762313961982727}, {"text": "data-oriented parsing)", "start_pos": 64, "end_pos": 86, "type": "TASK", "confidence": 0.7102197011311849}]}, {"text": "We show how constraints can be imposed on the shape of the TSG fragments to enable incremental processing.", "labels": [], "entities": []}, {"text": "We propose an efficient Earley-based algorithm for incremental TSG parsing and report an F-score competitive with other incremental parsers.", "labels": [], "entities": [{"text": "TSG parsing", "start_pos": 63, "end_pos": 74, "type": "TASK", "confidence": 0.8494615256786346}, {"text": "F-score", "start_pos": 89, "end_pos": 96, "type": "METRIC", "confidence": 0.9944697618484497}]}, {"text": "TSG fragments can be arbitrarily large and can contain multiple lexical items.", "labels": [], "entities": []}, {"text": "This property enables our incremental TSG parser to generate partial parse trees that include predictions about the upcoming words in a sentence.", "labels": [], "entities": [{"text": "TSG parser", "start_pos": 38, "end_pos": 48, "type": "TASK", "confidence": 0.7809178829193115}]}, {"text": "It can therefore be applied directly to the task of sentence prediction, simply by reading off the predicted items in a partial tree.", "labels": [], "entities": [{"text": "sentence prediction", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.7525492906570435}]}, {"text": "We show that our parser outperforms an n-gram model in predicting more than one upcoming word.", "labels": [], "entities": []}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we introduce the ITSG framework and relate it to the original TSG formalism.", "labels": [], "entities": []}, {"text": "Section 3 describes the chart-parser algorithm, while Section 4 details the experimental setup and results.", "labels": [], "entities": []}, {"text": "Sections 5 and 6 present related work and conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "For training and evaluating the ITSG parser, we employ the Penn WSJ Treebank ().", "labels": [], "entities": [{"text": "Penn WSJ Treebank", "start_pos": 59, "end_pos": 76, "type": "DATASET", "confidence": 0.9523184100786845}]}, {"text": "We use sections 2-21 for training, section 22 and 24 for development and section 23 for testing.", "labels": [], "entities": []}, {"text": "In addition to standard full-sentence parsing results, we propose a novel way of evaluating our ITSG on partial trees, i.e., those that the parser constructs for sentence prefixes.", "labels": [], "entities": [{"text": "full-sentence parsing", "start_pos": 24, "end_pos": 45, "type": "TASK", "confidence": 0.6898750513792038}]}, {"text": "More precisely, for each prefix of the input sentence (length two words or longer) we compute the parsing accuracy on the minimal structure spanning that prefix.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9616812467575073}]}, {"text": "The minimal structure is obtained from the subtree rooted in the minimum The fragment with no lexical items, and those with more than one substitution site at the beginning of the yield.", "labels": [], "entities": []}, {"text": "common ancestor of the prefix nodes, after pruning those nodes not yielding any word in the prefix.", "labels": [], "entities": []}, {"text": "As observed in the example derivations of, our ITSG generates partial trees fora given prefix which may include predictions about unseen parts of the sentence.", "labels": [], "entities": []}, {"text": "We propose three new measures for evaluating sentence prediction: Word prediction PRD(m): For every prefix of each test sentence, if the model predicts m \u2265 m words, the prediction is correct if the first m predicted words are identical to them words following the prefix in the original sentence.", "labels": [], "entities": [{"text": "sentence prediction", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.7178696244955063}, {"text": "Word prediction PRD", "start_pos": 66, "end_pos": 85, "type": "TASK", "confidence": 0.7292743027210236}]}, {"text": "Word presence PRS(m): For every prefix of each test sentence, if the model predicts m \u2265 m words, the prediction is correct if the first m predicted words are present, in the same order, in the words following the prefix in the original sentence (i.e., the predicted word sequence is a subsequence of the sequence of words following the prefix).", "labels": [], "entities": [{"text": "Word presence PRS", "start_pos": 0, "end_pos": 17, "type": "METRIC", "confidence": 0.4436839421590169}]}, {"text": "Longest common subsequence LCS: For every prefix of each test sentence, it computes the longest common subsequence between the sequence of predicted words (possibly none) and the words following the prefix in the original sentence.", "labels": [], "entities": [{"text": "LCS", "start_pos": 27, "end_pos": 30, "type": "METRIC", "confidence": 0.675421416759491}]}, {"text": "Recall and precision can be computed in the usual way for these three measures.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9922695159912109}, {"text": "precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.999322772026062}]}, {"text": "Recall is the total number (over all prefixes) of correctly predicted words (as defined by PRD(m), PRS(m), or LCS) over the total number of words expected to be predicted (according to m), while precision is the number of correctly predicted words over the number of words predicted by the model.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9853030443191528}, {"text": "precision", "start_pos": 195, "end_pos": 204, "type": "METRIC", "confidence": 0.9993067979812622}]}, {"text": "We compare the ITSG parser with the incremental parsers of and for full-sentence parsing, with the Roark (2001) parser 19 for full-sentence and partial pars- We also evaluated our ITSG model using perplexity; the results obtained were substantially worse than those obtained using Roark's parsers.", "labels": [], "entities": [{"text": "full-sentence parsing", "start_pos": 67, "end_pos": 88, "type": "TASK", "confidence": 0.6963488757610321}]}, {"text": "Note that neither PRD(m) nor PRS(m) correspond to word error rate (WER).", "labels": [], "entities": [{"text": "PRS(m)", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.8894352465867996}, {"text": "word error rate (WER)", "start_pos": 50, "end_pos": 71, "type": "METRIC", "confidence": 0.896550307671229}]}, {"text": "PRD requires the predicted word sequence to be identical to the original sequence, while PRS only requires the predicted words to be present in the original.", "labels": [], "entities": [{"text": "PRD", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.6675959229469299}]}, {"text": "In contrast, WER measures the minimum number of substitutions, insertions, and deletions needed to transform the predicted sequence into the original sequence.", "labels": [], "entities": [{"text": "WER", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.9907093048095703}]}, {"text": "Apart from reporting the results in Roark, we also run the latest version of Roark's parser, used in, which has higher results compared to the original work.", "labels": [], "entities": [{"text": "Roark", "start_pos": 36, "end_pos": 41, "type": "DATASET", "confidence": 0.8686372637748718}]}, {"text": "ing, and with a language model built using SRILM) for sentence prediction.", "labels": [], "entities": [{"text": "SRILM", "start_pos": 43, "end_pos": 48, "type": "DATASET", "confidence": 0.7507513165473938}, {"text": "sentence prediction", "start_pos": 54, "end_pos": 73, "type": "TASK", "confidence": 0.8161126375198364}]}, {"text": "We used a standard 3-gram model trained on the sentences of the training set using the default setting and smoothing (Kneser-Ney) provided by the SRILM package.", "labels": [], "entities": [{"text": "SRILM package", "start_pos": 146, "end_pos": 159, "type": "DATASET", "confidence": 0.9208441078662872}]}, {"text": "(Higher n-gram model do not seem appropriate, given the small size of the training corpus.)", "labels": [], "entities": []}, {"text": "For every prefix in the test set we compute the most probable continuation predicted by the n-gram model.", "labels": [], "entities": []}, {"text": "reports full-sentence parsing results for our parser and three comparable incremental parsers from the literature.", "labels": [], "entities": [{"text": "full-sentence parsing", "start_pos": 8, "end_pos": 29, "type": "TASK", "confidence": 0.5915588140487671}]}, {"text": "While Roark (2001) obtains the best results, the ITSG parser without smoothing performs on a par with, and outperforms.", "labels": [], "entities": []}, {"text": "Adding smoothing results in again of 1.2 points F-score over the Schuler parser.", "labels": [], "entities": [{"text": "F-score", "start_pos": 48, "end_pos": 55, "type": "METRIC", "confidence": 0.99937504529953}]}, {"text": "When we compare the different parsing objectives of the ITSG parser, MRP is the best one, followed by MPP and MPD.", "labels": [], "entities": [{"text": "parsing", "start_pos": 30, "end_pos": 37, "type": "TASK", "confidence": 0.9697290062904358}, {"text": "MRP", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.9390662908554077}]}], "tableCaptions": [{"text": " Table 1: Full-sentence parsing results for sentences in the  test set of length up to 40 words.", "labels": [], "entities": [{"text": "Full-sentence parsing", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.786113828420639}]}, {"text": " Table 2: Sentence prediction results.", "labels": [], "entities": [{"text": "Sentence prediction", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.9577338397502899}]}]}