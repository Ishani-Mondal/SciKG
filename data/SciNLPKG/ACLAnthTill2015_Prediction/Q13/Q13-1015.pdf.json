{"title": [], "abstractContent": [{"text": "We introduce anew approach to semantics which combines the benefits of distributional and formal logical semantics.", "labels": [], "entities": []}, {"text": "Distributional models have been successful in modelling the meanings of content words, but logical semantics is necessary to adequately represent many function words.", "labels": [], "entities": []}, {"text": "We follow formal semantics in mapping language to logical representations , but differ in that the relational constants used are induced by offline distri-butional clustering at the level of predicate-argument structure.", "labels": [], "entities": []}, {"text": "Our clustering algorithm is highly scalable, allowing us to run on corpora the size of Gigaword.", "labels": [], "entities": [{"text": "Gigaword", "start_pos": 87, "end_pos": 95, "type": "DATASET", "confidence": 0.9630129933357239}]}, {"text": "Different senses of a word are disambiguated based on their induced types.", "labels": [], "entities": []}, {"text": "We outperform a variety of existing approaches on a wide-coverage question answering task, and demonstrate the ability to make complex multi-sentence inferences involving quantifiers on the FraCaS suite.", "labels": [], "entities": [{"text": "question answering task", "start_pos": 66, "end_pos": 89, "type": "TASK", "confidence": 0.7812521954377493}, {"text": "FraCaS suite", "start_pos": 190, "end_pos": 202, "type": "DATASET", "confidence": 0.970719963312149}]}], "introductionContent": [{"text": "Mapping natural language to meaning representations is a central challenge of NLP.", "labels": [], "entities": []}, {"text": "There has been much recent progress in unsupervised distributional semantics, in which the meaning of a word is induced based on its usage in large corpora.", "labels": [], "entities": []}, {"text": "This approach is useful fora range of key applications including question answering and relation extraction ().", "labels": [], "entities": [{"text": "question answering", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.9334930181503296}, {"text": "relation extraction", "start_pos": 88, "end_pos": 107, "type": "TASK", "confidence": 0.8916420042514801}]}, {"text": "Because such a semantics can be automically induced, it escapes the limitation of depending on relations from hand-built training data, knowledge bases or ontologies, which have proved of limited use in capturing the huge variety of meanings that can be expressed in language.", "labels": [], "entities": []}, {"text": "However, distributional semantics has largely developed in isolation from the formal semantics literature.", "labels": [], "entities": [{"text": "distributional semantics", "start_pos": 9, "end_pos": 33, "type": "TASK", "confidence": 0.7727804183959961}]}, {"text": "Whilst distributional semantics has been effective in modelling the meanings of content words such as nouns and verbs, it is less clear that it can be applied to the meanings of function words.", "labels": [], "entities": []}, {"text": "Semantic operators, such as determiners, negation, conjunctions, modals, tense, mood, aspect, and plurals are ubiquitous in natural language, and are crucial for high performance on many practical applicationsbut current distributional models struggle to capture even simple examples.", "labels": [], "entities": []}, {"text": "Conversely, computational models of formal semantics have shown low recall on practical applications, stemming from their reliance on ontologies such as WordNet to model the meanings of content words All of these require knowledge of lexical semantics (e.g. that buy and purchase are synonyms), but some also need interpretation of quantifiers, negatives, modals and disjunction.", "labels": [], "entities": [{"text": "recall", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.9984545707702637}]}, {"text": "It seems unlikely that distributional or formal approaches can accomplish the task alone.", "labels": [], "entities": []}, {"text": "We propose a method for mapping natural language to first-order logic representations capable of capturing the meanings of function words such as every, not and or, but which also uses distributional statistics to model the meaning of content words.", "labels": [], "entities": []}, {"text": "Our approach differs from standard formal semantics in that the non-logical symbols used in the logical form are cluster identifiers.", "labels": [], "entities": []}, {"text": "Where standard semantic formalisms would map the verb write to a write' symbol, we map it to a cluster identifier such as relation37, which the noun author may also map to.", "labels": [], "entities": []}, {"text": "This mapping is learnt by offline clustering.", "labels": [], "entities": []}, {"text": "Unlike previous distributional approaches, we perform clustering at the level of predicate-argument structure, rather than syntactic dependency structure.", "labels": [], "entities": []}, {"text": "This means that we abstract away from many syntactic differences that are not present in the semantics, such as conjunctions, passives, relative clauses, and long-range dependencies.", "labels": [], "entities": []}, {"text": "This significantly reduces sparsity, so we have fewer predicates to cluster and more observations for each.", "labels": [], "entities": []}, {"text": "Of course, many practical inferences rely heavily on background knowledge about the world-such knowledge falls outside the scope of this work.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our approach aims to offer a strong model of both formal and lexical semantics.", "labels": [], "entities": []}, {"text": "We perform two evaluations, aiming to target each of these separately, but using the same semantic representations in each.", "labels": [], "entities": []}, {"text": "We train our system on Gigaword (, which contains around 4 billion words of newswire.", "labels": [], "entities": [{"text": "Gigaword", "start_pos": 23, "end_pos": 31, "type": "DATASET", "confidence": 0.9208939671516418}]}, {"text": "The type-model is trained using 15 types 7 , and 5,000 iterations of Gibbs sampling (using the distributions from the final sample).", "labels": [], "entities": []}, {"text": "The relation clustering uses only proper nouns, to improve precision (sparsity problems are partly offset by the large input corpus).", "labels": [], "entities": [{"text": "relation clustering", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.809442549943924}, {"text": "precision", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9983501434326172}]}, {"text": "Aside from parsing, the pipeline takes around a day to run using 12 cores.", "labels": [], "entities": [{"text": "parsing", "start_pos": 11, "end_pos": 18, "type": "TASK", "confidence": 0.9858157634735107}]}, {"text": "As yet, there is no standard way of evaluating lexical semantics.", "labels": [], "entities": []}, {"text": "Existing tasks like Recognising Textual Entailment (RTE;) rely heavily on background knowledge, which is beyond the scope of this work.", "labels": [], "entities": [{"text": "Recognising Textual Entailment (RTE", "start_pos": 20, "end_pos": 55, "type": "TASK", "confidence": 0.8054699659347534}]}, {"text": "Intrinsic evaluations of entailment relations have low inter-annotator agreement, due to the difficulty of evaluating relations out of context.", "labels": [], "entities": []}, {"text": "Our evaluation is based on that performed by.", "labels": [], "entities": []}, {"text": "We automatically construct a set of questions by sampling from text, and then evaluate how many answers can be found in a different corpus.", "labels": [], "entities": []}, {"text": "\u2192 Y patterns, where X and Y are proper nouns and the verb is not on a list of stop verbs, and deterministically convert these to questions.", "labels": [], "entities": []}, {"text": "For example, from Google bought YouTube we create the questions What did Google buy? and What bought YouTube?.", "labels": [], "entities": []}, {"text": "The task is to find proper-noun answers to these questions in a different corpus, which are then evaluated by human annotators based on the sentence the answer was retrieved from 8 . Systems can return multiple answers to the same question (e.g. What did Google buy?", "labels": [], "entities": []}, {"text": "may have many valid answers), and all of these contribute to the result.", "labels": [], "entities": []}, {"text": "As none of the systems model tense or temporal semantics, annotators were instructed to annotate answers as correct if they were true at anytime.", "labels": [], "entities": []}, {"text": "This approach means we evaluate on relations in proportion to corpus frequency.", "labels": [], "entities": []}, {"text": "We sample 1000 questions from the New York Times subset of Gigaword from 2010, and search for answers in the New York Times from 2009.", "labels": [], "entities": [{"text": "New York Times subset of Gigaword from 2010", "start_pos": 34, "end_pos": 77, "type": "DATASET", "confidence": 0.7992612794041634}, {"text": "New York Times from 2009", "start_pos": 109, "end_pos": 133, "type": "DATASET", "confidence": 0.9106500744819641}]}, {"text": "We evaluate the following approaches: \u2022 CCG-Baseline The logical form produced by our CCG derivation, without the clustering.", "labels": [], "entities": []}, {"text": "\u2022 CCG-WordNet The CCG logical form, plus WordNet as a model of lexical semantics.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 41, "end_pos": 48, "type": "DATASET", "confidence": 0.9322412610054016}]}, {"text": "\u2022 CCG-Distributional The logical form including the type model and clusters.", "labels": [], "entities": []}, {"text": "\u2022 Relational LDA An LDA based model for clustering dependency paths).", "labels": [], "entities": []}, {"text": "We train on New York Times subset of Gigaword 9 , using their setup of 50 iterations with 100 relation types.", "labels": [], "entities": [{"text": "New York Times subset of Gigaword 9", "start_pos": 12, "end_pos": 47, "type": "DATASET", "confidence": 0.8538907936641148}]}, {"text": "\u2022 Reverb A sophisticated Open Information Extraction system).", "labels": [], "entities": []}, {"text": "Unsupervised Semantic Parsing (USP;) would be another obvious baseline.", "labels": [], "entities": [{"text": "Unsupervised Semantic Parsing (USP", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.6704478681087493}]}, {"text": "However, memory requirements mean it is not possible to run at this scale (our system is trained on 4 orders of magnitude more data than the USP evaluation).", "labels": [], "entities": [{"text": "USP evaluation", "start_pos": 141, "end_pos": 155, "type": "DATASET", "confidence": 0.8660657405853271}]}, {"text": "found it had comparable performance to Relational LDA.", "labels": [], "entities": [{"text": "Relational LDA", "start_pos": 39, "end_pos": 53, "type": "TASK", "confidence": 0.5231769680976868}]}, {"text": "For the CCG models, rather than performing full first-order inference on a large corpus, we simply test whether the question predicate subsumes a candidate answer predicate, and whether the arguments match . In the case of CCG-Distributional, we calculate the probability that the two packed-predicates 94.1% CCG-Distributional@500 500 82.0%: Results on wide-coverage Question Answering task.", "labels": [], "entities": [{"text": "Question Answering task", "start_pos": 368, "end_pos": 391, "type": "TASK", "confidence": 0.7903794050216675}]}, {"text": "CCG-Distributional ranks question/answer pairs by confidence-@250 means we evaluate the top 250 of these.", "labels": [], "entities": []}, {"text": "It is not possible to give a recall as the total number of correct answers in the corpus is unknown.", "labels": [], "entities": [{"text": "recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9985488057136536}]}, {"text": "We are also interested in evaluating our approach as a model of formal semantics-demonstrating that it is possible to integrate the formal semantics of Steedman (2012) with our distributional clusters.", "labels": [], "entities": []}, {"text": "The FraCaS suite contains a hand-built set of entailment problems designed to be challenging in terms of formal semantics.", "labels": [], "entities": [{"text": "FraCaS suite", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.9289344251155853}]}, {"text": "We use Section 1, which contains 74 problems requiring an understanding of quantifiers 12 . They do not require any knowledge of lexical semantics, meaning we can evaluate the formal component of our system in isolation.", "labels": [], "entities": []}, {"text": "However, we use the same representations as in our previous experiment, even though the clusters provide no benefit on this task.", "labels": [], "entities": []}, {"text": "The only previous work we are aware of on this dataset is by.", "labels": [], "entities": []}, {"text": "This approach learns the monotonicity properties of words from a hand-built training set, and uses this to transform a sentence into a polarity annotated string.", "labels": [], "entities": []}, {"text": "The system then aims to transform the premise string into a hypothesis.", "labels": [], "entities": []}, {"text": "Positively polarized words can be replaced with less specific ones (e.g. by deleting adjuncts), whereas negatively polarized words can be replaced with more specific ones (e.g. by adding adjuncts).", "labels": [], "entities": []}, {"text": "Whilst this is highprecision and often useful, this logic is unable to perform inferences with multiple premise sentences (in contrast to our first-order logic).", "labels": [], "entities": []}, {"text": "Development consists of adding entries to our lexicon for quantifiers.", "labels": [], "entities": []}, {"text": "For simplicity, we treat multiword quantifiers like at least a few, as being multiword expressions-although a more compositional analysis maybe possible.", "labels": [], "entities": []}, {"text": "Following, we do not use held-out dataeach problem is designed to test a different issue, so it is not possible to generalize from one subset of the suite to another.", "labels": [], "entities": []}, {"text": "As we are interested in evaluating the semantics, not the parser, we manually supply gold-standard lexical categories for sentences with parser errors (any syntactic mistake causes incorrect semantics).", "labels": [], "entities": []}, {"text": "Our derivations produce a distribution over logical forms-we license the inference if it holds in any interpretation with non-zero probability.", "labels": [], "entities": []}, {"text": "We use the Prover9 () theorem prover for inference, returning yes if the premise implies the hypothesis, no if it implies the negation of the hypothesis, and unknown otherwise.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Most probable terms in some clusters induced  by the Type Model.", "labels": [], "entities": []}]}