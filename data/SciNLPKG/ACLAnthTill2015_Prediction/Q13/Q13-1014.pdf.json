{"title": [{"text": "Learning to translate with products of novices: a suite of open-ended challenge problems for teaching MT", "labels": [], "entities": [{"text": "MT", "start_pos": 102, "end_pos": 104, "type": "TASK", "confidence": 0.9236647486686707}]}], "abstractContent": [{"text": "Machine translation (MT) draws from several different disciplines, making it a complex subject to teach.", "labels": [], "entities": [{"text": "Machine translation (MT)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9184487819671631}]}, {"text": "There are excellent pedagogical texts, but problems in MT and current algorithms for solving them are best learned by doing.", "labels": [], "entities": [{"text": "MT", "start_pos": 55, "end_pos": 57, "type": "TASK", "confidence": 0.9754251837730408}]}, {"text": "As a centerpiece of our MT course, we devised a series of open-ended challenges for students in which the goal was to improve performance on carefully constrained instances of four key MT tasks: alignment, decoding, evaluation, and reranking.", "labels": [], "entities": [{"text": "MT", "start_pos": 24, "end_pos": 26, "type": "TASK", "confidence": 0.9925211071968079}, {"text": "MT tasks: alignment", "start_pos": 185, "end_pos": 204, "type": "TASK", "confidence": 0.7639625519514084}]}, {"text": "Students brought a diverse set of techniques to the problems , including some novel solutions which performed remarkably well.", "labels": [], "entities": []}, {"text": "A surprising and exciting outcome was that student solutions or their combinations fared competitively on some tasks, demonstrating that even newcomers to the field can help improve the state-of-the-art on hard NLP problems while simultaneously learning a great deal.", "labels": [], "entities": []}, {"text": "The problems, baseline code, and results are freely available.", "labels": [], "entities": []}], "introductionContent": [{"text": "A decade ago, students interested in natural language processing arrived at universities having been exposed to the idea of machine translation (MT) primarily through science fiction.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 37, "end_pos": 64, "type": "TASK", "confidence": 0.6794110536575317}, {"text": "machine translation (MT)", "start_pos": 124, "end_pos": 148, "type": "TASK", "confidence": 0.8624565243721009}]}, {"text": "Today, incoming students have been exposed to services like Google Translate since they were in secondary school or earlier.", "labels": [], "entities": []}, {"text": "For them, MT is science fact.", "labels": [], "entities": [{"text": "MT", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.9774667620658875}]}, {"text": "So it makes sense to teach statistical MT, either on its own or as a unit * The first five authors were instructors and the remaining authors were students in the worked described here.", "labels": [], "entities": [{"text": "statistical MT", "start_pos": 27, "end_pos": 41, "type": "TASK", "confidence": 0.6261403560638428}]}, {"text": "This research was conducted while Chris Callison-Burch was at Johns Hopkins in a class on natural language processing (NLP), machine learning (ML), or artificial intelligence (AI).", "labels": [], "entities": []}, {"text": "A course that promises to show students how Google Translate works and teach them how to build something like it is especially appealing, and several universities and summer schools now offer such classes.", "labels": [], "entities": []}, {"text": "There are excellent introductory texts-depending on the level of detail required, instructors can choose from a comprehensive MT textbook, a chapter of a popular NLP textbook), a tutorial survey (, or an intuitive tutorial on the IBM Models), among many others.", "labels": [], "entities": [{"text": "MT", "start_pos": 126, "end_pos": 128, "type": "TASK", "confidence": 0.9444410800933838}, {"text": "IBM Models", "start_pos": 230, "end_pos": 240, "type": "DATASET", "confidence": 0.8850418031215668}]}, {"text": "But MT is not just an object of academic study.", "labels": [], "entities": [{"text": "MT", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.9713601469993591}]}, {"text": "It's areal application that isn't fully perfected, and the best way to learn about it is to build an MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 101, "end_pos": 103, "type": "TASK", "confidence": 0.9633244276046753}]}, {"text": "This can be done with open-source toolkits such as Moses (), cdec, or Joshua (), but these systems are not designed for pedagogy.", "labels": [], "entities": []}, {"text": "They are mature codebases featuring tens of thousands of source code lines, making it difficult to focus on their core algorithms.", "labels": [], "entities": []}, {"text": "Most tutorials present them as black boxes.", "labels": [], "entities": []}, {"text": "But our goal is for students to learn the key techniques in MT, and ideally to learn by doing.", "labels": [], "entities": [{"text": "MT", "start_pos": 60, "end_pos": 62, "type": "TASK", "confidence": 0.9876356720924377}]}, {"text": "Black boxes are incompatible with this goal.", "labels": [], "entities": []}, {"text": "We solve this dilemma by presenting students with concise, fully-functioning, self-contained components of a statistical MT system: word alignment, decoding, evaluation, and reranking.", "labels": [], "entities": [{"text": "MT", "start_pos": 121, "end_pos": 123, "type": "TASK", "confidence": 0.9440433979034424}, {"text": "word alignment", "start_pos": 132, "end_pos": 146, "type": "TASK", "confidence": 0.7499350607395172}]}, {"text": "Each implementation consists of a na\u00a8\u0131vena\u00a8\u0131ve baseline algorithm in less than 150 lines of Python code.", "labels": [], "entities": []}, {"text": "We assign them to students as open-ended challenges in which the goal is to improve performance on objective evaluation metrics as much as possible.", "labels": [], "entities": []}, {"text": "This setting mirrors evaluations conducted by the NLP research community and by the engineering teams behind high-profile NLP projects such as Google Translate and IBM's Watson.", "labels": [], "entities": [{"text": "Watson", "start_pos": 170, "end_pos": 176, "type": "DATASET", "confidence": 0.8662557005882263}]}, {"text": "While we designate specific algorithms as benchmarks for each task, we encourage creativity by awarding more points for the best systems.", "labels": [], "entities": []}, {"text": "As additional incentive, we provide a webbased leaderboard to display standings in real time.", "labels": [], "entities": []}, {"text": "In our graduate class on MT, students took a variety of different approaches to the tasks, in some cases devising novel algorithms.", "labels": [], "entities": [{"text": "MT", "start_pos": 25, "end_pos": 27, "type": "TASK", "confidence": 0.9730404615402222}]}, {"text": "A more exciting result is that some student systems or combinations of systems rivaled the state of the art on some datasets.", "labels": [], "entities": []}], "datasetContent": [{"text": "We designed four assignments, each corresponding to areal subproblem in MT: alignment, decoding, evaluation, and reranking.", "labels": [], "entities": [{"text": "MT", "start_pos": 72, "end_pos": 74, "type": "TASK", "confidence": 0.9385818243026733}, {"text": "alignment", "start_pos": 76, "end_pos": 85, "type": "TASK", "confidence": 0.9126578569412231}]}, {"text": "1 From the more general perspective of AI, they emphasize the key problems of unsupervised learning, search, evaluation design, and supervised learning, respectively.", "labels": [], "entities": []}, {"text": "In real MT systems, these problems are highly interdependent, a point we emphasized in class and at the end of each assignment-for example, that alignment is an exercise in parameter estimation for translation models, that model choice is a tradeoff between expressivity and efficient inference, and that optimal search does not guarantee optimal accuracy.", "labels": [], "entities": [{"text": "MT", "start_pos": 8, "end_pos": 10, "type": "TASK", "confidence": 0.9715167284011841}, {"text": "accuracy", "start_pos": 347, "end_pos": 355, "type": "METRIC", "confidence": 0.9719573259353638}]}, {"text": "However, presenting each problem independently and holding all else constant enables more focused exploration.", "labels": [], "entities": []}, {"text": "For each problem we provided data, a na\u00a8\u0131vena\u00a8\u0131ve solution, and an evaluation program.", "labels": [], "entities": []}, {"text": "Following and, we implemented the challenges in Python, a high-level pro-gramming language that can be used to write very concise programs resembling pseudocode.", "labels": [], "entities": []}, {"text": "By default, each baseline system reads the test data and generates output in the evaluation format, so setup required zero configuration, and students could begin experimenting immediately.", "labels": [], "entities": []}, {"text": "For example, on receipt of the alignment code, aligning data and evaluating results required only typing: > align | grade Students could then run experiments within minutes of beginning the assignment.", "labels": [], "entities": []}, {"text": "Three of the four challenges also included unlabeled test data (except the decoding assignment, as explained in \u00a74).", "labels": [], "entities": []}, {"text": "We evaluated test results against a hidden key when assignments were submitted.", "labels": [], "entities": []}, {"text": "The third challenge was evaluation: given a test corpus with reference translations and the output of several MT systems, students were challenged to produce a ranking of the systems that closely correlated with a human ranking.", "labels": [], "entities": [{"text": "MT", "start_pos": 110, "end_pos": 112, "type": "TASK", "confidence": 0.9670741558074951}]}, {"text": "We received 212 submissions from 12 teams, again demonstrating a wide range of techniques.", "labels": [], "entities": []}, {"text": "\u2022 Experimentation with the maximum n-gram length and weights in BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.986361026763916}]}, {"text": "\u2022 Implementation of smoothed versions of BLEU ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9868983030319214}]}, {"text": "\u2022 Implementation of weighted F-measure to balance both precision and recall.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9085719585418701}, {"text": "precision", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.9993888139724731}, {"text": "recall", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.9978795051574707}]}, {"text": "\u2022 Implementation of several techniques used in AMBER).", "labels": [], "entities": []}, {"text": "The best submission, obtaining a correlation of 83.5, relied on the idea that the reference and machine translation should be good paraphrases of each other).", "labels": [], "entities": [{"text": "correlation", "start_pos": 33, "end_pos": 44, "type": "METRIC", "confidence": 0.9777742624282837}, {"text": "machine translation", "start_pos": 96, "end_pos": 115, "type": "TASK", "confidence": 0.6793959587812424}]}, {"text": "It employed a simple paraphrase system trained on the alignment challenge data, using the pivot technique of, and computing the optimal alignment between machine translation and reference under a simple model in which words could align if they were paraphrases.", "labels": [], "entities": []}, {"text": "When compared with the 20 systems submitted to the original task from which the data was obtained, this system would have ranked fifth, quite near the top-scoring competitors, whose correlations ranged from 88 to 94.", "labels": [], "entities": []}], "tableCaptions": []}