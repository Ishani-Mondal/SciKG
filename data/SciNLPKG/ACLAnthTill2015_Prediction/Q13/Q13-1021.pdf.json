{"title": [{"text": "Minimally-Supervised Morphological Segmentation using Adaptor Grammars", "labels": [], "entities": [{"text": "Minimally-Supervised Morphological Segmentation", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.7866326173146566}]}], "abstractContent": [{"text": "This paper explores the use of Adaptor Grammars , a nonparametric Bayesian modelling framework, for minimally supervised morphological segmentation.", "labels": [], "entities": []}, {"text": "We compare three training methods: unsupervised training, semi-supervised training, and a novel model selection method.", "labels": [], "entities": []}, {"text": "In the model selection method, we train unsupervised Adaptor Grammars using an over-articulated metagrammar, then use a small labelled data set to select which potential morph boundaries identified by the meta-grammar should be returned in the final output.", "labels": [], "entities": []}, {"text": "We evaluate on five languages and show that semi-supervised training provides a boost over unsupervised training, while the model selection method yields the best average results overall languages and is competitive with state-of-the-art semi-supervised systems.", "labels": [], "entities": []}, {"text": "Moreover, this method provides the potential to tune performance according to different evaluation met-rics or downstream tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Research into unsupervised learning of morphology has along history, starting with the work of.", "labels": [], "entities": []}, {"text": "While early research was mostly motivated by linguistic interests, more recent work in NLP often aims to reduce data sparsity in morphologically rich languages for tasks such as automatic speech recognition, statistical machine translation, or automatic text generation.", "labels": [], "entities": [{"text": "automatic speech recognition", "start_pos": 178, "end_pos": 206, "type": "TASK", "confidence": 0.6620087424914042}, {"text": "statistical machine translation", "start_pos": 208, "end_pos": 239, "type": "TASK", "confidence": 0.7269291083017985}, {"text": "automatic text generation", "start_pos": 244, "end_pos": 269, "type": "TASK", "confidence": 0.605582038561503}]}, {"text": "For these applications, however, completely unsupervised systems may not be ideal if even a small amount of segmented training data is available.", "labels": [], "entities": []}, {"text": "In this paper, we explore the use of Adaptor) for minimally supervised morphological segmentation.", "labels": [], "entities": [{"text": "morphological segmentation", "start_pos": 71, "end_pos": 97, "type": "TASK", "confidence": 0.7020537257194519}]}, {"text": "Adaptor Grammars (AGs) area nonparametric Bayesian modelling framework that can learn latent tree structures over an input corpus of strings.", "labels": [], "entities": []}, {"text": "For example, they can be used to define a morphological grammar where each word consists of zero or more prefixes, a stem, and zero or more suffixes; the actual forms of these morphs (and the segmentation of words into morphs) are learned from the data.", "labels": [], "entities": []}, {"text": "In this general approach AGs are similar to many other unsupervised morphological segmentation systems, such as Linguistica () and the Morfessor family.", "labels": [], "entities": []}, {"text": "A major difference, however, is that the morphological grammar is specified as an input to the program, rather than hard-coded, which allows different grammars to be explored easily.", "labels": [], "entities": []}, {"text": "For the task of segmenting utterances into words, for example, Johnson and colleagues have experimented with grammars encoding different kinds of sub-word and super-word structure (e.g., syllables and collocations), showing that the best grammars far outperform other systems on the same corpora.", "labels": [], "entities": []}, {"text": "These word segmentation papers demonstrated both the power of the AG approach and the synergistic behavior that occurs when learning multiple levels of structure simultaneously.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 6, "end_pos": 23, "type": "TASK", "confidence": 0.7449253499507904}]}, {"text": "However, the bestperforming grammars were selected using the same corpus that was used for final testing, and each paper dealt with only one language.", "labels": [], "entities": []}, {"text": "The ideal unsupervised learner would use a single grammar tuned on one or more development languages and still perform well on other languages where development data is unavailable.", "labels": [], "entities": []}, {"text": "Indeed, this is the basic principle behind Linguistica and Morfessor.", "labels": [], "entities": []}, {"text": "However, we know that different languages can have very different morphological properties, so using a single grammar for all languages may not be the best approach if there is a principled way to choose between grammars.", "labels": [], "entities": []}, {"text": "Though AGs make it easy to try many different possible grammars, the process of proposing and testing plausible options can still be time-consuming.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel method for automatically selecting good morphological grammars for different languages (English, Finnish, Turkish, German, and Estonian) using a small amount of gold-segmented data (1000 word types).", "labels": [], "entities": []}, {"text": "We use the AG framework to specify a very general binarybranching grammar of depth four with which we learn a parse tree of each word that contains several possible segmentation splits for the word.", "labels": [], "entities": []}, {"text": "Then, we use the gold-segmented data to learn, for each language, which of the proposed splits from the original grammar should actually be used in order to best segment that language.", "labels": [], "entities": []}, {"text": "We evaluate our approach on both a small development set and the full Morpho Challenge test set for each language-up to three million word types.", "labels": [], "entities": [{"text": "Morpho Challenge test set", "start_pos": 70, "end_pos": 95, "type": "DATASET", "confidence": 0.8414998352527618}]}, {"text": "In doing so, we demonstrate that using the posterior grammar of an AG model to decode unseen data is a feasible way to scale these models to large data sets.", "labels": [], "entities": []}, {"text": "We compare to several baselines which use the annotated data to different degrees: parameter tuning, grammar tuning, supervised training, or no use of annotated data.", "labels": [], "entities": [{"text": "parameter tuning", "start_pos": 83, "end_pos": 99, "type": "TASK", "confidence": 0.7153819054365158}, {"text": "grammar tuning", "start_pos": 101, "end_pos": 115, "type": "TASK", "confidence": 0.7823549807071686}]}, {"text": "In addition to existing approachesunsupervised and semi-supervised Morfessor, unsupervised Morsel, and unsupervised AGs-we also show how to use the annotated data to train semi-supervised AGs (using the data to accumulate rule statistics rather than for grammar selection).", "labels": [], "entities": []}, {"text": "The grammar selection method yields comparable results to the best of these other approaches.", "labels": [], "entities": []}, {"text": "To summarize, our contributions in this paper are: 1) scaling AGs to large data sets by using the posterior grammar to define an inductive model; 2) demonstrating how to train semi-supervised AG models, and showing that this improves morphological segmentation over unsupervised training; and 3) introducing a novel grammar selection method for AG models whose segmentation results are competitive with the best existing systems.", "labels": [], "entities": []}, {"text": "Before providing details of our methods and results, we first briefly review Adaptor Grammars.", "labels": [], "entities": []}, {"text": "For a formal definition, see.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our results with two measures: segment border F1-score (SBF1) and EMMA).", "labels": [], "entities": [{"text": "segment border F1-score (SBF1)", "start_pos": 43, "end_pos": 73, "type": "METRIC", "confidence": 0.7277520100275675}, {"text": "EMMA", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.9931280612945557}]}, {"text": "SBF1 is one of the simplest and most popular evaluation metrics for morphological segmentations.", "labels": [], "entities": [{"text": "morphological segmentations", "start_pos": 68, "end_pos": 95, "type": "TASK", "confidence": 0.6883077025413513}]}, {"text": "It computes F1-score from the precision and recall of ambiguous segment boundariesi.e., word edges are not counted.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9989223480224609}, {"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9991902709007263}, {"text": "recall", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.9968100190162659}]}, {"text": "It is easy and quick to compute but has the drawback that it gives no credit for one-morpheme words that have been segmented correctly (i.e., are assigned no segment borders).", "labels": [], "entities": []}, {"text": "Also it can only be used on systems and gold standards where the output is just a segmentation of the surface string (e.g., availabil+ity) rather than a morpheme analysis (e.g., available+ity).", "labels": [], "entities": []}, {"text": "For this reason we cannot report SBF1 on our German data, which annotations contain only analyses.", "labels": [], "entities": [{"text": "SBF1", "start_pos": 33, "end_pos": 37, "type": "TASK", "confidence": 0.7757090330123901}, {"text": "German data", "start_pos": 45, "end_pos": 56, "type": "DATASET", "confidence": 0.9470095336437225}]}, {"text": "EMMA is a newer measure that addresses both of these issues-correctly segmented one-morpheme words are reflected in the score, and it can evaluate both concatenative and non-concatenative morphology.", "labels": [], "entities": [{"text": "EMMA", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.5373148918151855}]}, {"text": "EMMA works by finding the best one-toone mapping between the hypothesized and true segments.", "labels": [], "entities": [{"text": "EMMA", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8665329217910767}]}, {"text": "The induced segments are then replaced with their mappings and based on that, F1-score on matching segments is calculated.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9991363883018494}]}, {"text": "Using EMMA we can evaluate the induced segmentations of German words against gold standard analyses.", "labels": [], "entities": [{"text": "EMMA", "start_pos": 6, "end_pos": 10, "type": "DATASET", "confidence": 0.7996895909309387}]}, {"text": "EMMA has a freely available implementation, 9 but is slow to compute because it uses Integer Linear Programming.", "labels": [], "entities": [{"text": "EMMA", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9690735936164856}]}, {"text": "For our dev results, we computed both scores using the entire dev set, but for the large test sets, the evaluation is done on batches of 1000 word types selected randomly from the test set.", "labels": [], "entities": []}, {"text": "This procedure is repeated 10 times and the average is reported, just as in the MC2010 competition ().", "labels": [], "entities": [{"text": "average", "start_pos": 44, "end_pos": 51, "type": "METRIC", "confidence": 0.9883190393447876}, {"text": "MC2010 competition", "start_pos": 80, "end_pos": 98, "type": "DATASET", "confidence": 0.9570917189121246}]}], "tableCaptions": [{"text": " Table 1: Number of word types in our data sets.", "labels": [], "entities": []}, {"text": " Table 2: Dev set results for all models in (a) transductive and (b) inductive mode. Unsupervised AG models and  baselines are shown in the top part of each table; semi-supervised AG models and grammar selection method are below.", "labels": [], "entities": []}, {"text": " Table 3: Test set results for unsupervised baselines Morfessor CatMAP (in transductive and inductive mode) and Morsel;  semi-supervised Morfessor; and AG semi-supervised ( marks the Compounding grammar,  \u2020 denotes SubMorphs  grammar, and  *  is the MorphSeq grammar) and grammar selection methods. Results are shown for each language,  averaged over all languages (when possible: Avg), and averaged over just the languages where scores are available for  all systems (-Est, -Est/Ger).", "labels": [], "entities": []}]}