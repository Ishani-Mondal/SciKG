{"title": [{"text": "Jointly Learning to Parse and Perceive: Connecting Natural Language to the Physical World", "labels": [], "entities": [{"text": "Parse and Perceive", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.6962815523147583}]}], "abstractContent": [{"text": "This paper introduces Logical Semantics with Perception (LSP), a model for grounded language acquisition that learns to map natural language statements to their referents in a physical environment.", "labels": [], "entities": [{"text": "grounded language acquisition", "start_pos": 75, "end_pos": 104, "type": "TASK", "confidence": 0.7103471358617147}]}, {"text": "For example, given an image, LSP can map the statement \"blue mug on the table\" to the set of image segments showing blue mugs on tables.", "labels": [], "entities": []}, {"text": "LSP learns physical representations for both categorical (\"blue,\" \"mug\") and relational (\"on\") language, and also learns to compose these representations to produce the referents of entire statements.", "labels": [], "entities": []}, {"text": "We further introduce a weakly supervised training procedure that estimates LSP's parameters using annotated referents for entire statements, without annotated ref-erents for individual words or the parse structure of the statement.", "labels": [], "entities": []}, {"text": "We perform experiments on two applications: scene understanding and geographical question answering.", "labels": [], "entities": [{"text": "scene understanding", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.8628076612949371}, {"text": "geographical question answering", "start_pos": 68, "end_pos": 99, "type": "TASK", "confidence": 0.6814375321070353}]}, {"text": "We find that LSP outperforms existing, less expressive models that cannot represent relational language.", "labels": [], "entities": []}, {"text": "We further find that weakly supervised training is competitive with fully supervised training while requiring significantly less annotation effort.", "labels": [], "entities": []}], "introductionContent": [{"text": "Learning the mapping from natural language to physical environments is a central problem for natural language semantics.", "labels": [], "entities": [{"text": "natural language semantics", "start_pos": 93, "end_pos": 119, "type": "TASK", "confidence": 0.6395694017410278}]}, {"text": "Understanding this mapping is necessary to enable natural language interactions with robots and other embodied systems.", "labels": [], "entities": []}, {"text": "For example, for an autonomous robot to understand the sentence \"The blue mug is on the table,\" it must be able to identify (1) the objects in its environment corresponding to \"blue mug\" and \"table,\" and (2) the objects which participate in the spatial relation denoted by \"on.\"", "labels": [], "entities": []}, {"text": "If the robot can successfully identify these objects, it understands the meaning of the sentence.", "labels": [], "entities": []}, {"text": "The problem of learning to map from natural language expressions to their referents in an environment is known as grounded language acquisition.", "labels": [], "entities": [{"text": "grounded language acquisition", "start_pos": 114, "end_pos": 143, "type": "TASK", "confidence": 0.6625223159790039}]}, {"text": "In embodied settings, environments consist of raw sensor data -for example, an environment could bean image collected from a robot's camera.", "labels": [], "entities": []}, {"text": "In such applications, grounded language acquisition has two subproblems: parsing, learning the compositional structure of natural language; and perception, learning the environmental referents of individual words.", "labels": [], "entities": [{"text": "grounded language acquisition", "start_pos": 22, "end_pos": 51, "type": "TASK", "confidence": 0.6751484076182047}, {"text": "parsing", "start_pos": 73, "end_pos": 80, "type": "TASK", "confidence": 0.9706277847290039}]}, {"text": "Acquiring both kinds of knowledge is necessary to understand novel language in novel environments.", "labels": [], "entities": []}, {"text": "Unfortunately, perception is often ignored in work on language acquisition.", "labels": [], "entities": [{"text": "language acquisition", "start_pos": 54, "end_pos": 74, "type": "TASK", "confidence": 0.7138905227184296}]}, {"text": "Other variants of grounded language acquisition eliminate the need for perception by assuming access to a logical representation of the environment).", "labels": [], "entities": []}, {"text": "The existing work which has jointly addressed both parsing and perception has significant drawbacks, including: (1) fully supervised models requiring large amounts of manual annotation and (2) limited semantic representations (.", "labels": [], "entities": [{"text": "parsing", "start_pos": 51, "end_pos": 58, "type": "TASK", "confidence": 0.9623074531555176}]}, {"text": "This paper introduces Logical Semantics with Perception (LSP), a model for grounded language acquisition that jointly learns to semantically parse language and perceive the world.", "labels": [], "entities": []}, {"text": "LSP models a mapping from natural language queries to sets of objects in a real-world environment.", "labels": [], "entities": []}, {"text": "The input to LSP is an environment containing objects, such as a seg-mented image, and a natural language query, such as \"the things to the right of the blue mug.\"", "labels": [], "entities": []}, {"text": "Given these inputs, LSP produces (1) a logical knowledge base describing objects and relationships in the environment and (2) a semantic parse of the query capturing its compositional structure.", "labels": [], "entities": []}, {"text": "LSP combines these two outputs to produce the query's grounding, which is the set of object referents of the query's noun phrases, and its denotation, which is the query's answer ().", "labels": [], "entities": []}, {"text": "Weakly supervised training estimates parameters for LSP using queries annotated with their denotations in an environment (.", "labels": [], "entities": [{"text": "LSP", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.9472527503967285}]}, {"text": "This work has two contributions.", "labels": [], "entities": []}, {"text": "The first contribution is LSP, which is more expressive than previous models, representing both one-argument categories and two-argument relations over sets of objects in the environment.", "labels": [], "entities": []}, {"text": "The second contribution is a weakly supervised training procedure that estimates LSP's parameters without annotated semantic parses, noun phrase/object mappings, or manuallyconstructed knowledge bases.", "labels": [], "entities": []}, {"text": "We perform experiments on two different applications.", "labels": [], "entities": []}, {"text": "The first application is scene understanding, where LSP grounds descriptions of images in image segments.", "labels": [], "entities": [{"text": "scene understanding", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.8492085337638855}]}, {"text": "The second application is geographical question answering, where LSP learns to answer questions about locations, represented as polygons on a map.", "labels": [], "entities": [{"text": "geographical question answering", "start_pos": 26, "end_pos": 57, "type": "TASK", "confidence": 0.6168778538703918}]}, {"text": "In geographical question answering, We treat declarative sentences as if they were queries about their subject, e.g., the denotation of \"the mug is on the table\" is the set of mugs on tables.", "labels": [], "entities": [{"text": "geographical question answering", "start_pos": 3, "end_pos": 34, "type": "TASK", "confidence": 0.6534277200698853}]}, {"text": "Typically, the denotation of a sentence is either true or false; our treatment is strictly more general, as a sentence's denotation is nonempty if and only if the sentence is true.", "labels": [], "entities": []}, {"text": "LSP correctly answers 34% more questions than the most comparable state-of-the-art model).", "labels": [], "entities": []}, {"text": "In scene understanding, accuracy similarly improves by 16%.", "labels": [], "entities": [{"text": "scene understanding", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.8737269639968872}, {"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.999685525894165}]}, {"text": "Furthermore, weakly supervised training achieves an accuracy within 6% of that achieved by fully supervised training, while requiring significantly less annotation effort.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9993914365768433}]}], "datasetContent": [{"text": "The evaluation function f eval deterministically scores denotations given a logical form and a logical knowledge base \u0393.", "labels": [], "entities": []}, {"text": "Intuitively, the evaluation function simply evaluates the query on the database \u0393 to produce a denotation.", "labels": [], "entities": []}, {"text": "The evaluation function then assigns score 0 to this denotation, and score \u2212\u221e to all other denotations.", "labels": [], "entities": []}, {"text": "We describe f eval by giving a recurrence for computing the denotation \u03b3 of a logical form on a logical knowledge base \u0393.", "labels": [], "entities": []}, {"text": "This evaluation takes the form of a tree, as in.", "labels": [], "entities": []}, {"text": "The base cases are: The denotations for more complex logical forms are computed recursively by decomposing according to its logical structure.", "labels": [], "entities": []}, {"text": "Our logical forms contain only conjunctions and existential quantifiers; the corresponding recursive computations are: \u2022 Note that a similar recurrence can be used to compute groundings: simply retain the satisfying assignments to existentially-quantified variables.", "labels": [], "entities": []}, {"text": "Our evaluation performs three major comparisons.", "labels": [], "entities": []}, {"text": "First, we examine the performance impact of weakly supervised training by comparing weakly and fully supervised variants of LSP.", "labels": [], "entities": []}, {"text": "Second, we examine the performance impact of modelling relations by comparing against a category-only baseline, which is an ablated version of LSP similar to the model of.", "labels": [], "entities": []}, {"text": "Finally, we examine the causes of errors by performing an error analysis of LSP's semantic parser and perceptual classifiers.", "labels": [], "entities": []}, {"text": "Before describing our results, we first describe some necessary set-up for the experiments.", "labels": [], "entities": []}, {"text": "These sections describe the data sets, features, construction of the CCG lexicon, and details of the models.", "labels": [], "entities": []}, {"text": "Our data sets and additional evaluation resources are available online from http://rtw.ml.cmu.edu/ tacl2013_lsp/.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the two data sets used in our  evaluation, and of the generated lexicons.", "labels": [], "entities": []}, {"text": " Table 2: Model performance on the SCENE and GEOQA datasets. LSP-CAT is an ablated version of LSP  that only learns categories (similar to Matuszek et al. (2012)), LSP-F is LSP trained with annotated seman- tic parses and logical knowledge bases, and LSP-W is LSP trained on sentences with annotated denotations.  Results are separated by the number of relations in each test natural language statement.", "labels": [], "entities": [{"text": "GEOQA datasets", "start_pos": 45, "end_pos": 59, "type": "DATASET", "confidence": 0.9632398188114166}]}, {"text": " Table 3: Accuracy of the semantic parser from each  trained model. Upper bound is the highest accu- racy achievable without modelling comparatives and  other linguistic phenomena not captured by LSP.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.981432318687439}]}, {"text": " Table 4: Perceptual classifier performance, mea- sured against the gold standard logical knowledge  bases. LSP-CAT is excluded from the relation eval- uations, since it does not learn relations. Relations  (rw) is the reweighted metric (see text for details).", "labels": [], "entities": [{"text": "mea- sured", "start_pos": 45, "end_pos": 55, "type": "METRIC", "confidence": 0.8018862803777059}]}]}