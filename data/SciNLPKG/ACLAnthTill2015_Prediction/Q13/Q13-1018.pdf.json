{"title": [{"text": "Joint Arc-factored Parsing of Syntactic and Semantic Dependencies", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper we introduce a joint arc-factored model for syntactic and semantic dependency parsing.", "labels": [], "entities": [{"text": "syntactic and semantic dependency parsing", "start_pos": 58, "end_pos": 99, "type": "TASK", "confidence": 0.6288393974304199}]}, {"text": "The semantic role labeler predicts the full syntactic paths that connect predicates with their arguments.", "labels": [], "entities": []}, {"text": "This process is framed as a linear assignment task, which allows to control some well-formedness constraints.", "labels": [], "entities": []}, {"text": "For the syntactic part, we define a standard arc-factored dependency model that predicts the full syntactic tree.", "labels": [], "entities": []}, {"text": "Finally, we employ dual decomposition techniques to produce consistent syntactic and predicate-argument structures while searching over a large space of syntactic configurations.", "labels": [], "entities": []}, {"text": "In experiments on the CoNLL-2009 English benchmark we observe very competitive results.", "labels": [], "entities": [{"text": "CoNLL-2009 English benchmark", "start_pos": 22, "end_pos": 50, "type": "DATASET", "confidence": 0.920022984345754}]}], "introductionContent": [{"text": "Semantic role labeling (SRL) is the task of identifying the arguments of lexical predicates in a sentence and labeling them with semantic roles (.", "labels": [], "entities": [{"text": "Semantic role labeling (SRL)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.849838395913442}]}, {"text": "SRL is an important shallow semantic task in NLP since predicate-argument relations directly represent semantic properties of the type \"who\" did \"what\" to \"whom\", \"how\", and \"why\" for events expressed by predicates (typically verbs and nouns).", "labels": [], "entities": [{"text": "SRL", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9508969187736511}]}, {"text": "Predicate-argument relations are strongly related to the syntactic structure of the sentence: the majority of predicate arguments correspond to some syntactic constituent, and the syntactic structure that connects an argument with the predicate is a strong indicator of its semantic role.", "labels": [], "entities": []}, {"text": "Actually, semantic roles represent an abstraction of the syntactic form of a predicative event.", "labels": [], "entities": []}, {"text": "While syntactic functions of arguments change with the form of the event (e.g., active vs. passive forms), the semantic roles of arguments remain invariant to their syntactic realization.", "labels": [], "entities": []}, {"text": "Consequently, since the first works, SRL systems have assumed access to the syntactic structure of the sentence (.", "labels": [], "entities": [{"text": "SRL", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.9710835218429565}]}, {"text": "A simple approach is to obtain the parse trees as a pre-process to the SRL system, which allows the use of unrestricted features of the syntax.", "labels": [], "entities": []}, {"text": "However, as in other pipeline approaches in NLP, it has been shown that the errors of the syntactic parser severely degrade the predictions of the SRL model (.", "labels": [], "entities": []}, {"text": "A common approach to alleviate this problem is to work with multiple alternative syntactic trees and let the SRL system optimize over any input tree or part of it (.", "labels": [], "entities": []}, {"text": "As a step further, more recent work has proposed parsing models that predict syntactic structure augmented with semantic predicate-argument relations (, which is the focus of this paper.", "labels": [], "entities": []}, {"text": "These joint models should favor the syntactic structure that is most consistent with the semantic predicate-argument structures of a sentence.", "labels": [], "entities": []}, {"text": "In principle, these models can exploit syntactic and semantic features simultaneously, and could potentially improve the accuracy for both syntactic and semantic relations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.9993308782577515}]}, {"text": "One difficulty in the design of joint syntacticsemantic parsing models is that there exist important structural divergences between the two layers.", "labels": [], "entities": [{"text": "joint syntacticsemantic parsing", "start_pos": 32, "end_pos": 63, "type": "TASK", "confidence": 0.7016733884811401}]}], "datasetContent": [{"text": "We present experiments using our syntacticsemantic parser on the CoNLL-2009 Shared Task English benchmark.", "labels": [], "entities": [{"text": "CoNLL-2009 Shared Task English benchmark", "start_pos": 65, "end_pos": 105, "type": "DATASET", "confidence": 0.8672708630561828}]}, {"text": "It consists of the usual WSJ training/development/test sections mapped to dependency trees, augmented with semantic predicate-argument relations from PropBank () and NomBank () also represented as dependencies.", "labels": [], "entities": [{"text": "WSJ training/development/test", "start_pos": 25, "end_pos": 54, "type": "TASK", "confidence": 0.6047185858090719}]}, {"text": "It also contains a PropBanked portion of the Brown corpus as an out-of-domain test set.", "labels": [], "entities": [{"text": "PropBanked portion of the Brown corpus", "start_pos": 19, "end_pos": 57, "type": "DATASET", "confidence": 0.8345732986927032}]}, {"text": "Our goal was to evaluate the contributions of parsing algorithms in the following configurations: Base Pipeline Runs a syntactic parser and then runs an SRL parser constrained to paths of the best syntactic tree.", "labels": [], "entities": [{"text": "parsing", "start_pos": 46, "end_pos": 53, "type": "TASK", "confidence": 0.9647031426429749}]}, {"text": "In the SRL it only enforces constraint cArg, by simply classifying the candidate argument in each path into one of the possible semantic roles or as NULL.", "labels": [], "entities": [{"text": "SRL", "start_pos": 7, "end_pos": 10, "type": "TASK", "confidence": 0.8784390687942505}]}, {"text": "Pipeline with Assignment Runs the assignment algorithm for SRL, enforcing constraints cRole and cArg, but constrained to paths of the best syntactic tree.", "labels": [], "entities": [{"text": "SRL", "start_pos": 59, "end_pos": 62, "type": "TASK", "confidence": 0.9226132035255432}]}, {"text": "Forest Runs the assignment algorithm for SRL on a large set of precomputed syntactic paths, described below.", "labels": [], "entities": [{"text": "SRL", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.9910227060317993}]}, {"text": "This configuration corresponds to running Dual Decomposition fora single iteration, and is not guaranteed to predict consistent syntactic and semantic structures.", "labels": [], "entities": []}, {"text": "Dual Decomposition (DD) Runs dual decomposition using the assignment algorithm on the set of precomputed paths.", "labels": [], "entities": []}, {"text": "Syntactic and semantic structures are consistent when it reaches convergence.", "labels": [], "entities": []}, {"text": "All four systems used the same type of discriminative scorers and features.", "labels": [], "entities": []}, {"text": "Next we provide details about these systems.", "labels": [], "entities": []}, {"text": "Then we present the results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on development for the baseline and as- signment pipelines, running first and second-order syn- tactic parsers, and the Forest method. o indicates the or- der of syntactic inference.", "labels": [], "entities": []}, {"text": " Table 3: Comparative results on the CoNLL-2009 En- glish test sets, namely the WSJ test (top table) and the  out of domain test from the Brown corpus (bottom table).", "labels": [], "entities": [{"text": "CoNLL-2009 En- glish test sets", "start_pos": 37, "end_pos": 67, "type": "DATASET", "confidence": 0.862235814332962}, {"text": "WSJ test", "start_pos": 80, "end_pos": 88, "type": "DATASET", "confidence": 0.7304844856262207}, {"text": "Brown corpus", "start_pos": 138, "end_pos": 150, "type": "DATASET", "confidence": 0.9623202979564667}]}]}