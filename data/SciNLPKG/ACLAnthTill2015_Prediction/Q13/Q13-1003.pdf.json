{"title": [], "abstractContent": [{"text": "Recent work has shown that the integration of visual information into text-based models can substantially improve model predictions, but so far only visual information extracted from static images has been used.", "labels": [], "entities": []}, {"text": "In this paper, we consider the problem of grounding sentences describing actions in visual information extracted from videos.", "labels": [], "entities": []}, {"text": "We present a general purpose corpus that aligns high quality videos with multiple natural language descriptions of the actions portrayed in the videos, together with an annotation of how similar the action descriptions are to each other.", "labels": [], "entities": []}, {"text": "Experimental results demonstrate that a text-based model of similarity between actions improves substantially when combined with visual information from videos depicting the described actions.", "labels": [], "entities": []}], "introductionContent": [{"text": "The estimation of semantic similarity between words and phrases is a basic task in computational semantics.", "labels": [], "entities": [{"text": "estimation of semantic similarity between words and phrases", "start_pos": 4, "end_pos": 63, "type": "TASK", "confidence": 0.8475115746259689}]}, {"text": "Vector-space models of meaning are one standard approach.", "labels": [], "entities": []}, {"text": "Following the distributional hypothesis, frequencies of context words are recorded in vectors, and semantic similarity is computed as a proximity measure in the underlying vector space.", "labels": [], "entities": []}, {"text": "Such distributional models are attractive because they are conceptually simple, easy to implement and relevant for various NLP tasks.", "labels": [], "entities": []}, {"text": "At the same time, they provide a substantially incomplete picture of word meaning, since they ignore the relation between language and extralinguistic information, which is constitutive for linguistic meaning.", "labels": [], "entities": []}, {"text": "In the last few years, a growing amount of work has been devoted to the task of grounding meaning in visual information, in particular by extending the distributional approach to jointly cover texts and images).", "labels": [], "entities": []}, {"text": "As a clear result, visual information improves the quality of distributional models.", "labels": [], "entities": []}, {"text": "show that visual information drawn from images is particularly relevant for concrete common nouns and adjectives.", "labels": [], "entities": []}, {"text": "A natural next step is to integrate visual information from videos into a semantic model of event and action verbs.", "labels": [], "entities": []}, {"text": "Psychological studies have shown the connection between action semantics and videos), but to our knowledge, we are the first to provide a suitable data source and to implement such a model.", "labels": [], "entities": []}, {"text": "The contribution of this paper is three-fold: \u2022 We present a multimodal corpus containing textual descriptions aligned with high-quality videos.", "labels": [], "entities": []}, {"text": "Starting from the video corpus of, which contains highresolution video recordings of basic cooking tasks, we collected multiple textual descriptions of each video via Mechanical Turk.", "labels": [], "entities": []}, {"text": "We also provide an accurate sentence-level alignment of the descriptions with their respective videos.", "labels": [], "entities": []}, {"text": "We expect the corpus to be a valuable resource for computational semantics, and moreover helpful fora variety of purposes, including video understanding and generation of text from videos.", "labels": [], "entities": [{"text": "video understanding", "start_pos": 133, "end_pos": 152, "type": "TASK", "confidence": 0.7141239494085312}, {"text": "generation of text from videos", "start_pos": 157, "end_pos": 187, "type": "TASK", "confidence": 0.8111026763916016}]}, {"text": "\u2022 We provide a gold-standard dataset for the evaluation of similarity models for action verbs and phrases.", "labels": [], "entities": []}, {"text": "The dataset has been designed as analogous to the Usage Similarity dataset of 25  and contains pairs of naturallanguage action descriptions plus their associated video segments.", "labels": [], "entities": [{"text": "Usage Similarity dataset of 25", "start_pos": 50, "end_pos": 80, "type": "DATASET", "confidence": 0.7551549851894379}]}, {"text": "Each of the pairs is annotated with a similarity score based on several manual annotations.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 38, "end_pos": 54, "type": "METRIC", "confidence": 0.9671206176280975}]}, {"text": "\u2022 We report an experiment on similarity modeling of action descriptions based on the video corpus and the gold standard annotation, which demonstrates the impact of scene information from videos.", "labels": [], "entities": [{"text": "similarity modeling of action descriptions", "start_pos": 29, "end_pos": 71, "type": "TASK", "confidence": 0.725988095998764}]}, {"text": "Visual similarity models outperform text-based models; the performance of combined models approaches the upper bound indicated by inter-annotator agreement.", "labels": [], "entities": []}, {"text": "The paper is structured as follows: We first place ourselves in the landscape of related work (Sec. 2), then we introduce our corpus (Sec. 3).", "labels": [], "entities": []}, {"text": "4 reports our action similarity annotation experiment and Sec.", "labels": [], "entities": [{"text": "action similarity annotation", "start_pos": 14, "end_pos": 42, "type": "TASK", "confidence": 0.6789792676766714}]}, {"text": "5 introduces the similarity measures we apply to the annotated data.", "labels": [], "entities": [{"text": "similarity", "start_pos": 17, "end_pos": 27, "type": "METRIC", "confidence": 0.9579004049301147}]}, {"text": "We outline the results of our evaluation in Sec.", "labels": [], "entities": []}, {"text": "6, and conclude the paper with a summary and directions for future work (Sec. 7).", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we present a gold standard dataset, as a basis for the evaluation of visually grounded models of action similarity.", "labels": [], "entities": []}, {"text": "We call it the \"Action Similarity Dataset\" (ASim) in analogy to the Usage Similarity dataset (USim) of  and.", "labels": [], "entities": []}, {"text": "Similarly to USim, ASim contains a collection of sentence pairs with numerical similarity scores assigned by human annotators.", "labels": [], "entities": []}, {"text": "We asked the annotators to focus on the similarity of the activities described rather than on assessing semantic similarity in general.", "labels": [], "entities": []}, {"text": "We use sentences from the TACOS corpus and record their timestamps.", "labels": [], "entities": [{"text": "TACOS corpus", "start_pos": 26, "end_pos": 38, "type": "DATASET", "confidence": 0.8674107491970062}]}, {"text": "Thus each sentence comes with the video segment which it describes (these were not shown to the annotators).", "labels": [], "entities": []}, {"text": "We evaluate the different similarity models introduced in Sec.", "labels": [], "entities": []}, {"text": "5 by calculating their correlation with the gold-standard similarity annotations of ASim (cf. Sec. 4).", "labels": [], "entities": []}, {"text": "For all correlations, we use Spearman's \u03c1 as a measure.", "labels": [], "entities": [{"text": "Spearman's \u03c1", "start_pos": 29, "end_pos": 41, "type": "METRIC", "confidence": 0.5796833336353302}]}, {"text": "We consider the two textual measures (JACCARD and TEXTUAL VECTORS) and their combination, as well as the two visual models (VISUAL RAW VECTORS and VISUAL CLAS-SIFIER) and their combination.", "labels": [], "entities": [{"text": "JACCARD", "start_pos": 38, "end_pos": 45, "type": "METRIC", "confidence": 0.6330939531326294}, {"text": "TEXTUAL", "start_pos": 50, "end_pos": 57, "type": "METRIC", "confidence": 0.9386755228042603}]}, {"text": "We also combined textual and visual features, in two variants: The first includes all models (ALL COMBINED), the second only the unsupervised components, omitting the visual classifier (ALL UNSUPERVISED).", "labels": [], "entities": [{"text": "ALL COMBINED)", "start_pos": 94, "end_pos": 107, "type": "METRIC", "confidence": 0.8378638426462809}]}, {"text": "To combine multiple similarity measures, we simply average their normalized scores (using z-scores).", "labels": [], "entities": []}, {"text": "shows the scores for all of these measures on the complete ASim dataset (OVERALL), along with the two subparts, where description pairs share either the object (SAME OBJECT) or the head verb (SAME VERB).", "labels": [], "entities": [{"text": "ASim dataset (OVERALL)", "start_pos": 59, "end_pos": 81, "type": "DATASET", "confidence": 0.691973739862442}, {"text": "VERB", "start_pos": 197, "end_pos": 201, "type": "METRIC", "confidence": 0.6903295516967773}]}, {"text": "In addition to the model results, the table also shows the average human interannotator agreement as UPPER BOUND.", "labels": [], "entities": [{"text": "UPPER", "start_pos": 101, "end_pos": 106, "type": "METRIC", "confidence": 0.9502369165420532}, {"text": "BOUND", "start_pos": 107, "end_pos": 112, "type": "METRIC", "confidence": 0.7439733147621155}]}, {"text": "On the complete set, both visual and textual measures have a highly significant correlation with the gold standard, whereas the combination of both clearly leads to the best performance (0.55).", "labels": [], "entities": []}, {"text": "The results on the SAME OBJECT and SAME VERB subsets shed light on the division of labor between the two information sources.", "labels": [], "entities": [{"text": "SAME OBJECT", "start_pos": 19, "end_pos": 30, "type": "DATASET", "confidence": 0.5645475089550018}, {"text": "SAME VERB subsets", "start_pos": 35, "end_pos": 52, "type": "DATASET", "confidence": 0.5601509312788645}]}, {"text": "While the textual measures show a comparable performance over the two subsets, there is a dramatic difference in the contribution of visual information: On the SAME OBJECT set, the visual models clearly outperform the textual ones, whereas the visual information has no positive effect on the SAME VERB set.", "labels": [], "entities": [{"text": "SAME OBJECT set", "start_pos": 160, "end_pos": 175, "type": "DATASET", "confidence": 0.6641574005285898}, {"text": "SAME VERB set", "start_pos": 293, "end_pos": 306, "type": "DATASET", "confidence": 0.5701392491658529}]}, {"text": "This is clear evidence that the visual model does not capture the similarity of the participating objects but rather genuine action similarity, which the visual features () we employ were designed for.", "labels": [], "entities": []}, {"text": "A direction for future work is to learn dedicated visual object detectors to recognize and capture similarities between objects more precisely.", "labels": [], "entities": []}, {"text": "The numbers shown in support this hypothesis, showing the two groups in the SAME OB-JECT class: For sentence pairs that share the same activity, the textual models seem to be much more suitable than the visual ones.", "labels": [], "entities": []}, {"text": "In general, visual models perform better on actions with different activity types, textual models on closely related activities.", "labels": [], "entities": []}, {"text": "Overall, the supervised classifier contributes a good part to the final results.", "labels": [], "entities": []}, {"text": "However, the supervision is not strictly necessary to arrive at a significant correlation; the raw visual features alone are sufficient for the main performance gain seen with the integration of visual information.", "labels": [], "entities": []}], "tableCaptions": []}