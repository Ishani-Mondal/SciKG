{"title": [{"text": "Modeling Child Divergences from Adult Grammar", "labels": [], "entities": [{"text": "Modeling Child Divergences from Adult Grammar", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.8831082681814829}]}], "abstractContent": [{"text": "During the course of first language acquisition , children produce linguistic forms that do not conform to adult grammar.", "labels": [], "entities": []}, {"text": "In this paper, we introduce a data set and approach for systematically modeling this child-adult grammar divergence.", "labels": [], "entities": [{"text": "child-adult grammar divergence", "start_pos": 85, "end_pos": 115, "type": "TASK", "confidence": 0.6351411640644073}]}, {"text": "Our corpus consists of child sentences with corrected adult forms.", "labels": [], "entities": []}, {"text": "We bridge the gap between these forms with a discrim-inatively reranked noisy channel model that translates child sentences into equivalent adult utterances.", "labels": [], "entities": []}, {"text": "Our method outperforms MT and ESL baselines, reducing child error by 20%.", "labels": [], "entities": [{"text": "MT", "start_pos": 23, "end_pos": 25, "type": "TASK", "confidence": 0.9335432052612305}, {"text": "error", "start_pos": 60, "end_pos": 65, "type": "METRIC", "confidence": 0.6136068105697632}]}, {"text": "Our model allows us to chart specific aspects of grammar development in longitudinal studies of children, and investigate the hypothesis that children share a common developmental path in language acquisition.", "labels": [], "entities": []}], "introductionContent": [{"text": "Since the publication of the Brown, the existence of standard stages of development has been an underlying assumption in the study of first language learning.", "labels": [], "entities": []}, {"text": "As a child moves towards language mastery, their language use grows predictably to include more complex syntactic structures, eventually converging to full adult usage.", "labels": [], "entities": []}, {"text": "In the course of this process, children may produce linguistic forms that do not conform to the grammatical standard.", "labels": [], "entities": []}, {"text": "From the adult point of view these are language errors, a label which implies a faulty production.", "labels": [], "entities": []}, {"text": "Considering the work-in-progress nature of a child language learner, these divergences could also be described as expressions of the structural differences between child and adult grammar.", "labels": [], "entities": []}, {"text": "The predictability of these divergences has been observed by psychologists, linguists and parents.", "labels": [], "entities": []}, {"text": "Our work leverages the differences between child and adult language to make two contributions towards the study of language acquisition.", "labels": [], "entities": [{"text": "language acquisition", "start_pos": 115, "end_pos": 135, "type": "TASK", "confidence": 0.7399839460849762}]}, {"text": "First, we provide a corpus of errorful child sentences annotated with adult-like rephrasings.", "labels": [], "entities": []}, {"text": "This data will allow researchers to test hypotheses and build models relating the development of child language to adult forms.", "labels": [], "entities": []}, {"text": "Our second contribution is a probabilistic model trained on our corpus that predicts a grammatical rephrasing given an errorful child sentence.", "labels": [], "entities": []}, {"text": "The generative assumption of our model is that sentences begin in underlying adult forms, and are then stochastically transformed into observed child utterances.", "labels": [], "entities": []}, {"text": "Given an observed child utterance s, we calculate the probability of the corrected adult translation t as P (t|s) \u221d P (s|t)P (t), where P (t) is an adult language model and P (s|t) is a noise model crafted to capture child grammar errors like omission of certain function words and corruptions of tense or declension.", "labels": [], "entities": []}, {"text": "The parameters of this noise model are estimated using our corpus of child and adult-form utterances, using EM to handle unobserved word alignments.", "labels": [], "entities": []}, {"text": "We use this generative model to produce n-best lists of candidate corrections which are then reranked using long range sentence features in a discriminative framework ().", "labels": [], "entities": []}, {"text": "One could argue that our noisy channel model mirrors the cognitive process of child language production by appealing to the hypothesis that children rapidly learn adult-like grammar but produce errors due to performance factors.", "labels": [], "entities": []}, {"text": "That being said, our primary goal in this paper is not cognitive plausibility, but rather the creation of a practical tool to aid in the empirical study of language acquisition.", "labels": [], "entities": [{"text": "language acquisition", "start_pos": 156, "end_pos": 176, "type": "TASK", "confidence": 0.7066170424222946}]}, {"text": "By automatically inferring adult-like forms of child sentences, our model can highlight and compare developmental trends of children overtime using large quantities of data, while minimizing the need for human annotation.", "labels": [], "entities": []}, {"text": "Besides this, our model's predictive success itself has theoretical implications.", "labels": [], "entities": []}, {"text": "By aggregating training and testing data across children, our model instantiates the Brown hypothesis of a shared developmental path.", "labels": [], "entities": []}, {"text": "Even when adequate per-child training data exists, using data only from other children leads to no degradation in performance, suggesting that the learned parameters capture general child language phenomena and not just individual habits.", "labels": [], "entities": []}, {"text": "Besides aggregating across children, our model coarsely lumps together all stages of development, providing a frozen snapshot of child grammar.", "labels": [], "entities": []}, {"text": "This establishes a baseline for more cognitively plausible and temporally dynamic models.", "labels": [], "entities": []}, {"text": "We compare our correction system against two baselines, a phrase-based Machine Translation (MT) system, and a model designed for English Second Language (ESL) error correction.", "labels": [], "entities": [{"text": "phrase-based Machine Translation (MT)", "start_pos": 58, "end_pos": 95, "type": "TASK", "confidence": 0.7686910430590311}, {"text": "English Second Language (ESL) error correction", "start_pos": 129, "end_pos": 175, "type": "TASK", "confidence": 0.6267059333622456}]}, {"text": "Relative to the best performing baseline, our approach achieves a 30% decrease in word error-rate and a four point increase in BLEU score.", "labels": [], "entities": [{"text": "word error-rate", "start_pos": 82, "end_pos": 97, "type": "METRIC", "confidence": 0.6454906165599823}, {"text": "BLEU score", "start_pos": 127, "end_pos": 137, "type": "METRIC", "confidence": 0.9840750694274902}]}, {"text": "We analyze the performance of our system on various child error categories, highlighting our model's strengths (correcting be drops and morphological overgeneralizations) as well as its weaknesses (correcting pronoun and auxiliary drops).", "labels": [], "entities": []}, {"text": "We also assess the learning rate of our model, showing that very little annotation is needed to achieve high performance.", "labels": [], "entities": []}, {"text": "Finally, to showcase a potential application, we use our model to chart one aspect of four children's grammar acquisition overtime.", "labels": [], "entities": [{"text": "four children's grammar acquisition overtime", "start_pos": 86, "end_pos": 130, "type": "TASK", "confidence": 0.6835415412982305}]}, {"text": "While generally vindicating the Brown thesis of a common developmental path, the results point to subtleties in variation across individuals that merit further investigation.", "labels": [], "entities": []}], "datasetContent": [{"text": "Baselines We compare our system's performance with two pre-existing baselines.", "labels": [], "entities": []}, {"text": "The first is a standard phrase-based machine translation system using MOSES () with GIZA++ (Och and Ney, 2003) word alignments.", "labels": [], "entities": [{"text": "phrase-based machine translation", "start_pos": 24, "end_pos": 56, "type": "TASK", "confidence": 0.6255564192930857}, {"text": "Och and Ney, 2003) word alignments", "start_pos": 92, "end_pos": 126, "type": "TASK", "confidence": 0.5269339419901371}]}, {"text": "We holdout 9% of the training data for tuning using the MERT algorithm with BLEU objective.", "labels": [], "entities": [{"text": "MERT", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.8828969597816467}, {"text": "BLEU", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.9989228844642639}]}, {"text": "The second baseline is our implementation of the ESL error correction system described by Park and Levy (2011).", "labels": [], "entities": [{"text": "ESL error correction", "start_pos": 49, "end_pos": 69, "type": "TASK", "confidence": 0.8280407190322876}]}, {"text": "Like our system, this baseline trains FST noise models using EM in the V-expectation semiring.", "labels": [], "entities": [{"text": "FST noise", "start_pos": 38, "end_pos": 47, "type": "TASK", "confidence": 0.7574481666088104}, {"text": "EM", "start_pos": 61, "end_pos": 63, "type": "METRIC", "confidence": 0.9480382204055786}]}, {"text": "Our noise model is crafted specifically for the child language domain, and so differs from Park and Levy's in several ways: First, we capture a wider range of word-swaps, with richer parameterization allowing many more translation options.", "labels": [], "entities": []}, {"text": "As a result, our model has 6,718 parameters, many more than the ESL model's 187.", "labels": [], "entities": []}, {"text": "These parameters correspond to learned probability distributions, whereas in the ESL model many of the distributions are fixed as uniform.", "labels": [], "entities": []}, {"text": "We also capture a larger class of errors, including deletions, change of auxiliary lemma, and inflectional overgeneralizations.", "labels": [], "entities": []}, {"text": "Finally, we use a discriminative reranking step to model long-range syntactic dependencies.", "labels": [], "entities": []}, {"text": "Although the ESL model is originally geared towards fully unsupervised training, we train this baseline in the same supervised framework as our model.", "labels": [], "entities": []}, {"text": "We train all models on 80% of our child-adult sentence pairs and test on the remaining 20%.", "labels": [], "entities": []}, {"text": "For illustration, selected output from our model is shown in.", "labels": [], "entities": []}, {"text": "Predictions are evaluated with BLEU score) and Word Error Rate (WER), defined as the minimum string edit distance (in words) between reference and predicted translations, divided by length of the reference.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 31, "end_pos": 41, "type": "METRIC", "confidence": 0.9832784831523895}, {"text": "Word Error Rate (WER)", "start_pos": 47, "end_pos": 68, "type": "METRIC", "confidence": 0.9323379198710123}]}, {"text": "As a control, we compare all results against scores for the uncorrected child sentences themselves.", "labels": [], "entities": []}, {"text": "As reported in, our model achieves the best scores for both metrics.", "labels": [], "entities": []}, {"text": "BLEU score increases from 50 for child sentences to 62, while WER is reduced from .271 to .224.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9493924975395203}, {"text": "WER", "start_pos": 62, "end_pos": 65, "type": "METRIC", "confidence": 0.9990525841712952}]}, {"text": "Interestingly, MOSES achieves a BLEU score of 58 -still four points below our model -but actually increases WER to .449.", "labels": [], "entities": [{"text": "MOSES", "start_pos": 15, "end_pos": 20, "type": "METRIC", "confidence": 0.9301515221595764}, {"text": "BLEU score", "start_pos": 32, "end_pos": 42, "type": "METRIC", "confidence": 0.9859528541564941}, {"text": "WER", "start_pos": 108, "end_pos": 111, "type": "METRIC", "confidence": 0.999430239200592}]}, {"text": "For both metrics, the ESL system increases error.", "labels": [], "entities": [{"text": "error", "start_pos": 43, "end_pos": 48, "type": "METRIC", "confidence": 0.9914947748184204}]}, {"text": "This is not surprising given that its intended application is in an entirely different domain.", "labels": [], "entities": []}, {"text": "Error Analysis We measured the performance of our model over the six most common categories of child divergence, including deletions of various function words and overgeneralizations of past tense forms (e.g. \"maked\" for \"made\").", "labels": [], "entities": [{"text": "Error Analysis", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7213226854801178}]}, {"text": "We first identified model parameters associated with each category, and then counted the number of correct and incorrect parameter firings on the test sentences.", "labels": [], "entities": []}, {"text": "As indicates, our model performs reasonably well on \"be\" verb deletions, preposition deletions, and overgeneralizations, but has difficulty correcting pronoun and auxiliary deletions.", "labels": [], "entities": []}, {"text": "In general, hypothesizing dropped words burdens the noise model by adding additional draws from multinomial distributions to the derivation.", "labels": [], "entities": []}, {"text": "To pre-  dicta deletion, either the language model or the reranker must strongly prefer including the omitted word.", "labels": [], "entities": []}, {"text": "A syntax-based noise model may achieve better performance in detecting and correcting child word drops.", "labels": [], "entities": [{"text": "detecting and correcting child word drops", "start_pos": 61, "end_pos": 102, "type": "TASK", "confidence": 0.8272842466831207}]}, {"text": "While our model parameterization and performance rely on the largely constrained nature of child language errors, we observe some instances in which it is overly restrictive.", "labels": [], "entities": []}, {"text": "For 10% of utterances in our corpus, it is impossible to recover the exact gold-standard adult sentence.", "labels": [], "entities": []}, {"text": "These sentences feature errors like reordering or lexical lemma swapsfor example \"I talk Mexican\" for \"I speak Spanish.\"", "labels": [], "entities": []}, {"text": "While our model may correct other errors in these sentences, a perfect correction is unattainable.", "labels": [], "entities": []}, {"text": "Sometimes, our model produces appropriate forms which by happenstance do not conform to the annotators' decision.", "labels": [], "entities": []}, {"text": "For example, in the second row of, the model corrects \"This one have water?\" to \"This one has water?\", instead of the more verbose correction chosen by the annotators (\"Does this one have water?\").", "labels": [], "entities": []}, {"text": "Similarly, our model sometimes produces corrections which seem appropriate in isolation, but do not preserve the meaning implied by the larger conversational context.", "labels": [], "entities": []}, {"text": "For example, in row three of, the sentence \"Want to read the book.\" is recognized both by our human annotators and the system as requiring a pronoun subject.", "labels": [], "entities": []}, {"text": "Unlike the annotators, however, the model has no knowledge of conversational context, so it chooses the highest probability pronoun -in this case \"you\" -instead of the contextually correct \"I.\"", "labels": [], "entities": []}, {"text": "Learning Curves In, we see that the learning curves for our model initially rise sharply, then remain relatively flat.", "labels": [], "entities": []}, {"text": "Using only 10% of our training data (80 sentences), we increase BLEU from 44 (using just the language model) to almost 61.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9990876913070679}]}, {"text": "We only reach our reported BLEU score of 62 when adding the final 20% of training data.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 27, "end_pos": 37, "type": "METRIC", "confidence": 0.9825656116008759}]}, {"text": "This result emphasizes the specificity of our parameterization.", "labels": [], "entities": []}, {"text": "Because our model is so tailored to the childlanguage scenario, only a few examples of each error type are needed to find good parameter values.", "labels": [], "entities": []}, {"text": "We suspect that more annotated data would lead to a continued but slow increase in performance.", "labels": [], "entities": []}, {"text": "Training and Testing across Children We use our system to investigate the hypothesis that language acquisition follows a similar path across children.", "labels": [], "entities": [{"text": "language acquisition", "start_pos": 90, "end_pos": 110, "type": "TASK", "confidence": 0.6915114521980286}]}, {"text": "We first note that models trained in both scenarios lead to large error reductions over the child sentences.", "labels": [], "entities": []}, {"text": "This provides evidence that our model captures general, and not child-specific, error patterns.", "labels": [], "entities": []}, {"text": "Although training exclusively on Adam does lead to increased BLEU score (72.58 vs 69.83), WER is minimized when using the larger volume of training data from other children (.186 vs .226).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 61, "end_pos": 71, "type": "METRIC", "confidence": 0.9557026624679565}, {"text": "WER", "start_pos": 90, "end_pos": 93, "type": "METRIC", "confidence": 0.9880346655845642}]}, {"text": "Taken as a whole, these results suggest that training and testing on separate children does not degrade performance.", "labels": [], "entities": []}, {"text": "This finding supports the general hypothesis of shared developmental paths.", "labels": [], "entities": []}, {"text": "Plotting Child Language Errors over Time After training on annotated data, we predict divergences in all available data from the children in Roger Brown's 1973 study -Adam, Eve and Sarah -as well as Abe, a child from a separate study over a similar age-range.", "labels": [], "entities": [{"text": "Roger Brown's 1973 study", "start_pos": 141, "end_pos": 165, "type": "DATASET", "confidence": 0.805043625831604}]}, {"text": "We plot each child's per-utterance frequency of preposition omissions in.", "labels": [], "entities": []}, {"text": "Since we evaluate over 65,000 utterances and reranking has no impact on preposition drop prediction, we skip the reranking step to save computation.", "labels": [], "entities": [{"text": "preposition drop prediction", "start_pos": 72, "end_pos": 99, "type": "TASK", "confidence": 0.7770785490671793}]}, {"text": "In, we see that Adam and Sarah's preposition drops spike early, and then gradually decrease in frequency as their preposition use moves towards that of an adult.", "labels": [], "entities": [{"text": "frequency", "start_pos": 95, "end_pos": 104, "type": "METRIC", "confidence": 0.9780035018920898}]}, {"text": "Although Eve's data covers an earlier time period, we see that her pattern of preposition drops shows a similar spike and gradual decrease.", "labels": [], "entities": []}, {"text": "This is consistent with Eve's general language precocity.", "labels": [], "entities": []}, {"text": "Brown's conclusion -that the language development of these three children advanced in similar stages at different times -is consistent with our predictions.", "labels": [], "entities": []}, {"text": "However, when we examine: Automatically detected preposition omissions in un-annotated utterances from four children overtime.", "labels": [], "entities": []}, {"text": "Assuming perfect model predictions, frequencies are \u00b1.002 at p = .05 under a binomial normal approximation interval.", "labels": [], "entities": []}, {"text": "Prediction error is given in.", "labels": [], "entities": [{"text": "Prediction error", "start_pos": 0, "end_pos": 16, "type": "METRIC", "confidence": 0.9677438735961914}]}, {"text": "Abe we do not observe the same pattern.", "labels": [], "entities": []}, {"text": "This points to a degree of variance across children, and suggests the use of our model as a tool for further empirical refinement of language development hypotheses.", "labels": [], "entities": []}, {"text": "Discussion Our error correction system is designed to be more constrained than a full-scale MT system, focusing parameter learning on errors that are known to be common to child language learners.", "labels": [], "entities": [{"text": "error correction", "start_pos": 15, "end_pos": 31, "type": "TASK", "confidence": 0.6979273706674576}, {"text": "MT", "start_pos": 92, "end_pos": 94, "type": "TASK", "confidence": 0.9694833755493164}]}, {"text": "Reorderings are prohibited, lexical word swaps are limited to inflectional changes, and deletions are restricted to function word categories.", "labels": [], "entities": [{"text": "lexical word swaps", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.6504701673984528}]}, {"text": "By highly restricting our hypothesis space, we provide an inductive bias for our model that matches the child language domain.", "labels": [], "entities": []}, {"text": "This is particularly important since the size of our training set is much smaller than that usually used in MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 108, "end_pos": 110, "type": "TASK", "confidence": 0.9752641916275024}]}, {"text": "Indeed, as shows, very little data is needed to achieve good performance.", "labels": [], "entities": []}, {"text": "In contrast, the ESL baseline suffers because its generative model is too restricted for the domain of transcribed child language.", "labels": [], "entities": [{"text": "ESL baseline", "start_pos": 17, "end_pos": 29, "type": "DATASET", "confidence": 0.7201410979032516}]}, {"text": "As shown above in, child deletions of function words are the most frequent error types in our data.", "labels": [], "entities": []}, {"text": "Since the ESL model does not capture word deletions, and has a more restricted notion of word swaps, 88% of child sentences in our training corpus cannot be translated to their reference adult versions.", "labels": [], "entities": [{"text": "word swaps", "start_pos": 89, "end_pos": 99, "type": "TASK", "confidence": 0.7212757617235184}]}, {"text": "The result is that the ESL model tends to rely too heavily on the language model.", "labels": [], "entities": []}, {"text": "For example, on the sentence \"I com-ing to you,\" the ESL model improves n-gram probability by producing \"I came to you\" instead of the correct \"I am coming to you\".", "labels": [], "entities": []}, {"text": "This increases error over the child sentence itself.", "labels": [], "entities": [{"text": "error", "start_pos": 15, "end_pos": 20, "type": "METRIC", "confidence": 0.9978063702583313}]}, {"text": "In addition to the domain-specific generative model, our approach has the advantage of longrange syntactic information encoded by reranking features.", "labels": [], "entities": []}, {"text": "Although the perceptron algorithm places high weight on the generative model probability, it alters the predictions in 17 out of 201 test sentences, in all cases an improvement.", "labels": [], "entities": [{"text": "generative model", "start_pos": 60, "end_pos": 76, "type": "TASK", "confidence": 0.8942958116531372}]}, {"text": "Three of these reranking changes add a noun subject, five enforce question structure, and nine add a main verb.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: WER and BLEU scores. Our system's perfor- mance using various reranking schemes (BLEU objec- tive, WER objective and none) is contrasted with Moses  MT and ESL error correction baselines, as well as un- corrected test sentences. Best performance under each  metric is shown in bold.", "labels": [], "entities": [{"text": "WER", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.8889226913452148}, {"text": "BLEU", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.9880997538566589}, {"text": "BLEU", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.9952322840690613}, {"text": "WER objective", "start_pos": 109, "end_pos": 122, "type": "METRIC", "confidence": 0.9471391439437866}, {"text": "ESL error correction baselines", "start_pos": 166, "end_pos": 196, "type": "METRIC", "confidence": 0.6146153435111046}]}, {"text": " Table 4: Frequency of the six most common error types  in test data, along with our model's corresponding F- measure, precision and recall. All counts are \u00b1.12 at  p = .05 under a binomial normal approximation inter- val.", "labels": [], "entities": [{"text": "Frequency", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9873606562614441}, {"text": "F- measure", "start_pos": 107, "end_pos": 117, "type": "METRIC", "confidence": 0.9947500030199686}, {"text": "precision", "start_pos": 119, "end_pos": 128, "type": "METRIC", "confidence": 0.9991149306297302}, {"text": "recall", "start_pos": 133, "end_pos": 139, "type": "METRIC", "confidence": 0.9992552399635315}]}, {"text": " Table 5: Performance on Adam's sentences training on  other children, versus training on himself. Best perfor- mance under each metric is shown in bold.", "labels": [], "entities": []}]}