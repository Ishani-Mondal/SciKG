{"title": [{"text": "Unsupervised Dependency Parsing with Acoustic Cues", "labels": [], "entities": [{"text": "Dependency Parsing", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.6463054120540619}]}], "abstractContent": [{"text": "Unsupervised parsing is a difficult task that infants readily perform.", "labels": [], "entities": [{"text": "Unsupervised parsing", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.5095916092395782}]}, {"text": "Progress has been made on this task using text-based models, but few computational approaches have considered how infants might benefit from acoustic cues.", "labels": [], "entities": []}, {"text": "This paper explores the hypothesis that word duration can help with learning syntax.", "labels": [], "entities": []}, {"text": "We describe how duration information can be incorporated into an unsupervised Bayesian dependency parser whose only other source of information is the words themselves (without punctuation or parts of speech).", "labels": [], "entities": []}, {"text": "Our results, evaluated on both adult-directed and child-directed utterances, show that using word duration can improve parse quality relative to words-only baselines.", "labels": [], "entities": []}, {"text": "These results support the idea that acoustic cues provide useful evidence about syntactic structure for language-learning infants , and motivate the use of word duration cues in NLP tasks with speech.", "labels": [], "entities": []}], "introductionContent": [{"text": "Unsupervised learning of syntax is difficult for NLP systems, yet infants perform this task routinely.", "labels": [], "entities": []}, {"text": "Previous work in NLP has focused on using the implicit syntactic information available in part-of-speech (POS) tags (), punctuation, and syntactic similarities between related languages).", "labels": [], "entities": []}, {"text": "However, these approaches likely use the data in a very different way from children: neither POS tags nor punctuation are observed during language acquisition (although see and for encouraging results using unsupervised POS tags), and many children learn in a broadly monolingual environment.", "labels": [], "entities": []}, {"text": "This paper explores a possible source of information that NLP systems typically ignore: word duration, or the length of time taken to pronounce each word.", "labels": [], "entities": []}, {"text": "There are good reasons to think that word duration might be useful for learning syntax.", "labels": [], "entities": [{"text": "learning syntax", "start_pos": 71, "end_pos": 86, "type": "TASK", "confidence": 0.783675879240036}]}, {"text": "First, the well-established Prosodic Bootstrapping hypothesis proposes that infants use acoustic-prosodic cues (such as word duration) to help them identify syntactic structure, because prosodic and syntactic structures sometimes coincide.", "labels": [], "entities": []}, {"text": "More recently, we proposed () that infants might use word duration as a direct cue to syntactic structure (i.e., without requiring intermediate prosodic structure), because words in high-probability syntactic structures tend to be pronounced more quickly (.", "labels": [], "entities": []}, {"text": "Like most recent work on unsupervised parsing, we focus on learning syntactic dependencies.", "labels": [], "entities": []}, {"text": "Our work is based on's Bayesian version of the Dependency Model with Valence (DMV) (), using interpolated backoff techniques to incorporate multiple information sources per token.", "labels": [], "entities": []}, {"text": "However, whereas Headden et al. used words and POS tags as input, we use words and word duration information, presenting three variants of their model that use this information in slightly different ways.", "labels": [], "entities": []}, {"text": "To our knowledge, this is the first work to incorporate acoustic cues into an unsupervised system for learning full syntactic parses.", "labels": [], "entities": [{"text": "learning full syntactic parses", "start_pos": 102, "end_pos": 132, "type": "TASK", "confidence": 0.5844436883926392}]}, {"text": "The methods in this paper were inspired by our previous approach (, which showed that word duration measurements could improve the performance of an unsupervised lexicalized syntactic chunker over a words-only baseline.", "labels": [], "entities": []}, {"text": "However, that work was limited to HMM-like sequence models, tested on adultdirected speech (ADS) only, and none of the models outperformed uniform-branching baselines.", "labels": [], "entities": []}, {"text": "Here, we extend our results to full dependency parsing, and experiment on transcripts of both spontaneous ADS and child-directed speech (CDS).", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.712696835398674}]}, {"text": "Our models using word duration outperform words-only baselines, along with the Common Cover Link parser of, and the Unsupervised Partial Parser of, unsupervised lexicalized parsers that have obtained state-of-the-art results on standard newswire treebanks (though their performance here is worse, as our input lacks punctuation).", "labels": [], "entities": []}, {"text": "We also outperform uniform-branching baselines.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate on three datasets: wsj10, sentences of length 10 or less from the Wall Street Journal portion of the Penn Treebank; swbdnxt10, sentences of length 10 or less from the Switchboard dataset of ADS used by Pate and Goldwater (2011); and brent, part of the Brent corpus of CDS).", "labels": [], "entities": [{"text": "Wall Street Journal portion of the Penn Treebank", "start_pos": 78, "end_pos": 126, "type": "DATASET", "confidence": 0.9426139295101166}, {"text": "Switchboard dataset of ADS", "start_pos": 179, "end_pos": 205, "type": "DATASET", "confidence": 0.8586159497499466}, {"text": "Brent corpus of CDS", "start_pos": 264, "end_pos": 283, "type": "DATASET", "confidence": 0.8671992868185043}]}, {"text": "In addition to evaluating the various incarnations of the DMV with backoff and input types, we compare to uniform branching baselines, the Common Cover Link (CCL) parser of, and the Unsupervised Partial Parser (UPP) of.", "labels": [], "entities": []}, {"text": "The UPP produces a constituency parse from words and punctuation using a series of finite-state chun-6 Available at http://homepages.inf.ed.ac.uk/s0930006/brentDep/ kers; we use the best-performing (Probabilistic Right Linear Grammar) version.", "labels": [], "entities": []}, {"text": "The CCL parser produces a constituency parse using a novel \"Cover Link\" representation, scoring these links heuristically.", "labels": [], "entities": [{"text": "constituency parse", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.7104075253009796}]}, {"text": "Both CCL and UPP rely on punctuation (though according to, UPP less so), which our input is missing.", "labels": [], "entities": []}, {"text": "The left-headed \"LH\" (right-headed \"RH\") baseline assumes that each word takes the first word to its right (left) as a dependent, and corresponds to a uniform right-branching (left-branching) constituency baseline.", "labels": [], "entities": []}, {"text": "We evaluate the output of all models in terms of both constituency scores and dependency accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9068644046783447}]}, {"text": "Our wsj10 and swbdnxt10 corpora are originally annotated for constituency structure, with the dependency gold standard derived as described above, while our brent corpus is originally annotated for dependency structure, with the constituency gold standard derived by defining a constituent to span ahead and each of its dependents (ignoring any one-word \"constituents\").", "labels": [], "entities": []}, {"text": "As the CCL and UPP parsers don't produce dependencies, only constituency scores are provided.", "labels": [], "entities": []}, {"text": "For constituency scores, we present the standard unlabeled Precision, Recall, and F-measure scores.", "labels": [], "entities": [{"text": "Precision", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9863690137863159}, {"text": "Recall", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.8874849677085876}, {"text": "F-measure", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.9959732890129089}]}, {"text": "For dependency scores, we present Directed attachment accuracy, Undirected attachment accuracy, and the \"Neutral Edge Detection\" (NED) score introduced by.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.7032119035720825}, {"text": "Undirected attachment accuracy", "start_pos": 64, "end_pos": 94, "type": "METRIC", "confidence": 0.7121281027793884}, {"text": "Neutral Edge Detection\" (NED) score", "start_pos": 105, "end_pos": 140, "type": "METRIC", "confidence": 0.5949265286326408}]}, {"text": "Directed attachment accuracy counts an arc as a true positive if it correctly identifies both ahead and a dependent, whereas undirected attachment accuracy ignores arc direction in counting true positives.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9364104270935059}, {"text": "accuracy", "start_pos": 147, "end_pos": 155, "type": "METRIC", "confidence": 0.8037197589874268}]}, {"text": "NED counts an arc as a true positive if it would be a true positive under the Undirected attachment score, or if the proposed head is the gold-standard grandparent of the proposed dependent.", "labels": [], "entities": [{"text": "NED", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8949699401855469}, {"text": "Undirected attachment score", "start_pos": 78, "end_pos": 105, "type": "METRIC", "confidence": 0.7674638827641805}]}, {"text": "This avoids penalizing parses for flipping an arc, such as making determiners, rather than nouns, the head of noun phrases.", "labels": [], "entities": []}, {"text": "To assess statistical significance, we carried out stratified shuffling tests, with 10, 000 random shuffles, for all measures.", "labels": [], "entities": []}, {"text": "Tables indicate significance differences between the backoff models and the most competitive baseline model on that measure, indicated by an italic score.", "labels": [], "entities": []}, {"text": "A star ( * ) indicates p < 0.05, and a dagger \u2020 indicates p < 0.01.", "labels": [], "entities": []}, {"text": "To seethe direction of a significant difference (i.e. whether the backoff model is better or worse than the baseline),  Significantly different from best non-uniform baseline (italics) by a stratified shuffling test, p < 0.01; * : p < 0.05.", "labels": [], "entities": []}, {"text": "look to the scores themselves.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics for our three corpora.", "labels": [], "entities": []}, {"text": " Table 2: Performance on wsj10 and swbdnxt10 for models using words and POS tags only. Bold scores indicate  the best performance of all models and baselines on that measure.", "labels": [], "entities": []}, {"text": " Table 3: Performance on swbdnxt10 for models using words and duration. The scatterplot includes a subset of the  information in the table: F-score and undirected attachment accuracy for backoff models and VB and LH baseline.  Bold, italics, and significance annotations as in", "labels": [], "entities": [{"text": "F-score", "start_pos": 140, "end_pos": 147, "type": "METRIC", "confidence": 0.9957743287086487}, {"text": "accuracy", "start_pos": 174, "end_pos": 182, "type": "METRIC", "confidence": 0.7911110520362854}]}, {"text": " Table 4: Performance on brent for models using words and duration. The scatterplot includes a subset of the  information in the table: F-score and undirected attachment accuracy for backoff models and VB and LH baseline.  Bold, italics, and significance annotations as in", "labels": [], "entities": [{"text": "F-score", "start_pos": 136, "end_pos": 143, "type": "METRIC", "confidence": 0.9963885545730591}, {"text": "accuracy", "start_pos": 170, "end_pos": 178, "type": "METRIC", "confidence": 0.7040284872055054}]}]}