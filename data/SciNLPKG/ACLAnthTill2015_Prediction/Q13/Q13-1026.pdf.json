{"title": [{"text": "Parsing entire discourses as very long strings: Capturing topic continuity in grounded language learning", "labels": [], "entities": [{"text": "Parsing entire discourses", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8956593672434489}, {"text": "Capturing topic continuity", "start_pos": 48, "end_pos": 74, "type": "TASK", "confidence": 0.9075043201446533}]}], "abstractContent": [{"text": "Grounded language learning, the task of mapping from natural language to a representation of meaning, has attracted more and more interest in recent years.", "labels": [], "entities": [{"text": "Grounded language learning", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6912352045377096}]}, {"text": "In most work on this topic, however, utterances in a conversation are treated independently and discourse structure information is largely ignored.", "labels": [], "entities": []}, {"text": "In the context of language acquisition, this independence assumption discards cues that are important to the learner, e.g., the fact that consecutive utterances are likely to share the same referent (Frank et al., 2013).", "labels": [], "entities": [{"text": "language acquisition", "start_pos": 18, "end_pos": 38, "type": "TASK", "confidence": 0.7086974382400513}]}, {"text": "The current paper describes an approach to the problem of simultaneously modeling grounded language at the sentence and discourse levels.", "labels": [], "entities": []}, {"text": "We combine ideas from parsing and grammar induction to produce a parser that can handle long input strings with thousands of tokens, creating parse trees that represent full discourses.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.7132553160190582}]}, {"text": "By casting grounded language learning as a grammatical inference task, we use our parser to extend the work of Johnson et al.", "labels": [], "entities": []}, {"text": "(2012), investigating the importance of discourse continuity in children's language acquisition and its interaction with social cues.", "labels": [], "entities": [{"text": "discourse continuity", "start_pos": 40, "end_pos": 60, "type": "TASK", "confidence": 0.7333049178123474}]}, {"text": "Our model boosts performance in a language acquisition task and yields good discourse segmentations compared with human annotators.", "labels": [], "entities": [{"text": "language acquisition task", "start_pos": 34, "end_pos": 59, "type": "TASK", "confidence": 0.760562797387441}]}], "introductionContent": [{"text": "Learning mappings between natural language (NL) and meaning representations (MR) is an important goal for both computational linguistics and cognitive science.", "labels": [], "entities": [{"text": "Learning mappings between natural language (NL) and meaning representations (MR)", "start_pos": 0, "end_pos": 80, "type": "TASK", "confidence": 0.8421068106378827}]}, {"text": "Accurately learning novel mappings is crucial in grounded language understanding tasks and such systems can suggest insights into the nature of children language learning.", "labels": [], "entities": [{"text": "language understanding tasks", "start_pos": 58, "end_pos": 86, "type": "TASK", "confidence": 0.7893058359622955}]}, {"text": "Two influential examples of grounded language learning tasks are the sportscasting task, RoboCup, where the NL is the set of running commentary and the MR is the set of logical forms representing actions like kicking or passing, and the cross-situational word-learning task, where the NL is the caregiver's utterances and the MR is the set of objects present in the context.", "labels": [], "entities": []}, {"text": "Work in these domains suggests that, based on the cooccurrence between words and their referents in context, it is possible to learn mappings between NL and MR even under substantial ambiguity.", "labels": [], "entities": []}, {"text": "Nevertheless, contexts like RoboCup-where every single utterance is grounded-are extremely rare.", "labels": [], "entities": []}, {"text": "Much more common are cases where a single topic is introduced and then discussed at length throughout a discourse.", "labels": [], "entities": []}, {"text": "Ina television news show, for example, a topic might be introduced by presenting a relevant picture or video clip.", "labels": [], "entities": []}, {"text": "Once the topic is introduced, the anchors can discuss it by name or even using a pronoun without showing a picture.", "labels": [], "entities": []}, {"text": "The discourse is grounded without having to ground every utterance.", "labels": [], "entities": []}, {"text": "Moreover, although previous work has largely treated utterance order as independent, the order of utterances is critical in grounded discourse contexts: if the order is scrambled, it can become impossible to recover the topic.", "labels": [], "entities": []}, {"text": "Supporting this idea, found that topic continuity-the tendency to talk about the same topic in multiple utterances that are contiguous in time-is both prevalent and informative for word learning.", "labels": [], "entities": [{"text": "topic continuity-the tendency to talk about the same topic in multiple utterances", "start_pos": 33, "end_pos": 114, "type": "TASK", "confidence": 0.7905932491024336}, {"text": "word learning", "start_pos": 181, "end_pos": 194, "type": "TASK", "confidence": 0.8039869368076324}]}, {"text": "This paper examines the importance of topic continuity through a grammatical inference problem.", "labels": [], "entities": [{"text": "topic continuity", "start_pos": 38, "end_pos": 54, "type": "TASK", "confidence": 0.7274999022483826}]}, {"text": "We build on's work that used grammatical inference to) -shown is a parse tree of the input utterance \"wheres the piggie\" accompanied with social cue prefixes, indicating that the caregiver is holding a pig toy while the child is looking at it; at the same time, a dog toy is present in the screen.", "labels": [], "entities": []}, {"text": "learn word-object mappings and to investigate the role of social information (cues like eye-gaze and pointing) in a child language acquisition task.", "labels": [], "entities": [{"text": "child language acquisition task", "start_pos": 116, "end_pos": 147, "type": "TASK", "confidence": 0.6914410442113876}]}, {"text": "Our main contribution lies in the novel integration of existing techniques and algorithms in parsing and grammar induction to offer a complete solution for simultaneously modeling grounded language at the sentence and discourse levels.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 105, "end_pos": 122, "type": "TASK", "confidence": 0.6927144974470139}]}, {"text": "Specifically, we: (1) use the Earley algorithm to exploit the special structure of our grammars, which are deterministic or have at most bounded ambiguity, to achieve approximately linear parsing time; (2) suggest a rescaling approach that enables us to build a PCFG parser capable of handling very long strings with thousands of tokens; and (3) employ Variational Bayes for grammatical inference to obtain better grammars than those given by the EM algorithm.", "labels": [], "entities": []}, {"text": "By parsing entire discourses at once, we shed light on a scientifically interesting question about why the child's own gaze is a positive cue for word learning ().", "labels": [], "entities": [{"text": "word learning", "start_pos": 146, "end_pos": 159, "type": "TASK", "confidence": 0.8389592468738556}]}, {"text": "Our data provide support for the hypothesis (from previous work) that caregivers \"follow in\": they name objects that the child is already looking at (.", "labels": [], "entities": []}, {"text": "In addition, our discourse model produces a performance improvement in a language acquisition task and yields good discourse segmentations compared with human annotators.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments apply sentence-and discourselevel models to the annotated corpus of childdirected speech described in Section 3.", "labels": [], "entities": []}, {"text": "Each model is evaluated on (a) topic accuracy-how many utterances are labeled with correct topics (including the null), (b) topic metrics (f-scores/precision/recall)-how well the model predicts non-null topical utterances, (c) word metrics-how well the model predicts topical words, 9 and (d) lexicon metrics-how well word types are assigned to the topic that they attach to most frequently.", "labels": [], "entities": [{"text": "accuracy-how", "start_pos": 37, "end_pos": 49, "type": "METRIC", "confidence": 0.7822516560554504}, {"text": "precision/recall)-", "start_pos": 148, "end_pos": 166, "type": "METRIC", "confidence": 0.7657331079244614}]}, {"text": "For example, in, the model assigns topic pig to the entire utterance.", "labels": [], "entities": []}, {"text": "At the word level, it labels piggie with topic pig and assigns null topic to wheres and the.", "labels": [], "entities": []}, {"text": "See () for more details of these metrics.", "labels": [], "entities": []}, {"text": "In Section 7.1, we examine baseline models that do not make use of social cues (mother and child's eye-gaze and hand position) to discover the topic; these baselines are contrasted with a range of social cues ( \u00a77.2 and \u00a77.3).", "labels": [], "entities": []}, {"text": "In Section 7.4, we evaluate the discourse structures discovered by our models.", "labels": [], "entities": []}, {"text": "While the discourse model performs well using metrics from previous work, these metrics do not fully reflect an important strength of the model: its ability to capture inter-utterance structure.", "labels": [], "entities": []}, {"text": "For exam-  ple, consider the sequence of utterances in.", "labels": [], "entities": [{"text": "exam-  ple", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.7076921661694845}]}, {"text": "Our previous evaluation is based on the raw annotation, which labels as topical only utterances containing topical words or pronouns referring to an object.", "labels": [], "entities": []}, {"text": "As a result, classifying \"there\" as car is incorrect.", "labels": [], "entities": []}, {"text": "From the perspective of a human listener, however, \"there\" is part of a broader discourse about the car, and labeling it with the same topic captures the fact that it encodes useful information for learners.", "labels": [], "entities": []}, {"text": "To differentiate these cases, Frank and Rohde (under review) added anew set of annotations (to the dataset used in Section 7) based on the discourse structure perceived by human, similar to column discourse, . We utilize these new annotations to judge topics predicted by our discourse model and adopt previous metrics for discourse segmentation evaluation: a=b, a simple proportion equivalence of discourse assignments; pk , a window method) to measure the probability of two random utterances correctly classified as being in the same discourse), an improved version of pk which gives \"partial credit\" to boundaries close to the correct ones.", "labels": [], "entities": [{"text": "discourse segmentation evaluation", "start_pos": 323, "end_pos": 356, "type": "TASK", "confidence": 0.7897848089536031}]}, {"text": "Results in demonstrate that our model is in better agreement with human annotation (modelhuman) than the raw annotation (raw-human) across all metrics.", "labels": [], "entities": []}, {"text": "As is visible from the limited change in the a=b metric, relatively few topic assignments are altered; yet these alterations create much more coherent discourses that allow for far better segmentation performance under pk and WindowDiff.", "labels": [], "entities": []}, {"text": "To put an upper bound on possible discourse segmentation results, we further evaluated performance on a subset of 634 utterances for which multiple annotations were collected.", "labels": [], "entities": [{"text": "discourse segmentation", "start_pos": 34, "end_pos": 56, "type": "TASK", "confidence": 0.7247262001037598}]}, {"text": "Results in demonstrate that our model predicts discourse topics (m-h 1 , m-h 1 ) at a level quite close to the level of agreement between human annotators (column h 1 -h 2 ).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Parsing time (dense grammars) -to compute  surprisal values for a 22-word sentence using Levy's  parser and ours (Earleyx).", "labels": [], "entities": []}, {"text": " Table 2: Social-cue models. Comparison of sentence-and discourse-level models (init: initialized from the VB  sentence-level model) over full metrics. Free energies are shown to compare VB-based models.", "labels": [], "entities": []}, {"text": " Table 3: Baseline (non-social) models. Comparison of  sentence-level models (MCMC (Johnson et al., 2012),  EM, VB) and the discourse-level model.", "labels": [], "entities": []}, {"text": " Table 2. For the sentence-level models using social", "labels": [], "entities": []}, {"text": " Table 4: Social cue influence. Ablation test results across models without discourse (MCMC, VB) and with discourse  (discourse+init). We start with the full set of social cues and drop one at a time. Each cell contains results for metrics:  topic accuracy/topic F 1 /word F 1 /lexicon F 1 . For row discourse+init, we compare models with/without a social cue  using chi-square tests and denote statistically significant results (p < .05) at the utterance (  *  ) and word ( + ) levels.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.977678656578064}]}, {"text": " Table 5: Social cue influence. Add-one test results across models without discourse (MCMC, VB) and with discourse  (discourse). We start with no social information and add one cue at a time. Each cell contains results for metrics:  topic accuracy/topic F 1 /word F 1 /lexicon F 1 . For row discourse, we compare models with/without a social cue using  chi-square tests and denote statistically significant results (p < .05) at the utterance (  *  ) and word ( + ) levels.", "labels": [], "entities": []}, {"text": " Table 7: Discourse evaluation. Single annotator sample,  comparison between topics assigned by the raw annota- tion, our discourse model, and a human coder.", "labels": [], "entities": []}, {"text": " Table 8: Discourse evaluation. Multiple annotator sam- ple, comparison between raw annotations (r), our model  (m), and two independent human coders (h 1 , h 2 ).", "labels": [], "entities": []}]}