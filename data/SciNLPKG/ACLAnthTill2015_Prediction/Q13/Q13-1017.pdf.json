{"title": [{"text": "Dual Coordinate Descent Algorithms for Efficient Large Margin Structured Prediction", "labels": [], "entities": [{"text": "Efficient Large Margin Structured Prediction", "start_pos": 39, "end_pos": 83, "type": "TASK", "confidence": 0.668690288066864}]}], "abstractContent": [{"text": "Due to the nature of complex NLP problems, structured prediction algorithms have been important modeling tools fora wide range of tasks.", "labels": [], "entities": []}, {"text": "While there exists evidence showing that linear Structural Support Vector Machine (SSVM) algorithm performs better than struc-tured Perceptron, the SSVM algorithm is still less frequently chosen in the NLP community because of its relatively slow training speed.", "labels": [], "entities": []}, {"text": "In this paper, we propose a fast and easy-to-implement dual coordinate descent algorithm for SSVMs.", "labels": [], "entities": [{"text": "dual coordinate descent", "start_pos": 55, "end_pos": 78, "type": "TASK", "confidence": 0.6221148570378622}]}, {"text": "Unlike algorithms such as Per-ceptron and stochastic gradient descent, our method keeps track of dual variables and updates the weight vector more aggressively.", "labels": [], "entities": [{"text": "stochastic gradient descent", "start_pos": 42, "end_pos": 69, "type": "TASK", "confidence": 0.7315592169761658}]}, {"text": "As a result, this training process is as efficient as existing online learning methods, and yet derives consistently better models, as evaluated on four benchmark NLP datasets for part-of-speech tagging, named-entity recognition and dependency parsing.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 180, "end_pos": 202, "type": "TASK", "confidence": 0.7321041226387024}, {"text": "named-entity recognition", "start_pos": 204, "end_pos": 228, "type": "TASK", "confidence": 0.7505051791667938}, {"text": "dependency parsing", "start_pos": 233, "end_pos": 251, "type": "TASK", "confidence": 0.8426474630832672}]}], "introductionContent": [{"text": "Complex natural language processing tasks are inherently structured.", "labels": [], "entities": []}, {"text": "From sequence labeling problems like part-of-speech tagging and named entity recognition to tree construction tasks like syntactic parsing, strong dependencies exist among the labels of individual components.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 5, "end_pos": 22, "type": "TASK", "confidence": 0.6566110253334045}, {"text": "part-of-speech tagging", "start_pos": 37, "end_pos": 59, "type": "TASK", "confidence": 0.731160968542099}, {"text": "named entity recognition", "start_pos": 64, "end_pos": 88, "type": "TASK", "confidence": 0.6231733163197836}, {"text": "syntactic parsing", "start_pos": 121, "end_pos": 138, "type": "TASK", "confidence": 0.7539604306221008}]}, {"text": "By modeling such relations in the output space, structured output prediction algorithms have been shown to outperform significantly simple binary or multi-class classifiers ().", "labels": [], "entities": []}, {"text": "Among the existing structured output prediction algorithms, the linear Structural Support Vector Machine (SSVM) algorithm has shown outstanding performance in several NLP tasks, such as bilingual word alignment (), constituency and dependency parsing (), sentence compression) and document summarization (.", "labels": [], "entities": [{"text": "bilingual word alignment", "start_pos": 186, "end_pos": 210, "type": "TASK", "confidence": 0.60518150528272}, {"text": "constituency and dependency parsing", "start_pos": 215, "end_pos": 250, "type": "TASK", "confidence": 0.6225714534521103}, {"text": "sentence compression", "start_pos": 255, "end_pos": 275, "type": "TASK", "confidence": 0.8119648098945618}, {"text": "document summarization", "start_pos": 281, "end_pos": 303, "type": "TASK", "confidence": 0.7516035735607147}]}, {"text": "Nevertheless, as a learning method for NLP, the SSVM algorithm has been less than popular algorithms such as the structured Perceptron).", "labels": [], "entities": []}, {"text": "This maybe due to the fact that current SSVM implementations often suffer from several practical issues.", "labels": [], "entities": []}, {"text": "First, state-of-the-art implementations of SSVM such as cutting plane methods ( ) are typically complicated.", "labels": [], "entities": []}, {"text": "Second, while methods like stochastic gradient descent are simple to implement, tuning learning rates can be difficult.", "labels": [], "entities": [{"text": "stochastic gradient descent", "start_pos": 27, "end_pos": 54, "type": "TASK", "confidence": 0.6814525127410889}]}, {"text": "Finally, while SSVM models can achieve superior accuracy, this often requires long training time.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9977045655250549}]}, {"text": "In this paper, we propose a novel optimization method for efficiently training linear SSVMs.", "labels": [], "entities": []}, {"text": "Our method not only is easy to implement, but also has excellent training speed, competitive with both structured Perceptron) and MIRA ( ).", "labels": [], "entities": [{"text": "MIRA", "start_pos": 130, "end_pos": 134, "type": "METRIC", "confidence": 0.8751605153083801}]}, {"text": "When evaluated on several NLP tasks, including POS tagging, NER and dependency parsing, this optimization method also outperforms other approaches in terms of prediction accuracy.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 47, "end_pos": 58, "type": "TASK", "confidence": 0.8101970255374908}, {"text": "dependency parsing", "start_pos": 68, "end_pos": 86, "type": "TASK", "confidence": 0.8276258111000061}, {"text": "accuracy", "start_pos": 170, "end_pos": 178, "type": "METRIC", "confidence": 0.9143303632736206}]}, {"text": "Our final algorithm is a dual coordinate descent (DCD) algorithm for solving a structured output SVM problem with a 2-norm hinge loss function.", "labels": [], "entities": [{"text": "dual coordinate descent (DCD)", "start_pos": 25, "end_pos": 54, "type": "TASK", "confidence": 0.736775795618693}]}, {"text": "The algorithm consists of two main components.", "labels": [], "entities": []}, {"text": "One component behaves analogously to online learning methods and updates the weight vector immediately after inference is performed.", "labels": [], "entities": []}, {"text": "The other component is similar to the cutting plane method and updates the dual variables (and the weight vector) without running inference.", "labels": [], "entities": []}, {"text": "Conceptually, this hybrid approach operates at a balanced trade-off point between inference and weight update, performing better than with either component alone.", "labels": [], "entities": []}, {"text": "Our contributions in this work can be summarized as follows.", "labels": [], "entities": []}, {"text": "Firstly, our proposed algorithm shows that even for structured output prediction, an SSVM model can be trained as efficiently as a structured Perceptron one.", "labels": [], "entities": [{"text": "structured output prediction", "start_pos": 52, "end_pos": 80, "type": "TASK", "confidence": 0.7700762748718262}]}, {"text": "Secondly, we conducted a careful experimental study on three NLP tasks using four different benchmark datasets.", "labels": [], "entities": []}, {"text": "When compared with previous methods for training, our method achieves similar performance using less training time.", "labels": [], "entities": []}, {"text": "When compared to commonly used learning algorithms such as Perceptron and MIRA, the model trained by our algorithm performs consistently better when given the same amount of training time.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 74, "end_pos": 78, "type": "DATASET", "confidence": 0.5854731798171997}]}, {"text": "We believe our method can be a powerful tool for many different NLP tasks.", "labels": [], "entities": []}, {"text": "The rest of our paper is organized as follows.", "labels": [], "entities": []}, {"text": "We first describe our approach by formally defining the problem and notation in Sec.", "labels": [], "entities": []}, {"text": "2, where we also review some existing, closely-related structuredoutput learning algorithms and optimization techniques.", "labels": [], "entities": []}, {"text": "We introduce the detailed algorithmic design in Sec.", "labels": [], "entities": []}, {"text": "3. The experimental comparisons of variations of our approach and the existing methods on several NLP benchmark tasks and datasets are reported in Sec.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to verify the effectiveness of the proposed algorithm, we conduct a set of experiments on different optimization and learning algorithms.", "labels": [], "entities": []}, {"text": "Before going to the experimental results, we first introduce the tasks and settings used in the experiments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance of online learning algorithms and  the DCD-SSVM algorithm on the testing sets. NER is  measured by F 1 while others by accuracy.", "labels": [], "entities": [{"text": "NER", "start_pos": 101, "end_pos": 104, "type": "METRIC", "confidence": 0.5551639199256897}, {"text": "F 1", "start_pos": 121, "end_pos": 124, "type": "METRIC", "confidence": 0.9932583868503571}, {"text": "accuracy", "start_pos": 141, "end_pos": 149, "type": "METRIC", "confidence": 0.9989532232284546}]}]}