{"title": [{"text": "Token and Type Constraints for Cross-Lingual Part-of-Speech Tagging", "labels": [], "entities": [{"text": "Token and Type Constraints", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6610157787799835}, {"text": "Cross-Lingual Part-of-Speech Tagging", "start_pos": 31, "end_pos": 67, "type": "TASK", "confidence": 0.664736936489741}]}], "abstractContent": [{"text": "We consider the construction of part-of-speech taggers for resource-poor languages.", "labels": [], "entities": [{"text": "part-of-speech taggers", "start_pos": 32, "end_pos": 54, "type": "TASK", "confidence": 0.7083400636911392}]}, {"text": "Recently, manually constructed tag dictionaries from Wiktionary and dictionaries projected via bitext have been used as type constraints to overcome the scarcity of annotated data in this setting.", "labels": [], "entities": [{"text": "Wiktionary", "start_pos": 53, "end_pos": 63, "type": "DATASET", "confidence": 0.9243296980857849}]}, {"text": "In this paper, we show that additional token constraints can be projected from a resource-rich source language to a resource-poor target language via word-aligned bitext.", "labels": [], "entities": []}, {"text": "We present several models to this end; in particular a partially observed conditional random field model, where coupled token and type constraints provide a partial signal for training.", "labels": [], "entities": []}, {"text": "Averaged across eight previously studied Indo-European languages, our model achieves a 25% relative error reduction over the prior state of the art.", "labels": [], "entities": [{"text": "relative error", "start_pos": 91, "end_pos": 105, "type": "METRIC", "confidence": 0.6800425052642822}]}, {"text": "We further present successful results on seven additional languages from different families, empirically demonstrating the applicability of coupled token and type constraints across a diverse set of languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "Supervised part-of-speech (POS) taggers are available for more than twenty languages and achieve accuracies of around 95% on in-domain data).", "labels": [], "entities": [{"text": "part-of-speech (POS) taggers", "start_pos": 11, "end_pos": 39, "type": "TASK", "confidence": 0.6325831770896911}, {"text": "accuracies", "start_pos": 97, "end_pos": 107, "type": "METRIC", "confidence": 0.9964195489883423}]}, {"text": "Thanks to their efficiency and robustness, supervised taggers are routinely employed in many natural language processing applications, such as syntactic and semantic parsing, named-entity recognition and machine translation.", "labels": [], "entities": [{"text": "supervised taggers", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.7065037786960602}, {"text": "syntactic and semantic parsing", "start_pos": 143, "end_pos": 173, "type": "TASK", "confidence": 0.6519063040614128}, {"text": "named-entity recognition", "start_pos": 175, "end_pos": 199, "type": "TASK", "confidence": 0.7746960818767548}, {"text": "machine translation", "start_pos": 204, "end_pos": 223, "type": "TASK", "confidence": 0.7907763719558716}]}, {"text": "Unfortunately, the resources required to train supervised taggers are expensive to create and unlikely to exist for the majority of written * Work primarily carried out while at Google Research. languages.", "labels": [], "entities": [{"text": "Google Research. languages", "start_pos": 178, "end_pos": 204, "type": "DATASET", "confidence": 0.8113196939229965}]}, {"text": "The necessity of building NLP tools for these resource-poor languages has been part of the motivation for research on unsupervised learning of POS taggers.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 143, "end_pos": 154, "type": "TASK", "confidence": 0.7580299377441406}]}, {"text": "In this paper, we instead take a weakly supervised approach towards this problem.", "labels": [], "entities": []}, {"text": "Recently, learning POS taggers with type-level tag dictionary constraints has gained popularity.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 19, "end_pos": 30, "type": "TASK", "confidence": 0.7720259428024292}]}, {"text": "Tag dictionaries, noisily projected via word-aligned bitext, have bridged the gap between purely unsupervised and fully supervised taggers, resulting in an average accuracy of over 83% on a benchmark of eight Indo-European languages ().", "labels": [], "entities": [{"text": "accuracy", "start_pos": 164, "end_pos": 172, "type": "METRIC", "confidence": 0.9986793398857117}]}, {"text": "further improved upon this result by employing Wiktionary 1 as a tag dictionary source, resulting in the hitherto best published result of almost 85% on the same setup.", "labels": [], "entities": []}, {"text": "Although the aforementioned weakly supervised approaches have resulted in significant improvements over fully unsupervised approaches, they have not exploited the benefits of token-level cross-lingual projection methods, which are possible with wordaligned bitext between a target language of interest and a resource-rich source language, such as English.", "labels": [], "entities": [{"text": "token-level cross-lingual projection", "start_pos": 175, "end_pos": 211, "type": "TASK", "confidence": 0.5896783371766409}]}, {"text": "This is the setting we consider in this paper ( \u00a72).", "labels": [], "entities": []}, {"text": "While prior work has successfully considered both token-and type-level projection across word-aligned bitext for estimating the model parameters of generative tagging models, inter alia), a key observation underlying the present work is that token-and type-level information offer different and complementary signals.", "labels": [], "entities": [{"text": "generative tagging", "start_pos": 148, "end_pos": 166, "type": "TASK", "confidence": 0.8732733428478241}]}, {"text": "On the one hand, high confidence token-level projections offer precise constraints on a tag in a particular context.", "labels": [], "entities": []}, {"text": "On the other hand, manually cre-ated type-level dictionaries can have broad coverage and do not suffer from word-alignment errors; they can therefore be used to filter systematic as well as random noise in token-level projections.", "labels": [], "entities": []}, {"text": "In order to reap these potential benefits, we propose a partially observed conditional random field (CRF) model () that couples token and type constraints in order to guide learning ( \u00a73).", "labels": [], "entities": []}, {"text": "In essence, the model is given the freedom to push probability mass towards hypotheses consistent with both types of information.", "labels": [], "entities": []}, {"text": "This approach is flexible: we can use either noisy projected or manually constructed dictionaries to generate type constraints; furthermore, we can incorporate arbitrary features over the input.", "labels": [], "entities": []}, {"text": "In addition to standard (contextual) lexical features and transition features, we observe that adding features from a monolingual word clustering can significantly improve accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 172, "end_pos": 180, "type": "METRIC", "confidence": 0.9917735457420349}]}, {"text": "While most of these features can also be used in a generative feature-based hidden Markov model (HMM), we achieve the best accuracy with a globally normalized discriminative CRF model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.9991353154182434}]}, {"text": "To evaluate our approach, we present extensive results on standard publicly available datasets for 15 languages: the eight Indo-European languages previously studied in this context by and, and seven additional languages from different families, for which no comparable study exists.", "labels": [], "entities": []}, {"text": "In \u00a74 we compare various features, constraints and model types.", "labels": [], "entities": []}, {"text": "Our best model uses type constraints derived from Wiktionary, together with token constraints derived from high-confidence word alignments.", "labels": [], "entities": []}, {"text": "When averaged across the eight languages studied by and, we achieve an accuracy of 88.8%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9996650218963623}]}, {"text": "This is a 25% relative error reduction over the previous state of the art.", "labels": [], "entities": [{"text": "error", "start_pos": 23, "end_pos": 28, "type": "METRIC", "confidence": 0.5357831120491028}]}, {"text": "Averaged across all 15 languages, our model obtains an accuracy of 84.5% compared to 78.5% obtained by a strong generative baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9995782971382141}]}, {"text": "Finally, we provide an in depth analysis of the relative contributions of the two types of constraints in \u00a75.", "labels": [], "entities": []}], "datasetContent": [{"text": "Before delving into the experimental details, we present our setup and datasets.", "labels": [], "entities": []}, {"text": "We evaluate on eight target languages used in previous work () and on seven additional languages (see).", "labels": [], "entities": []}, {"text": "While the former eight languages all belong to the Indo-European family, we broaden the coverage to language families more distant from the source language (for example, Chinese, Japanese and Turkish).", "labels": [], "entities": []}, {"text": "We use the treebanks from the CoNLL shared tasks on dependency parsing () for evaluation.", "labels": [], "entities": [{"text": "CoNLL shared tasks", "start_pos": 30, "end_pos": 48, "type": "DATASET", "confidence": 0.8496317466100057}, {"text": "dependency parsing", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.7173803746700287}]}, {"text": "The twoletter abbreviations from the ISO 639-1 standard are used when referring to these languages in tables and figures.", "labels": [], "entities": [{"text": "ISO 639-1 standard", "start_pos": 37, "end_pos": 55, "type": "DATASET", "confidence": 0.7827346126238505}]}, {"text": "In all cases, we map the language-specific POS tags to universal POS tags using the mapping of.", "labels": [], "entities": []}, {"text": "Since we use indirect supervision via projected tags or Wiktionary, the model states induced by all models correspond directly to POS tags, enabling us to compute tagging accuracy without a greedy 1-to-1 or many-to-1 mapping.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 171, "end_pos": 179, "type": "METRIC", "confidence": 0.9735480546951294}]}, {"text": "For all experiments, we use English as the source language.", "labels": [], "entities": []}, {"text": "Depending on availability, there are between 1M and 5M parallel sentences for each language.", "labels": [], "entities": []}, {"text": "The majority of the parallel data is gathered automatically from the web using the method of.", "labels": [], "entities": []}, {"text": "We further include data from Europarl () and from the UN parallel corpus), for languages covered by these corpora.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 29, "end_pos": 37, "type": "DATASET", "confidence": 0.9814354181289673}, {"text": "UN parallel corpus", "start_pos": 54, "end_pos": 72, "type": "DATASET", "confidence": 0.8365076581637064}]}, {"text": "The English side of the bitext is POS tagged with a standard supervised CRF tagger, trained on the Penn Treebank (, with tags mapped to universal tags.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 99, "end_pos": 112, "type": "DATASET", "confidence": 0.9959372282028198}]}, {"text": "The parallel sen-tences are word aligned with the aligner of DeNero and Macherey (2011).", "labels": [], "entities": []}, {"text": "Intersected high-confidence alignments (confidence > 0.95) are extracted and aggregated into projected type-level dictionaries.", "labels": [], "entities": []}, {"text": "For purely practical reasons, the training data with tokenlevel projections is created by randomly sampling target-side sentences with a total of 500K tokens.", "labels": [], "entities": []}, {"text": "We use a snapshot of the Wiktionary word definitions, and follow the heuristics of for creating the Wiktionary dictionary by mapping the Wiktionary tags to universal POS tags.", "labels": [], "entities": []}, {"text": "For all models, we use only an identity feature for tag-pair transitions.", "labels": [], "entities": []}, {"text": "We use five features that couple the current tag and the observed word (analogous to the emission in an HMM): word identity, suffixes of up to length 3, and three indicator features that fire when the word starts with a capital letter, contains a hyphen or contains a digit.", "labels": [], "entities": []}, {"text": "These are the same features as those used by.", "labels": [], "entities": []}, {"text": "Finally, for some models we add a word cluster feature that couples the current tag and the word cluster identity of the word.", "labels": [], "entities": []}, {"text": "These (monolingual) word clusters are induced with the exchange algorithm.", "labels": [], "entities": []}, {"text": "We set the number of clusters to 256 across all languages, as this has previously been shown to produce robust results for similar tasks).", "labels": [], "entities": []}, {"text": "The clusters for each language are learned on a large monolingual newswire corpus.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Tagging accuracies for type-constrained HMM  models. D&P is the \"With LP\" model in", "labels": [], "entities": [{"text": "Tagging", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9716290235519409}]}, {"text": " Table 2: Tagging accuracies for models with token constraints and coupled token and type constraints. All models use  cluster features (. . . +C) and are trained on large training sets each containing 500k tokens with (partial) token-level  projections (. . . +L). The best type-constrained model, trained on the larger datasets, Y HMM  union +C+L, is included for  comparison. The remaining columns correspond to HMM and CRF models trained only with token constraints (\u02dc y . . .)  and with coupled token and type constraints (  Y . . .). The latter are trained using the projected dictionary (\u00b7 proj. ),  Wiktionary (\u00b7 wik. ) and the union of these dictionaries (\u00b7 union ), respectively. The search spaces of the models trained with  coupled constraints (  Y . . .) are each pruned with the respective tag dictionary used to derive the coupled constraints.  The observed difference between  Y CRF  wik. +C+L and Y HMM  union +C+L is statistically significant at p < 0.01 (**) and p < 0.015  (*) according to a paired bootstrap test", "labels": [], "entities": []}]}