{"title": [{"text": "Exploring Correlation of Dependency Relation Paths for Answer Extraction", "labels": [], "entities": [{"text": "Exploring Correlation of Dependency Relation", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.7404973864555359}, {"text": "Answer Extraction", "start_pos": 55, "end_pos": 72, "type": "TASK", "confidence": 0.9539926648139954}]}], "abstractContent": [{"text": "In this paper, we explore correlation of dependency relation paths to rank candidate answers in answer extraction.", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 96, "end_pos": 113, "type": "TASK", "confidence": 0.8374302089214325}]}, {"text": "Using the correlation measure, we compare dependency relations of a candidate answer and mapped question phrases in sentence with the corresponding relations in question.", "labels": [], "entities": [{"text": "correlation", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9432178735733032}]}, {"text": "Different from previous studies, we propose an approximate phrase mapping algorithm and incorporate the mapping score into the correlation measure.", "labels": [], "entities": [{"text": "phrase mapping", "start_pos": 59, "end_pos": 73, "type": "TASK", "confidence": 0.7001833766698837}]}, {"text": "The correlations are further incorporated into a Maximum Entropy-based ranking model which estimates path weights from training.", "labels": [], "entities": []}, {"text": "Experimental results show that our method significantly outperforms state-of-the-art syntactic relation-based methods by up to 20% in MRR.", "labels": [], "entities": [{"text": "MRR", "start_pos": 134, "end_pos": 137, "type": "TASK", "confidence": 0.7172450423240662}]}], "introductionContent": [{"text": "Answer Extraction is one of basic modules in open domain Question Answering (QA).", "labels": [], "entities": [{"text": "Answer Extraction", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9005527198314667}, {"text": "open domain Question Answering (QA)", "start_pos": 45, "end_pos": 80, "type": "TASK", "confidence": 0.7361884372574943}]}, {"text": "It is to further process relevant sentences extracted with Passage / Sentence Retrieval and pinpoint exact answers using more linguistic-motivated analysis.", "labels": [], "entities": [{"text": "Passage / Sentence Retrieval", "start_pos": 59, "end_pos": 87, "type": "TASK", "confidence": 0.7201940417289734}]}, {"text": "Since QA turns to find exact answers rather than text snippets in recent years, answer extraction becomes more and more crucial.", "labels": [], "entities": [{"text": "QA", "start_pos": 6, "end_pos": 8, "type": "TASK", "confidence": 0.9189661741256714}, {"text": "answer extraction", "start_pos": 80, "end_pos": 97, "type": "TASK", "confidence": 0.9371365904808044}]}, {"text": "Typically, answer extraction works in the following steps: \u2022 Recognize expected answer type of a question.", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.8944128155708313}]}, {"text": "\u2022 Annotate relevant sentences with various types of named entities.", "labels": [], "entities": []}, {"text": "\u2022 Regard the phrases annotated with the expected answer type as candidate answers.", "labels": [], "entities": []}, {"text": "In the above workflow, answer extraction heavily relies on named entity recognition (NER).", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.9454942643642426}, {"text": "named entity recognition (NER)", "start_pos": 59, "end_pos": 89, "type": "TASK", "confidence": 0.7532306611537933}]}, {"text": "On one hand, NER reduces the number of candidate answers and eases answer ranking.", "labels": [], "entities": [{"text": "NER", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.6597558259963989}, {"text": "answer ranking", "start_pos": 67, "end_pos": 81, "type": "TASK", "confidence": 0.8397261202335358}]}, {"text": "On the other hand, the errors from NER directly degrade answer extraction performance.", "labels": [], "entities": [{"text": "NER", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.6538920402526855}, {"text": "answer extraction", "start_pos": 56, "end_pos": 73, "type": "TASK", "confidence": 0.8874132037162781}]}, {"text": "To our knowledge, most top ranked QA systems in TREC are supported by effective NER modules which may identify and classify more than 20 types of named entities (NE), such as abbreviation, music, movie, etc.", "labels": [], "entities": []}, {"text": "However, developing such named entity recognizer is not trivial.", "labels": [], "entities": []}, {"text": "Up to now, we haven't found any paper relevant to QA-specific NER development.", "labels": [], "entities": [{"text": "QA-specific NER development", "start_pos": 50, "end_pos": 77, "type": "TASK", "confidence": 0.7899008393287659}]}, {"text": "So, it is hard to follow their work.", "labels": [], "entities": []}, {"text": "In this paper, we just use a general MUC-based NER, which makes our results reproducible.", "labels": [], "entities": [{"text": "MUC-based NER", "start_pos": 37, "end_pos": 50, "type": "DATASET", "confidence": 0.5941967368125916}]}, {"text": "A general MUC-based NER can't annotate a large number of NE classes.", "labels": [], "entities": [{"text": "MUC-based NER ca", "start_pos": 10, "end_pos": 26, "type": "DATASET", "confidence": 0.8332978288332621}]}, {"text": "In this case, all noun phrases in sentences are regarded as candidate answers, which makes candidate answer sets much larger than those filtered by a well developed NER.", "labels": [], "entities": []}, {"text": "The larger candidate answer sets result in the more difficult answer extraction.", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 62, "end_pos": 79, "type": "TASK", "confidence": 0.7440009713172913}]}, {"text": "Previous methods working on surface word level, such as density-based ranking and pattern matching, may not perform well.", "labels": [], "entities": [{"text": "pattern matching", "start_pos": 82, "end_pos": 98, "type": "TASK", "confidence": 0.7740465402603149}]}, {"text": "Deeper linguistic analysis has to be conducted.", "labels": [], "entities": [{"text": "linguistic analysis", "start_pos": 7, "end_pos": 26, "type": "TASK", "confidence": 0.7151519656181335}]}, {"text": "This paper proposes a statistical method which exploring correlation of dependency relation paths to rank candidate answers.", "labels": [], "entities": []}, {"text": "It is motivated by the observation that relations between proper answers and question phrases in candidate sentences are always similar to the corresponding relations in question.", "labels": [], "entities": []}, {"text": "For example, the question \"What did Alfred Nobel invent?\" and the candidate sentence \"...", "labels": [], "entities": []}, {"text": "in the will of Swedish industrialist Alfred Nobel, who invented dynamite.\"", "labels": [], "entities": []}, {"text": "For each question, firstly, dependency relation paths are defined and extracted from the question and each of its candidate sentences.", "labels": [], "entities": []}, {"text": "Secondly, the paths from the question and the candidate sentence are paired according to question phrase mapping score.", "labels": [], "entities": []}, {"text": "Thirdly, correlation between two paths of each pair is calculated by employing Dynamic Time Warping algorithm.", "labels": [], "entities": [{"text": "correlation", "start_pos": 9, "end_pos": 20, "type": "METRIC", "confidence": 0.9857467412948608}, {"text": "Dynamic Time Warping", "start_pos": 79, "end_pos": 99, "type": "TASK", "confidence": 0.47856395443280536}]}, {"text": "The input of the calculation is correlations between dependency relations, which are estimated from a set of training path pairs.", "labels": [], "entities": []}, {"text": "Lastly, a Maximum Entropy-based ranking model is proposed to incorporate the path correlations and rank candidate answers.", "labels": [], "entities": []}, {"text": "Furthermore, sentence supportive measure are presented according to correlations of relation paths among question phrases.", "labels": [], "entities": []}, {"text": "It is applied to re-rank the candidate answers extracted from the different candidate sentences.", "labels": [], "entities": []}, {"text": "Considering phrases may provide more accurate information than individual words, we extract dependency relations on phrase level instead of word level.", "labels": [], "entities": []}, {"text": "The experiment on TREC questions shows that our method significantly outperforms a densitybased method by 50% in MRR and three stateof-the-art syntactic-based methods by up to 20% in MRR.", "labels": [], "entities": [{"text": "TREC", "start_pos": 18, "end_pos": 22, "type": "TASK", "confidence": 0.7743343114852905}]}, {"text": "Furthermore, we classify questions by judging whether NER is used.", "labels": [], "entities": [{"text": "NER", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.6945352554321289}]}, {"text": "We investigate how these methods perform on the two question sets.", "labels": [], "entities": []}, {"text": "The results indicate that our method achieves better performance than the other syntactic-based methods on both question sets.", "labels": [], "entities": []}, {"text": "Especially for more difficult questions, for which NER may not help, our method improves MRR by up to 31%.", "labels": [], "entities": [{"text": "MRR", "start_pos": 89, "end_pos": 92, "type": "METRIC", "confidence": 0.7705866098403931}]}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 discusses related work and clarifies what is new in this paper.", "labels": [], "entities": []}, {"text": "Section 3 presents relation path correlation in detail.", "labels": [], "entities": [{"text": "relation path correlation", "start_pos": 19, "end_pos": 44, "type": "TASK", "confidence": 0.8767144680023193}]}, {"text": "Section 4 and 5 discuss how to incorporate the correlations for answer ranking and re-ranking.", "labels": [], "entities": [{"text": "answer ranking", "start_pos": 64, "end_pos": 78, "type": "TASK", "confidence": 0.9134307503700256}, {"text": "re-ranking", "start_pos": 83, "end_pos": 93, "type": "METRIC", "confidence": 0.8926036357879639}]}, {"text": "Section 6 reports experiment and results.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we setup experiments on TREC factoid questions and report evaluation results.", "labels": [], "entities": [{"text": "TREC factoid questions", "start_pos": 41, "end_pos": 63, "type": "DATASET", "confidence": 0.5898849467436472}]}, {"text": "The goal of answer extraction is to identify exact answers from given candidate sentence collections for questions.", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.8230374753475189}]}, {"text": "The candidate sentences are regarded as the most relevant sentences to the questions and retrieved by IR techniques.", "labels": [], "entities": []}, {"text": "Qualities of the candidate sentences have a strong impact on answer extraction.", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 61, "end_pos": 78, "type": "TASK", "confidence": 0.9290100336074829}]}, {"text": "It is meaningless to evaluate the questions of which none candidate sentences contain proper answer in answer extraction experiment.", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 103, "end_pos": 120, "type": "TASK", "confidence": 0.7319906055927277}]}, {"text": "To our knowledge, most of current QA systems lose about half of questions in sentence retrieval stage.", "labels": [], "entities": [{"text": "QA", "start_pos": 34, "end_pos": 36, "type": "TASK", "confidence": 0.9474682211875916}, {"text": "sentence retrieval", "start_pos": 77, "end_pos": 95, "type": "TASK", "confidence": 0.7242049723863602}]}, {"text": "To make more questions evaluated in our experiments, for each of questions, we automatically build a candidate sentence set from TREC judgements rather than use sentence retrieval output.", "labels": [], "entities": []}, {"text": "We use TREC99-03 questions for training and TREC04 questions for testing.", "labels": [], "entities": [{"text": "TREC99-03", "start_pos": 7, "end_pos": 16, "type": "METRIC", "confidence": 0.8232058882713318}, {"text": "TREC04", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.8476355075836182}]}, {"text": "As to build training data, we retrieve all of the sentences which contain proper answers from relevant documents according to TREC judgements and answer patterns.", "labels": [], "entities": []}, {"text": "Then, We manually check the sentences and remove those in which answers cannot be supported.", "labels": [], "entities": []}, {"text": "As to build candidate sentence sets for testing, we retrieve all of the sentences from relevant documents in judgements and keep those which contain at least one question keyword.", "labels": [], "entities": []}, {"text": "Therefore, each question has at least one proper candidate sentence which contains proper answer in its candidate sentence set.", "labels": [], "entities": []}, {"text": "There are 230 factoid questions (27 NIL questions) in TREC04.", "labels": [], "entities": [{"text": "TREC04", "start_pos": 54, "end_pos": 60, "type": "DATASET", "confidence": 0.7390052080154419}]}, {"text": "NIL questions are excluded from our test set because TREC doesn't supply relevant documents and answer patterns for them.", "labels": [], "entities": []}, {"text": "Therefore, we will evaluate 203 TREC04 questions.", "labels": [], "entities": [{"text": "TREC04", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.7437126040458679}]}, {"text": "Five answer extraction methods are evaluated for comparison: \u2022 Density: Density-based method is used as baseline, in which we choose candidate answer with the shortest surface distance to question phrases.", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 5, "end_pos": 22, "type": "TASK", "confidence": 0.708976149559021}]}, {"text": "\u2022 SynPattern: Syntactic relation patterns) are automatically extracted from training set and are partially matched using tree kernel.", "labels": [], "entities": []}, {"text": "\u2022 StrictMatch: Strict relation matching follows the assumption in ().", "labels": [], "entities": [{"text": "Strict relation matching", "start_pos": 15, "end_pos": 39, "type": "TASK", "confidence": 0.6801720758279165}]}, {"text": "We implement it by adapting relation correlation score.", "labels": [], "entities": [{"text": "relation correlation score", "start_pos": 28, "end_pos": 54, "type": "METRIC", "confidence": 0.6569962004820505}]}, {"text": "In stead of learning relation correlations during training, we predefine them as: Cor(r 1 , r 2 ) = 1 if r 1 = r 2 ; 0, otherwise.", "labels": [], "entities": [{"text": "Cor", "start_pos": 82, "end_pos": 85, "type": "METRIC", "confidence": 0.9799038767814636}]}, {"text": "\u2022 ApprMatch: Approximate relation matching () aligns two relation paths using fuzzy matching and ranks candidates according to the sum of all path similarities.", "labels": [], "entities": [{"text": "ApprMatch", "start_pos": 2, "end_pos": 11, "type": "METRIC", "confidence": 0.9963037967681885}]}, {"text": "\u2022 CorME: It is the method proposed in this paper.", "labels": [], "entities": []}, {"text": "Different from ApprMatch, ME-based ranking model is implemented to incorporate path correlations which assigns different weights for different paths respectively.", "labels": [], "entities": []}, {"text": "Furthermore, phrase mapping score is incorporated into the path correlation measure.", "labels": [], "entities": [{"text": "phrase mapping", "start_pos": 13, "end_pos": 27, "type": "TASK", "confidence": 0.792864054441452}]}, {"text": "These methods are briefly described in Section 2.", "labels": [], "entities": []}, {"text": "Performance is evaluated with Mean Reciprocal Rank (MRR).", "labels": [], "entities": [{"text": "Mean Reciprocal Rank (MRR)", "start_pos": 30, "end_pos": 56, "type": "METRIC", "confidence": 0.9689253270626068}]}, {"text": "Furthermore, we list percentages of questions correctly answered in terms of top 5 answers and top 1 answer returned respectively.", "labels": [], "entities": []}, {"text": "No answer validations are used to adjust answers.", "labels": [], "entities": []}, {"text": "shows the overall performance of the five methods.", "labels": [], "entities": []}, {"text": "The main observations from the table are as follows: 1.", "labels": [], "entities": []}, {"text": "The methods SynPattern, StrictMatch, ApprMatch and CorME significantly improve MRR by 25.0%, 26.8%, 34.5% and 50.1% over the baseline method Density.", "labels": [], "entities": [{"text": "ApprMatch", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9945736527442932}, {"text": "CorME", "start_pos": 51, "end_pos": 56, "type": "METRIC", "confidence": 0.9502076506614685}, {"text": "MRR", "start_pos": 79, "end_pos": 82, "type": "TASK", "confidence": 0.6251751184463501}]}, {"text": "The improvements may benefit from the various explorations of syntactic relations.", "labels": [], "entities": []}, {"text": "2. The performance of SynPattern (0.56MRR) and StrictMatch (0.57MRR) are close.", "labels": [], "entities": []}, {"text": "SynPattern matches relation sequences of candidate answers with the predefined relation sequences extracted from a training data set, while StrictMatch matches relation sequences of candidate answers with the corresponding relation sequences in questions.", "labels": [], "entities": []}, {"text": "But, both of them are based on the assumption that the more number of same relations between two sequences, the more similar the sequences are.", "labels": [], "entities": []}, {"text": "Furthermore, since most TREC04 questions only have one or two phrases and many questions have similar expressions, SynPattern and StrictMatch don't make essential difference.", "labels": [], "entities": []}, {"text": "3. ApprMatch and CorME outperform SynPattern and StrictMatch by about 6.1% and 18.4% improvement in MRR.", "labels": [], "entities": [{"text": "ApprMatch", "start_pos": 3, "end_pos": 12, "type": "METRIC", "confidence": 0.9986838698387146}, {"text": "CorME", "start_pos": 17, "end_pos": 22, "type": "METRIC", "confidence": 0.8476601839065552}, {"text": "MRR", "start_pos": 100, "end_pos": 103, "type": "TASK", "confidence": 0.5273037552833557}]}, {"text": "Strict matching often fails due to various relation representations in syntactic trees.", "labels": [], "entities": [{"text": "Strict matching", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.816473513841629}]}, {"text": "However, such variations of syntactic relations maybe captured by ApprMatch and CorME using a MI-based statistical method.", "labels": [], "entities": []}, {"text": "4. CorME achieves the better performance by 11.6% than ApprMatch.", "labels": [], "entities": [{"text": "ApprMatch", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.5302747488021851}]}, {"text": "The improvement may benefit from two aspects: 1) ApprMatch assigns equal weights to the paths of a candidate answer and question phrases, while CorME estimate the weights according to phrase type and path length.", "labels": [], "entities": [{"text": "ApprMatch", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9426451921463013}]}, {"text": "After training a ME model, the weights are assigned, such as 5.72 for topic path ; 3.44 for constraints path and 1.76 for target path.", "labels": [], "entities": []}, {"text": "2) CorME incorporates approximate phrase mapping scores into path correlation measure.", "labels": [], "entities": [{"text": "phrase mapping", "start_pos": 34, "end_pos": 48, "type": "TASK", "confidence": 0.7011866569519043}]}, {"text": "We further divide the questions into two classes according to whether NER is used in answer extraction.", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 85, "end_pos": 102, "type": "TASK", "confidence": 0.8034473955631256}]}, {"text": "If the expected answer type of a question is unknown, such as \"How did James Dean die?\" or the type cannot be annotated by NER, such as \"What ethnic group/race are Crip members?\", we put the question in Qw/oNE set, otherwise, we put it in QwNE.", "labels": [], "entities": [{"text": "James Dean die?\"", "start_pos": 71, "end_pos": 87, "type": "DATASET", "confidence": 0.7311470732092857}, {"text": "NER", "start_pos": 123, "end_pos": 126, "type": "DATASET", "confidence": 0.8469031453132629}]}, {"text": "For the questions in Qw/oNE, we extract all basic noun phrases and verb phrases as candidate answers.", "labels": [], "entities": []}, {"text": "Then, answer extraction module has to work on the larger candidate sets.", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 6, "end_pos": 23, "type": "TASK", "confidence": 0.9014090299606323}]}, {"text": "Using a MUC-based NER, the recognized types include person, location, organization, date, time and money.", "labels": [], "entities": [{"text": "MUC-based NER", "start_pos": 8, "end_pos": 21, "type": "DATASET", "confidence": 0.663025975227356}]}, {"text": "In TREC04 questions, 123 questions are put in QwNE and 80 questions in Qw/oNE.", "labels": [], "entities": [{"text": "TREC04 questions", "start_pos": 3, "end_pos": 19, "type": "DATASET", "confidence": 0.7057022154331207}]}, {"text": "We evaluate the performance on QwNE and Qw/oNE respectively, as shown in.", "labels": [], "entities": []}, {"text": "The density-based method Density (0.11MRR) loses many questions in Qw/oNE, which indicates that using only surface word information is not sufficient for large candidate answer sets.", "labels": [], "entities": []}, {"text": "On the contrary, SynPattern(0.36MRR), StrictPattern(0.36MRR), ApprMatch(0.42MRR) and CorME (0.47MRR) which capture syntactic information, perform much better than Density.", "labels": [], "entities": [{"text": "ApprMatch", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.9698552489280701}]}, {"text": "Our method CorME outperforms the other syntacticbased methods on both QwNE and Qw/oNE.", "labels": [], "entities": []}, {"text": "Es-pecially for more difficult questions Qw/oNE, the improvements (up to 31% in MRR) are more obvious.", "labels": [], "entities": [{"text": "Es-pecially", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9405385851860046}, {"text": "MRR", "start_pos": 80, "end_pos": 83, "type": "METRIC", "confidence": 0.9133583307266235}]}, {"text": "It indicates that our method can be used to further enhance state-of-the-art QA systems even if they have a good NER.", "labels": [], "entities": []}, {"text": "In addition, we evaluate component contributions of our method based on the main idea of relation path correlation.", "labels": [], "entities": [{"text": "relation path correlation", "start_pos": 89, "end_pos": 114, "type": "TASK", "confidence": 0.6951686143875122}]}, {"text": "Three components are tested: 1.", "labels": [], "entities": []}, {"text": "We replace approximate question phrase mapping with exact phrase mapping and withdraw the phrase mapping scores from path correlation measure.", "labels": [], "entities": [{"text": "question phrase mapping", "start_pos": 23, "end_pos": 46, "type": "TASK", "confidence": 0.6339320242404938}]}, {"text": "2. Answer Ranking (Section 4).", "labels": [], "entities": [{"text": "Answer Ranking", "start_pos": 3, "end_pos": 17, "type": "METRIC", "confidence": 0.8404411673545837}]}, {"text": "Instead of using ME model, we sum all of the path correlations to rank candidate answers, which is similar to ().", "labels": [], "entities": [{"text": "ME", "start_pos": 17, "end_pos": 19, "type": "METRIC", "confidence": 0.9222944974899292}]}, {"text": "3. Answer Re-ranking (Section 5).", "labels": [], "entities": [{"text": "Answer Re-ranking", "start_pos": 3, "end_pos": 20, "type": "TASK", "confidence": 0.7986882328987122}]}, {"text": "We disable this component and select top 5 answers according to answer ranking scores.", "labels": [], "entities": []}, {"text": "The contribution of each component is evaluated with the overall performance degradation after it is removed or replaced.", "labels": [], "entities": []}, {"text": "Some findings are concluded from.", "labels": [], "entities": []}, {"text": "Performances degrade when replacing approximate phrase mapping or ME-based answer ranking, which indicates that both of them have positive effects on the systems.", "labels": [], "entities": [{"text": "phrase mapping", "start_pos": 48, "end_pos": 62, "type": "TASK", "confidence": 0.6660042256116867}, {"text": "ME-based answer ranking", "start_pos": 66, "end_pos": 89, "type": "METRIC", "confidence": 0.7646286090215048}]}, {"text": "This maybe also used to explain why CorME outperforms ApprMatch in.", "labels": [], "entities": []}, {"text": "However, removing answer re-ranking doesn't affect much.", "labels": [], "entities": []}, {"text": "Since short questions, such as \"What does AARP stand for?\", frequently occur in TREC04, exploring the phrase relations for such questions isn't helpful.", "labels": [], "entities": [{"text": "TREC04", "start_pos": 80, "end_pos": 86, "type": "DATASET", "confidence": 0.7672443985939026}]}], "tableCaptions": [{"text": " Table 1: Overall performance  Density  SynPattern  StrictMatch  ApprMatch  CorME  MRR  0.45  0.56  0.57  0.60  0.67  Top1  0.36  0.53  0.49  0.53  0.62  Top5  0.56  0.60  0.67  0.70  0.74", "labels": [], "entities": [{"text": "ApprMatch  CorME  MRR  0.45  0.56  0.57  0.60", "start_pos": 65, "end_pos": 110, "type": "METRIC", "confidence": 0.8583899225507464}]}, {"text": " Table 2: Performance on two question sets QwNE  and Qw/oNE  QwNE  Qw/oNE  Density  0.66  0.11  SynPattern  0.71  0.36  StrictMatch  0.70  0.36  ApprMatch  0.72  0.42  CorME  0.79  0.47", "labels": [], "entities": [{"text": "ApprMatch", "start_pos": 145, "end_pos": 154, "type": "METRIC", "confidence": 0.9257205128669739}]}, {"text": " Table 2.  The density-based method Density (0.11MRR)  loses many questions in Qw/oNE, which indi- cates that using only surface word information  is not sufficient for large candidate answer sets.  On the contrary, SynPattern(0.36MRR), Strict- Pattern(0.36MRR), ApprMatch(0.42MRR) and  CorME (0.47MRR) which capture syntactic infor- mation, perform much better than Density. Our  method CorME outperforms the other syntactic- based methods on both QwNE and Qw/oNE. Es-", "labels": [], "entities": [{"text": "ApprMatch", "start_pos": 263, "end_pos": 272, "type": "METRIC", "confidence": 0.9792121648788452}]}, {"text": " Table 3: Component Contributions  MRR  Overall  0.67  -Appr. Mapping  0.63  -Answer Ranking  0.62  -Answer Re-ranking  0.66", "labels": [], "entities": [{"text": "MRR  Overall  0.67  -Appr. Mapping  0.63  -Answer Ranking  0.62  -Answer Re-ranking  0.66", "start_pos": 35, "end_pos": 124, "type": "METRIC", "confidence": 0.7301665637642145}]}]}