{"title": [{"text": "Low-cost Enrichment of Spanish WordNet with Automatically Translated Glosses: Combining General and Specialized Models", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper studies the enrichment of Span-ish WordNet with synset glosses automatically obtained from the English Word-Net glosses using a phrase-based Statistical Machine Translation system.", "labels": [], "entities": [{"text": "phrase-based Statistical Machine Translation", "start_pos": 139, "end_pos": 183, "type": "TASK", "confidence": 0.5930877402424812}]}, {"text": "We construct the English-Spanish translation system from a parallel corpus of proceedings of the European Parliament, and study how to adapt statistical models to the domain of dictionary definitions.", "labels": [], "entities": []}, {"text": "We build specialized language and translation models from a small set of parallel definitions and experiment with robust manners to combine them.", "labels": [], "entities": []}, {"text": "A statistically significant increase in performance is obtained.", "labels": [], "entities": []}, {"text": "The best system is finally used to generate a definition for all Spanish synsets, which are currently ready fora manual revision.", "labels": [], "entities": []}, {"text": "As a complementary issue, we analyze the impact of the amount of in-domain data needed to improve a system trained entirely on out-of-domain data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Statistical Machine Translation (SMT) is today a very promising approach.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8726891080538431}]}, {"text": "It allows to build very quickly and fully automatically Machine Translation (MT) systems, exhibiting very competitive results, only from a parallel corpus aligning sentences from the two languages involved.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 56, "end_pos": 80, "type": "TASK", "confidence": 0.8637513399124146}]}, {"text": "In this work we approach the task of enriching Spanish WordNet with automatically translated glosses . The source glosses for these translations are taken from the English WordNet (Fellbaum, Glosses are short dictionary definitions that accompany WordNet synsets.", "labels": [], "entities": []}, {"text": "See examples in 1998), which is linked, at the synset level, to Spanish WordNet.", "labels": [], "entities": [{"text": "Spanish WordNet", "start_pos": 64, "end_pos": 79, "type": "DATASET", "confidence": 0.8021539747714996}]}, {"text": "This resource is available, among other sources, through the Multilingual Central Repository (MCR) developed by the MEANING project (.", "labels": [], "entities": []}, {"text": "We start by empirically testing the performance of a previously developed English-Spanish SMT system, built from the large Europarl corpus 2.", "labels": [], "entities": [{"text": "SMT", "start_pos": 90, "end_pos": 93, "type": "TASK", "confidence": 0.9220036864280701}, {"text": "Europarl corpus 2", "start_pos": 123, "end_pos": 140, "type": "DATASET", "confidence": 0.9856343666712443}]}, {"text": "The first observation is that this system completely fails to translate the specific WordNet glosses, due to the large language variations in both domains (vocabulary, style, grammar, etc.).", "labels": [], "entities": [{"text": "WordNet glosses", "start_pos": 85, "end_pos": 100, "type": "DATASET", "confidence": 0.9150874018669128}]}, {"text": "Actually, this is confirming one of the main criticisms against SMT, which is its strong domain dependence.", "labels": [], "entities": [{"text": "SMT", "start_pos": 64, "end_pos": 67, "type": "TASK", "confidence": 0.988714873790741}]}, {"text": "Since parameters are estimated from a corpus in a concrete domain, the performance of the system on a different domain is often much worse.", "labels": [], "entities": []}, {"text": "This flaw of statistical and machine learning approaches is well known and has been largely described in the NLP literature, fora variety of tasks (e.g., parsing, word sense disambiguation, and semantic role labeling).", "labels": [], "entities": [{"text": "parsing", "start_pos": 154, "end_pos": 161, "type": "TASK", "confidence": 0.9683775305747986}, {"text": "word sense disambiguation", "start_pos": 163, "end_pos": 188, "type": "TASK", "confidence": 0.6075234909852346}, {"text": "semantic role labeling", "start_pos": 194, "end_pos": 216, "type": "TASK", "confidence": 0.6309655706087748}]}, {"text": "Fortunately, we count on a small set of Spanish hand-developed glosses in MCR . Thus, we move to a working scenario in which we introduce a small corpus of aligned translations from the concrete domain of WordNet glosses.", "labels": [], "entities": [{"text": "MCR", "start_pos": 74, "end_pos": 77, "type": "DATASET", "confidence": 0.8722695112228394}, {"text": "WordNet glosses", "start_pos": 205, "end_pos": 220, "type": "DATASET", "confidence": 0.9230826795101166}]}, {"text": "This in-domain corpus could be itself used as a source for constructing a specialized SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 86, "end_pos": 89, "type": "TASK", "confidence": 0.9919620156288147}]}, {"text": "Again, experiments show that this small corpus alone does not suffice, since it does not allow to estimate good translation parameters.", "labels": [], "entities": []}, {"text": "However, it is well suited for combination with the Europarl corpus, to generate combined Language and Translation Models.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 52, "end_pos": 67, "type": "DATASET", "confidence": 0.9927099049091339}]}, {"text": "A substantial increase in performance is achieved, according to several standard MT evaluation metrics.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 81, "end_pos": 94, "type": "TASK", "confidence": 0.8504035770893097}]}, {"text": "Although moderate, this boost in performance is statistically significant according to the bootstrap resampling test described by and applied to the BLEU metric.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 149, "end_pos": 153, "type": "METRIC", "confidence": 0.9745014905929565}]}, {"text": "The main reason behind this improvement is that the large out-of-domain corpus contributes mainly with coverage and recall and the in-domain corpus provides more precise translations.", "labels": [], "entities": [{"text": "coverage", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.8877497315406799}, {"text": "recall", "start_pos": 116, "end_pos": 122, "type": "METRIC", "confidence": 0.9976407289505005}]}, {"text": "We present a qualitative error analysis to support these claims.", "labels": [], "entities": []}, {"text": "Finally, we also address the important question of how much in-domain data is needed to be able to improve the baseline results.", "labels": [], "entities": []}, {"text": "Apart from the experimental findings, our study has generated a very valuable resource.", "labels": [], "entities": []}, {"text": "Currently, we have the complete Spanish WordNet enriched with one gloss per synset, which, far from being perfect, constitutes an axcellent starting point fora posterior manual revision.", "labels": [], "entities": []}, {"text": "Finally, we note that the construction of a SMT system when few domain-specific data are available has been also investigated by other authors.", "labels": [], "entities": [{"text": "SMT", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.9960002303123474}]}, {"text": "For instance, studied whether an SMT system for speech-to-speech translation built on top of a small parallel corpus can be improved by adding knowledge sources which are not domain specific.", "labels": [], "entities": [{"text": "SMT", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.9918518662452698}, {"text": "speech-to-speech translation", "start_pos": 48, "end_pos": 76, "type": "TASK", "confidence": 0.7307974696159363}]}, {"text": "In this work, we look at the same problem the other way around.", "labels": [], "entities": []}, {"text": "We study how to adapt an out-of-domain SMT system using in-domain data.", "labels": [], "entities": [{"text": "SMT", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.9780057668685913}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2 the fundamentals of SMT and the components of our MT architecture are described.", "labels": [], "entities": [{"text": "SMT", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.9928727149963379}, {"text": "MT", "start_pos": 63, "end_pos": 65, "type": "TASK", "confidence": 0.959011435508728}]}, {"text": "The experimental setting is described in Section 3.", "labels": [], "entities": []}, {"text": "Evaluation is carried out in Section 4.", "labels": [], "entities": []}, {"text": "Finally, Section 5 contains error analysis and Section 6 concludes and outlines future work.", "labels": [], "entities": [{"text": "error analysis", "start_pos": 28, "end_pos": 42, "type": "METRIC", "confidence": 0.903811514377594}]}], "datasetContent": [{"text": "As a general source of English-Spanish parallel text, we used a collection of 730,740 parallel sentences extracted from the Europarl corpus.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 124, "end_pos": 139, "type": "DATASET", "confidence": 0.9896252751350403}]}, {"text": "These correspond exactly to the training data from the Shared Task 2: Exploiting Parallel Texts for Statistical Machine Translation from the ACL-2005 Workshop on Building and Using Parallel Texts: Data-Driven Machine Translation and Beyond 5 . To be used as specialized source, we extracted, from the MCR , the set of 6,519 English-Spanish parallel glosses corresponding to the already defined synsets in Spanish WordNet.", "labels": [], "entities": [{"text": "Statistical Machine Translation from the ACL-2005 Workshop on Building and Using Parallel Texts: Data-Driven Machine Translation", "start_pos": 100, "end_pos": 228, "type": "TASK", "confidence": 0.7533283268704134}]}, {"text": "These definitions corresponded to 5,698 nouns, 87 verbs, and 734 adjectives.", "labels": [], "entities": []}, {"text": "Examples and parenthesized texts were removed.", "labels": [], "entities": []}, {"text": "Parallel glosses were tokenized and case lowered.", "labels": [], "entities": [{"text": "Parallel glosses", "start_pos": 0, "end_pos": 16, "type": "METRIC", "confidence": 0.8928464651107788}]}, {"text": "We discarded some of these parallel glosses based on the difference in length between the source and the target.", "labels": [], "entities": []}, {"text": "The gloss average length for the resulting 5,843 glosses was 8.25 words for English and 8.13 for Spanish.", "labels": [], "entities": [{"text": "gloss average length", "start_pos": 4, "end_pos": 24, "type": "METRIC", "confidence": 0.8188701669375101}]}, {"text": "Finally, gloss pairs were randomly split into training (4,843), development (500) and test (500) sets.", "labels": [], "entities": []}, {"text": "Additionally, we counted on two large monolingual Spanish electronic dictionaries, consisting of 142,892 definitions (2,112,592 tokens) ('D1') and 168,779 definitons (1,553,674 tokens) ('D2'), respectively.", "labels": [], "entities": []}, {"text": "Regarding evaluation, we used up to four different metrics with the aim of showing whether the improvements attained are consistent or not.", "labels": [], "entities": []}, {"text": "We have computed the BLEU score (accumulated up to 4-grams) (), the NIST score (accumulated up to 5-grams)), the General Text Matching (GTM) F-measure (e = 1, 2) (, and the METEOR measure ().", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 21, "end_pos": 31, "type": "METRIC", "confidence": 0.9826734066009521}, {"text": "NIST score", "start_pos": 68, "end_pos": 78, "type": "DATASET", "confidence": 0.7177743911743164}, {"text": "General Text Matching (GTM) F-measure", "start_pos": 113, "end_pos": 150, "type": "TASK", "confidence": 0.6647537733827319}, {"text": "METEOR measure", "start_pos": 173, "end_pos": 187, "type": "METRIC", "confidence": 0.9803890287876129}]}, {"text": "These metrics work at the lexical level by rewarding n-gram matches between the candidate translation and a set of human references.", "labels": [], "entities": []}, {"text": "Additionally, METEOR considers stemming, and allows for WordNet synonymy lookup.", "labels": [], "entities": []}, {"text": "The discussion of the significance of the results will be based on the BLEU score, for which we computed a bootstrap resampling test of significance.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 71, "end_pos": 81, "type": "METRIC", "confidence": 0.9412182867527008}]}, {"text": "http://www.statmt.org/wpt05/.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: MT Results on development and test sets, for the two baseline systems compared to SYSTRAN and to the 'EU'", "labels": [], "entities": [{"text": "MT", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.9152722954750061}, {"text": "SYSTRAN", "start_pos": 92, "end_pos": 99, "type": "DATASET", "confidence": 0.8093677759170532}]}, {"text": " Table 2: MT Results on development set, for several translation/language model configurations. 'EU' and 'WNG' refer to", "labels": [], "entities": [{"text": "MT", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.8469942212104797}]}, {"text": " Table 3: MT Results on development and test sets after tuning for the 'EU + D1 + D2 + WNG' language model configuration", "labels": [], "entities": [{"text": "MT", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.9096150398254395}]}, {"text": " Table 4: MT Results on development and test sets for the two strategies for combining translations models.", "labels": [], "entities": [{"text": "MT", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.9133113026618958}]}, {"text": " Table 5: MT output analysis of the 'EU', 'WNG' and 'EU+WNG' systems. FE, FW and FEW refer to the GTM (e = 1)", "labels": [], "entities": [{"text": "MT", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.9514685273170471}, {"text": "FE", "start_pos": 70, "end_pos": 72, "type": "METRIC", "confidence": 0.9766595363616943}, {"text": "FW", "start_pos": 74, "end_pos": 76, "type": "METRIC", "confidence": 0.6916284561157227}, {"text": "FEW", "start_pos": 81, "end_pos": 84, "type": "METRIC", "confidence": 0.940476655960083}]}]}