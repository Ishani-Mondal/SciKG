{"title": [{"text": "Unsupervised Topic Modelling for Multi-Party Spoken Discourse", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a method for unsupervised topic modelling which adapts methods used in document classification (Blei et al., 2003; Griffiths and Steyvers, 2004) to unsegmented multi-party discourse transcripts.", "labels": [], "entities": [{"text": "topic modelling", "start_pos": 37, "end_pos": 52, "type": "TASK", "confidence": 0.6513331532478333}, {"text": "document classification", "start_pos": 82, "end_pos": 105, "type": "TASK", "confidence": 0.7233149260282516}]}, {"text": "We show how Bayesian inference in this generative model can be used to simultaneously address the problems of topic segmentation and topic identification: automatically segmenting multi-party meetings into topically coherent segments with performance which compares well with previous unsuper-vised segmentation-only methods (Galley et al., 2003) while simultaneously extracting topics which rate highly when assessed for coherence by human judges.", "labels": [], "entities": [{"text": "topic segmentation", "start_pos": 110, "end_pos": 128, "type": "TASK", "confidence": 0.7234676778316498}, {"text": "topic identification", "start_pos": 133, "end_pos": 153, "type": "TASK", "confidence": 0.8055976331233978}]}, {"text": "We also show that this method appears robust in the face of off-topic dialogue and speech recognition errors.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 83, "end_pos": 101, "type": "TASK", "confidence": 0.7577569782733917}]}], "introductionContent": [{"text": "Topic segmentation -division of a text or discourse into topically coherent segments -and topic identification -classification of those segments by subject matter -are joint problems.", "labels": [], "entities": [{"text": "Topic segmentation -division of a text or discourse into topically coherent segments", "start_pos": 0, "end_pos": 84, "type": "TASK", "confidence": 0.830631251518543}, {"text": "topic identification -classification of those segments by subject matter", "start_pos": 90, "end_pos": 162, "type": "TASK", "confidence": 0.8116129517555237}]}, {"text": "Both are necessary steps in automatic indexing, retrieval and summarization from large datasets, whether spoken or written.", "labels": [], "entities": [{"text": "summarization from large", "start_pos": 62, "end_pos": 86, "type": "TASK", "confidence": 0.8682742913564047}]}, {"text": "Both have received significant attention in the past (see Section 2), but most approaches have been targeted at either text or monologue, and most address only one of the two issues (usually for the very good reason that the dataset itself provides the other, for example by the explicit separation of individual documents or news stories in a collection).", "labels": [], "entities": []}, {"text": "Spoken multi-party meetings pose a difficult problem: firstly, neither the segmentation nor the discussed topics can betaken as given; secondly, the discourse is by nature less tidily structured and less restricted in domain; and thirdly, speech recognition results have unavoidably high levels of error due to the noisy multispeaker environment.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 239, "end_pos": 257, "type": "TASK", "confidence": 0.7759227752685547}]}, {"text": "In this paper we present a method for unsupervised topic modelling which allows us to approach both problems simultaneously, inferring a set of topics while providing a segmentation into topically coherent segments.", "labels": [], "entities": [{"text": "topic modelling", "start_pos": 51, "end_pos": 66, "type": "TASK", "confidence": 0.6793760806322098}]}, {"text": "We show that this model can address these problems over multi-party discourse transcripts, providing good segmentation performance on a corpus of meetings (comparable to the best previous unsupervised method that we are aware of (), while also inferring a set of topics rated as semantically coherent by human judges.", "labels": [], "entities": []}, {"text": "We then show that its segmentation performance appears relatively robust to speech recognition errors, giving us confidence that it can be successfully applied in areal speech-processing system.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 76, "end_pos": 94, "type": "TASK", "confidence": 0.7546563744544983}]}, {"text": "The plan of the paper is as follows.", "labels": [], "entities": []}, {"text": "Section 2 below briefly discusses previous approaches to the identification and segmentation problems.", "labels": [], "entities": [{"text": "identification and segmentation", "start_pos": 61, "end_pos": 92, "type": "TASK", "confidence": 0.6350958546002706}]}, {"text": "Section 3 then describes the model we use here.", "labels": [], "entities": []}, {"text": "Section 4 then details our experiments and results, and conclusions are drawn in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "To analyze the properties of this algorithm we first applied it to a simulated dataset: a sequence of 10,000 words chosen from a vocabulary of 25.", "labels": [], "entities": []}, {"text": "Each segment of 100 successive words had a con- stant topic distribution (with distributions for different segments drawn from a Dirichlet distribution with \u03b2 = 0.1), and each subsequence of 10 words was taken to be one utterance.", "labels": [], "entities": []}, {"text": "The topicword assignments were chosen such that when the vocabulary is aligned in a 5\u00d75 grid the topics were binary bars.", "labels": [], "entities": []}, {"text": "The inference algorithm was then run for 200,000 iterations, with samples collected after every 1,000 iterations to minimize autocorrelation.", "labels": [], "entities": []}, {"text": "shows the inferred topic-word distributions and segment boundaries, which correspond well with those used to generate the data.", "labels": [], "entities": []}, {"text": "We applied the algorithm to the ICSI meeting corpus transcripts (, consisting of manual transcriptions of 75 meetings.", "labels": [], "entities": [{"text": "ICSI meeting corpus transcripts", "start_pos": 32, "end_pos": 63, "type": "DATASET", "confidence": 0.9597290754318237}]}, {"text": "For evaluation, we use ('s set of human-annotated segmentations, which covers a sub-portion of 25 meetings and takes a relatively coarse-grained approach to topic with an average of 5-6 topic segments per meeting.", "labels": [], "entities": []}, {"text": "Note that these segmentations were not used in training the model: topic inference and segmentation was unsupervised, with the human annotations used only to provide some knowledge of the overall segmentation density and to evaluate performance.", "labels": [], "entities": []}, {"text": "The transcripts from all 75 meetings were linearized by utterance start time and merged into a single dataset that contained 607,263 word tokens.", "labels": [], "entities": []}, {"text": "We sampled for 200,000 iterations of MCMC, taking samples every 1,000 iterations, and then averaged the sampled cu variables over the last 100 samples to derive an estimate for the posterior probability of a segmentation boundary at each utterance start.", "labels": [], "entities": [{"text": "MCMC", "start_pos": 37, "end_pos": 41, "type": "DATASET", "confidence": 0.9108726382255554}]}, {"text": "This probability was then thresholded to derive a final segmentation which was compared to the manual annotations.", "labels": [], "entities": []}, {"text": "More precisely, we apply a small amount of smoothing (Gaussian kernel convolution) and take the midpoints of any areas above a set threshold to be the segment boundaries.", "labels": [], "entities": []}, {"text": "Varying this threshold allows us to segment the discourse in a more or less finegrained way (and we anticipate that this could be user-settable in a meeting browsing application).", "labels": [], "entities": []}, {"text": "If the correct number of segments is known fora meeting, this can be used directly to determine the optimum threshold, increasing performance; if not, we must set it at a level which corresponds to the desired general level of granularity.", "labels": [], "entities": []}, {"text": "For each set of annotations, we therefore performed two sets of segmentations: one in which the threshold was set for each meeting to give the known goldstandard number of segments, and one in which the threshold was set on a separate development set to give the overall corpus-wide average number of segments, and held constant for all test meetings.", "labels": [], "entities": []}, {"text": "This also allows us to compare our results with those of (, who apply a similar threshold to their lexical cohesion function and give corresponding results produced with known/unknown numbers of segments.", "labels": [], "entities": []}, {"text": "Segmentation We assessed segmentation performance using the P k and WindowDiff (W D ) error measures proposed by and) respectively; both intuitively provide a measure of the probability that two points drawn from the meeting will be incorrectly separated by a hypothesized segment boundary -thus, lower P k and W D figures indicate better agreement with the human-annotated results.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 25, "end_pos": 37, "type": "TASK", "confidence": 0.9611061215400696}, {"text": "WindowDiff (W D ) error", "start_pos": 68, "end_pos": 91, "type": "METRIC", "confidence": 0.6727441251277924}]}, {"text": "For the numbers of segments we are dealing with, a baseline of segmenting the discourse into equal-length segments gives both P k and W D about 50%.", "labels": [], "entities": []}, {"text": "In order to investigate the effect of the number of underlying topics T , we tested models using 2, 5, 10 and 20 topics.", "labels": [], "entities": []}, {"text": "We then compared performance with ()'s LCSeg tool, and with a 10-state HMM model as described above.", "labels": [], "entities": []}, {"text": "Results are shown in, averaged over the 25 test meetings.", "labels": [], "entities": []}, {"text": "Results show that our model significantly outperforms the HMM equivalent -because the HMM cannot combine different topics, it places a lot of segmentation boundaries, resulting in inferior performance.", "labels": [], "entities": []}, {"text": "Using stemming and a bigram: Results from the ICSI corpus: A) the words most indicative for each topic; B) Probability of a segment boundary, compared with human segmentation, for an arbitrary subset of the data; C) Receiveroperator characteristic (ROC) curves for predicting human segmentation, and conditional probabilities of placing a boundary at an offset from a human boundary; D) subjective topic coherence ratings.", "labels": [], "entities": [{"text": "ICSI corpus", "start_pos": 46, "end_pos": 57, "type": "DATASET", "confidence": 0.9437550902366638}, {"text": "Receiveroperator characteristic (ROC)", "start_pos": 216, "end_pos": 253, "type": "METRIC", "confidence": 0.949279510974884}, {"text": "predicting human segmentation", "start_pos": 265, "end_pos": 294, "type": "TASK", "confidence": 0.8045530716578165}]}, {"text": "representation, however, might improve its performance (), although similar benefits might equally apply to our model.", "labels": [], "entities": []}, {"text": "It also performs comparably to ()'s unsupervised performance (exceeding it for some settings of T ).", "labels": [], "entities": []}, {"text": "It does not perform as well as their hybrid supervised system, which combined LCSeg with supervised learning over discourse features (P k = .23); but we expect that a similar approach would be possible here, combining our segmentation probabilities with other discourse-based features in a supervised way for improved performance.", "labels": [], "entities": []}, {"text": "Interestingly, segmentation quality, at least at this relatively coarse-grained level, seems hardly affected by the overall number of topics T . shows an example for one meeting of how the inferred topic segmentation probabilities at each utterance compare with the gold-standard segment boundaries.", "labels": [], "entities": []}, {"text": "illustrates the performance difference between our model and the HMM equivalent at an example segment boundary: for this example, the HMM model gives almost no discrimination.", "labels": [], "entities": []}, {"text": "Identification shows the most indicative words fora subset of the topics inferred at the last iteration.", "labels": [], "entities": []}, {"text": "Encouragingly, most topics seem intuitively to reflect the subjects we know were discussed in the ICSI meetings -the majority of them (67 meetings) are taken from the weekly meetings of 3 distinct research groups, where discussions centered around speech recognition techniques (topics 2, 5), meeting recording, annotation and hardware setup (topics 6, 3, 1, 8), robust language processing (topic 7).", "labels": [], "entities": [{"text": "ICSI meetings", "start_pos": 98, "end_pos": 111, "type": "DATASET", "confidence": 0.8066932559013367}, {"text": "speech recognition", "start_pos": 248, "end_pos": 266, "type": "TASK", "confidence": 0.741218239068985}]}, {"text": "Others reflect general classes of words which are independent of subject matter (topic 4).", "labels": [], "entities": []}, {"text": "To compare the quality of these inferred topics we performed an experiment in which 7 human observers rated (on a scale of 1 to 9) the semantic coherence of 50 lists of 10 words each.", "labels": [], "entities": []}, {"text": "Of these lists, 40 contained the most indicative words for each of the 10 topics from different models: the topic segmentation model; a topic model that had the same number of segments but with fixed evenly spread segmentation boundaries; an equiv-alent with randomly placed segmentation boundaries; and the HMM.", "labels": [], "entities": []}, {"text": "The other 10 lists contained random samples of 10 words from the other 40 lists.", "labels": [], "entities": []}, {"text": "Results are shown in, with the topic segmentation model producing the most coherent topics and the HMM model and random words scoring less well.", "labels": [], "entities": [{"text": "topic segmentation", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.6976566463708878}]}, {"text": "Interestingly, using an even distribution of boundaries but allowing the topic model to infer topics performs similarly well with even segmentation, but badly with random segmentation -topic quality is thus not very susceptible to the precise segmentation of the text, but does require some reasonable approximation (on ICSI data, an even segmentation gives a P k of about 50%, while random segmentations can do much worse).", "labels": [], "entities": [{"text": "ICSI data", "start_pos": 320, "end_pos": 329, "type": "DATASET", "confidence": 0.929389625787735}]}, {"text": "However, note that the full topic segmentation model is able to identify meaningful segmentation boundaries at the same time as inferring topics.", "labels": [], "entities": [{"text": "topic segmentation", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.7245136499404907}]}, {"text": "Meetings often include off-topic dialogue, in particular at the beginning and end, where informal chat and meta-dialogue are common.", "labels": [], "entities": []}, {"text": "annotated these sections explicitly, together with the ICSI \"digit-task\" sections (participants read sequences of digits to provide data for speech recognition experiments), and removed them from their data, as did we in Experiment 1 above.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 141, "end_pos": 159, "type": "TASK", "confidence": 0.641466811299324}]}, {"text": "While this seems reasonable for the purposes of investigating ideal algorithm performance, in real situations we will be faced with such off-topic dialogue, and would obviously prefer segmentation performance not to be badly affected (and ideally, enabling segmentation of the off-topic sections from the meeting proper).", "labels": [], "entities": []}, {"text": "One might suspect that an unsupervised generative model such as ours might not be robust in the presence of numerous off-topic words, as spurious topics might be inferred and used in the mixture model throughout.", "labels": [], "entities": []}, {"text": "In order to investigate this, we therefore also tested on the full dataset without removing these sections (806,026 word tokens in total), and added the section boundaries as further desired gold-standard segmentation boundaries.", "labels": [], "entities": []}, {"text": "shows the results: performance is not significantly affected, and again is very similar for both our model and LCSeg.", "labels": [], "entities": []}, {"text": "The experiments so far have all used manual word transcriptions.", "labels": [], "entities": []}, {"text": "Of course, in real meeting pro-  cessing systems, we will have to deal with speech recognition (ASR) errors.", "labels": [], "entities": [{"text": "speech recognition (ASR)", "start_pos": 76, "end_pos": 100, "type": "TASK", "confidence": 0.8280965328216553}]}, {"text": "We therefore also tested on 1-best ASR output provided by ICSI, and results are shown in.", "labels": [], "entities": [{"text": "ASR", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.4958317279815674}, {"text": "ICSI", "start_pos": 58, "end_pos": 62, "type": "DATASET", "confidence": 0.8989236354827881}]}, {"text": "The \"off-topic\" and \"digits\" sections were removed in this test, so results are comparable with Experiment 1.", "labels": [], "entities": []}, {"text": "Segmentation accuracy seems extremely robust; interestingly, LCSeg's results are less robust (the drop in performance is higher), especially when the number of segments in a meeting is unknown.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9404249787330627}]}, {"text": "It is surprising to notice that the segmentation accuracy in this experiment was actually slightly higher than achieved in Experiment 1 (especially given that ASR word error rates were generally above 20%).", "labels": [], "entities": [{"text": "segmentation", "start_pos": 36, "end_pos": 48, "type": "TASK", "confidence": 0.9534175395965576}, {"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9247432947158813}, {"text": "ASR word error rates", "start_pos": 159, "end_pos": 179, "type": "METRIC", "confidence": 0.7871520668268204}]}, {"text": "This may simply be a smoothing effect: differences in vocabulary and its distribution can effectively change the prior towards sparsity instantiated in the Dirichlet distributions.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on the ICSI meeting corpus.", "labels": [], "entities": [{"text": "ICSI meeting corpus", "start_pos": 25, "end_pos": 44, "type": "DATASET", "confidence": 0.9256314635276794}]}, {"text": " Table 2: Results for Experiments 2 & 3: robust- ness to off-topic and ASR data.", "labels": [], "entities": [{"text": "ASR", "start_pos": 71, "end_pos": 74, "type": "TASK", "confidence": 0.9198315739631653}]}]}