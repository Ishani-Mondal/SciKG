{"title": [{"text": "A Comparison of Alternative Parse Tree Paths for Labeling Semantic Roles", "labels": [], "entities": [{"text": "Labeling Semantic Roles", "start_pos": 49, "end_pos": 72, "type": "TASK", "confidence": 0.8036017417907715}]}], "abstractContent": [{"text": "The integration of sophisticated inference based techniques into natural language processing applications first requires a reliable method of encoding the predicate-argument structure of the pro-positional content of text.", "labels": [], "entities": []}, {"text": "Recent statistical approaches to automated predicate-argument annotation have utilized parse tree paths as predictive features, which encode the path between a verb predicate and anode in the parse tree that governs its argument.", "labels": [], "entities": [{"text": "predicate-argument annotation", "start_pos": 43, "end_pos": 72, "type": "TASK", "confidence": 0.7656066119670868}]}, {"text": "In this paper, we explore a number of alternatives for how these parse tree paths are encoded, focusing on the difference between automatically generated constituency parses and dependency parses.", "labels": [], "entities": []}, {"text": "After describing five alternatives for encoding parse tree paths, we investigate how well each can be aligned with the argument substrings in annotated text corpora, their relative precision and recall performance, and their comparative learning curves.", "labels": [], "entities": [{"text": "precision", "start_pos": 181, "end_pos": 190, "type": "METRIC", "confidence": 0.9652218818664551}, {"text": "recall", "start_pos": 195, "end_pos": 201, "type": "METRIC", "confidence": 0.9748650193214417}]}, {"text": "Results indicate that constituency parsers produce parse tree paths that can more easily be aligned to argument substrings, perform better in precision and recall, and have more favorable learning curves than those produced by a dependency parser.", "labels": [], "entities": [{"text": "precision", "start_pos": 142, "end_pos": 151, "type": "METRIC", "confidence": 0.9993088245391846}, {"text": "recall", "start_pos": 156, "end_pos": 162, "type": "METRIC", "confidence": 0.9980212450027466}]}], "introductionContent": [{"text": "A persistent goal of natural language processing research has been the automated transformation of natural language texts into representations that unambiguously encode their propositional content informal notation.", "labels": [], "entities": []}, {"text": "Increasingly, firstorder predicate calculus representations of textual meaning have been used in natural lanugage processing applications that involve automated inference.", "labels": [], "entities": []}, {"text": "For example, demonstrate how predicate-argument formulations of questions and candidate answer sentences are unified using logical inference in a top-performing question-answering application.", "labels": [], "entities": []}, {"text": "The importance of robust techniques for predicate-argument transformation has motivated the development of large-scale text corpora with predicate-argument annotations such as PropBank () and FrameNet ().", "labels": [], "entities": [{"text": "predicate-argument transformation", "start_pos": 40, "end_pos": 73, "type": "TASK", "confidence": 0.8861035108566284}]}, {"text": "These corpora typically take a pragmatic approach to the predicate-argument representations of sentences, where predicates correspond to single word triggers in the surface form of the sentence (typically verb lemmas), and arguments can be identified as substrings of the sentence.", "labels": [], "entities": []}, {"text": "Along with the development of annotated corpora, researchers have developed new techniques for automatically identifying the arguments of predications by labeling text segments in sentences with semantic roles.", "labels": [], "entities": [{"text": "automatically identifying the arguments of predications", "start_pos": 95, "end_pos": 150, "type": "TASK", "confidence": 0.7330109675725301}]}, {"text": "Both and describe statistical labeling algorithms that achieve high accuracy in assigning semantic role labels to appropropriate constituents in a parse tree of a sentence.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9945969581604004}]}, {"text": "Each of these efforts employed the use of parse tree paths as predictive features, encoding the series of up and down transitions through a parse tree to move from the node of the verb (predicate) to the governing node of the constituent (argument).", "labels": [], "entities": []}, {"text": "demonstrate that utilizing the gold-standard parse trees of the Penn treebank () to encode parse tree paths yields significantly better labeling accuracy than when using an automatic syntactical parser, namely that of Collins (1999).", "labels": [], "entities": [{"text": "Penn treebank", "start_pos": 64, "end_pos": 77, "type": "DATASET", "confidence": 0.9896820485591888}, {"text": "accuracy", "start_pos": 145, "end_pos": 153, "type": "METRIC", "confidence": 0.972746729850769}]}, {"text": "Parse tree paths (between verbs and arguments that fill semantic roles) are particularly interesting because they symbolically encode the relationship between the syntactic and semantic aspects of verbs, and are potentially generalized across other verbs within the same class.", "labels": [], "entities": []}, {"text": "However, the encoding of individual parse tree paths for predicates is wholly dependent on the characteristics of the parse tree of a sentence, for which competing approaches could betaken.", "labels": [], "entities": []}, {"text": "The research effort described in this paper further explores the role of parse tree paths in identifying the argument structure of verb-based predications.", "labels": [], "entities": []}, {"text": "We are particularly interested in exploring alternatives to the constituency parses that were used in previous research, including parsing approaches that employ dependency grammars.", "labels": [], "entities": [{"text": "constituency parses", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.6869028359651566}]}, {"text": "Specifically, our aim is to answer four important questions: 1.", "labels": [], "entities": []}, {"text": "How can parse tree paths be encoded when employing different automated constituency parsers, i.e.,, or a dependency parser (Lin, 1998)?", "labels": [], "entities": []}, {"text": "2. Given that each of these alternatives creates a different formulation of the parse tree of a sentence, which of them encodes branches that are easiest to align with substrings that have been annotated with semantic role information?", "labels": [], "entities": []}, {"text": "3. What is the relative precision and recall performance of parse tree paths formulated using these alternative automated parsing techniques, and do the results vary depending on argument type?", "labels": [], "entities": [{"text": "precision", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.986247718334198}, {"text": "recall", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.9938753247261047}]}, {"text": "4. How many examples of parse tree paths are necessary to provide as training examples in order to achieve high labeling accuracy when employing each of these parsing alternatives?", "labels": [], "entities": [{"text": "accuracy", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.9699426293373108}]}, {"text": "Each of these four questions is addressed in the four subsequent sections of this paper, followed by a discussion of the implications of our findings and directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to evaluate the comparative performance of the parse tree paths for each of the five encodings, we divided the corpus in to equal-sized training and test sets (50 training and 50 test examples for each of the four predicates).", "labels": [], "entities": []}, {"text": "We then constructed a system that identified the parse tree paths for each of the 10 arguments in the training sets, and applied them to the sentences in each corresponding test sets.", "labels": [], "entities": []}, {"text": "When applying the 50 training parse tree paths to anyone of the 50 test sentences fora given predicate-argument pair, a set of zero or more candidate answer nodes were returned.", "labels": [], "entities": []}, {"text": "For the purpose of calculating precision and recall scores, credit was given when the correct answer appeared in this set.", "labels": [], "entities": [{"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9970473647117615}, {"text": "recall scores", "start_pos": 45, "end_pos": 58, "type": "METRIC", "confidence": 0.9778013825416565}, {"text": "credit", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.9754464030265808}]}, {"text": "Precision scores were calculated as the number of correct answers found divided by the number of all candidate answer nodes returned.", "labels": [], "entities": [{"text": "Precision scores", "start_pos": 0, "end_pos": 16, "type": "METRIC", "confidence": 0.9764581322669983}]}, {"text": "Recall scores were calculated as the number of correct answers found divided by the total number of correct answers possible.", "labels": [], "entities": [{"text": "Recall scores", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.9707858264446259}]}, {"text": "F-scores were calculated as the equallyweighted harmonic mean of precision and recall.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9631179571151733}, {"text": "precision", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9995918869972229}, {"text": "recall", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.9974284768104553}]}, {"text": "Our calculation of recall scores represents the best-possible performance of systems using only these types of parse-tree paths.", "labels": [], "entities": [{"text": "recall", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.9980093836784363}]}, {"text": "This level of performance could be obtained if a system could always select the correct answer from the set of candidates returned.", "labels": [], "entities": []}, {"text": "However, it is also informative to estimate the performance that could be achieved by randomly selecting among the candidate answers, representing a lower-bound on performance.", "labels": [], "entities": []}, {"text": "Accordingly, we computed an adjusted recall score that awarded only fractional credit in cases where more than one candidate answer was returned (one divided by the set size).", "labels": [], "entities": [{"text": "recall score", "start_pos": 37, "end_pos": 49, "type": "METRIC", "confidence": 0.9824577867984772}]}, {"text": "Adjusted recall is the sum of all of these adjusted credits divided by the total number of correct answers possible.", "labels": [], "entities": [{"text": "recall", "start_pos": 9, "end_pos": 15, "type": "METRIC", "confidence": 0.8403695225715637}]}, {"text": "summarizes the comparative recall, precision, f-score, and adjusted recall performance for each of the five parse tree path formulations.", "labels": [], "entities": [{"text": "recall", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9827006459236145}, {"text": "precision", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9994860887527466}, {"text": "recall", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.8659589886665344}]}, {"text": "The Charniak parser achieved the highest overall scores (precision=.49, recall=.68, fscore=.57, adjusted recall=.48), followed closely by the Stanford parser (precision=.47, recall=.67, f-score=.55, adjusted recall=.48).", "labels": [], "entities": [{"text": "precision", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.9991422891616821}, {"text": "recall", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.9992961883544922}, {"text": "fscore", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.9974877834320068}, {"text": "adjusted", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.9521369934082031}, {"text": "recall", "start_pos": 105, "end_pos": 111, "type": "METRIC", "confidence": 0.5452425479888916}, {"text": "precision", "start_pos": 159, "end_pos": 168, "type": "METRIC", "confidence": 0.9985741376876831}, {"text": "recall", "start_pos": 174, "end_pos": 180, "type": "METRIC", "confidence": 0.9985405206680298}, {"text": "f-score", "start_pos": 186, "end_pos": 193, "type": "METRIC", "confidence": 0.9907914400100708}, {"text": "adjusted recall", "start_pos": 199, "end_pos": 214, "type": "METRIC", "confidence": 0.7582730948925018}]}, {"text": "Our expectation was that the short, semantically descriptive parse tree paths produced by Minipar would yield the highest performance.", "labels": [], "entities": [{"text": "Minipar", "start_pos": 90, "end_pos": 97, "type": "DATASET", "confidence": 0.9528075456619263}]}, {"text": "However, these results indicate the opposite; the constituency parsers produce the most accurate parse tree paths.", "labels": [], "entities": []}, {"text": "Only Minipar C offers better recall (0.71) than the constituency parsers, but at the expense of extremely low precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9996716976165771}, {"text": "precision", "start_pos": 110, "end_pos": 119, "type": "METRIC", "confidence": 0.9982107877731323}]}, {"text": "Minipar A offers excellent precision (0.62), but with extremely low recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9995500445365906}, {"text": "recall", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.9984536170959473}]}, {"text": "Minipar B provides a balance between recall and precision performance, but falls short of being competitive with the parse tree paths generated by the two constituency parsers, with an f-score of .44.", "labels": [], "entities": [{"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9985271692276001}, {"text": "precision", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9981733560562134}, {"text": "f-score", "start_pos": 185, "end_pos": 192, "type": "METRIC", "confidence": 0.9650307297706604}]}, {"text": "We utilized the Sign Test in order to determine the statistical significance of these differences.", "labels": [], "entities": []}, {"text": "Rank orderings between pairs of systems were determined based on the adjusted credit that each system achieved for each test sentence.", "labels": [], "entities": [{"text": "credit", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.9443220496177673}]}, {"text": "Significant differences were found between the performance of every system (p<0.05), with the exception of the Charniak and Stanford parsers.", "labels": [], "entities": []}, {"text": "Interestingly, by comparing weighted values for each test example, Minipar C more frequently scores higher than Minipar A, even though the sum of these scores favors Minipar A. In addition to overall performance, we were interested in determining whether performance varied depending on the type of the argument that is being labeled.", "labels": [], "entities": []}, {"text": "In assigning labels to arguments in the corpus, we followed the general principles set out by for labeling arguments arg0, arg1 and arg2.", "labels": [], "entities": [{"text": "arg1", "start_pos": 123, "end_pos": 127, "type": "METRIC", "confidence": 0.9451093077659607}, {"text": "arg2", "start_pos": 132, "end_pos": 136, "type": "METRIC", "confidence": 0.9229421019554138}]}, {"text": "Across each of our four predicates, arg0 is the agent of the predication (e.g. the person that has the belief or is doing the giving), and arg1 is the thing that is acted upon by the agent (e.g. the thing that is believed or the thing that is given).", "labels": [], "entities": [{"text": "arg0", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.9832747578620911}, {"text": "arg1", "start_pos": 139, "end_pos": 143, "type": "METRIC", "confidence": 0.979197084903717}]}, {"text": "Arg2 is used only for the predications based on the verbs give and receive, where it is used to indicate the other party of the action.", "labels": [], "entities": [{"text": "Arg2", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9888184666633606}]}, {"text": "Our interest was in determining whether these five approaches yielded different results depending on the semantic type of the argument.", "labels": [], "entities": []}, {"text": "presents the f-scores for each of these encodings across each argument type.", "labels": [], "entities": []}, {"text": "Results indicate that the Charniak and Stanford parsers continue to produce parse tree paths that outperform each of the Minipar-based approaches.", "labels": [], "entities": []}, {"text": "In each approach argument 0 is the easiest to identify.", "labels": [], "entities": []}, {"text": "Minipar A retains the general trends of Charniak and Stanford, with argument.", "labels": [], "entities": [{"text": "Minipar A", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8808531761169434}]}, {"text": "Precision, recall, f-scores, and adjusted recall for five parse tree path types.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.975047767162323}, {"text": "recall", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.9995777010917664}, {"text": "adjusted", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9414462447166443}, {"text": "recall", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.8234838247299194}]}, {"text": "Comparative f-scores for arguments 0, 1, and 2 for five parse tree path types 1 easier to identify than argument 2, while Minipar B and C show the reverse.", "labels": [], "entities": []}, {"text": "The highest fscores for argument 0 were achieved Stanford (f=.65), while Charniak achieved the highest scores for argument 1 (f=.55) and argument 2 (f=.49).", "labels": [], "entities": []}], "tableCaptions": []}