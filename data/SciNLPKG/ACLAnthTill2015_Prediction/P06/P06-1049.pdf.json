{"title": [{"text": "A Bottom-up Approach to Sentence Ordering for Multi-document Summarization", "labels": [], "entities": [{"text": "Sentence Ordering", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.9211414158344269}, {"text": "Summarization", "start_pos": 61, "end_pos": 74, "type": "TASK", "confidence": 0.8229995369911194}]}], "abstractContent": [{"text": "Ordering information is a difficult but important task for applications generating natural-language text.", "labels": [], "entities": []}, {"text": "We present a bottom-up approach to arranging sentences extracted for multi-document sum-marization.", "labels": [], "entities": []}, {"text": "To capture the association and order of two textual segments (eg, sentences), we define four criteria, chronology , topical-closeness, precedence, and succession.", "labels": [], "entities": [{"text": "succession", "start_pos": 151, "end_pos": 161, "type": "METRIC", "confidence": 0.9546352624893188}]}, {"text": "These criteria are integrated into a criterion by a supervised learning approach.", "labels": [], "entities": []}, {"text": "We repeatedly concatenate two textual segments into one segment based on the criterion until we obtain the overall segment with all sentences arranged.", "labels": [], "entities": []}, {"text": "Our experimental results show a significant improvement over existing sentence ordering strategies.", "labels": [], "entities": [{"text": "sentence ordering", "start_pos": 70, "end_pos": 87, "type": "TASK", "confidence": 0.7389566898345947}]}], "introductionContent": [{"text": "Multi-document summarization (MDS)) tackles the information overload problem by providing a condensed version of a set of documents.", "labels": [], "entities": [{"text": "Multi-document summarization (MDS))", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8567196488380432}]}, {"text": "Among a number of sub-tasks involved in MDS, eg, sentence extraction, topic detection, sentence ordering, information extraction, sentence generation, etc., most MDS systems have been based on an extraction method, which identifies important textual segments (eg, sentences or paragraphs) in source documents.", "labels": [], "entities": [{"text": "sentence extraction", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.7833433449268341}, {"text": "topic detection", "start_pos": 70, "end_pos": 85, "type": "TASK", "confidence": 0.7865862548351288}, {"text": "sentence ordering", "start_pos": 87, "end_pos": 104, "type": "TASK", "confidence": 0.7525389194488525}, {"text": "information extraction", "start_pos": 106, "end_pos": 128, "type": "TASK", "confidence": 0.7959041595458984}, {"text": "sentence generation", "start_pos": 130, "end_pos": 149, "type": "TASK", "confidence": 0.7622231245040894}]}, {"text": "It is important for such MDS systems to determine a coherent arrangement of the textual segments extracted from multi-documents in order to reconstruct the text structure for summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 175, "end_pos": 188, "type": "TASK", "confidence": 0.9794651865959167}]}, {"text": "Ordering information is also essential for * Research Fellow of the Japan Society for the Promotion of other text-generation applications such as Question Answering.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 146, "end_pos": 164, "type": "TASK", "confidence": 0.8033817410469055}]}, {"text": "A summary with improperly ordered sentences confuses the reader and degrades the quality/reliability of the summary itself. has provided empirical evidence that proper order of extracted sentences improves their readability significantly.", "labels": [], "entities": []}, {"text": "However, ordering a set of sentences into a coherent text is a nontrivial task.", "labels": [], "entities": [{"text": "ordering a set of sentences into a coherent text", "start_pos": 9, "end_pos": 57, "type": "TASK", "confidence": 0.785981125301785}]}, {"text": "For example, identifying rhetorical relations () in an ordered text has been a difficult task for computers, whereas our task is even more complicated: to reconstruct such relations from unordered sets of sentences.", "labels": [], "entities": [{"text": "identifying rhetorical relations", "start_pos": 13, "end_pos": 45, "type": "TASK", "confidence": 0.8661391337712606}]}, {"text": "Source documents fora summary may have been written by different authors, by different writing styles, on different dates, and based on different background knowledge.", "labels": [], "entities": []}, {"text": "We cannot expect that a set of extracted sentences from such diverse documents will be coherent on their own.", "labels": [], "entities": []}, {"text": "Several strategies to determine sentence ordering have been proposed as described in section 2.", "labels": [], "entities": [{"text": "determine sentence ordering", "start_pos": 22, "end_pos": 49, "type": "TASK", "confidence": 0.6447562674681345}]}, {"text": "However, the appropriate way to combine these strategies to achieve more coherent summaries remains unsolved.", "labels": [], "entities": []}, {"text": "In this paper, we propose four criteria to capture the association of sentences in the context of multi-document summarization for newspaper articles.", "labels": [], "entities": [{"text": "multi-document summarization for newspaper articles", "start_pos": 98, "end_pos": 149, "type": "TASK", "confidence": 0.7095743179321289}]}, {"text": "These criteria are integrated into one criterion by a supervised learning approach.", "labels": [], "entities": []}, {"text": "We also propose a bottom-up approach in arranging sentences, which repeatedly concatenates textual segments until the overall segment with all sentences arranged, is achieved.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated the proposed method by using the 3rd Text Summarization Challenge (TSC-3) corpus 2 . The TSC-3 corpus contains 30 sets of extracts, each of which consists of unordered sentences 3 extracted from Japanese newspaper articles relevant to a topic (query).", "labels": [], "entities": [{"text": "3rd Text Summarization Challenge (TSC-3)", "start_pos": 46, "end_pos": 86, "type": "TASK", "confidence": 0.7580886653491429}, {"text": "TSC-3 corpus", "start_pos": 102, "end_pos": 114, "type": "DATASET", "confidence": 0.8242961466312408}]}, {"text": "We arrange the extracts by using different algorithms and evaluate the readability of the ordered extracts by a subjective grading and several metrics.", "labels": [], "entities": []}, {"text": "In order to construct training data applicable to the proposed method, we asked two human subjects to arrange the extracts and obtained 30(topics) \u00d7 2(humans) = 60 sets of ordered extracts.", "labels": [], "entities": []}, {"text": "shows the agreement of the ordered extracts between the two subjects.", "labels": [], "entities": [{"text": "agreement", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9498464465141296}]}, {"text": "The correlation is measured by three metrics, Spearman's rank correlation, Kendall's rank correlation, and average continuity (described later).", "labels": [], "entities": [{"text": "correlation", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9549347162246704}, {"text": "Spearman's rank correlation", "start_pos": 46, "end_pos": 73, "type": "METRIC", "confidence": 0.5330692678689957}, {"text": "Kendall's rank correlation", "start_pos": 75, "end_pos": 101, "type": "METRIC", "confidence": 0.8418303281068802}, {"text": "average continuity", "start_pos": 107, "end_pos": 125, "type": "METRIC", "confidence": 0.9635548889636993}]}, {"text": "The mean correlation values (0.74 for Spearman's rank correlation and 0.69 for Kendall's rank correlation) indicate a certain level of agreement in sentence orderings made by the two subjects.", "labels": [], "entities": [{"text": "Kendall's rank correlation", "start_pos": 79, "end_pos": 105, "type": "METRIC", "confidence": 0.588078111410141}, {"text": "agreement", "start_pos": 135, "end_pos": 144, "type": "METRIC", "confidence": 0.9570227861404419}]}, {"text": "8 out of 30 extracts were actually identical.", "labels": [], "entities": []}, {"text": "We applied the leave-one-out method to the proposed method to produce a set of sentence orderings.", "labels": [], "entities": []}, {"text": "In this experiment, the leave-out-out method arranges an extract by using an SVM model trained from the rest of the 29 extracts.", "labels": [], "entities": []}, {"text": "Repeating this process 30 times with a different topic for each iteration, we generated a set of 30 extracts for evaluation.", "labels": [], "entities": []}, {"text": "In addition to the proposed method, we prepared six sets of sentence orderings produced by different algorithms for comparison.", "labels": [], "entities": []}, {"text": "We describe briefly the seven algorithms (including the proposed method): Agglomerative ordering (AGL) is an ordering arranged by the proposed method; Random ordering (RND) is the lowest anchor, in which sentences are arranged randomly; Human-made ordering (HUM) is the highest anchor, in which sentences are arranged by a human subject; Chronological ordering (CHR) arranges sentences with the chronology criterion defined in Formula 8.", "labels": [], "entities": [{"text": "Random ordering (RND)", "start_pos": 151, "end_pos": 172, "type": "METRIC", "confidence": 0.7630460023880005}, {"text": "Chronological ordering (CHR) arranges sentences", "start_pos": 338, "end_pos": 385, "type": "TASK", "confidence": 0.713465758732387}]}, {"text": "Sentences are arranged in chronological order of their publication date; Topical-closeness ordering (TOP) arranges sentences with the topical-closeness criterion defined in Formula 9; Suceedence ordering (SUC) arranges sentences with the succession criterion defined in Formula 11.", "labels": [], "entities": [{"text": "Suceedence ordering (SUC", "start_pos": 184, "end_pos": 208, "type": "TASK", "confidence": 0.813420906662941}]}, {"text": "The last four algorithms (CHR, TOP, PRE, and SUC) arrange sentences by the corresponding criterion alone, each of which uses the association strength directly to arrange sentences without the integration of other criteria.", "labels": [], "entities": [{"text": "TOP", "start_pos": 31, "end_pos": 34, "type": "METRIC", "confidence": 0.765769898891449}]}, {"text": "These orderings are expected to show the performance of each expert independently and their contribution to solving the sentence ordering problem.", "labels": [], "entities": [{"text": "sentence ordering problem", "start_pos": 120, "end_pos": 145, "type": "TASK", "confidence": 0.8099022308985392}]}, {"text": "We also evaluated sentence orderings by reusing two sets of gold-standard orderings made for the training data.", "labels": [], "entities": [{"text": "sentence orderings", "start_pos": 18, "end_pos": 36, "type": "TASK", "confidence": 0.7218113094568253}]}, {"text": "In general, subjective grading consumes much time and effort, even though we cannot reproduce the evaluation afterwards.", "labels": [], "entities": []}, {"text": "The previous studies ( employ rank correlation coefficients such as Spearman's rank correlation and Kendall's rank correlation, assuming a sentence ordering to be a rank.", "labels": [], "entities": [{"text": "Kendall's rank correlation", "start_pos": 100, "end_pos": 126, "type": "METRIC", "confidence": 0.5814711004495621}]}, {"text": "propose a metric that assess continuity of pairwise sentences compared with the gold standard.", "labels": [], "entities": [{"text": "continuity", "start_pos": 29, "end_pos": 39, "type": "METRIC", "confidence": 0.9849299788475037}]}, {"text": "In addition to Spearman's and Kendall's rank correlation coefficients, we propose an average continuity metric, which extends the idea of the continuity metric to continuous k sentences.", "labels": [], "entities": [{"text": "average continuity metric", "start_pos": 85, "end_pos": 110, "type": "METRIC", "confidence": 0.7504479090372721}]}, {"text": "A text with sentences arranged in proper order does not interrupt a human's reading while moving from one sentence to the next.", "labels": [], "entities": []}, {"text": "Hence, the quality of a sentence ordering can be estimated by the number of continuous sentences that are also reproduced in the reference sentence ordering.", "labels": [], "entities": []}, {"text": "This is equivalent to measuring a precision of continuous sentences in an ordering against the reference ordering.", "labels": [], "entities": [{"text": "precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9904679656028748}]}, {"text": "We define P n to measure the precision of n continuous sentences in an ordering to be evaluated as, Here, N is the number of sentences in the reference ordering; n is the length of continuous sentences on which we are evaluating; m is the number of continuous sentences that appear in both the evaluation and reference orderings.", "labels": [], "entities": [{"text": "precision", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9936466217041016}]}, {"text": "In, the precision of 3 continuous sentences P 3 is calculated as: The Average Continuity (AC) is defined as the logarithmic average of P n over 2 to k: Here, k is a parameter to control the range of the logarithmic average; and \u03b1 is a small value in case if P n is zero.", "labels": [], "entities": [{"text": "precision", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.9969601631164551}, {"text": "Average Continuity (AC)", "start_pos": 70, "end_pos": 93, "type": "METRIC", "confidence": 0.9737290978431702}]}, {"text": "We set k = 4 (ie, more than five continuous sentences are not included for evaluation) and \u03b1 = 0.01.", "labels": [], "entities": []}, {"text": "Average Continuity becomes 0 when evaluation and reference orderings share no continuous sentences and 1 when the two orderings are identical.", "labels": [], "entities": [{"text": "Continuity", "start_pos": 8, "end_pos": 18, "type": "METRIC", "confidence": 0.8060361742973328}]}, {"text": "In, Average Continuity is calculated as 0.63.", "labels": [], "entities": [{"text": "Average", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9977961778640747}, {"text": "Continuity", "start_pos": 12, "end_pos": 22, "type": "METRIC", "confidence": 0.5313142538070679}]}, {"text": "The underlying idea of Formula 14 was proposed by as the BLEU metric for the semi-automatic evaluation of machine-translation systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9929821491241455}]}, {"text": "The original definition of the BLEU metric is to compare a machine-translated text with its reference translation by using the word n-grams.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.984126627445221}]}, {"text": "rest in all evaluation metrics, although the chronological ordering (CHR) appeared to play the major role.", "labels": [], "entities": [{"text": "chronological ordering (CHR)", "start_pos": 45, "end_pos": 73, "type": "METRIC", "confidence": 0.7480410516262055}]}, {"text": "The one-way analysis of variance (ANOVA) verified the effects of different algorithms for sentence orderings with all metrics (p < 0.01).", "labels": [], "entities": []}, {"text": "We performed Tukey Honest Significant Differences (HSD) test to compare differences among these algorithms.", "labels": [], "entities": [{"text": "Tukey", "start_pos": 13, "end_pos": 18, "type": "DATASET", "confidence": 0.6805130243301392}, {"text": "Honest Significant Differences (HSD) test", "start_pos": 19, "end_pos": 60, "type": "METRIC", "confidence": 0.7768802898270744}]}, {"text": "The Tukey test revealed that AGL was significantly better than the rest.", "labels": [], "entities": [{"text": "AGL", "start_pos": 29, "end_pos": 32, "type": "METRIC", "confidence": 0.9982033967971802}]}, {"text": "Even though we could not compare our experiment with the probabilistic approach directly due to the difference of the text corpora, the Kendall coefficient reported higher agreement than Lapata's experiment (Kendall=0.48 with lemmatized nouns and Kendall=0.56 with verb-noun dependencies).", "labels": [], "entities": [{"text": "agreement", "start_pos": 172, "end_pos": 181, "type": "METRIC", "confidence": 0.9961657524108887}]}, {"text": "shows precision P n with different length values of continuous sentence n for the six methods compared in.", "labels": [], "entities": [{"text": "precision", "start_pos": 6, "end_pos": 15, "type": "METRIC", "confidence": 0.9733130931854248}]}, {"text": "The number of continuous sentences becomes sparse fora higher value of length n.", "labels": [], "entities": []}, {"text": "Therefore, the precision values decrease as the length n increases.", "labels": [], "entities": [{"text": "precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9995867609977722}]}, {"text": "Although RND ordering reported some continuous sentences for lower n values, no continuous sentences could be observed for the higher n values.", "labels": [], "entities": [{"text": "RND ordering", "start_pos": 9, "end_pos": 21, "type": "TASK", "confidence": 0.8191773593425751}]}, {"text": "Four criteria described in Section 3 (ie, CHR, TOP, PRE, SUC) produce segments of continuous sentences at all values of n.", "labels": [], "entities": [{"text": "TOP", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.8731051087379456}, {"text": "PRE", "start_pos": 52, "end_pos": 55, "type": "METRIC", "confidence": 0.864753246307373}]}], "tableCaptions": [{"text": " Table 1: Correlation between two sets of human- ordered extracts", "labels": [], "entities": []}, {"text": " Table 2: Comparison with human-made ordering", "labels": [], "entities": []}]}