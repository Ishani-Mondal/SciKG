{"title": [{"text": "BiTAM: Bilingual Topic AdMixture Models for Word Alignment", "labels": [], "entities": [{"text": "Word Alignment", "start_pos": 44, "end_pos": 58, "type": "TASK", "confidence": 0.7287114858627319}]}], "abstractContent": [{"text": "We propose a novel bilingual topical ad-mixture (BiTAM) formalism for word alignment in statistical machine translation.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 70, "end_pos": 84, "type": "TASK", "confidence": 0.7681359052658081}, {"text": "statistical machine translation", "start_pos": 88, "end_pos": 119, "type": "TASK", "confidence": 0.665564755598704}]}, {"text": "Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model.", "labels": [], "entities": []}, {"text": "Three BiTAM models are proposed to capture topic sharing at different levels of linguistic granularity (i.e., at the sentence or word levels).", "labels": [], "entities": []}, {"text": "These models enable word-alignment process to leverage topical contents of document-pairs.", "labels": [], "entities": []}, {"text": "Efficient vari-ational approximation algorithms are designed for inference and parameter estimation.", "labels": [], "entities": [{"text": "parameter estimation", "start_pos": 79, "end_pos": 99, "type": "TASK", "confidence": 0.6827833503484726}]}, {"text": "With the inferred latent topics, BiTAM models facilitate coherent pairing of bilingual linguistic entities that share common topical aspects.", "labels": [], "entities": []}, {"text": "Our preliminary experiments show that the proposed models improve word alignment accuracy, and lead to better translation quality.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 66, "end_pos": 80, "type": "TASK", "confidence": 0.8076342344284058}, {"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.8739291429519653}]}], "introductionContent": [{"text": "Parallel data has been treated as sets of unrelated sentence-pairs in state-of-the-art statistical machine translation (SMT) models.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 87, "end_pos": 124, "type": "TASK", "confidence": 0.7838103771209717}]}, {"text": "Most current approaches emphasize within-sentence dependencies such as the distortion in (, the dependency of alignment in HMM (, and syntax mappings in).", "labels": [], "entities": []}, {"text": "Beyond the sentence-level, corpuslevel word-correlation and contextual-level topical information may help to disambiguate translation candidates and word-alignment choices.", "labels": [], "entities": []}, {"text": "For example, the most frequent source words (e.g., functional words) are likely to be translated into words which are also frequent on the target side; words of the same topic generally bear correlations and similar translations.", "labels": [], "entities": []}, {"text": "Extended contextual information is especially useful when translation models are vague due to their reliance solely on word-pair cooccurrence statistics.", "labels": [], "entities": []}, {"text": "For example, the word shot in \"It was a nice shot.\" should be translated differently depending on the context of the sentence: a goal in the context of sports, or a photo within the context of sightseeing.", "labels": [], "entities": []}, {"text": "stated that sentence-pairs are tied by the logic-flow in a document-pair; in other words, the document-pair should be word-aligned as one entity instead of being uncorrelated instances.", "labels": [], "entities": []}, {"text": "In this paper, we propose a probabilistic admixture model to capture latent topics underlying the context of documentpairs.", "labels": [], "entities": []}, {"text": "With such topical information, the translation models are expected to be sharper and the word-alignment process less ambiguous.", "labels": [], "entities": []}, {"text": "Previous works on topical translation models concern mainly explicit logical representations of semantics for machine translation.", "labels": [], "entities": [{"text": "topical translation", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.7759579122066498}, {"text": "machine translation", "start_pos": 110, "end_pos": 129, "type": "TASK", "confidence": 0.7632836997509003}]}, {"text": "This include knowledge-based and interlingua-based ( approaches.", "labels": [], "entities": []}, {"text": "These approaches can be expensive, and they do not emphasize stochastic translation aspects.", "labels": [], "entities": []}, {"text": "Recent investigations along this line includes using word-disambiguation schemes) and non-overlapping bilingual word-clusters () with particular translation models, which showed various degrees of success.", "labels": [], "entities": []}, {"text": "We propose anew statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT.", "labels": [], "entities": [{"text": "topic-based word alignment", "start_pos": 96, "end_pos": 122, "type": "TASK", "confidence": 0.6110292573769888}, {"text": "SMT", "start_pos": 126, "end_pos": 129, "type": "TASK", "confidence": 0.8753932118415833}]}, {"text": "Variants of admixture models have appeared in population genetics () and text modeling (.", "labels": [], "entities": [{"text": "text modeling", "start_pos": 73, "end_pos": 86, "type": "TASK", "confidence": 0.8312839865684509}]}, {"text": "Statistically, an object is said to be derived from an admixture if it consists of a bag of elements, each sampled independently or coupled in someway, from a mixture model.", "labels": [], "entities": []}, {"text": "Ina typical SMT setting, each documentpair corresponds to an object; depending on a chosen modeling granularity, all sentence-pairs or word-pairs in the document-pair correspond to the elements constituting the object.", "labels": [], "entities": [{"text": "SMT", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.9882398843765259}]}, {"text": "Correspondingly, a latent topic is sampled for each pair from a prior topic distribution to induce topic-specific translations; and the resulting sentence-pairs and wordpairs are marginally dependent.", "labels": [], "entities": []}, {"text": "Generatively, this admixture formalism enables word translations to be instantiated by topic-specific bilingual models and/or monolingual models, depending on their contexts.", "labels": [], "entities": [{"text": "word translations", "start_pos": 47, "end_pos": 64, "type": "TASK", "confidence": 0.7416701018810272}]}, {"text": "In this paper we investigate three instances of the BiTAM model, They are data-driven and do not need hand-crafted knowledge engineering.", "labels": [], "entities": []}, {"text": "The remainder of the paper is as follows: in section 2, we introduce notations and baselines; in section 3, we propose the topic admixture models; in section 4, we present the learning and inference algorithms; and in section 5 we show experiments of our models.", "labels": [], "entities": []}, {"text": "We conclude with a brief discussion in section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate BiTAM models on the word alignment accuracy and the translation quality.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 32, "end_pos": 46, "type": "TASK", "confidence": 0.7510459423065186}, {"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.8719095587730408}]}, {"text": "For word alignment accuracy, F-measure is reported, i.e., the harmonic mean of precision and recall against a gold-standard reference set; for translation quality,) and its variation of NIST scores are reported.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.7593169808387756}, {"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.878728449344635}, {"text": "F-measure", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9988648891448975}, {"text": "precision", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.9577729105949402}, {"text": "recall", "start_pos": 93, "end_pos": 99, "type": "METRIC", "confidence": 0.9831233620643616}]}, {"text": "We have two training data settings with different sizes (see).", "labels": [], "entities": []}, {"text": "The small one consists of 316 document-pairs from Treebank (LDC2002E17).", "labels": [], "entities": [{"text": "Treebank (LDC2002E17)", "start_pos": 50, "end_pos": 71, "type": "DATASET", "confidence": 0.8164787441492081}]}, {"text": "For the large training data setting, we collected additional documentpairs from FBIS (LDC2003E14, Beijing part), Sinorama (LDC2002E58), and Xinhua News (LDC2002E18, document boundaries are kept in our sentence-aligner ( To evaluate translation quality, TIDES'02 Eval.", "labels": [], "entities": [{"text": "FBIS", "start_pos": 80, "end_pos": 84, "type": "DATASET", "confidence": 0.9176111817359924}, {"text": "Sinorama (LDC2002E58)", "start_pos": 113, "end_pos": 134, "type": "DATASET", "confidence": 0.8102962672710419}, {"text": "Xinhua News", "start_pos": 140, "end_pos": 151, "type": "DATASET", "confidence": 0.7960709929466248}, {"text": "TIDES'02 Eval", "start_pos": 253, "end_pos": 266, "type": "DATASET", "confidence": 0.4976240396499634}]}, {"text": "testis used as development set, and TIDES'03 Eval.", "labels": [], "entities": [{"text": "TIDES'03 Eval", "start_pos": 36, "end_pos": 49, "type": "DATASET", "confidence": 0.5348754078149796}]}, {"text": "testis used as the unseen test data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Training and Test Data Statistics", "labels": [], "entities": [{"text": "Training and Test Data", "start_pos": 10, "end_pos": 32, "type": "DATASET", "confidence": 0.4923744350671768}]}, {"text": " Table 2: Topic-specific translation lexicons are learned by a 3-topic BiTAM-1. The third lexicon (Topic-3) prefers to translate", "labels": [], "entities": [{"text": "Topic-specific translation lexicons", "start_pos": 10, "end_pos": 45, "type": "TASK", "confidence": 0.7930762966473898}]}, {"text": " Table 3: Three most distinctive topics are displayed. The English words for each topic are ranked according to p(e|z)", "labels": [], "entities": []}, {"text": " Table 4: Word Alignment Accuracy (F-measure) and Machine Translation Quality for BiTAM Models, comparing with IBM", "labels": [], "entities": [{"text": "Word Alignment Accuracy (F-measure)", "start_pos": 10, "end_pos": 45, "type": "METRIC", "confidence": 0.6807160278161367}, {"text": "Machine Translation", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.7261730879545212}]}]}