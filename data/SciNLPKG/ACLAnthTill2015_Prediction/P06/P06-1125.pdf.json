{"title": [{"text": "A Phonetic-Based Approach to Chinese Chat Text Normalization", "labels": [], "entities": [{"text": "Chinese Chat Text Normalization", "start_pos": 29, "end_pos": 60, "type": "TASK", "confidence": 0.5734691917896271}]}], "abstractContent": [{"text": "Chatting is a popular communication media on the Internet via ICQ, chat rooms, etc.", "labels": [], "entities": []}, {"text": "Chat language is different from natural language due to its anomalous and dynamic natures, which renders conventional NLP tools inapplicable.", "labels": [], "entities": []}, {"text": "The dynamic problem is enormously troublesome because it makes static chat language corpus outdated quickly in representing contemporary chat language.", "labels": [], "entities": []}, {"text": "To address the dynamic problem, we propose the phonetic mapping models to present mappings between chat terms and standard words via phonetic transcription , i.e. Chinese Pinyin in our case.", "labels": [], "entities": []}, {"text": "Different from character mappings, the pho-netic mappings can be constructed from available standard Chinese corpus.", "labels": [], "entities": []}, {"text": "To perform the task of dynamic chat language term normalization, we extend the source channel model by incorporating the phonetic mapping models.", "labels": [], "entities": [{"text": "dynamic chat language term normalization", "start_pos": 23, "end_pos": 63, "type": "TASK", "confidence": 0.6527224123477936}]}, {"text": "Experimental results show that this method is effective and stable in normalizing dynamic chat language terms.", "labels": [], "entities": [{"text": "normalizing dynamic chat language terms", "start_pos": 70, "end_pos": 109, "type": "TASK", "confidence": 0.8675108551979065}]}], "introductionContent": [{"text": "Internet facilitates online chatting by providing ICQ, chat rooms, BBS, email, blogs, etc.", "labels": [], "entities": []}, {"text": "Chat language becomes ubiquitous due to the rapid proliferation of Internet applications.", "labels": [], "entities": []}, {"text": "Chat language text appears frequently in chat logs of online education), customer relationship management, etc.", "labels": [], "entities": [{"text": "customer relationship management", "start_pos": 73, "end_pos": 105, "type": "TASK", "confidence": 0.7699604034423828}]}, {"text": "On the other hand, wed-based chat rooms and BBS systems are often abused by solicitors of terrorism, pornography and crime.", "labels": [], "entities": []}, {"text": "Thus there is asocial urgency to understand online chat language text.", "labels": [], "entities": []}, {"text": "Chat language is anomalous and dynamic.", "labels": [], "entities": []}, {"text": "Many words in chat text are anomalous to natural language.", "labels": [], "entities": []}, {"text": "Chat text comprises of ill-edited terms and anomalous writing styles.", "labels": [], "entities": []}, {"text": "We refer chat terms to the anomalous words in chat text.", "labels": [], "entities": []}, {"text": "The dynamic nature reflects that chat language changes more frequently than natural languages.", "labels": [], "entities": []}, {"text": "For example, many popular chat terms used in last year have been discarded and replaced by new ones in this year.", "labels": [], "entities": []}, {"text": "Details on these two features are provided in Section 2.", "labels": [], "entities": []}, {"text": "The anomalous nature of Chinese chat language is investigated in ().", "labels": [], "entities": []}, {"text": "Pattern matching and SVM are proposed to recognize the ambiguous chat terms.", "labels": [], "entities": [{"text": "Pattern matching", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8366302251815796}]}, {"text": "Experiments show that F-1 measure of recognition reaches 87.1% with the biggest training set.", "labels": [], "entities": [{"text": "F-1 measure of recognition", "start_pos": 22, "end_pos": 48, "type": "METRIC", "confidence": 0.7948409542441368}]}, {"text": "However, it is also disclosed that quality of both methods drops significantly when training set is older.", "labels": [], "entities": []}, {"text": "The dynamic nature is investigated in (, in which an error-driven approach is proposed to detect chat terms in dynamic Chinese chat terms by combining standard Chinese corpora and NIL corpus ().", "labels": [], "entities": [{"text": "NIL corpus", "start_pos": 180, "end_pos": 190, "type": "DATASET", "confidence": 0.8246324062347412}]}, {"text": "Language texts in standard Chinese corpora are used as negative samples and chat text pieces in the NIL corpus as positive ones.", "labels": [], "entities": [{"text": "NIL corpus", "start_pos": 100, "end_pos": 110, "type": "DATASET", "confidence": 0.9377352893352509}]}, {"text": "The approach calculates confidence and entropy values for the input text.", "labels": [], "entities": [{"text": "confidence", "start_pos": 24, "end_pos": 34, "type": "METRIC", "confidence": 0.9891976714134216}]}, {"text": "Then threshold values estimated from the training data are applied to identify chat terms.", "labels": [], "entities": []}, {"text": "Performance equivalent to the methods in existence is achieved consistently.", "labels": [], "entities": []}, {"text": "However, the issue of normalization is addressed in their work.", "labels": [], "entities": [{"text": "normalization", "start_pos": 22, "end_pos": 35, "type": "TASK", "confidence": 0.9801764488220215}]}, {"text": "Dictionary based chat term normalization is not a good solution because the dictionary cannot cover new chat terms appearing in the dynamic chat language.", "labels": [], "entities": [{"text": "Dictionary based chat term normalization", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.5554659426212311}]}, {"text": "In the early stage of this work, a method based on source channel model is implemented for chat term normalization.", "labels": [], "entities": [{"text": "chat term normalization", "start_pos": 91, "end_pos": 114, "type": "TASK", "confidence": 0.7437846064567566}]}, {"text": "The problem we encounter is addressed as follows.", "labels": [], "entities": []}, {"text": "To deal with the anomalous nature, a chat language corpus is constructed with chat text collected from the Internet.", "labels": [], "entities": []}, {"text": "How-ever, the dynamic nature renders the static corpus outdated quickly in representing contemporary chat language.", "labels": [], "entities": []}, {"text": "The dilemma is that timely chat language corpus is nearly impossible to obtain.", "labels": [], "entities": []}, {"text": "The sparse data problem and dynamic problem become crucial in chat term normalization.", "labels": [], "entities": [{"text": "chat term normalization", "start_pos": 62, "end_pos": 85, "type": "TASK", "confidence": 0.7532928784688314}]}, {"text": "We believe that some information beyond character should be discovered to help addressing these two problems.", "labels": [], "entities": []}, {"text": "Observation on chat language text reveals that most Chinese chat terms are created via phonetic transcription, i.e. Chinese Pinyin in our case.", "labels": [], "entities": []}, {"text": "A more exciting finding is that the phonetic mappings between standard Chinese words and chat terms remain stable in dynamic chat language.", "labels": [], "entities": []}, {"text": "We are thus enlightened to make use of the phonetic mapping models, instead of character mapping models, to design a normalization algorithm to translate chat terms to their standard counterparts.", "labels": [], "entities": []}, {"text": "Different from the character mapping models constructed from chat language corpus, the phonetic mapping models are learned from a standard language corpus because they attempt to model mappings probabilities between any two Chinese characters in terms of phonetic transcription.", "labels": [], "entities": []}, {"text": "Now the sparse data problem can thus be appropriately addressed.", "labels": [], "entities": []}, {"text": "To normalize the dynamic chat language text, we extend the source channel model by incorporating phonetic mapping models.", "labels": [], "entities": []}, {"text": "We believe that the dynamic problem can be resolved effectively and robustly because the phonetic mapping models are stable.", "labels": [], "entities": []}, {"text": "The remaining sections of this paper are organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, features of chat language are analyzed with evidences.", "labels": [], "entities": []}, {"text": "In Section 3, we present methodology and problems of the source channel model approach to chat term normalization.", "labels": [], "entities": [{"text": "chat term normalization", "start_pos": 90, "end_pos": 113, "type": "TASK", "confidence": 0.7470022241274515}]}, {"text": "In Section 4, we present definition, justification, formalization and parameter estimation for the phonetic mapping model.", "labels": [], "entities": []}, {"text": "In Section 5, we present the extended source channel model that incorporates the phonetic mapping models.", "labels": [], "entities": []}, {"text": "Experiments and results are presented in Section 6 as well as discussions and error analysis.", "labels": [], "entities": []}, {"text": "We conclude this paper in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate two tasks in our experiments, i.e. recognition and normalization.", "labels": [], "entities": [{"text": "recognition", "start_pos": 47, "end_pos": 58, "type": "TASK", "confidence": 0.9481847882270813}]}, {"text": "In recognition, we use precision (p), recall (r) and f-1 measure (f) defined as follows.", "labels": [], "entities": [{"text": "precision (p)", "start_pos": 23, "end_pos": 36, "type": "METRIC", "confidence": 0.9330055564641953}, {"text": "recall (r)", "start_pos": 38, "end_pos": 48, "type": "METRIC", "confidence": 0.9485100656747818}, {"text": "f-1 measure (f)", "start_pos": 53, "end_pos": 68, "type": "METRIC", "confidence": 0.7816109955310822}]}, {"text": "where x denotes the number of true positives, y the false positives and z the true negatives.", "labels": [], "entities": []}, {"text": "For normalization, we use accuracy (a), which is commonly accepted by machine translation researchers as a standard evaluation criterion.", "labels": [], "entities": [{"text": "normalization", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.9780673384666443}, {"text": "accuracy (a)", "start_pos": 26, "end_pos": 38, "type": "METRIC", "confidence": 0.942354828119278}, {"text": "machine translation", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.7732624709606171}]}, {"text": "Every output of the normalization methods is compared to the standard answer so that normalization accuracy on each test set is produced.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9011492133140564}]}, {"text": "In this experiment we investigate on quality of XSCM and SCM using same size-varying training data.", "labels": [], "entities": []}, {"text": "We intend to prove that chat language is dynamic and phonetic mapping models used in XSCM are helpful in addressing the dynamic problem.", "labels": [], "entities": []}, {"text": "As no standard Chinese corpus is used in this experiment, we use standard Chinese text in chat language corpora to construct phonetic mapping models in XSCM.", "labels": [], "entities": []}, {"text": "This violates the basic assumption that the phonetic mapping models should be constructed with standard Chinese corpus.", "labels": [], "entities": []}, {"text": "So results in this experiment should be used only for comparison purpose.", "labels": [], "entities": []}, {"text": "It would be unfair to make any conclusion on general performance of XSCM method based on results in this experiments.", "labels": [], "entities": []}, {"text": "We train the two methods with each of the six chat language corpora, i.e. C#1 ~ C#6 and test them on six time-varying test sets, i.e. T#1 ~ T#6.", "labels": [], "entities": []}, {"text": "F-1 measure values produced by SCM and XSCM in this experiment are present in.", "labels": [], "entities": [{"text": "F-1 measure", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9253335297107697}]}, {"text": "Three tendencies should be pointed out according to.", "labels": [], "entities": []}, {"text": "The first tendency is that f-1 measure in both methods drops on time-varying test sets (see) using same training chat language corpora.", "labels": [], "entities": []}, {"text": "For example, both SCM and XSCM perform best on the earliest test set T#1 and worst on newest T#4.", "labels": [], "entities": []}, {"text": "We find that the quality drop is caused by the dynamic nature of chat language.", "labels": [], "entities": []}, {"text": "It is thus revealed that chat language is indeed dynamic.", "labels": [], "entities": []}, {"text": "We also find that quality of XSCM drops less than that of SCM.", "labels": [], "entities": []}, {"text": "This proves that phonetic mapping models used in XSCM are helpful in addressing the dynamic problem.", "labels": [], "entities": []}, {"text": "However, quality of XSCM in this experiment still drops by 0.05 on the six time-varying test sets.", "labels": [], "entities": []}, {"text": "This is because chat language text corpus is used as standard language corpus to model the phonetic mappings.", "labels": [], "entities": []}, {"text": "Phonetic mapping models constructed with chat language corpus are far from sufficient.", "labels": [], "entities": [{"text": "Phonetic mapping", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7660991847515106}]}, {"text": "We will investigate in Experiment-II to prove that stable phonetic mapping models can be constructed with real standard language corpus, i.e. CNGIGA.", "labels": [], "entities": []}, {"text": "The second tendency is f-1 measure of both methods on same test sets drops when trained with size-varying chat language corpora.", "labels": [], "entities": []}, {"text": "For example, both SCM and XSCM perform best on the largest training chat language corpus C#6 and worst on the smallest corpus C#1.", "labels": [], "entities": []}, {"text": "This tendency reveals that both methods favor bigger training chat language corpus.", "labels": [], "entities": []}, {"text": "So extending the chat language corpus should be one choice to improve quality of chat language term normalization.", "labels": [], "entities": [{"text": "chat language term normalization", "start_pos": 81, "end_pos": 113, "type": "TASK", "confidence": 0.6530563607811928}]}, {"text": "The last tendency is found on quality gap between SCM and XSCM.", "labels": [], "entities": [{"text": "quality", "start_pos": 30, "end_pos": 37, "type": "METRIC", "confidence": 0.964740514755249}]}, {"text": "We calculate f-1 measure gaps between two methods using same training sets on same test sets (see.", "labels": [], "entities": []}, {"text": "Then the tendency is made clear.", "labels": [], "entities": []}, {"text": "Quality gap between SCM and XSCM becomes bigger when test set becomes newer.", "labels": [], "entities": [{"text": "Quality", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9745657444000244}]}, {"text": "On the oldest test set T#1, the gap is smallest, while on the newest test set T#6, the gap reaches biggest value, i.e. around 0.09.", "labels": [], "entities": []}, {"text": "This tendency reveals excellent capability of XSCM in addressing dynamic problem using the phonetic mapping models.", "labels": [], "entities": []}, {"text": "In this experiment we investigate on quality of SCM and XSCM when areal standard Chinese language corpus is incorporated.", "labels": [], "entities": []}, {"text": "We want to prove that the dynamic problem can be addressed effectively and robustly when CNGIGA is used as standard Chinese corpus.", "labels": [], "entities": []}, {"text": "We train the two methods on CNGIGA and each of the six chat language corpora, i.e. C#1 ~ C#6.", "labels": [], "entities": []}, {"text": "We then test the two methods on six timevarying test sets, i.e. T#1 ~ T#6.", "labels": [], "entities": [{"text": "T#1", "start_pos": 64, "end_pos": 67, "type": "METRIC", "confidence": 0.8823447823524475}]}, {"text": "F-1 measure values produced by SCM and XSCM in this experiment are present in  Three observations are conducted on our results.", "labels": [], "entities": [{"text": "F-1 measure", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9188343286514282}]}, {"text": "First, according to, f-1 measure of SCM with same training chat language corpora drops on time-varying test sets, but XSCM produces much better f-1 measure consistently using CNGIGA and same training chat language corpora (see).", "labels": [], "entities": [{"text": "SCM", "start_pos": 36, "end_pos": 39, "type": "TASK", "confidence": 0.9667851328849792}]}, {"text": "This proves that phonetic mapping models are helpful in XSCM method.", "labels": [], "entities": []}, {"text": "The phonetic mapping models contribute in two aspects.", "labels": [], "entities": [{"text": "phonetic mapping", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.7549574971199036}]}, {"text": "On the one hand, they improve quality of chat term normalization on individual test sets.", "labels": [], "entities": [{"text": "chat term normalization", "start_pos": 41, "end_pos": 64, "type": "TASK", "confidence": 0.6009654502073923}]}, {"text": "On the other hand, satisfactory robustness is achieved consistently.", "labels": [], "entities": []}, {"text": "The second observation is conducted on phonetic mapping models constructed with CNGIGA.", "labels": [], "entities": [{"text": "CNGIGA", "start_pos": 80, "end_pos": 86, "type": "DATASET", "confidence": 0.865323007106781}]}, {"text": "We find that 4,056,766 phonetic mapping models are constructed in this experiment, while only 1,303,227 models are constructed with NIL corpus in Experiment I. This reveals that coverage of standard Chinese corpus is crucial to phonetic mapping modeling.", "labels": [], "entities": [{"text": "NIL corpus", "start_pos": 132, "end_pos": 142, "type": "DATASET", "confidence": 0.8579263091087341}, {"text": "phonetic mapping modeling", "start_pos": 228, "end_pos": 253, "type": "TASK", "confidence": 0.7988230288028717}]}, {"text": "We then compare two character lists constructed with two corpora.", "labels": [], "entities": []}, {"text": "The 100 characters most frequently used in NIL corpus are rather different from those extracted from CNGIGA.", "labels": [], "entities": [{"text": "NIL corpus", "start_pos": 43, "end_pos": 53, "type": "DATASET", "confidence": 0.8869204521179199}]}, {"text": "We can conclude that phonetic mapping models should be constructed with a sound corpus that can represent standard language.", "labels": [], "entities": []}, {"text": "The last observation is conducted on f-1 measure achieved by same methods on same test sets using size-varying training chat language corpora.", "labels": [], "entities": []}, {"text": "Both methods produce best f-1 measure with biggest training chat language corpus C#6 on same test sets.", "labels": [], "entities": []}, {"text": "This again proves that bigger training chat language corpus could be helpful to improve quality of chat language term normalization.", "labels": [], "entities": [{"text": "chat language term normalization", "start_pos": 99, "end_pos": 131, "type": "TASK", "confidence": 0.6172976270318031}]}, {"text": "One question might be asked whether quality of XSCM converges on size of the training chat language corpus.", "labels": [], "entities": []}, {"text": "This question remains open due to limited chat language corpus available to us.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Chat term re-occurring rates. The rows  represent the earlier chat term sets and the col- umns the later ones.  The surprising finding in Table 1 is that 29.4%  of chat terms are replaced with new ones within  two years and about 18.5% within one year. The  changing speed is much faster than that in stan- dard language. This thus proves that chat text is  dynamic indeed. The dynamic nature renders the  static corpus outdated quickly. It poses a chal- lenging issue on chat language processing.", "labels": [], "entities": []}, {"text": " Table 2. Chat term distribution in terms of map- ping type.", "labels": [], "entities": []}, {"text": " Table 3. F-1 measure by SCM and XSCM on six  test sets with six chat language corpora.", "labels": [], "entities": []}, {"text": " Table 4. F-1 measure by SCM and XSCM on six  test sets with six chat language corpora and  CNGIGA.", "labels": [], "entities": []}]}