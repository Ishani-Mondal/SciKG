{"title": [{"text": "Translating HPSG-style Outputs of a Robust Parser into Typed Dynamic Logic", "labels": [], "entities": []}], "abstractContent": [{"text": "The present paper proposes a method by which to translate outputs of a robust HPSG parser into semantic representations of Typed Dynamic Logic (TDL), a dynamic plural semantics defined in typed lambda calculus.", "labels": [], "entities": []}, {"text": "With its higher-order representations of contexts , TDL analyzes and describes the inherently inter-sentential nature of quantification and anaphora in a strictly lexicalized and compositional manner.", "labels": [], "entities": []}, {"text": "The present study shows that the proposed translation method successfully combines robustness and descriptive adequacy of contemporary semantics.", "labels": [], "entities": [{"text": "translation", "start_pos": 42, "end_pos": 53, "type": "TASK", "confidence": 0.9741386771202087}]}, {"text": "The present implementation achieves high coverage, approximately 90%, for the real text of the Penn Treebank corpus.", "labels": [], "entities": [{"text": "coverage", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9833106994628906}, {"text": "Penn Treebank corpus", "start_pos": 95, "end_pos": 115, "type": "DATASET", "confidence": 0.9930063486099243}]}], "introductionContent": [{"text": "Robust parsing technology is one result of the recent fusion between symbolic and statistical approaches in natural language processing and has been applied to tasks such as information extraction, information retrieval and machine translation).", "labels": [], "entities": [{"text": "Robust parsing", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8163553476333618}, {"text": "information extraction", "start_pos": 174, "end_pos": 196, "type": "TASK", "confidence": 0.7865313589572906}, {"text": "information retrieval", "start_pos": 198, "end_pos": 219, "type": "TASK", "confidence": 0.8013885617256165}, {"text": "machine translation", "start_pos": 224, "end_pos": 243, "type": "TASK", "confidence": 0.7609888315200806}]}, {"text": "However, reflecting the field boundary and unestablished interfaces between syntax and semantics informal theory of grammar, this fusion has achieved less in semantics than in syntax.", "labels": [], "entities": []}, {"text": "For example, a system that translates the output of a robust CCG parser into semantic representations has been developed ().", "labels": [], "entities": []}, {"text": "While its corpus-oriented parser attained high coverage with respect to real text, the expressive power of the resulting semantic representations is confined to first-order predicate logic.", "labels": [], "entities": []}, {"text": "The more elaborate tasks tied to discourse information and plurality, such as resolution of anaphora antecedent, scope ambiguity, presupposition, topic and focus, are required to refer to 'deeper' semantic structures, such as dynamic semantics.", "labels": [], "entities": [{"text": "resolution of anaphora antecedent", "start_pos": 78, "end_pos": 111, "type": "TASK", "confidence": 0.8240699172019958}]}, {"text": "However, most dynamic semantic theories are not equipped with large-scale syntax that covers more than a small fragment of target languages.", "labels": [], "entities": []}, {"text": "One of a few exceptions is Minimal Recursion Semantics (MRS) (), which is compatible with largescale HPSG syntax and has affinities with UDRS.", "labels": [], "entities": [{"text": "Minimal Recursion Semantics (MRS)", "start_pos": 27, "end_pos": 60, "type": "TASK", "confidence": 0.7315966983636221}]}, {"text": "For real text, however, its implementation, as in the case of the ERG parser), restricts its target to the static fragment of MRS and yet has a lower coverage than corpus-oriented parsers (Baldwin, to appear).", "labels": [], "entities": []}, {"text": "The lack of transparency between syntax and discourse semantics appears to have created a tension between the robustness of syntax and the descriptive adequacy of semantics.", "labels": [], "entities": []}, {"text": "In the present paper, we will introduce a robust method to obtain dynamic semantic representations based on Typed Dynamic Logic (TDL)) from real text by translating the outputs of a robust HPSG parser ().", "labels": [], "entities": []}, {"text": "Typed Dynamic Logic is a dynamic plural semantics that formalizes the structure underlying the semantic interactions between quantification, plurality, bound variable/E-type anaphora: Propositions of TDL ( and presuppositions.", "labels": [], "entities": []}, {"text": "All of this complex discourse/plurality-related information is encapsulated within higher-order structures in TDL, and the analysis remains strictly lexical and compositional, which makes its interface with syntax transparent and straightforward.", "labels": [], "entities": []}, {"text": "This is a significant advantage for achieving robustness in natural language processing.", "labels": [], "entities": []}, {"text": "shows a number of propositions defined in, including atomic predicate, negation, conjunction, and anaphoric expression.", "labels": [], "entities": []}, {"text": "Typed Dynamic Logic is described in typed lambda calculus (G\u00f6del's System T) with four ground types: e(entity), i(index), n(natural number), and t(truth).", "labels": [], "entities": [{"text": "Typed Dynamic Logic", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7639033595720927}]}, {"text": "While assignment functions in static logic are functions in metalanguage from type e variables (in the case of first-order logic) to objects in the domain D e , assignment functions in TDL are functions in object-language from indices to entities.", "labels": [], "entities": []}, {"text": "Typed Dynamic Logic defines the notion context as a set of assignment functions (an object of type (i 7 \u2192 e) 7 \u2192 t) and a proposition as a function from context to context (an object of type ((i 7 \u2192 e) 7 \u2192 t) 7 \u2192 (i 7 \u2192 e) 7 \u2192 t).", "labels": [], "entities": [{"text": "Typed Dynamic Logic", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7487282951672872}]}, {"text": "The conjunctions of two propositions are then defined as composite functions thereof.", "labels": [], "entities": []}, {"text": "This setting conforms to the view of \"propositions as information flow\", which is widely accepted in dynamic semantics.", "labels": [], "entities": []}], "datasetContent": [{"text": "The number of rules we have implemented is shown in.", "labels": [], "entities": []}, {"text": "We used the Penn Treebank (Marcus, 1994) Section 22 (1,527 sentences) to develop and evaluate the proposed method and Section 23 (2,144 sentences) as the final test set.", "labels": [], "entities": [{"text": "Penn Treebank (Marcus, 1994) Section 22", "start_pos": 12, "end_pos": 51, "type": "DATASET", "confidence": 0.9646949436929491}]}, {"text": "We measured the coverage of the construction of TDL semantic representations, in the manner described in a previous study).", "labels": [], "entities": [{"text": "TDL semantic representations", "start_pos": 48, "end_pos": 76, "type": "TASK", "confidence": 0.6688169638315836}]}, {"text": "Although the best method for strictly evaluating the proposed method is to measure the agreement between the obtained semantic representations and the intuitions of the speaker/writer of the texts, this type of evaluation could not be performed because of insufficient resources.", "labels": [], "entities": []}, {"text": "Instead, we measured the rate of successful derivations as an indicator of the coverage of the proposed system.", "labels": [], "entities": []}, {"text": "The sentences in the test set were parsed by a robust HPSG parser (), and HPSG parse trees were successfully generated for 2,122 (98.9%) sentences.", "labels": [], "entities": [{"text": "HPSG", "start_pos": 54, "end_pos": 58, "type": "DATASET", "confidence": 0.8974403142929077}]}, {"text": "The proposed method was then applied to these parse trees.", "labels": [], "entities": []}, {"text": "shows that 88.3% of the un-  seen sentences are assigned TDL semantic representations.", "labels": [], "entities": []}, {"text": "Although this number is slightly less than 92.3%, as reported by, it seems reasonable to say that the proposed method attained a relatively high coverage, given the expressive power of TDL.", "labels": [], "entities": [{"text": "coverage", "start_pos": 145, "end_pos": 153, "type": "METRIC", "confidence": 0.9869170188903809}]}, {"text": "The construction of TDL semantic representations failed for 11.7% of the sentences.", "labels": [], "entities": [{"text": "TDL semantic representations", "start_pos": 20, "end_pos": 48, "type": "TASK", "confidence": 0.6273774107297262}]}, {"text": "We classified the causes of the failure into two types.", "labels": [], "entities": []}, {"text": "One of which is application failure of the assignment rules (assignment failure); that is, no assignment rules are applied to a number of HPSG lexical items, and so no TDLESs are assigned to these items.", "labels": [], "entities": []}, {"text": "The other is application failure of the composition rules (composition failure).", "labels": [], "entities": []}, {"text": "In this case, a type mismatch occurred in the composition, and so a TDLES was not derived.", "labels": [], "entities": [{"text": "TDLES", "start_pos": 68, "end_pos": 73, "type": "METRIC", "confidence": 0.7072728276252747}]}, {"text": "shows further classification of the causes categorized into the two classes.", "labels": [], "entities": []}, {"text": "We manually investigated all of the failures in the development set.", "labels": [], "entities": []}, {"text": "Assignment failures are caused by three factors.", "labels": [], "entities": [{"text": "Assignment", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.9371629953384399}]}, {"text": "Most assignment failures occurred due to the limitation in the number of the assignment rules (as indicated by \"unimplemented words\" in the table).", "labels": [], "entities": []}, {"text": "In this experiment, we did not implement rules for infrequent HPSG lexical items.", "labels": [], "entities": []}, {"text": "We believe that this type of failure will be resolved by increasing the number of  assignment rules.", "labels": [], "entities": []}, {"text": "The second factor in the table, \"TDL unsupported words\", refers to expressions that are not covered by the current theory of TDL.", "labels": [], "entities": []}, {"text": "In order to resolve this type of failure, the development of TDL is required.", "labels": [], "entities": []}, {"text": "The third factor, \"nonlinguistic HPSG lexical items\" includes a small number of cases in which TDLESs are not assigned to the words that are categorized as nonlinguistic syntactic categories by the HPSG parser.", "labels": [], "entities": []}, {"text": "This problem is caused by ill-formed outputs of the parser.", "labels": [], "entities": []}, {"text": "The composition failures can be further classified into three classes according to their causative factors.", "labels": [], "entities": []}, {"text": "The first factor is the existence of HPSG schemata for which we have not yet implemented composition rules.", "labels": [], "entities": []}, {"text": "These failures will be fixed by extending of the definition of our composition rules.", "labels": [], "entities": []}, {"text": "The second factor is type mismatches due to the unintended assignments of TDLESs to lexical items.", "labels": [], "entities": []}, {"text": "We need to further elaborate the assignment rules in order to deal with this problem.", "labels": [], "entities": []}, {"text": "The third factor is parse trees that are linguistically invalid.", "labels": [], "entities": []}, {"text": "The error analysis given above indicates that we can further increase the coverage through the improvement of the assignment/composition rules.", "labels": [], "entities": [{"text": "coverage", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.971872091293335}]}, {"text": "shows an example of the output fora sentence in the development set.", "labels": [], "entities": []}, {"text": "The variables $1, ..", "labels": [], "entities": []}, {"text": ",$11 are indices that represent entities, events and situations.", "labels": [], "entities": []}, {"text": "For example, $3 represents a situation and $2 represents the lecturing event that exists in $3.", "labels": [], "entities": []}, {"text": "past($3) requires that the situation is past.", "labels": [], "entities": []}, {"text": "agent($2,$1) requires that the entity $1 is the agent of $2.", "labels": [], "entities": []}, {"text": "content($2,$4) requires that $4 (as a set of possible worlds) is the content of $2.", "labels": [], "entities": []}, {"text": "be($11,$4) refers to $4.", "labels": [], "entities": []}, {"text": "Finally, every($6)[ball($6,$4)][see($7,$4) ...] represents a generalized quantifier \"every ball\".", "labels": [], "entities": []}, {"text": "The index $6 serves as an antecedent both for bound-variable anaphora within its scope and for E-type anaphora outside its scope.", "labels": [], "entities": []}, {"text": "The entities that correspond to the two occurrences of \"you\" are represented by $8 and $5.", "labels": [], "entities": []}, {"text": "Their unification is left as an anaphora resolution task that can be easily solved by existing statistical or rule-based methods, given the structural information of the TDL semantic representation.", "labels": [], "entities": [{"text": "anaphora resolution task", "start_pos": 32, "end_pos": 56, "type": "TASK", "confidence": 0.8438611229260763}]}], "tableCaptions": [{"text": " Table 1: Number of implemented rules", "labels": [], "entities": []}, {"text": " Table 2: Coverage with respect to the test set", "labels": [], "entities": []}, {"text": " Table 3: Error analysis: the development set", "labels": [], "entities": [{"text": "Error analysis", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.8581040501594543}]}]}