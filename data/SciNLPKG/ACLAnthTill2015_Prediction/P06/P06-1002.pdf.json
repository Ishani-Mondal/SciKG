{"title": [{"text": "Going Beyond AER: An Extensive Analysis of Word Alignments and Their Impact on MT", "labels": [], "entities": [{"text": "AER", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.9386634230613708}, {"text": "Word Alignments", "start_pos": 43, "end_pos": 58, "type": "TASK", "confidence": 0.6626808345317841}, {"text": "MT", "start_pos": 79, "end_pos": 81, "type": "TASK", "confidence": 0.9302479028701782}]}], "abstractContent": [{"text": "This paper presents an extensive evaluation of five different alignments and investigates their impact on the corresponding MT system output.", "labels": [], "entities": [{"text": "MT", "start_pos": 124, "end_pos": 126, "type": "TASK", "confidence": 0.9851377010345459}]}, {"text": "We introduce new measures for intrinsic evaluations and examine the distribution of phrases and untranslated words during decoding to identify which characteristics of different alignments affect translation.", "labels": [], "entities": []}, {"text": "We show that precision-oriented alignments yield better MT output (translating more words and using longer phrases) than recall-oriented alignments.", "labels": [], "entities": [{"text": "precision-oriented", "start_pos": 13, "end_pos": 31, "type": "METRIC", "confidence": 0.9642479419708252}, {"text": "MT", "start_pos": 56, "end_pos": 58, "type": "TASK", "confidence": 0.9883930087089539}]}], "introductionContent": [{"text": "Word alignments area by-product of statistical machine translation (MT) and play a crucial role in MT performance.", "labels": [], "entities": [{"text": "Word alignments", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.6961334347724915}, {"text": "statistical machine translation (MT)", "start_pos": 35, "end_pos": 71, "type": "TASK", "confidence": 0.7574769010146459}, {"text": "MT", "start_pos": 99, "end_pos": 101, "type": "TASK", "confidence": 0.9950892329216003}]}, {"text": "In recent years, researchers have proposed several algorithms to generate word alignments.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 74, "end_pos": 89, "type": "TASK", "confidence": 0.6940222978591919}]}, {"text": "However, evaluating word alignments is difficult because even humans have difficulty performing this task.", "labels": [], "entities": [{"text": "evaluating word alignments", "start_pos": 9, "end_pos": 35, "type": "TASK", "confidence": 0.662185808022817}]}, {"text": "The state-of-the art evaluation metricalignment error rate (AER)-attempts to balance the precision and recall scores at the level of alignment links).", "labels": [], "entities": [{"text": "metricalignment error rate (AER)-", "start_pos": 32, "end_pos": 65, "type": "METRIC", "confidence": 0.900217076142629}, {"text": "precision", "start_pos": 89, "end_pos": 98, "type": "METRIC", "confidence": 0.9992021918296814}, {"text": "recall", "start_pos": 103, "end_pos": 109, "type": "METRIC", "confidence": 0.9931271076202393}]}, {"text": "Other metrics assess the impact of alignments externally, e.g., different alignments are tested by comparing the corresponding MT outputs using automated evaluation metrics (e.g., BLEU () or METEOR ().", "labels": [], "entities": [{"text": "MT", "start_pos": 127, "end_pos": 129, "type": "TASK", "confidence": 0.9542664289474487}, {"text": "BLEU", "start_pos": 180, "end_pos": 184, "type": "METRIC", "confidence": 0.9979161620140076}, {"text": "METEOR", "start_pos": 191, "end_pos": 197, "type": "METRIC", "confidence": 0.9381507039070129}]}, {"text": "However, these studies showed that AER and BLEU do not correlate well).", "labels": [], "entities": [{"text": "AER", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.9992740750312805}, {"text": "BLEU", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9985777139663696}]}, {"text": "Despite significant AER improvements achieved by several researchers, the improvements in BLEU scores are insignificant or, at best, small.", "labels": [], "entities": [{"text": "AER", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.9985393285751343}, {"text": "BLEU scores", "start_pos": 90, "end_pos": 101, "type": "METRIC", "confidence": 0.9724444448947906}]}, {"text": "This paper demonstrates the difficulty in assessing whether alignment quality makes a difference in MT performance.", "labels": [], "entities": [{"text": "MT", "start_pos": 100, "end_pos": 102, "type": "TASK", "confidence": 0.9957208037376404}]}, {"text": "We describe the impact of certain alignment characteristics on MT performance but also identify several alignment-related factors that impact MT performance regardless of the quality of the initial alignments.", "labels": [], "entities": [{"text": "MT", "start_pos": 63, "end_pos": 65, "type": "TASK", "confidence": 0.9958315491676331}, {"text": "MT", "start_pos": 142, "end_pos": 144, "type": "TASK", "confidence": 0.995612621307373}]}, {"text": "In so doing, we begin to answer long-standing questions about the value of alignment in the context of MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 103, "end_pos": 105, "type": "TASK", "confidence": 0.9845787286758423}]}, {"text": "We first evaluate 5 different word alignments intrinsically, using: (1) community-standard metrics-precision, recall and AER; and (2) anew measure called consistent phrase error rate (CPER).", "labels": [], "entities": [{"text": "recall", "start_pos": 110, "end_pos": 116, "type": "METRIC", "confidence": 0.9994173049926758}, {"text": "AER", "start_pos": 121, "end_pos": 124, "type": "METRIC", "confidence": 0.9980831146240234}, {"text": "consistent phrase error rate (CPER)", "start_pos": 154, "end_pos": 189, "type": "METRIC", "confidence": 0.7822884236063276}]}, {"text": "Next, we observe the impact of different alignments on MT performance.", "labels": [], "entities": [{"text": "MT", "start_pos": 55, "end_pos": 57, "type": "TASK", "confidence": 0.9955074787139893}]}, {"text": "We present BLEU scores on a phrase-based MT system, Pharaoh), using five different alignments to extract phrases.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.9984592199325562}]}, {"text": "We investigate the impact of different settings for phrase extraction, lexical weighting, maximum phrase length and training data.", "labels": [], "entities": [{"text": "phrase extraction", "start_pos": 52, "end_pos": 69, "type": "TASK", "confidence": 0.8921713531017303}]}, {"text": "Finally, we present a quantitative analysis of which phrases are chosen during the actual decoding process and show how the distribution of the phrases differ from one alignment into another.", "labels": [], "entities": []}, {"text": "Our experiments show that precision-oriented alignments yield better phrases for MT than recalloriented alignments.", "labels": [], "entities": [{"text": "MT", "start_pos": 81, "end_pos": 83, "type": "TASK", "confidence": 0.9948028326034546}]}, {"text": "Specifically, they cover a higher percentage of our test sets and result in fewer untranslated words and selection of longer phrases during decoding.", "labels": [], "entities": []}, {"text": "The next section describes work related to our alignment evaluation approach.", "labels": [], "entities": [{"text": "alignment evaluation", "start_pos": 47, "end_pos": 67, "type": "TASK", "confidence": 0.9880299866199493}]}, {"text": "Following this we outline different intrinsic evaluation measures of alignment and we propose anew measure to evaluate word alignments within phrase-based MT framework.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 119, "end_pos": 134, "type": "TASK", "confidence": 0.7191109955310822}, {"text": "MT", "start_pos": 155, "end_pos": 157, "type": "TASK", "confidence": 0.8320581316947937}]}, {"text": "We then present several experiments to measure the impact of different word alignments on a phrase-based MT system, and investigate how different alignments change the phrase selection in the same MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 105, "end_pos": 107, "type": "TASK", "confidence": 0.8825670480728149}]}], "datasetContent": [{"text": "Our goal is to compare different alignments and to investigate how their characteristics affect the MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 100, "end_pos": 102, "type": "TASK", "confidence": 0.9799439311027527}]}, {"text": "We evaluate alignments in terms of precision, recall, alignment error rate (AER), and anew measure called consistent phrase error rate (CPER).", "labels": [], "entities": [{"text": "precision", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9995899796485901}, {"text": "recall", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.999460756778717}, {"text": "alignment error rate (AER)", "start_pos": 54, "end_pos": 80, "type": "METRIC", "confidence": 0.9588800966739655}, {"text": "consistent phrase error rate (CPER)", "start_pos": 106, "end_pos": 141, "type": "METRIC", "confidence": 0.8190957435539791}]}, {"text": "We focus on 5 different alignments obtained by combining two uni-directional alignments.", "labels": [], "entities": []}, {"text": "Each uni-directional alignment is the result of running GIZA++) in one of two directions (source-to-target and vice versa) with default configurations.", "labels": [], "entities": []}, {"text": "The combined alignments that are used in this paper are as follows: 1.", "labels": [], "entities": []}, {"text": "Union of both directions (S U ), 2.", "labels": [], "entities": []}, {"text": "Intersection of both directions (S I ), 3.", "labels": [], "entities": [{"text": "Intersection", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.949357271194458}]}, {"text": "A heuristic based combination technique called grow-diag-final (S G ), which is the default alignment combination heuristic employed in Pharaoh), 4-5.", "labels": [], "entities": [{"text": "grow-diag-final (S G )", "start_pos": 47, "end_pos": 69, "type": "METRIC", "confidence": 0.7770577669143677}]}, {"text": "Two supervised alignment combination techniques (S A and S B ) using 2 and 4 input alignments as described in).", "labels": [], "entities": []}, {"text": "This paper examines the impact of alignments according to their orientation toward precision or recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 83, "end_pos": 92, "type": "METRIC", "confidence": 0.998741090297699}, {"text": "recall", "start_pos": 96, "end_pos": 102, "type": "METRIC", "confidence": 0.9613146185874939}]}, {"text": "Among the five alignments above, S U and S G are recall-oriented while the other three are precision-oriented.", "labels": [], "entities": [{"text": "recall-oriented", "start_pos": 49, "end_pos": 64, "type": "METRIC", "confidence": 0.998461127281189}, {"text": "precision-oriented", "start_pos": 91, "end_pos": 109, "type": "METRIC", "confidence": 0.9967203736305237}]}, {"text": "S B is an improved version of SA which attempts to increase recall without a significant sacrifice in precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.9992493987083435}, {"text": "precision", "start_pos": 102, "end_pos": 111, "type": "METRIC", "confidence": 0.9983483552932739}]}, {"text": "Manually aligned data from two language pairs are used in our intrinsic evaluations using the five combinations above.", "labels": [], "entities": []}, {"text": "A summary of the training and test data is presented in.", "labels": [], "entities": []}, {"text": "Our gold standard for each language pair is a manually aligned corpus.", "labels": [], "entities": []}, {"text": "English-Chinese an-notations distinguish between sure and probable alignment links, but English-Arabic annotations do not.", "labels": [], "entities": []}, {"text": "The details of how the annotations are done can be found in () and (Ittycheriah and).", "labels": [], "entities": []}, {"text": "presents the precision, recall, and AER for 5 different alignments on 2 language pairs.", "labels": [], "entities": [{"text": "precision", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9996919631958008}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9991611242294312}, {"text": "AER", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.99965500831604}]}, {"text": "For each of these metrics, a different system achieves the best score -respectively, these are S I , S U , and S B . S U and S G yield low precision, high recall alignments.", "labels": [], "entities": [{"text": "precision", "start_pos": 139, "end_pos": 148, "type": "METRIC", "confidence": 0.9909281134605408}, {"text": "recall", "start_pos": 155, "end_pos": 161, "type": "METRIC", "confidence": 0.9983845949172974}]}, {"text": "In contrast, S I yields very high precision but very low recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9985408782958984}, {"text": "recall", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9989834427833557}]}, {"text": "SA and S B attempt to balance these two measures but their precision is still higher than their recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9996936321258545}, {"text": "recall", "start_pos": 96, "end_pos": 102, "type": "METRIC", "confidence": 0.9991720914840698}]}, {"text": "Both systems have nearly the same precision but S B yields significantly higher recall than SA .", "labels": [], "entities": [{"text": "precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9992445707321167}, {"text": "recall", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.9996845722198486}]}], "tableCaptions": [{"text": " Table 1: Test and Training Data Used for Experiments", "labels": [], "entities": []}, {"text": " Table 2: Comparison of 5 Different Alignments using AER", "labels": [], "entities": [{"text": "AER", "start_pos": 53, "end_pos": 56, "type": "METRIC", "confidence": 0.9419640302658081}]}, {"text": " Table 3: Consistent Phrase Error Rates with Maximum", "labels": [], "entities": [{"text": "Consistent Phrase Error Rates", "start_pos": 10, "end_pos": 39, "type": "METRIC", "confidence": 0.6213040128350258}]}, {"text": " Table 4: BLEU Scores on English-Chinese with Different Lexical Weightings, Maximum Phrase Lengths and Training Data", "labels": [], "entities": [{"text": "BLEU Scores", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9568023681640625}, {"text": "Maximum Phrase Lengths", "start_pos": 76, "end_pos": 98, "type": "METRIC", "confidence": 0.66569850842158}]}, {"text": " Table 5: BLEU Scores on English-Arabic with Different", "labels": [], "entities": [{"text": "BLEU Scores", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9595891833305359}]}, {"text": " Table 6: BLEU Scores with Loose vs. Tight Phrases", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9982061386108398}]}, {"text": " Table 7: Number of Phrases in the Phrase Table Filtered for", "labels": [], "entities": []}, {"text": " Table 8: The average length of the phrases that are used in", "labels": [], "entities": []}, {"text": " Table 9: Coverage of Chinese MTEval'2003 Test Set Using", "labels": [], "entities": [{"text": "Chinese MTEval'2003 Test Set", "start_pos": 22, "end_pos": 50, "type": "DATASET", "confidence": 0.8615566939115524}]}]}