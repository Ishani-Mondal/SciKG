{"title": [{"text": "Answer Extraction, Semantic Clustering, and Extractive Summarization for Clinical Question Answering", "labels": [], "entities": [{"text": "Answer Extraction", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9483112394809723}, {"text": "Extractive Summarization", "start_pos": 44, "end_pos": 68, "type": "TASK", "confidence": 0.6595326364040375}, {"text": "Clinical Question Answering", "start_pos": 73, "end_pos": 100, "type": "TASK", "confidence": 0.5702990194161733}]}], "abstractContent": [{"text": "This paper presents a hybrid approach to question answering in the clinical domain that combines techniques from summarization and information retrieval.", "labels": [], "entities": [{"text": "question answering", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.8763288259506226}, {"text": "summarization", "start_pos": 113, "end_pos": 126, "type": "TASK", "confidence": 0.9835885763168335}, {"text": "information retrieval", "start_pos": 131, "end_pos": 152, "type": "TASK", "confidence": 0.7715353071689606}]}, {"text": "We tackle a frequently-occurring class of questions that takes the form \"What is the best drug treatment for X?\"", "labels": [], "entities": []}, {"text": "Starting from an initial set of MEDLINE citations, our system first identifies the drugs understudy.", "labels": [], "entities": []}, {"text": "Abstracts are then clustered using semantic classes from the UMLS on-tology.", "labels": [], "entities": [{"text": "UMLS", "start_pos": 61, "end_pos": 65, "type": "DATASET", "confidence": 0.9365448355674744}]}, {"text": "Finally, a short extractive summary is generated for each abstract to populate the clusters.", "labels": [], "entities": []}, {"text": "Two evaluations-a manual one focused on short answers and an automatic one focused on the supporting abstracts-demonstrate that our system compares favorably to PubMed, the search system most widely used by physicians today.", "labels": [], "entities": []}], "introductionContent": [{"text": "Complex information needs can rarely be addressed by single documents, but rather require the integration of knowledge from multiple sources.", "labels": [], "entities": []}, {"text": "This suggests that modern information retrieval systems, which excel at producing ranked lists of documents sorted by relevance, may not be sufficient to provide users with a good overview of the \"information landscape\".", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 26, "end_pos": 47, "type": "TASK", "confidence": 0.6947812438011169}]}, {"text": "Current question answering systems aspire to address this shortcoming by gathering relevant \"facts\" from multiple documents in response to information needs.", "labels": [], "entities": [{"text": "question answering", "start_pos": 8, "end_pos": 26, "type": "TASK", "confidence": 0.846024215221405}]}, {"text": "The so-called \"definition\" or \"other\" questions at recent TREC evaluations () serve as good examples: \"good answers\" to these questions include interesting \"nuggets\" about a particular person, organization, entity, or event.", "labels": [], "entities": []}, {"text": "The importance of cross-document information synthesis has not escaped the attention of other researchers.", "labels": [], "entities": [{"text": "cross-document information synthesis", "start_pos": 18, "end_pos": 54, "type": "TASK", "confidence": 0.7077716787656149}]}, {"text": "The last few years have seen a convergence between the question answering and summarization communities), as highlighted by the shift from generic to queryfocused summaries in the 2005 DUC evaluation (.", "labels": [], "entities": [{"text": "question answering", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.8190419673919678}, {"text": "summarization", "start_pos": 78, "end_pos": 91, "type": "TASK", "confidence": 0.7031464576721191}, {"text": "DUC evaluation", "start_pos": 185, "end_pos": 199, "type": "DATASET", "confidence": 0.935463011264801}]}, {"text": "Despite a focus on document ranking, different techniques for organizing search results have been explored by information retrieval researchers, as exemplified by techniques based on clustering.", "labels": [], "entities": [{"text": "document ranking", "start_pos": 19, "end_pos": 35, "type": "TASK", "confidence": 0.7066482156515121}]}, {"text": "Our work, which is situated in the domain of clinical medicine, lies at the intersection of question answering, information retrieval, and summarization.", "labels": [], "entities": [{"text": "question answering", "start_pos": 92, "end_pos": 110, "type": "TASK", "confidence": 0.8701915442943573}, {"text": "information retrieval", "start_pos": 112, "end_pos": 133, "type": "TASK", "confidence": 0.8482085466384888}, {"text": "summarization", "start_pos": 139, "end_pos": 152, "type": "TASK", "confidence": 0.9922005534172058}]}, {"text": "We employ answer extraction to identify short answers, semantic clustering to group similar results, and extractive summarization to produce supporting evidence.", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.8396095037460327}, {"text": "semantic clustering", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.7096345871686935}]}, {"text": "This paper describes how each of these capabilities contributes to an information system tailored to the requirements of physicians.", "labels": [], "entities": []}, {"text": "Two separate evaluations demonstrate the effectiveness of our approach.", "labels": [], "entities": []}], "datasetContent": [{"text": "Given that our work draws from QA, IR, and summarization, a proper evaluation that captures the salient characteristics of our system proved to be quite challenging.", "labels": [], "entities": [{"text": "IR", "start_pos": 35, "end_pos": 37, "type": "TASK", "confidence": 0.8374658226966858}, {"text": "summarization", "start_pos": 43, "end_pos": 56, "type": "TASK", "confidence": 0.9783574342727661}]}, {"text": "Overall, evaluation can be decomposed into two separate components: locating a suitable resource to serve as ground truth and leveraging it to assess system responses.", "labels": [], "entities": []}, {"text": "It is not difficult to find disease-specific pharmacology resources.", "labels": [], "entities": []}, {"text": "We employed Clinical Evidence (CE), a periodic report created by the British Medical Journal (BMJ) Publishing Group that summarizes the best known drugs fora few dozen diseases.", "labels": [], "entities": [{"text": "Clinical Evidence (CE)", "start_pos": 12, "end_pos": 34, "type": "METRIC", "confidence": 0.698323768377304}, {"text": "British Medical Journal (BMJ) Publishing Group", "start_pos": 69, "end_pos": 115, "type": "DATASET", "confidence": 0.9595962762832642}]}, {"text": "Note that the existence of such secondary sources does not obviate the need for automated systems because they are perpetually falling out of date due to rapid advances in medicine.", "labels": [], "entities": []}, {"text": "Furthermore, such reports are currently created by highlyexperienced physicians, which is an expensive and time-consuming process.", "labels": [], "entities": []}, {"text": "For each disease, CE classifies drugs into one of six categories: beneficial, likely beneficial, tradeoffs (i.e., may have adverse side effects), unknown, unlikely beneficial, and harmful.", "labels": [], "entities": []}, {"text": "Included with each entry is a list of references-citations consulted by the editors in compiling the resource.", "labels": [], "entities": []}, {"text": "Although the completeness of the drugs enumerated in CE is questionable, it nevertheless can be viewed as \"authoritative\".", "labels": [], "entities": []}, {"text": "We adapted existing techniques to evaluate our system in two separate ways: a factoid-style manual evaluation focused on short answers and an automatic evaluation with ROUGE using CE-cited abstracts as the reference summaries.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 168, "end_pos": 173, "type": "METRIC", "confidence": 0.9797674417495728}]}, {"text": "The setup and results for both are detailed below.", "labels": [], "entities": []}, {"text": "In our manual evaluation, system outputs were assessed as if they were answers to factoid questions.", "labels": [], "entities": []}, {"text": "We gathered three different sets of answers.", "labels": [], "entities": []}, {"text": "For the baseline, we used the main intervention from each of the first three PubMed citations.", "labels": [], "entities": [{"text": "PubMed citations", "start_pos": 77, "end_pos": 93, "type": "DATASET", "confidence": 0.9440757930278778}]}, {"text": "For our test condition, we considered the three largest clusters, taking the main intervention from the first abstract in each cluster.", "labels": [], "entities": []}, {"text": "This yields three drugs that are at the same level of ontological granularity as those extracted from the unclustered PubMed citations.", "labels": [], "entities": [{"text": "PubMed citations", "start_pos": 118, "end_pos": 134, "type": "DATASET", "confidence": 0.9136019051074982}]}, {"text": "For our third condition, we assumed the existence of an oracle which selects the three best clusters (as determined by the first author, a medical doctor).", "labels": [], "entities": []}, {"text": "From each of these three clusters, we extracted the main intervention of the first abstracts.", "labels": [], "entities": []}, {"text": "This oracle condition represents an achievable upper bound with a human in the loop.", "labels": [], "entities": []}, {"text": "Physicians are highly-trained professionals that already have significant domain knowledge.", "labels": [], "entities": []}, {"text": "Faced with a small number of choices, it is likely that they will be able to select the most promising cluster, even if they did not previously know it.", "labels": [], "entities": []}, {"text": "This preparation yielded up to nine drug names, three from each experimental condition.", "labels": [], "entities": []}, {"text": "For short, we refer to these as PubMed, Cluster, and Oracle, respectively.", "labels": [], "entities": [{"text": "PubMed", "start_pos": 32, "end_pos": 38, "type": "DATASET", "confidence": 0.9671345949172974}]}, {"text": "After blinding the source of the drugs and removing duplicates, each short answer was presented to the first author for evaluation.", "labels": [], "entities": []}, {"text": "Since: Manual evaluation of short answers: distribution of system answers with respect to CE categories (left side) and with respect to the assessor's own expertise (right side).", "labels": [], "entities": []}, {"text": "(Key: B=beneficial, LB=likely beneficial, T=tradeoffs, U=unknown, UB=unlikely beneficial, H=harmful, N=not in CE) the assessor had no idea from which condition an answer came, this process guarded against assessor bias.", "labels": [], "entities": [{"text": "CE", "start_pos": 110, "end_pos": 112, "type": "METRIC", "confidence": 0.9746955633163452}]}, {"text": "Each answer was evaluated in two different ways: first, with respect to the ground truth in CE, and second, using the assessor's own medical expertise.", "labels": [], "entities": []}, {"text": "In the first set of judgments, the assessor determined which of the six categories (beneficial, likely beneficial, tradeoffs, unknown, unlikely beneficial, harmful) the system answer belonged to, based on the CE recommendations.", "labels": [], "entities": []}, {"text": "As we have discussed previously, a human (with sufficient domain knowledge) is required to perform this matching due to synonymy and differences in ontological granularity.", "labels": [], "entities": []}, {"text": "However, note that the assessor only considered the drug name when making this categorization.", "labels": [], "entities": []}, {"text": "In the second set of judgments, the assessor separately determined if the short answer was \"good\", \"okay\" (marginal), or \"bad\" based both on CE and her own experience, taking into account the abstract title and the topscoring outcome sentence (and if necessary, the entire abstract text).", "labels": [], "entities": []}, {"text": "Results of this manual evaluation are presented in, which shows the distribution of judgments for the three experimental conditions.", "labels": [], "entities": []}, {"text": "For baseline PubMed, 20% of the examined drugs fell in the beneficial category; the values are 39% for the Cluster condition and 40% for the Oracle condition.", "labels": [], "entities": []}, {"text": "In terms of short answers, our system returns approximately twice as many beneficial drugs as the baseline, a marked increase in answer accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.9670614004135132}]}, {"text": "Note that a large fraction of the drugs evaluated were not found in CE at all, which provides an estimate of its coverage.", "labels": [], "entities": [{"text": "coverage", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.9669198393821716}]}, {"text": "In terms of the assessor's own judgments, 60% of PubMed short answers were found to be \"good\", compared to 83% and 89% for the Cluster and Oracle conditions, respectively.", "labels": [], "entities": []}, {"text": "From a factoid QA point of view, we can conclude that our system outperforms the PubMed baseline.", "labels": [], "entities": [{"text": "PubMed baseline", "start_pos": 81, "end_pos": 96, "type": "DATASET", "confidence": 0.9492464959621429}]}, {"text": "A major limitation of the factoid-based evaluation methodology is that it does not measure the quality of the abstracts from which the short answers were extracted.", "labels": [], "entities": []}, {"text": "Since we lacked the necessary resources to manually gather abstract-level judgments for evaluation, we sought an alternative.", "labels": [], "entities": []}, {"text": "Fortunately, CE can be leveraged to assess the \"goodness\" of abstracts automatically.", "labels": [], "entities": [{"text": "CE", "start_pos": 13, "end_pos": 15, "type": "METRIC", "confidence": 0.963413417339325}]}, {"text": "We assume that references cited in CE are examples of high quality abstracts, since they were used in generating the drug recommendations.", "labels": [], "entities": []}, {"text": "Following standard assumptions made in summarization evaluation, we considered abstracts that are similar in content with these \"reference abstracts\" to also be \"good\" (i.e., relevant).", "labels": [], "entities": [{"text": "summarization evaluation", "start_pos": 39, "end_pos": 63, "type": "TASK", "confidence": 0.9193190038204193}]}, {"text": "Similarity in content can be quantified with ROUGE.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 45, "end_pos": 50, "type": "METRIC", "confidence": 0.984372079372406}]}, {"text": "Since physicians demand high precision, we assess the cumulative relevance after the first, second, and third abstract that the clinician is likely to have examined (where the relevance for each individual abstract is given by its ROUGE-1 precision score).", "labels": [], "entities": [{"text": "precision", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9979674220085144}, {"text": "ROUGE-1 precision score", "start_pos": 231, "end_pos": 254, "type": "METRIC", "confidence": 0.8242354194323221}]}, {"text": "For the baseline PubMed condition, the examined abstracts simply correspond to the first three hits in the result set.", "labels": [], "entities": []}, {"text": "For our test system, we developed three different orderings.", "labels": [], "entities": []}, {"text": "The first, which we term cluster round-robin, selects the first abstract from the top three clusters (by size).", "labels": [], "entities": []}, {"text": "The second, which we term oracle cluster order, selects three abstracts from the best cluster, assuming the existence of an oracle that informs the system.", "labels": [], "entities": []}, {"text": "The third, which we term oracle round-robin, selects the first abstract from each of the three best clusters (also determined by an oracle).", "labels": [], "entities": []}, {"text": "Results of this evaluation are shown in.", "labels": [], "entities": []}, {"text": "The columns show the cumulative relevance (i.e., ROUGE score) after examining the first, second, and third abstract, under the different ordering conditions.", "labels": [], "entities": [{"text": "ROUGE score)", "start_pos": 49, "end_pos": 61, "type": "METRIC", "confidence": 0.9811594486236572}]}, {"text": "To determine statistical significance, we applied the Wilcoxon signed-rank test, the: Cumulative relevance after examining the first, second, and third abstracts, according to different orderings.", "labels": [], "entities": []}, {"text": "( \u2022 denotes n.s., denotes sig.", "labels": [], "entities": []}, {"text": "at 0.90, denotes sig.", "labels": [], "entities": []}, {"text": "at 0.95) standard non-parametric test for applications of this type.", "labels": [], "entities": []}, {"text": "Due to the relatively small test set (only 25 questions), the increase in cumulative relevance exhibited by the cluster round-robin condition is not statistically significant.", "labels": [], "entities": []}, {"text": "However, differences for the oracle conditions were significant.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Manual evaluation of short answers: distribution of system answers with respect to CE cat- egories (left side) and with respect to the assessor's own expertise (right side). (Key: B=beneficial,  LB=likely beneficial, T=tradeoffs, U=unknown, UB=unlikely beneficial, H=harmful, N=not in CE)", "labels": [], "entities": []}, {"text": " Table 3: Cumulative relevance after examining the first, second, and third abstracts, according to different  orderings. ( \u2022 denotes n.s., denotes sig. at 0.90, denotes sig. at 0.95)", "labels": [], "entities": []}]}