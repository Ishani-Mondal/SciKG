{"title": [{"text": "Minimum Cut Model for Spoken Lecture Segmentation", "labels": [], "entities": [{"text": "Spoken Lecture Segmentation", "start_pos": 22, "end_pos": 49, "type": "TASK", "confidence": 0.9019882281621298}]}], "abstractContent": [{"text": "We consider the task of unsupervised lecture segmentation.", "labels": [], "entities": [{"text": "unsupervised lecture segmentation", "start_pos": 24, "end_pos": 57, "type": "TASK", "confidence": 0.691421906153361}]}, {"text": "We formalize segmen-tation as a graph-partitioning task that optimizes the normalized cut criterion.", "labels": [], "entities": []}, {"text": "Our approach moves beyond localized comparisons and takes into account long-range cohesion dependencies.", "labels": [], "entities": []}, {"text": "Our results demonstrate that global analysis improves the segmentation accuracy and is robust in the presence of speech recognition errors.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 58, "end_pos": 70, "type": "TASK", "confidence": 0.9455817937850952}, {"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9665379524230957}, {"text": "speech recognition", "start_pos": 113, "end_pos": 131, "type": "TASK", "confidence": 0.7275254428386688}]}], "introductionContent": [{"text": "The development of computational models of text structure is a central concern in natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 82, "end_pos": 109, "type": "TASK", "confidence": 0.6467899481455485}]}, {"text": "Text segmentation is an important instance of such work.", "labels": [], "entities": [{"text": "Text segmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7237306386232376}]}, {"text": "The task is to partition a text into a linear sequence of topically coherent segments and thereby induce a content structure of the text.", "labels": [], "entities": []}, {"text": "The applications of the derived representation are broad, encompassing information retrieval, question-answering and summarization.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 71, "end_pos": 92, "type": "TASK", "confidence": 0.7931521236896515}, {"text": "summarization", "start_pos": 117, "end_pos": 130, "type": "TASK", "confidence": 0.9874539375305176}]}, {"text": "Not surprisingly, text segmentation has been extensively investigated over the last decade.", "labels": [], "entities": [{"text": "text segmentation", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.8453506231307983}]}, {"text": "Following the first unsupervised segmentation approach by, most algorithms assume that variations in lexical distribution indicate topic changes.", "labels": [], "entities": []}, {"text": "When documents exhibit sharp variations in lexical distribution, these algorithms are likely to detect segment boundaries accurately.", "labels": [], "entities": []}, {"text": "For example, most algorithms achieve high performance on synthetic collections, generated by concatenation of random text blocks).", "labels": [], "entities": []}, {"text": "The difficulty arises, however, when transitions between topics are smooth and distributional variations are subtle.", "labels": [], "entities": []}, {"text": "This is evident in the performance of existing unsupervised algorithms on less structured datasets, such as spoken meeting transcripts (.", "labels": [], "entities": []}, {"text": "Therefore, a more refined analysis of lexical distribution is needed.", "labels": [], "entities": []}, {"text": "Our work addresses this challenge by casting text segmentation in a graph-theoretic framework.", "labels": [], "entities": [{"text": "text segmentation", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.7294082045555115}]}, {"text": "We abstract a text into a weighted undirected graph, where the nodes of the graph correspond to sentences and edge weights represent the pairwise sentence similarity.", "labels": [], "entities": []}, {"text": "In this framework, text segmentation corresponds to a graph partitioning that optimizes the normalized-cut criterion).", "labels": [], "entities": [{"text": "text segmentation", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.7695153057575226}]}, {"text": "This criterion measures both the similarity within each partition and the dissimilarity across different partitions.", "labels": [], "entities": [{"text": "similarity", "start_pos": 33, "end_pos": 43, "type": "METRIC", "confidence": 0.967315137386322}]}, {"text": "Thus, our approach moves beyond localized comparisons and takes into account long-range changes in lexical distribution.", "labels": [], "entities": []}, {"text": "Our key hypothesis is that global analysis yields more accurate segmentation results than local models.", "labels": [], "entities": []}, {"text": "We tested our algorithm on a corpus of spoken lectures.", "labels": [], "entities": []}, {"text": "Segmentation in this domain is challenging in several respects.", "labels": [], "entities": []}, {"text": "Being less structured than written text, lecture material exhibits digressions, disfluencies, and other artifacts of spontaneous communication.", "labels": [], "entities": []}, {"text": "In addition, the output of speech recognizers is fraught with high word error rates due to specialized technical vocabulary and lack of in-domain spoken data for training.", "labels": [], "entities": [{"text": "speech recognizers", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.7356465756893158}]}, {"text": "Finally, pedagogical considerations call for fluent transitions between different topics in a lecture, further complicating the segmentation task.", "labels": [], "entities": []}, {"text": "Our experimental results confirm our hypothesis: considering long-distance lexical dependencies yields substantial gains in segmentation performance.", "labels": [], "entities": []}, {"text": "Our graph-theoretic approach compares favorably to state-of-the-art segmentation algorithms and attains results close to the range of human agreement scores.", "labels": [], "entities": []}, {"text": "Another attractive prop-erty of the algorithm is its robustness to noise: the accuracy of our algorithm does not deteriorate significantly when applied to speech recognition output.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9995937943458557}, {"text": "speech recognition output", "start_pos": 155, "end_pos": 180, "type": "TASK", "confidence": 0.7635274330774943}]}], "datasetContent": [{"text": "In this section we present the different corpora used to evaluate our model and provide a brief overview of the evaluation metrics.", "labels": [], "entities": []}, {"text": "Next, we describe our human segmentation study on the corpus of spoken lecture data.", "labels": [], "entities": []}, {"text": "We use the P k and WindowDiff measures to evaluate our system ().", "labels": [], "entities": [{"text": "WindowDiff", "start_pos": 19, "end_pos": 29, "type": "DATASET", "confidence": 0.7823114395141602}]}, {"text": "The P k measure estimates the probability that a randomly chosen pair of words within a window of length k words is inconsistently classified.", "labels": [], "entities": []}, {"text": "The WindowDiff metric is a variant of the P k measure, which penalizes false positives on an equal basis with near misses.", "labels": [], "entities": []}, {"text": "Both of these metrics are defined with respect to the average segment length of texts and exhibit high variability on real data.", "labels": [], "entities": []}, {"text": "We follow and compute the mean segment length used in determining the parameter k on each reference text separately.", "labels": [], "entities": []}, {"text": "We also plot the Receiver Operating Characteristic (ROC) curve to gauge performance at a finer level of discrimination.", "labels": [], "entities": [{"text": "Receiver Operating Characteristic (ROC) curve", "start_pos": 17, "end_pos": 62, "type": "METRIC", "confidence": 0.63137258376394}]}, {"text": "The ROC plot is the plot of the true positive rate against the false positive rate for various settings of a decision criterion.", "labels": [], "entities": [{"text": "ROC plot", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9538545310497284}]}, {"text": "In our case, the true positive rate is the fraction of boundaries correctly classified, and the false positive rate is the fraction of non-boundary positions incorrectly classified as boundaries.", "labels": [], "entities": [{"text": "true positive rate", "start_pos": 17, "end_pos": 35, "type": "METRIC", "confidence": 0.7079455057779948}, {"text": "false positive rate", "start_pos": 96, "end_pos": 115, "type": "METRIC", "confidence": 0.7616071303685507}]}, {"text": "In computing the true and false positive rates, we vary the threshold distance to the true boundary within which a hypothesized boundary is considered correct.", "labels": [], "entities": []}, {"text": "Larger areas under the ROC curve of a classifier indicate better discriminative performance.", "labels": [], "entities": [{"text": "ROC", "start_pos": 23, "end_pos": 26, "type": "METRIC", "confidence": 0.9819740056991577}]}, {"text": "key hypothesis is that considering long-distance lexical relations contributes to the effectiveness of the algorithm.", "labels": [], "entities": []}, {"text": "To test this hypothesis, we discard edges between nodes that are more than a certain number of sentences apart.", "labels": [], "entities": []}, {"text": "We test the system on a range of data sets, including the Physics and AI lectures and the synthetic corpus created by.", "labels": [], "entities": [{"text": "Physics and AI lectures", "start_pos": 58, "end_pos": 81, "type": "DATASET", "confidence": 0.5311352387070656}]}, {"text": "We also include segmentation results on Physics ASR transcripts.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 16, "end_pos": 28, "type": "TASK", "confidence": 0.9562958478927612}, {"text": "Physics ASR transcripts", "start_pos": 40, "end_pos": 63, "type": "DATASET", "confidence": 0.8038908044497172}]}, {"text": "The results in confirm our hypothesistaking into account non-local lexical dependencies helps across different domains.", "labels": [], "entities": []}, {"text": "On manually transcribed Physics lecture data, for example, the algorithm yields 0.394 P k measure when taking into account edges separated by up to ten sentences.", "labels": [], "entities": [{"text": "Physics lecture data", "start_pos": 24, "end_pos": 44, "type": "DATASET", "confidence": 0.7447546223799387}, {"text": "P k measure", "start_pos": 86, "end_pos": 97, "type": "METRIC", "confidence": 0.759641190369924}]}, {"text": "When dependencies up to a hundred sentences are considered, the algorithm yields a 25% reduction in P k measure.", "labels": [], "entities": []}, {"text": "shows the ROC plot for the segmentation of the Physics lecture data with different cutoff parameters, again demonstrating clear gains attained by employing longrange dependencies.", "labels": [], "entities": [{"text": "ROC", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9913923740386963}, {"text": "Physics lecture data", "start_pos": 47, "end_pos": 67, "type": "DATASET", "confidence": 0.8498032291730245}]}, {"text": "As shows, the improvement is consistent across all lecture datasets.", "labels": [], "entities": [{"text": "lecture datasets", "start_pos": 51, "end_pos": 67, "type": "DATASET", "confidence": 0.7012710422277451}]}, {"text": "We note, however, that after some point increasing the threshold degrades performance, because it introduces too many spurious dependencies (see the last column of).", "labels": [], "entities": []}, {"text": "The speaker will occasionally return to a topic described at the beginning of the lecture, and this will bias the algorithm to put the segment boundary closer to the end of the lecture.", "labels": [], "entities": []}, {"text": "Long-range dependencies do not improve the performance on the synthetic dataset.", "labels": [], "entities": []}, {"text": "This is expected since the segments in the synthetic dataset are randomly selected from widely-varying documents in the Brown corpus, even spanning different genres of written language.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 120, "end_pos": 132, "type": "DATASET", "confidence": 0.9091184437274933}]}, {"text": "So, effectively, there are no genuine long-range dependencies that can be exploited by the algorithm.", "labels": [], "entities": []}, {"text": "Comparison with local dependency models We compare our system with the state-of-the-art similarity-based segmentation system developed by.", "labels": [], "entities": []}, {"text": "We use the publicly available implementation of the system and optimize the system on a range of mask-sizes and different parameter settings described in) on a heldout development set of three lectures.", "labels": [], "entities": []}, {"text": "To control for segmentation granularity, we specify the number of segments in the reference (\"O\") segmentation for both our system and the baseline.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 15, "end_pos": 27, "type": "TASK", "confidence": 0.9704763293266296}]}, {"text": "Table 5 shows that the Minimum Cut algorithm consistently outperforms the similarity-based baseline on all the lecture datasets.", "labels": [], "entities": [{"text": "Minimum Cut algorithm", "start_pos": 23, "end_pos": 44, "type": "METRIC", "confidence": 0.7144226431846619}]}, {"text": "We attribute this gain to the presence of more attenuated topic transitions in spoken language.", "labels": [], "entities": []}, {"text": "Since spoken language is more spontaneous and less structured than written language, the speaker needs to keep the listener abreast of the changes in topic content by introducing subtle cues and references to prior topics in the course of topical transitions.", "labels": [], "entities": []}, {"text": "Non-local dependencies help to elucidate shifts in focus, because the strength of a particular transition is measured with respect to other local and long-distance contextual discourse relationships.", "labels": [], "entities": []}, {"text": "Our system does not outperform Choi's algorithm on the synthetic data.", "labels": [], "entities": []}, {"text": "This again can be attributed to the discrepancy in distributional properties of the synthetic corpus which lacks coherence in its thematic shifts and the lecture corpus of spontaneous speech with smooth distributional variations.", "labels": [], "entities": []}, {"text": "We also note that we did not try to adjust our model to optimize its performance on the synthetic data.", "labels": [], "entities": []}, {"text": "The smoothing method developed for lecture segmentation may not be appropriate for short segments ranging from three to eleven sentences that constitute the synthetic set.", "labels": [], "entities": [{"text": "lecture segmentation", "start_pos": 35, "end_pos": 55, "type": "TASK", "confidence": 0.7154531329870224}]}, {"text": "We also compared our method with another state-of-the-art algorithm which does not explicitly rely on pairwise similarity analysis.", "labels": [], "entities": []}, {"text": "This algorithm () (UI) computes the optimal segmentation by estimating changes in the language model predictions over different partitions.", "labels": [], "entities": []}, {"text": "We used the publicly available implemen-tation of the system that does not require parameter tuning on a heldout development set.", "labels": [], "entities": []}, {"text": "Again, our method achieves favorable performance on a range of lecture data sets (See Table 5), and both algorithms attain results close to the range of human agreement scores.", "labels": [], "entities": []}, {"text": "The attractive feature of our algorithm, however, is robustness to recognition errors -testing it on the ASR transcripts caused only 7.8% relative increase in P k measure (from 0.298 to 0.322), compared to a 13.5% relative increase for the UI system.", "labels": [], "entities": [{"text": "P k measure", "start_pos": 159, "end_pos": 170, "type": "METRIC", "confidence": 0.847456713517507}]}, {"text": "We attribute this feature to the fact that the model is less dependent on individual recognition errors, which have a detrimental effect on the local segment language modeling probability estimates for the UI system.", "labels": [], "entities": []}, {"text": "The block-level similarity function is not as sensitive to individual word errors, because the partition volume normalization factor dampens their overall effect on the derived models.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Lecture Corpus Statistics", "labels": [], "entities": [{"text": "Lecture Corpus Statistics", "start_pos": 10, "end_pos": 35, "type": "DATASET", "confidence": 0.9783284862836202}]}, {"text": " Table 2: Annotator Segmentation Statistics for the  first ten Physics lectures.", "labels": [], "entities": [{"text": "Annotator Segmentation", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.741971343755722}]}, {"text": " Table 3: P k annotation agreement between differ- ent pairs of annotators.", "labels": [], "entities": []}, {"text": " Table 4: Edges between nodes separated beyond a  certain threshold distance are removed.", "labels": [], "entities": []}, {"text": " Table 5: Performance analysis of different algo- rithms using the P k and WindowDiff measures,  with three lectures heldout for development.", "labels": [], "entities": []}]}