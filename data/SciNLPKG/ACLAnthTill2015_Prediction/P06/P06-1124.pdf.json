{"title": [{"text": "A Hierarchical Bayesian Language Model based on Pitman-Yor Processes", "labels": [], "entities": []}], "abstractContent": [{"text": "We propose anew hierarchical Bayesian n-gram model of natural languages.", "labels": [], "entities": []}, {"text": "Our model makes use of a generalization of the commonly used Dirichlet distributions called Pitman-Yor processes which produce power-law distributions more closely resembling those in natural languages.", "labels": [], "entities": []}, {"text": "We show that an approximation to the hierarchical Pitman-Yor language model recovers the exact formulation of interpolated Kneser-Ney, one of the best smoothing methods for n-gram language models.", "labels": [], "entities": []}, {"text": "Experiments verify that our model gives cross entropy results superior to interpolated Kneser-Ney and comparable to modified Kneser-Ney.", "labels": [], "entities": []}], "introductionContent": [{"text": "Probabilistic language models are used extensively in a variety of linguistic applications, including speech recognition, handwriting recognition, optical character recognition, and machine translation.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 102, "end_pos": 120, "type": "TASK", "confidence": 0.7693255841732025}, {"text": "handwriting recognition", "start_pos": 122, "end_pos": 145, "type": "TASK", "confidence": 0.8965682089328766}, {"text": "optical character recognition", "start_pos": 147, "end_pos": 176, "type": "TASK", "confidence": 0.6708147724469503}, {"text": "machine translation", "start_pos": 182, "end_pos": 201, "type": "TASK", "confidence": 0.8171372711658478}]}, {"text": "Most language models fall into the class of n-gram models, which approximate the distribution over sentences using the conditional distribution of each word given a context consisting of only the previous n \u2212 1 words, P (word i | word i\u22121 i\u2212n+1 ) (1) with n = 3 (trigram models) being typical.", "labels": [], "entities": []}, {"text": "Even for such a modest value of n the number of parameters is still tremendous due to the large vocabulary size.", "labels": [], "entities": []}, {"text": "As a result direct maximum-likelihood parameter fitting severely overfits to the training data, and smoothing methods are indispensible for proper training of n-gram models.", "labels": [], "entities": []}, {"text": "A large number of smoothing methods have been proposed in the literature (see) for good overviews).", "labels": [], "entities": []}, {"text": "Most methods take a rather ad hoc approach, where n-gram probabilities for various values of n are combined together, using either interpolation or back-off schemes.", "labels": [], "entities": []}, {"text": "Though some of these methods are intuitively appealing, the main justification has always been empirical-better perplexities or error rates on test data.", "labels": [], "entities": []}, {"text": "Though arguably this should be the only real justification, it only answers the question of whether a method performs better, not how nor why it performs better.", "labels": [], "entities": []}, {"text": "This is unavoidable given that most of these methods are not based on internally coherent Bayesian probabilistic models, which have explicitly declared prior assumptions and whose merits can be argued in terms of how closely these fit in with the known properties of natural languages.", "labels": [], "entities": []}, {"text": "Bayesian probabilistic models also have additional advantages-it is relatively straightforward to improve these models by incorporating additional knowledge sources and to include them in larger models in a principled manner.", "labels": [], "entities": []}, {"text": "Unfortunately the performance of previously proposed Bayesian language models had been dismal compared to other smoothing methods.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel language model based on a hierarchical Bayesian model where each hidden variable is distributed according to a Pitman-Yor process, a nonparametric generalization of the Dirichlet distribution that is widely studied in the statistics and probability theory communities).", "labels": [], "entities": []}, {"text": "Our model is a direct generalization of the hierarchical Dirichlet language model of.", "labels": [], "entities": []}, {"text": "Inference in our model is however not as straightforward and we propose an efficient Markov chain Monte Carlo sampling scheme.", "labels": [], "entities": []}, {"text": "Pitman-Yor processes produce power-law distributions that more closely resemble those seen in natural languages, and it has been argued that as a result they are more suited to applications in natural language processing).", "labels": [], "entities": []}, {"text": "We show experimentally that our hierarchical Pitman-Yor language model does indeed produce results superior to interpolated Kneser-Ney and comparable to modified Kneser-Ney, two of the currently best performing smoothing methods).", "labels": [], "entities": []}, {"text": "In fact we show a stronger result-that interpolated Kneser-Ney can be interpreted as a particular approximate inference scheme in the hierarchical Pitman-Yor language model.", "labels": [], "entities": []}, {"text": "Our interpretation is more useful than past interpretations involving marginal constraints ( or maximum-entropy models) as it can recover the exact formulation of interpolated Kneser-Ney, and actually produces superior results.", "labels": [], "entities": []}, {"text": "() has independently noted the correspondence between the hierarchical Pitman-Yor language model and interpolated Kneser-Ney, and conjectured improved performance in the hierarchical Pitman-Yor language model, which we verify here.", "labels": [], "entities": []}, {"text": "Thus the contributions of this paper are threefold: in proposing a langauge model with excellent performance and the accompanying advantages of Bayesian probabilistic models, in proposing a novel and efficient inference scheme for the model, and in establishing the direct correspondence between interpolated Kneser-Ney and the Bayesian approach.", "labels": [], "entities": []}, {"text": "We describe the Pitman-Yor process in Section 2, and propose the hierarchical Pitman-Yor language model in Section 3.", "labels": [], "entities": []}, {"text": "In Sections 4 and 5 we give a high level description of our sampling based inference scheme, leaving the details to a technical report).", "labels": [], "entities": []}, {"text": "We also show how interpolated Kneser-Ney can be interpreted as approximate inference in the model.", "labels": [], "entities": []}, {"text": "We show experimental comparisons to interpolated and modified Kneser-Ney, and the hierarchical Dirichlet language model in Section 6 and conclude in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "We performed experiments on the hierarchical Pitman-Yor language model on a 16 million word corpus derived from APNews.", "labels": [], "entities": [{"text": "APNews", "start_pos": 112, "end_pos": 118, "type": "DATASET", "confidence": 0.8492634892463684}]}, {"text": "This is the same dataset as in (.", "labels": [], "entities": []}, {"text": "The training, validation and test sets consist of about 14 million, 1 million and 1 million words respectively, while the vocabulary size is 17964.", "labels": [], "entities": []}, {"text": "For trigrams with n = 3, we varied the training set size between approximately 2 million and 14 million words by six equal increments, while we also experimented with n = 2 and 4 on the full 14 million word training set.", "labels": [], "entities": []}, {"text": "We compared the hierarchical Pitman-Yor language model trained using the proposed Gibbs sampler (HPYLM) against interpolated KneserNey (IKN), modified Kneser-Ney (MKN) with maximum discount cut-off c (max) = 3 as recommended in, and the hierarchical Dirichlet language model (HDLM).", "labels": [], "entities": []}, {"text": "For the various variants of Kneser-Ney, we first determined the parameters by conjugate gradient descent in the cross-entropy on the validation set.", "labels": [], "entities": []}, {"text": "At the optimal values, we folded the validation set into the training set to obtain the final n-gram probability estimates.", "labels": [], "entities": []}, {"text": "This procedure is as recommended in, and takes approximately 10 minutes on the full training set with n = 3 on a 1.4 Ghz PIII.", "labels": [], "entities": []}, {"text": "For HPYLM we inferred the posterior distribution over the latent variables and parameters given both the training and validation sets using the proposed Gibbs sampler.", "labels": [], "entities": [{"text": "HPYLM", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.8920667171478271}]}, {"text": "Since the posterior is well-behaved and the sampler converges quickly, we only used 125 iterations for burn-in, and 175 iterations to collect posterior samples.", "labels": [], "entities": []}, {"text": "On the full training set with n = 3 this took about 1.5 hours.", "labels": [], "entities": []}, {"text": "Perplexities on the test set are given in eters \u03b8 |u| 's are allowed to be positive and optimized over along with the discount parameters using cross-validation.", "labels": [], "entities": []}, {"text": "Seating arrangements are Gibbs sampled as in Section 5 with the parameter values fixed.", "labels": [], "entities": []}, {"text": "We find that HPYCV performs better than MKN (except marginally worse on small problems), and has best performance overall.", "labels": [], "entities": [{"text": "HPYCV", "start_pos": 13, "end_pos": 18, "type": "DATASET", "confidence": 0.7262839078903198}, {"text": "MKN", "start_pos": 40, "end_pos": 43, "type": "DATASET", "confidence": 0.6887865662574768}]}, {"text": "Note that the parameter values in HPYCV are still not the optimal ones since they are obtained by cross-validation using IKN, an approximation to a hierarchical Pitman-Yor language model.", "labels": [], "entities": [{"text": "HPYCV", "start_pos": 34, "end_pos": 39, "type": "DATASET", "confidence": 0.9365302324295044}]}, {"text": "Unfortunately cross-validation using a hierarchical Pitman-Yor language model inferred using Gibbs sampling is currently too costly to be practical.", "labels": [], "entities": []}, {"text": "In (right) we broke down the contributions to the cross-entropies in terms of how many times each word appears in the test set.", "labels": [], "entities": []}, {"text": "We see that most of the differences between the methods appear as differences among rare words, with the contribution of more common words being negligible.", "labels": [], "entities": []}, {"text": "HPYLM performs worse than MKN on words that occurred only once (on average) and better on other words, while HPYCV is reversed and performs better than MKN on words that occurred only once or twice and worse on other words.", "labels": [], "entities": [{"text": "HPYLM", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8115822076797485}]}], "tableCaptions": [{"text": " Table 1: Perplexities of various methods and for  various sizes of training set T and length of n- grams.", "labels": [], "entities": [{"text": "length", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.9661263823509216}]}]}