{"title": [{"text": "Subword-based Tagging for Confidence-dependent Chinese Word Segmentation", "labels": [], "entities": [{"text": "Subword-based Tagging", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7728325724601746}, {"text": "Chinese Word Segmentation", "start_pos": 47, "end_pos": 72, "type": "TASK", "confidence": 0.5187652707099915}]}], "abstractContent": [{"text": "We proposed a subword-based tagging for Chinese word segmentation to improve the existing character-based tagging.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 40, "end_pos": 65, "type": "TASK", "confidence": 0.598202258348465}]}, {"text": "The subword-based tagging was implemented using the maximum entropy (MaxEnt) and the conditional random fields (CRF) methods.", "labels": [], "entities": [{"text": "subword-based tagging", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.5505984574556351}]}, {"text": "We found that the proposed subword-based tagging outperformed the character-based tagging in all comparative experiments.", "labels": [], "entities": [{"text": "subword-based tagging", "start_pos": 27, "end_pos": 48, "type": "TASK", "confidence": 0.6210546493530273}]}, {"text": "In addition, we proposed a confidence measure approach to combine the results of a dictionary-based and a subword-tagging-based segmenta-tion.", "labels": [], "entities": []}, {"text": "This approach can produce an ideal tradeoff between the in-vocaulary rate and out-of-vocabulary rate.", "labels": [], "entities": []}, {"text": "Our techniques were evaluated using the test data from Sighan Bakeoff 2005.", "labels": [], "entities": [{"text": "Sighan Bakeoff 2005", "start_pos": 55, "end_pos": 74, "type": "DATASET", "confidence": 0.813588281472524}]}, {"text": "We achieved higher F-scores than the best results in three of the four corpora: PKU(0.951), CITYU(0.950) and MSR(0.971).", "labels": [], "entities": [{"text": "F-scores", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.999224066734314}, {"text": "PKU(0.951)", "start_pos": 80, "end_pos": 90, "type": "METRIC", "confidence": 0.7681465893983841}, {"text": "CITYU(0.950)", "start_pos": 92, "end_pos": 104, "type": "METRIC", "confidence": 0.8510033190250397}]}], "introductionContent": [{"text": "Many approaches have been proposed in Chinese word segmentation in the past decades.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 38, "end_pos": 63, "type": "TASK", "confidence": 0.621722529331843}]}, {"text": "Segmentation performance has been improved significantly, from the earliest maximal match (dictionary-based) approaches to HMM-based () approaches and recent state-of-the-art machine learning approaches such as maximum entropy (MaxEnt), support vector machine * Now the second author is affiliated with NTT.", "labels": [], "entities": [{"text": "NTT", "start_pos": 303, "end_pos": 306, "type": "DATASET", "confidence": 0.9692736864089966}]}, {"text": "(SVM) (), conditional random fields (CRF) (, and minimum error rate training ().", "labels": [], "entities": [{"text": "minimum error rate training", "start_pos": 49, "end_pos": 76, "type": "METRIC", "confidence": 0.7781743705272675}]}, {"text": "By analyzing the top results in the first and second Bakeoffs, and), we found the top results were produced by director indirect use of so-called \"IOB\" tagging, which converts the problem of word segmentation into one of character tagging so that part-of-speech tagging approaches can be used for word segmentation.", "labels": [], "entities": [{"text": "IOB\" tagging", "start_pos": 147, "end_pos": 159, "type": "TASK", "confidence": 0.5436098674933115}, {"text": "word segmentation", "start_pos": 191, "end_pos": 208, "type": "TASK", "confidence": 0.7339476644992828}, {"text": "character tagging", "start_pos": 221, "end_pos": 238, "type": "TASK", "confidence": 0.7397588193416595}, {"text": "part-of-speech tagging", "start_pos": 247, "end_pos": 269, "type": "TASK", "confidence": 0.7510187029838562}, {"text": "word segmentation", "start_pos": 297, "end_pos": 314, "type": "TASK", "confidence": 0.7730830311775208}]}, {"text": "This approach was also called \"LMR\" or \"BIES\" () tagging.", "labels": [], "entities": [{"text": "BIES", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.9696444272994995}]}, {"text": "Under the scheme, each character of a word is labeled as \"B\" if it is the first character of a multiple-character word, or \"I\" otherwise, and \"O\" if the character functioned as an independent word.", "labels": [], "entities": []}, {"text": "For example, \"(whole) (Beijing city)\" is labeled as \"/O /B /I /I\".", "labels": [], "entities": [{"text": "Beijing city)\"", "start_pos": 23, "end_pos": 37, "type": "DATASET", "confidence": 0.8711676994959513}]}, {"text": "Thus, the training data in word sequences are turned into IOB-labeled data in character sequences, which are then used as the training data for tagging.", "labels": [], "entities": []}, {"text": "For new test data, word boundaries are determined based on the results of tagging.", "labels": [], "entities": []}, {"text": "While the IOB tagging approach has been widely used in Chinese word segmentation, we found that so far all the existing implementations were using character-based IOB tagging.", "labels": [], "entities": [{"text": "IOB tagging", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.7603628039360046}, {"text": "Chinese word segmentation", "start_pos": 55, "end_pos": 80, "type": "TASK", "confidence": 0.5835101306438446}]}, {"text": "In this work we propose a subword-based IOB tagging, which assigns tags to a pre-defined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters.", "labels": [], "entities": [{"text": "IOB tagging", "start_pos": 40, "end_pos": 51, "type": "TASK", "confidence": 0.8533008098602295}]}, {"text": "If only Chinese characters are used, the subword-based IOB tagging is downgraded to a character-based one.", "labels": [], "entities": [{"text": "IOB tagging", "start_pos": 55, "end_pos": 66, "type": "TASK", "confidence": 0.7787721455097198}]}, {"text": "Taking the same example mentioned above, \"\" is la-beled as \"/O /B /I\" in the subword-based tagging, where \"/B\" is labeled as one unit.", "labels": [], "entities": []}, {"text": "We will give a detailed description of this approach in Section 2.", "labels": [], "entities": []}, {"text": "There exists a clear weakness with the IOB tagging approach: It yields a very low in-vocabulary rate (R-iv) in return fora higher out-of-vocabulary (OOV) rate (R-oov).", "labels": [], "entities": [{"text": "IOB tagging", "start_pos": 39, "end_pos": 50, "type": "TASK", "confidence": 0.8527027368545532}, {"text": "in-vocabulary rate (R-iv)", "start_pos": 82, "end_pos": 107, "type": "METRIC", "confidence": 0.8162135004997253}, {"text": "out-of-vocabulary (OOV) rate (R-oov)", "start_pos": 130, "end_pos": 166, "type": "METRIC", "confidence": 0.7612110748887062}]}, {"text": "In the results of the closed test in, the work of (), using CRFs for the IOB tagging, yielded a very high R-oov in all of the four corpora used, but the R-iv rates were lower.", "labels": [], "entities": [{"text": "IOB tagging", "start_pos": 73, "end_pos": 84, "type": "TASK", "confidence": 0.7233952581882477}, {"text": "R-oov", "start_pos": 106, "end_pos": 111, "type": "METRIC", "confidence": 0.9934622049331665}, {"text": "R-iv", "start_pos": 153, "end_pos": 157, "type": "METRIC", "confidence": 0.9831672310829163}]}, {"text": "While OOV recognition is very important in word segmentation, a higher IV rate is also desired.", "labels": [], "entities": [{"text": "OOV recognition", "start_pos": 6, "end_pos": 21, "type": "TASK", "confidence": 0.7908415496349335}, {"text": "word segmentation", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.8180085122585297}, {"text": "IV rate", "start_pos": 71, "end_pos": 78, "type": "METRIC", "confidence": 0.9870051145553589}]}, {"text": "In this work we propose a confidence measure approach to lessen this weakness.", "labels": [], "entities": []}, {"text": "By this approach we can change the R-oov and R-iv and find an optimal tradeoff.", "labels": [], "entities": []}, {"text": "This approach will be described in Section 2.3.", "labels": [], "entities": []}, {"text": "In addition, we illustrate our word segmentation process in Section 2, where the subword-based tagging is described by the MaxEnt method.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.7346015870571136}]}, {"text": "Section 3 presents our experimental results.", "labels": [], "entities": []}, {"text": "The effects using the MaxEnts and CRFs are shown in this section.", "labels": [], "entities": [{"text": "MaxEnts", "start_pos": 22, "end_pos": 29, "type": "DATASET", "confidence": 0.856241762638092}]}, {"text": "Section 4 describes current state-of-the-art methods with Chinese word segmentation, with which our results were compared.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 58, "end_pos": 83, "type": "TASK", "confidence": 0.658818523089091}]}, {"text": "Section 5 provides the concluding remarks and outlines future goals.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used the data provided by Sighan Bakeoff 2005 to test our approaches described in the previous sections.", "labels": [], "entities": [{"text": "Sighan Bakeoff 2005", "start_pos": 29, "end_pos": 48, "type": "DATASET", "confidence": 0.7634192109107971}]}, {"text": "The data contain four corpora from different sources: Academia sinica, City University of Hong Kong, Peking University and Microsoft Research (Beijing).", "labels": [], "entities": [{"text": "Academia sinica", "start_pos": 54, "end_pos": 69, "type": "DATASET", "confidence": 0.9569151401519775}]}, {"text": "The statistics concerning the corpora is listed in.", "labels": [], "entities": []}, {"text": "The corpora provided both unicode coding and Big5/GB coding.", "labels": [], "entities": []}, {"text": "We used the Big5 and CP936 encodings.", "labels": [], "entities": [{"text": "CP936 encodings", "start_pos": 21, "end_pos": 36, "type": "DATASET", "confidence": 0.7880959510803223}]}, {"text": "Since the main purpose of this work is to evaluate the proposed subwordbased IOB tagging, we carried out the closed test only.", "labels": [], "entities": [{"text": "IOB tagging", "start_pos": 77, "end_pos": 88, "type": "TASK", "confidence": 0.745443195104599}]}, {"text": "Five metrics were used to evaluate the segmentation results: recall (R), precision (P), F-score (F), OOV rate (R-oov) and IV rate (R-iv).", "labels": [], "entities": [{"text": "recall (R)", "start_pos": 61, "end_pos": 71, "type": "METRIC", "confidence": 0.9379073679447174}, {"text": "precision (P)", "start_pos": 73, "end_pos": 86, "type": "METRIC", "confidence": 0.9516186714172363}, {"text": "F-score (F)", "start_pos": 88, "end_pos": 99, "type": "METRIC", "confidence": 0.950069859623909}, {"text": "OOV rate (R-oov)", "start_pos": 101, "end_pos": 117, "type": "METRIC", "confidence": 0.9509871125221252}, {"text": "IV rate (R-iv)", "start_pos": 122, "end_pos": 136, "type": "METRIC", "confidence": 0.9448969960212708}]}, {"text": "For a detailed explanation of these metrics, refer to", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Corpus statistics in Sighan Bakeoff 2005", "labels": [], "entities": [{"text": "Sighan Bakeoff 2005", "start_pos": 31, "end_pos": 50, "type": "DATASET", "confidence": 0.8469854593276978}]}, {"text": " Table 4: Segmentation results by the pure subword-based IOB tagging. The separator \"/\" divides the results by three lexicon sizes  as illustrated in", "labels": [], "entities": [{"text": "Segmentation", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.9597366452217102}, {"text": "IOB tagging", "start_pos": 57, "end_pos": 68, "type": "TASK", "confidence": 0.7151064276695251}]}, {"text": " Table 3. The first is character-based (s1), while the other two are subword-based with different lexicons (s2/s3).", "labels": [], "entities": []}, {"text": " Table 5: Effects of combination using the confidence measure. Here we used \u03b1 = 0.8 and confidence threshold t = 0.7. The  separator \"/\" divides the results of s1, s2, and s3.", "labels": [], "entities": [{"text": "confidence threshold t", "start_pos": 88, "end_pos": 110, "type": "METRIC", "confidence": 0.9514534076054891}]}, {"text": " Table 6: Effects of using CRF. The separator \"/\" divides the results of s1, and s3.", "labels": [], "entities": []}]}