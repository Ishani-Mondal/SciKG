{"title": [{"text": "Automatic Classification of Verbs in Biomedical Texts", "labels": [], "entities": [{"text": "Automatic Classification of Verbs in Biomedical Texts", "start_pos": 0, "end_pos": 53, "type": "TASK", "confidence": 0.7921698476587024}]}], "abstractContent": [{"text": "Lexical classes, when tailored to the application and domain in question, can provide an effective means to deal with a number of natural language processing (NLP) tasks.", "labels": [], "entities": []}, {"text": "While manual construction of such classes is difficult, recent research shows that it is possible to automatically induce verb classes from cross-domain corpora with promising accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 176, "end_pos": 184, "type": "METRIC", "confidence": 0.961506187915802}]}, {"text": "We report a novel experiment where similar technology is applied to the important, challenging domain of biomedicine.", "labels": [], "entities": []}, {"text": "We show that the resulting classification, acquired from a corpus of biomedical journal articles, is highly accurate and strongly domain-specific.", "labels": [], "entities": []}, {"text": "It can be used to aid BIO-NLP directly or as useful material for investigating the syntax and semantics of verbs in biomedical texts.", "labels": [], "entities": []}], "introductionContent": [{"text": "Lexical classes which capture the close relation between the syntax and semantics of verbs have attracted considerable interest in NLP).", "labels": [], "entities": []}, {"text": "Such classes are useful for their ability to capture generalizations about a range of linguistic properties.", "labels": [], "entities": []}, {"text": "For example, verbs which share the meaning of 'manner of motion' (such as travel, run, walk), behave similarly also in terms of subcategorization (I traveled/ran/walked, I traveled/ran/walked to London, I traveled/ran/walked five miles).", "labels": [], "entities": []}, {"text": "Although the correspondence between the syntax and semantics of words is not perfect and the classes do not provide means for full semantic inferencing, their predictive power is nevertheless considerable.", "labels": [], "entities": []}, {"text": "NLP systems can benefit from lexical classes in many ways.", "labels": [], "entities": []}, {"text": "Such classes define the mapping from surface realization of arguments to predicateargument structure, and are therefore an important component of any system which needs the latter.", "labels": [], "entities": []}, {"text": "As the classes can capture higher level abstractions they can be used as a means to abstract away from individual words when required.", "labels": [], "entities": []}, {"text": "They are also helpful in many operational contexts where lexical information must be acquired from small application-specific corpora.", "labels": [], "entities": []}, {"text": "Their predictive power can help compensate for lack of data fully exemplifying the behavior of relevant words.", "labels": [], "entities": []}, {"text": "Lexical verb classes have been used to support various (multilingual) tasks, such as computational lexicography, language generation, machine translation, word sense disambiguation, semantic role labeling, and subcategorization acquisition).", "labels": [], "entities": [{"text": "language generation", "start_pos": 113, "end_pos": 132, "type": "TASK", "confidence": 0.7412071228027344}, {"text": "machine translation", "start_pos": 134, "end_pos": 153, "type": "TASK", "confidence": 0.7933084070682526}, {"text": "word sense disambiguation", "start_pos": 155, "end_pos": 180, "type": "TASK", "confidence": 0.6835360328356425}, {"text": "semantic role labeling", "start_pos": 182, "end_pos": 204, "type": "TASK", "confidence": 0.6661833922068278}, {"text": "subcategorization acquisition", "start_pos": 210, "end_pos": 239, "type": "TASK", "confidence": 0.7984457910060883}]}, {"text": "However, large-scale exploitation of the classes in real-world or domain-sensitive tasks has not been possible because the existing classifications, e.g., are incomprehensive and unsuitable for specific domains.", "labels": [], "entities": []}, {"text": "While manual classification of large numbers of words has proved difficult and time-consuming, recent research shows that it is possible to automatically induce lexical classes from corpus data with promising accuracy; Brew and Schulte im.", "labels": [], "entities": [{"text": "manual classification of large numbers of words", "start_pos": 6, "end_pos": 53, "type": "TASK", "confidence": 0.7749293616839817}]}, {"text": "A number of ML methods have been applied to classify words using features pertaining to mainly syntactic structure (e.g. statistical distributions of subcategorization frames (SCFs) or general patterns of syntactic behaviour, e.g. transitivity, passivisability) which have been extracted from corpora using e.g. part-of-speech tagging or robust statistical parsing techniques.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 312, "end_pos": 334, "type": "TASK", "confidence": 0.7172668725252151}]}, {"text": "This research has been encouraging but it has so far concentrated on general language.", "labels": [], "entities": []}, {"text": "Domainspecific lexical classification remains unexplored, although it is arguably important: existing classifications are unsuitable for domain-specific applications and these often challenging applications might benefit from improved performance by utilizing lexical classes the most.", "labels": [], "entities": [{"text": "Domainspecific lexical classification", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.7614150842030843}]}, {"text": "In this paper, we extend an existing approach to lexical classification ( and apply it (without any domain specific tuning) to the domain of biomedicine.", "labels": [], "entities": [{"text": "lexical classification", "start_pos": 49, "end_pos": 71, "type": "TASK", "confidence": 0.7094690650701523}]}, {"text": "We focus on biomedicine for several reasons: (i) NLP is critically needed to assist the processing, mining and extraction of knowledge from the rapidly growing literature in this area, (ii) the domain lexical resources (e.g. UMLS metathesaurus and lexicon 1 ) do not provide sufficient information about verbs and (iii) being linguistically challenging, the domain provides a good test case for examining the potential of automatic classification.", "labels": [], "entities": [{"text": "automatic classification", "start_pos": 422, "end_pos": 446, "type": "TASK", "confidence": 0.6228498071432114}]}, {"text": "We report an experiment where a classification is induced for 192 relatively frequent verbs from a corpus of 2230 biomedical journal articles.", "labels": [], "entities": []}, {"text": "The results, evaluated with domain experts, show that the approach is capable of acquiring classes with accuracy higher than that reported in previous work on general language.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9989684820175171}]}, {"text": "We discuss reasons for this and show that the resulting classes differ substantially from those in extant lexical resources.", "labels": [], "entities": []}, {"text": "They constitute the first syntactic-semantic verb classification for the biomedical domain and could be readily applied to support We discuss the domain-specific issues related to our task in section 2.", "labels": [], "entities": []}, {"text": "The approach to automatic classification is presented in section 3.", "labels": [], "entities": [{"text": "automatic classification", "start_pos": 16, "end_pos": 40, "type": "TASK", "confidence": 0.6388729959726334}]}, {"text": "Details of the experimental evaluation are supplied in section 4.", "labels": [], "entities": []}, {"text": "Section 5 provides discussion and section 6 concludes with directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "IB and ID also have the advantage (over NN) that they can be used to produce a hierarchical verb classification.", "labels": [], "entities": []}, {"text": "shows the results for IB and ID for the informative values of K.", "labels": [], "entities": [{"text": "IB", "start_pos": 22, "end_pos": 24, "type": "METRIC", "confidence": 0.9927009344100952}, {"text": "ID", "start_pos": 29, "end_pos": 31, "type": "METRIC", "confidence": 0.9829738140106201}]}, {"text": "The bold font indicates the results when the match between the values of K and the number of classes at the particular level of the gold standard is the closest.", "labels": [], "entities": [{"text": "K", "start_pos": 73, "end_pos": 74, "type": "METRIC", "confidence": 0.9528537392616272}]}, {"text": "IB is clearly better than ID at all levels of gold standard.", "labels": [], "entities": [{"text": "IB", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.4933592975139618}, {"text": "ID", "start_pos": 26, "end_pos": 28, "type": "METRIC", "confidence": 0.9027913212776184}]}, {"text": "It yields its best results at the medium level (34 classes) with K = 33: F = 77 and APP = 69 (the results for ID are F = 72 and APP = 65).", "labels": [], "entities": [{"text": "F", "start_pos": 73, "end_pos": 74, "type": "METRIC", "confidence": 0.9934566617012024}, {"text": "APP", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.997410237789154}, {"text": "F", "start_pos": 117, "end_pos": 118, "type": "METRIC", "confidence": 0.9632973074913025}, {"text": "APP", "start_pos": 128, "end_pos": 131, "type": "METRIC", "confidence": 0.9565272331237793}]}, {"text": "At the most fine-grained level (50 classes), IB is equally good according to F with K = 33, but APP is 8% lower.", "labels": [], "entities": [{"text": "IB", "start_pos": 45, "end_pos": 47, "type": "METRIC", "confidence": 0.9694141149520874}, {"text": "F", "start_pos": 77, "end_pos": 78, "type": "METRIC", "confidence": 0.9977633953094482}, {"text": "APP", "start_pos": 96, "end_pos": 99, "type": "METRIC", "confidence": 0.9992111921310425}]}, {"text": "Although ID is occasionally better than IB according to APP and mPUR (see e.g. the results for 16 classes with K = 53) this never happens in the case where the correspondence between the number of gold standard classes and the values of K is the closest.", "labels": [], "entities": [{"text": "ID", "start_pos": 9, "end_pos": 11, "type": "METRIC", "confidence": 0.9981821775436401}, {"text": "IB", "start_pos": 40, "end_pos": 42, "type": "METRIC", "confidence": 0.9977201819419861}, {"text": "APP", "start_pos": 56, "end_pos": 59, "type": "METRIC", "confidence": 0.5020319819450378}, {"text": "mPUR", "start_pos": 64, "end_pos": 68, "type": "DATASET", "confidence": 0.7603908777236938}]}, {"text": "In other words, the informative values of K prove really informative for IB.", "labels": [], "entities": [{"text": "IB", "start_pos": 73, "end_pos": 75, "type": "TASK", "confidence": 0.8931869268417358}]}, {"text": "The lower performance of ID seems to be due to its tendency to create evenly sized clusters.", "labels": [], "entities": []}, {"text": "All the methods perform significantly better than our random baseline.", "labels": [], "entities": []}, {"text": "The significance of the results with respect to two swaps was at the 2\u03c3 level, corresponding to a 97% confidence that the results are above random.", "labels": [], "entities": [{"text": "significance", "start_pos": 4, "end_pos": 16, "type": "METRIC", "confidence": 0.9765676856040955}]}, {"text": "We performed further, qualitative analysis of clusters produced by the best performing method IB.", "labels": [], "entities": [{"text": "IB", "start_pos": 94, "end_pos": 96, "type": "DATASET", "confidence": 0.8048057556152344}]}, {"text": "Consider the following clusters: A: inject, transfect, microinfect, contransfect (6) B: harvest, select, collect (7.1) centrifuge, process, recover (7.2) C: wash, rinse (4.1.1) immunoblot (4.1.3) overlap (5) When looking at coarse-grained outputs, interestingly, K as low as 8 learned the broad distinction between biomedical and general language verbs (the two verb types appeared only rarely in the same clusters) and produced large semantically meaningful groups of classes (e.g. the coarse-grained classes EXPERIMENTAL PROCE-DURES, TRANSFECT and COLLECT were mapped together).", "labels": [], "entities": [{"text": "overlap", "start_pos": 196, "end_pos": 203, "type": "METRIC", "confidence": 0.9790678024291992}]}, {"text": "K = 12 was sufficient to identify several classes with very particular syntax One of them was TRANSFECT (see A above) whose members were distinguished easily because of their typical SCFs (e.g. inject /transfect/microinfect/contransfect X with/into Y).", "labels": [], "entities": []}, {"text": "On the other hand, even K = 53 could not identify classes with very similar (yet un-identical) syntax.", "labels": [], "entities": []}, {"text": "These included many semantically similar sub-classes (e.g. the two sub-classes of COLLECT shown in B whose members take similar NP and PP SCFs).", "labels": [], "entities": []}, {"text": "However, also a few semantically different verbs clustered wrongly because of this reason, such as the ones exemplified in C.", "labels": [], "entities": []}, {"text": "In C, immunoblot (from the LABEL class) is still somewhat related to wash and rinse (the WASH class) because they all belong to the larger EXPERIMENTAL PRO-CEDURES class, but overlap (from the PROCESS class) shows up in the cluster merely because of syntactic idiosyncracy.", "labels": [], "entities": [{"text": "WASH", "start_pos": 89, "end_pos": 93, "type": "METRIC", "confidence": 0.8611499667167664}]}, {"text": "While parser errors caused by the challenging biomedical texts were visible in some SCFs (e.g. looking at a sample of SCFs, some adjunct instances were listed in the argument slots of the frames), the cases where this resulted in incorrect classification were not numerous . One representative singleton resulting from these errors is exemplified in D.", "labels": [], "entities": []}, {"text": "Activate appears in relatively complicated sentence structures, which gives rise to incorrect SCFs.", "labels": [], "entities": []}, {"text": "For example, MECs cultured on 2D planar substrates transiently activate MAP kinase in response to EGF, whereas...", "labels": [], "entities": [{"text": "EGF", "start_pos": 98, "end_pos": 101, "type": "DATASET", "confidence": 0.7640525698661804}]}, {"text": "gets incorrectly analysed as SCF NP-NP, while The effect of the constitutively activated ARF6-Q67L mutant was investigated...", "labels": [], "entities": []}, {"text": "receives the incorrect SCF analysis NP-SCOMP.", "labels": [], "entities": [{"text": "incorrect SCF analysis NP-SCOMP", "start_pos": 13, "end_pos": 44, "type": "METRIC", "confidence": 0.6819491684436798}]}, {"text": "Most parser errors are caused by unknown domainspecific words and phrases.", "labels": [], "entities": []}], "tableCaptions": []}