{"title": [{"text": "Enhancing electronic dictionaries with an index based on associations", "labels": [], "entities": [{"text": "Enhancing electronic dictionaries", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8730495572090149}]}], "abstractContent": [{"text": "A good dictionary contains not only many entries and a lot of information concerning each one of them, but also adequate means to reveal the stored information.", "labels": [], "entities": []}, {"text": "Information access depends crucially on the quality of the index.", "labels": [], "entities": []}, {"text": "We will present here some ideas of how a dictionary could be enhanced to support a speaker/writer to find the word s/he is looking for.", "labels": [], "entities": []}, {"text": "To this end we suggest to add to an existing electronic resource an index based on the notion of association.", "labels": [], "entities": []}, {"text": "We will also present preliminary work of how a subset of such associations, for example , topical associations, can be acquired by filtering a network of lexical co-occurrences extracted from a corpus.", "labels": [], "entities": []}], "introductionContent": [{"text": "A dictionary user typically pursues one of two goals: as a decoder (reading, listening), he may look for the definition or the translation of a specific target word, while as an encoder (speaker, writer) he may want to find a word that expresses well not only a given concept, but is also appropriate in a given context.", "labels": [], "entities": []}, {"text": "Obviously, readers and writers come to the dictionary with different mindsets, information and expectations concerning input and output.", "labels": [], "entities": []}, {"text": "While the decoder can provide the word he wants additional information for, the encoder (language producer) provides the meaning of a word for which he lacks the corresponding form.", "labels": [], "entities": []}, {"text": "In sum, users with different goals need access to different indexes, one that is based on form (decoding), In alphabetical order the other being based on meaning or meaning relations (encoding).", "labels": [], "entities": []}, {"text": "Our concern here is more with the encoder, i.e. lexical access in language production, a feature largely neglected in lexicographical work.", "labels": [], "entities": []}, {"text": "Yet, a good dictionary contains not only many entries and a lot of information concerning each one of them, but also efficient means to reveal the stored information.", "labels": [], "entities": []}, {"text": "Because, what is a huge dictionary good for, if one cannot access the information it contains?", "labels": [], "entities": []}], "datasetContent": [{"text": "We applied the method described hereto an initial co-occurrence network extracted from a corpus of 24 months of Le Monde, a major French newspaper.", "labels": [], "entities": [{"text": "co-occurrence network extracted from a corpus of 24 months of Le Monde, a major French newspaper", "start_pos": 50, "end_pos": 146, "type": "DATASET", "confidence": 0.8527016692301806}]}, {"text": "The size of the corpus was around 39 million words.", "labels": [], "entities": []}, {"text": "The initial network contained 18,958 words and 341,549 relations.", "labels": [], "entities": []}, {"text": "The first run produced 382,208 Topical Units.", "labels": [], "entities": []}, {"text": "After filtering, we kept 59% of them.", "labels": [], "entities": []}, {"text": "The network built from these Topical Units was made of 11,674 words and 2,864,473 co-occurrences.", "labels": [], "entities": []}, {"text": "70% of these cooccurrences were new with regard to the initial network and were discarded.", "labels": [], "entities": []}, {"text": "Finally, we got a filtered network of 7,160 words and 183,074 relations, which represents a cut of 46% of the initial network.", "labels": [], "entities": []}, {"text": "A qualitative study showed that most of the discarded relations are non-topical.", "labels": [], "entities": []}, {"text": "This is illustrated by, which gives the cooccurrents of the word acteur (actor) that are filtered by our method among its co-occurrents with a high cohesion (equal to 0.16).", "labels": [], "entities": []}, {"text": "For instance, the words cynique (cynical) or allocataire (beneficiary) are cohesive co-occurrents of the word actor, even though they are not topically linked to it.", "labels": [], "entities": []}, {"text": "These words are filtered out, while we keep words like gros_plan (close-up) or sc\u00e9-nique (theatrical), which topically cohere with acteur (actor) despite their lower frequency than the discarded words.", "labels": [], "entities": []}, {"text": "In order to evaluate more objectively our work, we compared the quantitative results of TOPICOLL with the initial network and its filtered version.", "labels": [], "entities": [{"text": "TOPICOLL", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.5709123015403748}]}, {"text": "The evaluation showed that the performance of the segmenter remains stable, even if we use a topically filtered network (see).", "labels": [], "entities": []}, {"text": "Moreover, it became obvious that a network filtered only by frequency and cohesion performs significantly less well, even with a comparable size.", "labels": [], "entities": []}, {"text": "For testing the statistical significance of these results, we applied to the P k values a one-side t-test with a null hypothesis of equal means.", "labels": [], "entities": []}, {"text": "Levels lower or equal to 0.05 are considered as statistically significant: p val (I-T): 0.08 p val (I-F): 0.02 p val (T-F): 0.05 These values confirm that the difference between the initial network (I) and the topically filtered one (T) is actually not significant, whereas the filtering based on co-occurrence frequencies leads to significantly lower results, both compared to the initial network and the topically filtered one.", "labels": [], "entities": []}, {"text": "Hence, one may conclude that our 8 Precision is given by N t / Nb and recall by N t / D, with D being the number of document breaks, Nb the number of boundaries found by TOPICOLL and N t the number of boundaries that are document breaks (the boundary should not be farther than 9 plain words from the document break).", "labels": [], "entities": [{"text": "recall", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.9972296357154846}, {"text": "TOPICOLL", "start_pos": 170, "end_pos": 178, "type": "METRIC", "confidence": 0.6793726086616516}]}, {"text": "9 P k) evaluates the probability that a randomly chosen pair of words, separated by k words, is wrongly classified, i.e. they are found in the same segment by TOPICOLL, while they are actually in different ones (miss of a document break), or they are found in different segments, while they are actually in the same one (false alarm).", "labels": [], "entities": [{"text": "TOPICOLL", "start_pos": 159, "end_pos": 167, "type": "METRIC", "confidence": 0.5205144882202148}]}, {"text": "method is an effective way of selecting topical relations by preference.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: TOPICOLL's results  with different networks", "labels": [], "entities": [{"text": "TOPICOLL", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.5119336247444153}]}]}