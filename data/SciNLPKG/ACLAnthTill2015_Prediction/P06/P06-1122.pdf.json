{"title": [{"text": "Modelling lexical redundancy for machine translation", "labels": [], "entities": [{"text": "machine translation", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.78212571144104}]}], "abstractContent": [{"text": "Certain distinctions made in the lexicon of one language maybe redundant when translating into another language.", "labels": [], "entities": []}, {"text": "We quantify redundancy among source types by the similarity of their distributions over target types.", "labels": [], "entities": []}, {"text": "We propose a language-independent framework for minimising lexical redundancy that can be optimised directly from parallel text.", "labels": [], "entities": [{"text": "minimising lexical redundancy", "start_pos": 48, "end_pos": 77, "type": "TASK", "confidence": 0.8258901238441467}]}, {"text": "Optimisation of the source lexicon fora given target language is viewed as model selection over a set of cluster-based translation models.", "labels": [], "entities": []}, {"text": "Redundant distinctions between types may exhibit monolingual regularities, for example , inflexion patterns.", "labels": [], "entities": []}, {"text": "We define a prior over model structure using a Markov random field and learn features over sets of monolingual types that are predictive of bilingual redundancy.", "labels": [], "entities": []}, {"text": "The prior makes model selection more robust without the need for language-specific assumptions regarding redundancy.", "labels": [], "entities": []}, {"text": "Using these models in a phrase-based SMT system, we show significant improvements in translation quality for certain language pairs.", "labels": [], "entities": [{"text": "SMT", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.865474283695221}]}], "introductionContent": [{"text": "Data-driven machine translation (MT) relies on models that can be efficiently estimated from parallel text.", "labels": [], "entities": [{"text": "Data-driven machine translation (MT)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7761933406194051}]}, {"text": "Token-level independence assumptions based on word-alignments can be used to decompose parallel corpora into manageable units for parameter estimation.", "labels": [], "entities": []}, {"text": "However, if training data is scarce or language pairs encode significantly different information in the lexicon, such as Czech and English, additional independence assumptions may assist the model estimation process.", "labels": [], "entities": []}, {"text": "Standard statistical translation models use separate parameters for each pair of source and target types.", "labels": [], "entities": [{"text": "statistical translation", "start_pos": 9, "end_pos": 32, "type": "TASK", "confidence": 0.6448531448841095}]}, {"text": "In these models, distinctions in either lexicon that are redundant to the translation process will result in unwarranted model complexity and make parameter estimation from limited parallel data more difficult.", "labels": [], "entities": [{"text": "parameter estimation", "start_pos": 147, "end_pos": 167, "type": "TASK", "confidence": 0.679249718785286}]}, {"text": "A natural way to eliminate such lexical redundancy is to group types into homogeneous clusters that do not differ significantly in their distributions over types in the other language.", "labels": [], "entities": []}, {"text": "Cluster-based translation models capture the corresponding independence assumptions.", "labels": [], "entities": []}, {"text": "Previous work on bilingual clustering has focused on coarse partitions of the lexicon that resemble automatically induced part-of-speech classes.", "labels": [], "entities": [{"text": "bilingual clustering", "start_pos": 17, "end_pos": 37, "type": "TASK", "confidence": 0.6581570506095886}]}, {"text": "These were used to model generic word-alignment patterns such as noun-adjective re-ordering between.", "labels": [], "entities": []}, {"text": "In contrast, we induce fine-grained partitions of the lexicon, conceptually closer to automatic lemmatisation, optimised specifically to assign translation probabilities.", "labels": [], "entities": []}, {"text": "Unlike lemmatisation or stemming, our method specifically quantifies lexical redundancy in a bilingual setting and does not make language-specific assumptions.", "labels": [], "entities": []}, {"text": "We tackle the problem of redundancy in the translation lexicon via Bayesian model selection over a set of cluster-based translation models.", "labels": [], "entities": []}, {"text": "We search for the model, defined by a clustering of the source lexicon, that maximises the marginal likelihood of target tokens in parallel data.", "labels": [], "entities": []}, {"text": "In this optimisation, source types are combined into clusters if their distributions over target types are too similar to warrant distinct parameters.", "labels": [], "entities": []}, {"text": "Redundant distinctions between types may exhibit regularities within a language, for instance, inflexion patterns.", "labels": [], "entities": []}, {"text": "These can be used to guide model selection.", "labels": [], "entities": [{"text": "model selection", "start_pos": 27, "end_pos": 42, "type": "TASK", "confidence": 0.6740336418151855}]}, {"text": "Here we show that the inclusion of a model 'prior' over the lexicon structure leads to more robust translation models.", "labels": [], "entities": []}, {"text": "Although a priori we do not know which monolingual features characterise redundancy fora given language pair, by defining a model over the prior monolingual space of source types and cluster assignments, we can introduce an inductive bias that allows clustering decisions in different parts of the lexicon to influence one another via monolingual features.", "labels": [], "entities": []}, {"text": "We use an EM-type algorithm to learn weights fora Markov random field parameterisation of this prior over lexicon structure.", "labels": [], "entities": []}, {"text": "We obtain significant improvements in translation quality as measured by BLEU, incorporating these optimised model within a phrase-based SMT system for three different language pairs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.9894355535507202}, {"text": "SMT", "start_pos": 137, "end_pos": 140, "type": "TASK", "confidence": 0.8470191955566406}]}, {"text": "The MRF prior improves the results and picks up features that appear to agree with linguistic intuitions of redundancy for the language pairs considered.", "labels": [], "entities": [{"text": "MRF prior", "start_pos": 4, "end_pos": 13, "type": "DATASET", "confidence": 0.6862249374389648}]}], "datasetContent": [{"text": "The two sets of experiments evaluate the baseline models and optimised lexicon models during word-alignment and phrase-level translation model estimation respectively.", "labels": [], "entities": [{"text": "phrase-level translation model estimation", "start_pos": 112, "end_pos": 153, "type": "TASK", "confidence": 0.8355278521776199}]}, {"text": "1: map the parallel corpus, perform word-alignment; estimate the phrase translation model using the original corpus.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.7428890466690063}]}, {"text": "2: smooth the phrase translation model, Here e, f and c e , cf are phrases mapped under the standard model and the model being tested respectively; \u03b3 is set once for all experiments on development data.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.7814091742038727}]}, {"text": "Wordalignments were generated using the optimal max-pref mapping for each training set.", "labels": [], "entities": []}, {"text": "shows the changes in BLEU when we incorporate the lexicon mappings during the wordalignment process.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9981416463851929}]}, {"text": "The standard SMT lexicon model is not optimal, as measured by BLEU, for any of the languages or training set sizes considered.", "labels": [], "entities": [{"text": "SMT lexicon", "start_pos": 13, "end_pos": 24, "type": "TASK", "confidence": 0.9386959671974182}, {"text": "BLEU", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.9985352754592896}]}, {"text": "Increases over this baseline, however, diminish with more training data.", "labels": [], "entities": []}, {"text": "For both Czech and Welsh, the explicit model selection procedure that we have proposed results in better translations than all of the baseline models when the MRF prior is used; again these increases diminish with larger training sets.", "labels": [], "entities": []}, {"text": "We note that the stemming baseline models appear to be more effective for Czech than for Welsh.", "labels": [], "entities": []}, {"text": "The impact of the MRF prior is also greater for smaller training sets.", "labels": [], "entities": [{"text": "MRF", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.7919585108757019}]}, {"text": "shows the results of using these models to smooth the phrase translation table.", "labels": [], "entities": [{"text": "phrase translation table", "start_pos": 54, "end_pos": 78, "type": "TASK", "confidence": 0.7988409399986267}]}, {"text": "With the exception of Czech, the improvements are smaller than for Exp 1.", "labels": [], "entities": [{"text": "Czech", "start_pos": 22, "end_pos": 27, "type": "DATASET", "confidence": 0.9573480486869812}, {"text": "Exp", "start_pos": 67, "end_pos": 70, "type": "DATASET", "confidence": 0.8517441749572754}]}, {"text": "For all source languages and models we found that it was optimal to leave the target lexicon unmapped when smoothing the phrase translation model.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 121, "end_pos": 139, "type": "TASK", "confidence": 0.7201228588819504}]}, {"text": "The system we use is described in).", "labels": [], "entities": []}, {"text": "The phrase-based translation model includes phrase-level and lexical weightings in both directions.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.699939489364624}]}, {"text": "We use the decoder's default behaviour for unknown words copying them verbatim to the output.", "labels": [], "entities": []}, {"text": "Smoothed trigram language models are estimated on training sections of the parallel corpus.", "labels": [], "entities": []}, {"text": "We used the parallel sections of the Prague Treebank (), French and English sections of the Europarl corpus () and parallel text from the Welsh Assembly 4 (see.", "labels": [], "entities": [{"text": "Prague Treebank", "start_pos": 37, "end_pos": 52, "type": "DATASET", "confidence": 0.9905588328838348}, {"text": "Europarl corpus", "start_pos": 92, "end_pos": 107, "type": "DATASET", "confidence": 0.9941023290157318}, {"text": "parallel text from the Welsh Assembly 4", "start_pos": 115, "end_pos": 154, "type": "DATASET", "confidence": 0.7511986153466361}]}, {"text": "The source languages, Czech, French and Welsh, were chosen on the basis that they may exhibit different degrees of redundancy with respect to English and that they differ morphologically.", "labels": [], "entities": []}, {"text": "Only the Czech corpus has explicit morphological annotation.", "labels": [], "entities": [{"text": "Czech corpus", "start_pos": 9, "end_pos": 21, "type": "DATASET", "confidence": 0.9422062039375305}]}], "tableCaptions": [{"text": " Table 2: BLEU scores with optimised lexicon applied during word-alignment (Exp. 1)  Czech-English  French-English  Welsh-English  Model  10K sent. 21K  10K  25K 100K 250K 10K  25K 100K 250K  standard  32.31 36.17 20.76 23.17 26.61 27.63 35.45 39.92 45.02 46.47  max-pref  34.18 37.34 21.63 23.94 26.45 28.25 35.88 41.03 44.82 46.11  min-freq  33.95 36.98 21.22 23.77 26.74 27.98 36.23 40.65 45.38 46.35  src  33.95 37.27 21.43 24.42 26.99 27.82 36.98 40.98 45.81 46.45  src+mrf  33.97 37.89 21.63 24.38 26.74 28.39 37.36 41.13 46.50 46.56  src+trg  34.24 38.28 22.05 24.02 26.53 27.80 36.83 41.31 45.22 46.51  src+trg+mrf  34.70 38.44 22.33 23.95 26.69 27.75 37.56 42.19 45.18 46.48", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9978635907173157}]}, {"text": " Table 3: BLEU scores with optimised lexicon used to smooth phrase-based translation model (Exp. 2)  Czech-English  French-English  Welsh-English  Model  10K sent. 21K  10K  25K 100K 250K 10K  25K 100K 250K  (standard) 5  34.18 37.34 21.63 23.94 26.45 28.25 35.88 41.03 44.82 46.11  max-pref  35.63 38.81 22.49 24.10 26.99 28.26 37.31 40.09 45.57 46.41  min-freq  34.65 37.75 21.14 23.41 26.29 27.47 36.40 40.84 45.75 46.45  src  34.38 37.98 21.28 24.17 26.88 28.35 36.94 39.99 45.75 46.65  src+mrf  36.24 39.70 22.02 24.10 26.82 28.09 37.81 41.04 46.16 46.51", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9974115490913391}]}, {"text": " Table 4: System output (Welsh 25K; Exp. 2)", "labels": [], "entities": [{"text": "Welsh 25K; Exp.", "start_pos": 25, "end_pos": 40, "type": "DATASET", "confidence": 0.9371176362037659}]}, {"text": " Table 5: Features learned by MRF prior  Czech  French  Welsh", "labels": [], "entities": [{"text": "MRF prior  Czech  French  Welsh", "start_pos": 30, "end_pos": 61, "type": "DATASET", "confidence": 0.7534851670265198}]}, {"text": " Table 6: Optimal lexicon size (ratio of raw vocab.)  Czech French Welsh  Word-alignment  0.26  0.22  0.24  TM smoothing  0.28  0.38  0.51", "labels": [], "entities": []}]}