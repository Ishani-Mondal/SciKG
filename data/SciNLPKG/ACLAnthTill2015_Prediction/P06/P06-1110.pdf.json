{"title": [], "abstractContent": [{"text": "The present work advances the accuracy and training speed of discrimina-tive parsing.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.999055802822113}, {"text": "discrimina-tive parsing", "start_pos": 61, "end_pos": 84, "type": "TASK", "confidence": 0.5820294320583344}]}, {"text": "Our discriminative parsing method has no generative component, yet surpasses a generative baseline on constituent parsing, and does so with minimal linguistic cleverness.", "labels": [], "entities": [{"text": "constituent parsing", "start_pos": 102, "end_pos": 121, "type": "TASK", "confidence": 0.755811333656311}]}, {"text": "Our model can incorporate arbitrary features of the input and parse state, and performs feature selection incrementally over an exponential feature space during training.", "labels": [], "entities": []}, {"text": "We demonstrate the flexibility of our approach by testing it with several parsing strategies and various feature sets.", "labels": [], "entities": []}, {"text": "Our implementation is freely available at: http://nlp.cs.nyu.edu/parser/.", "labels": [], "entities": []}], "introductionContent": [{"text": "Discriminative machine learning methods have improved accuracy on many NLP tasks, including POS-tagging, shallow parsing, relation extraction, and machine translation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9982491731643677}, {"text": "shallow parsing", "start_pos": 105, "end_pos": 120, "type": "TASK", "confidence": 0.5905523300170898}, {"text": "relation extraction", "start_pos": 122, "end_pos": 141, "type": "TASK", "confidence": 0.8881397545337677}, {"text": "machine translation", "start_pos": 147, "end_pos": 166, "type": "TASK", "confidence": 0.8156847953796387}]}, {"text": "Some advances have also been made on full syntactic constituent parsing.", "labels": [], "entities": [{"text": "full syntactic constituent parsing", "start_pos": 37, "end_pos": 71, "type": "TASK", "confidence": 0.6193865984678268}]}, {"text": "Successful discriminative parsers have relied on generative models to reduce training time and raise accuracy above generative baselines).", "labels": [], "entities": [{"text": "generative", "start_pos": 49, "end_pos": 59, "type": "TASK", "confidence": 0.959724485874176}, {"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9988583326339722}]}, {"text": "However, relying on information from a generative model might prevent these approaches from realizing the accuracy gains achieved by discriminative methods on other NLP tasks.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9968751668930054}]}, {"text": "Another problem is training speed: Discriminative parsers are notoriously slow to train.", "labels": [], "entities": []}, {"text": "In the present work, we make progress towards overcoming these obstacles.", "labels": [], "entities": []}, {"text": "We propose a flexible, end-to-end discriminative method for training parsers, demonstrating techniques that might also be useful for other structured prediction problems.", "labels": [], "entities": []}, {"text": "The proposed method does model selection without ad-hoc smoothing or frequency-based feature cutoffs.", "labels": [], "entities": [{"text": "model selection", "start_pos": 25, "end_pos": 40, "type": "TASK", "confidence": 0.72468501329422}]}, {"text": "It requires no heuristics or human effort to optimize the single important hyper-parameter.", "labels": [], "entities": []}, {"text": "The training regime can use all available information from the entire parse history.", "labels": [], "entities": []}, {"text": "The learning algorithm projects the hand-provided features into a compound feature space and performs incremental feature selection over this large feature space.", "labels": [], "entities": []}, {"text": "The resulting parser achieves higher accuracy than a generative baseline, despite not using a generative model as a feature.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9987130165100098}]}, {"text": "Section 2 describes the parsing algorithm.", "labels": [], "entities": [{"text": "parsing", "start_pos": 24, "end_pos": 31, "type": "TASK", "confidence": 0.9830294251441956}]}, {"text": "Section 3 presents the learning method.", "labels": [], "entities": []}, {"text": "Section 4 presents experiments with discriminative parsers built using these methods.", "labels": [], "entities": []}, {"text": "Section 5 compares our approach to related work.", "labels": [], "entities": []}], "datasetContent": [{"text": "Discriminative parsers are notoriously slow to train.", "labels": [], "entities": [{"text": "Discriminative parsers", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7253749370574951}]}, {"text": "For example, took several months to train on the \u2264 15 word sentences in the English Penn Treebank (Dan Klein, p.c.).", "labels": [], "entities": [{"text": "English Penn Treebank", "start_pos": 76, "end_pos": 97, "type": "DATASET", "confidence": 0.7343479593594869}]}, {"text": "The present work makes progress towards faster discriminative parser training: our slowest classifier took fewer than 5 days to train.", "labels": [], "entities": []}, {"text": "Even so, it would have taken much longer to train on the entire treebank.", "labels": [], "entities": []}, {"text": "We follow in training and testing on \u2264 15 word sentences in the English Penn Treebank (.", "labels": [], "entities": [{"text": "English Penn Treebank", "start_pos": 64, "end_pos": 85, "type": "DATASET", "confidence": 0.7256504893302917}]}, {"text": "We used sections 02-21 for training, section 22 for development, and section 23 for testing, preprocessed as per.", "labels": [], "entities": []}, {"text": "We evaluated our parser using the standard PARSEVAL measures: labelled precision, labelled recall, and labelled F-measure (Prec., Rec., and F 1 , respectively), which are based on the number of nonterminal items in the parser's output that match those in the gold-standard parse.", "labels": [], "entities": [{"text": "PARSEVAL", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.6080244183540344}, {"text": "precision", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.7732039093971252}, {"text": "recall", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.9133776426315308}, {"text": "F-measure", "start_pos": 112, "end_pos": 121, "type": "METRIC", "confidence": 0.7931388020515442}, {"text": "F 1", "start_pos": 140, "end_pos": 143, "type": "METRIC", "confidence": 0.9913206398487091}]}, {"text": "As mentioned in Section 2, items are inferred bottom-up and the parser cannot infer any item that crosses an item already in the state.", "labels": [], "entities": []}, {"text": "Although there are O(n 2 ) possible (span, label) pairs over a frontier containing n items, we reduce this to the \u2248 5 \u00b7 n inferences that have at most five children.", "labels": [], "entities": []}, {"text": "6 Steps for preprocessing the data.", "labels": [], "entities": []}, {"text": "Starred steps are performed only when parse trees are available in the data (e.g. not on test data).", "labels": [], "entities": []}, {"text": "1, removing outermost punctuation might discard useful information.", "labels": [], "entities": []}, {"text": "saw a LFMS improvement of 0.8% over their baseline discriminative parser after adding punctuation features, one of which encoded the sentence-final punctuation.", "labels": [], "entities": []}, {"text": "To ensure the parser does not enter an infinite loop, no two items in a state can have both the same span and the same label.", "labels": [], "entities": []}, {"text": "Given these restrictions on candidate inferences, there were roughly 40 million training examples generated in the training set.", "labels": [], "entities": []}, {"text": "These were partitioned among the 26 constituent label classifiers.", "labels": [], "entities": []}, {"text": "Building a decision tree (Steps 1.5-1.9 in Listing 1) using the entire example set I can be very expensive.", "labels": [], "entities": []}, {"text": "We estimate loss gradients (Equation 13) using a sample of the inference set, which gives a 100-fold increase in training speed).", "labels": [], "entities": []}, {"text": "Our atomic feature set A contains 300K features, each of the form \"is there an item in group J whose label/headword/headtag/headtagclass is 'X'?\".", "labels": [], "entities": []}, {"text": "Possible values of 'X' for each predicate are collected from the training data.", "labels": [], "entities": []}, {"text": "For 1 \u2264 n \u2264 3, possible values for J are: \u2022 the first/last n child items \u2022 the first n left/right context items \u2022 then children items left/right of the head \u2022 the head item.", "labels": [], "entities": []}, {"text": "The left and right context items are the frontier items to the left and right of the children of the candidate inference, respectively.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2 Results on the development set, training  and testing using only \u2264 15 word sentences.", "labels": [], "entities": []}, {"text": " Table 4 Results of parsers on the test set, training  and testing using only \u2264 15 word sentences.  % Rec. % Prec. F 1  Turian and Melamed (2005) 86.47 87.80 87.13  Bikel (2004)  87.85 88.75 88.30  Taskar et al. (2004)  89.10 89.14 89.12  kitchen sink  89.26 89.55 89.40", "labels": [], "entities": [{"text": "Rec", "start_pos": 102, "end_pos": 105, "type": "METRIC", "confidence": 0.9740788340568542}, {"text": "Prec. F 1  Turian and Melamed (2005) 86.47 87.80 87.13  Bikel (2004)  87.85 88.75 88.30  Taskar et al. (2004)  89.10 89.14 89.12  kitchen sink  89.26 89.55 89.40", "start_pos": 109, "end_pos": 270, "type": "DATASET", "confidence": 0.855120621408735}]}]}