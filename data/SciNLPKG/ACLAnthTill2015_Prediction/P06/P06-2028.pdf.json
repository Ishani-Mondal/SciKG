{"title": [{"text": "Using Lexical Dependency and Ontological Knowledge to Improve a Detailed Syntactic and Semantic Tagger of English", "labels": [], "entities": [{"text": "Syntactic and Semantic Tagger of English", "start_pos": 73, "end_pos": 113, "type": "TASK", "confidence": 0.7468384802341461}]}], "abstractContent": [], "introductionContent": [{"text": "Part-of-speech (POS) tagging has been one of the fundamental areas of research in natural language processing for many years.", "labels": [], "entities": [{"text": "Part-of-speech (POS) tagging", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.5896494805812835}]}, {"text": "Most of the prior research has focussed on the task of labeling text with tags that reflect the words' syntactic role in the sentence.", "labels": [], "entities": []}, {"text": "In parallel to this, the task of word sense disambiguation (WSD), the process of deciding in which semantic sense the word is being used, has been actively researched.", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 33, "end_pos": 64, "type": "TASK", "confidence": 0.8331459760665894}]}, {"text": "This paper addresses a combination of these two fields, that is: labeling running words with tags that comprise, in addition to their syntactic function, abroad semantic class that signifies the semantics of the word in the context of the sentence, but does not necessarily provide information that is sufficiently finegrained as to disambiguate its sense.", "labels": [], "entities": []}, {"text": "This differs * National Institute of Information and Communications Technology \u2020", "labels": [], "entities": [{"text": "National Institute of Information and Communications Technology", "start_pos": 15, "end_pos": 78, "type": "DATASET", "confidence": 0.9170422554016113}]}], "datasetContent": [{"text": "The primary corpus used for the experiments presented in this paper is the ATR General English Treebank.", "labels": [], "entities": [{"text": "ATR General English Treebank", "start_pos": 75, "end_pos": 103, "type": "DATASET", "confidence": 0.953220784664154}]}, {"text": "This consists of 518,080 words (approximately 20 words per sentence, on average) of text annotated with a detailed semantic and syntactic tagset.", "labels": [], "entities": []}, {"text": "To understand the nature of the task involved in the experiments presented in this paper, one needs some familiarity with the ATR General English Tagset.", "labels": [], "entities": [{"text": "ATR General English Tagset", "start_pos": 126, "end_pos": 152, "type": "DATASET", "confidence": 0.8395452052354813}]}, {"text": "For detailed presentations, see).", "labels": [], "entities": []}, {"text": "An apercu can be gained, however, from, which shows two sample sentences from the ATR Treebank (and originally from a Chinese take-out food flier), tagged with respect to the ATR General English Tagset.", "labels": [], "entities": [{"text": "apercu", "start_pos": 3, "end_pos": 9, "type": "METRIC", "confidence": 0.9597687125205994}, {"text": "ATR Treebank", "start_pos": 82, "end_pos": 94, "type": "DATASET", "confidence": 0.9870577156543732}, {"text": "ATR General English Tagset", "start_pos": 175, "end_pos": 201, "type": "DATASET", "confidence": 0.9136694073677063}]}, {"text": "Each verb, noun, adjective and adverb in the ATR tagset includes a semantic label, chosen from 42 noun/adjective/adverb categories and 29 verb/verbal categories, some overlap existing between these category sets.", "labels": [], "entities": [{"text": "ATR tagset", "start_pos": 45, "end_pos": 55, "type": "DATASET", "confidence": 0.8416155576705933}]}, {"text": "Proper nouns, plus certain adjectives and certain numerical expressions, are further categorized via an additional 35 \"proper-noun\" categories.", "labels": [], "entities": []}, {"text": "These semantic categories are intended for any \"Standard-American-English\" text, in any domain.", "labels": [], "entities": []}, {"text": "Sample categories include: \"physical.attribute\" (nouns/adjectives/adverbs), \"alter\" (verbs/verbals), \"interpersonal.act\" (nouns/adjectives/adverbs/verbs/verbals), \"orgname\" (proper nouns), and \"zipcode\" (numericals).", "labels": [], "entities": []}, {"text": "They were developed by the ATR grammarian and then proven and refined via day-in-day-out tagging for six months at ATR by two human \"treebankers\", then via four months of tagset-testing-only work at Lancaster University (UK) by five treebankers, with daily interactions among treebankers, and between the treebankers and the ATR grammarian.", "labels": [], "entities": [{"text": "ATR", "start_pos": 115, "end_pos": 118, "type": "DATASET", "confidence": 0.9559369683265686}]}, {"text": "The semantic categorization is, of course, in addition to an extensive syntactic classification, involving some 165 basic syntactic tags.", "labels": [], "entities": []}, {"text": "The test corpus has been designed specifically to cope with the ambiguity of the tagset.", "labels": [], "entities": []}, {"text": "It is possible to correctly assign anyone of a number of 'allowable' tags to a word in context.", "labels": [], "entities": []}, {"text": "For example, the tag of the word battle in the phrase \"a legal battle\" could be either NN1PROBLEM or NN1INTER-ACT, indicating that the semantics is either a problem, or an inter-personal action.", "labels": [], "entities": []}, {"text": "The test corpus consists of 53,367 words sampled from the same domains as, and in approximately the same proportions as the training data, and labeled with a set of up to 6 allowable tags for each word.", "labels": [], "entities": []}, {"text": "During testing, only if the predicted tag fails to match any of the allowed tags is it considered an error.", "labels": [], "entities": []}, {"text": "The results of our experiments are shown in Table 1.", "labels": [], "entities": []}, {"text": "The task of assigning semantic and syntactic tags is considerably more difficult than simply assigning syntactic tags due to the inherent ambiguity of the tagset.", "labels": [], "entities": []}, {"text": "To gauge the level of human performance on this task, experiments were conducted to determine inter-annotator consistency; in addition, annotator accuracy was measured on 5,000 words of data.", "labels": [], "entities": [{"text": "consistency", "start_pos": 110, "end_pos": 121, "type": "METRIC", "confidence": 0.8197681307792664}, {"text": "accuracy", "start_pos": 146, "end_pos": 154, "type": "METRIC", "confidence": 0.9459481835365295}]}, {"text": "Both the agreement and accuracy were found to be approximately 97%, with all of the inconsistencies and tagging errors arising from the semantic component of the tags.", "labels": [], "entities": [{"text": "agreement", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.9918431639671326}, {"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9963830709457397}]}, {"text": "97% accuracy is therefore an approximate upper bound for the performance one would expect from an automatic tagger.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.998350977897644}]}, {"text": "As a point of reference fora lower bound, the overall accuracy of a tagger which uses only a single feature representing the identity of the word being tagged is approximately 73%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9994813799858093}]}, {"text": "The overall baseline accuracy was 82.58% with only 30.58% of OOV's being tagged correctly.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9302233457565308}, {"text": "OOV", "start_pos": 61, "end_pos": 64, "type": "METRIC", "confidence": 0.6748700737953186}]}, {"text": "Of the two lexical dependency-based approaches, the features derived from Collins' parser were the most effective, improving accuracy by 0.8% overall.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 125, "end_pos": 133, "type": "METRIC", "confidence": 0.9988442659378052}]}, {"text": "To put the magnitude of this gain into perspective, dropping the features for the identity of the previous word from the baseline model, only degraded performance by 0.2%.", "labels": [], "entities": []}, {"text": "The features from the link grammar parser were handicapped due to the fact that only 31% of the sentences were able to be parsed.", "labels": [], "entities": []}, {"text": "When the model (Model 3 in Table 1) was evaluated on only the parsable portion on the test set, the accuracy obtained was roughly comparable to that using the dependencies from Collins' parses.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9994420409202576}]}, {"text": "To control for the differences between these parseable sentences and the full test set, Model 4 was tested on the same 31% of sentence that parsed.", "labels": [], "entities": []}, {"text": "Its accuracy was within 0.2% of the accuracy on the whole test set in all cases.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9997007846832275}, {"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9993466734886169}]}, {"text": "Neither of the lexical dependency-based approaches had a particularly strong effect on the performance on OOV's.", "labels": [], "entities": []}, {"text": "This is inline with our intuition, since these features rely on the identity of the word being tagged, and the performance gain we see is due to the improvement in labeling accuracy of the context around the OOV.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 173, "end_pos": 181, "type": "METRIC", "confidence": 0.9388646483421326}, {"text": "OOV", "start_pos": 208, "end_pos": 211, "type": "DATASET", "confidence": 0.788072943687439}]}, {"text": "In contrast to this, for the word-ontology-based feature sets, one would hope to see a marked improvement on OOV's, since these features were designed specifically to address this issue.", "labels": [], "entities": []}, {"text": "We do see a strong response to these features in the accuracy of the models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9992515444755554}]}, {"text": "The overall accuracy when using the automatically acquired ontology is only 0.1% higher than the accuracy using dependencies from Collins' parser.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9995649456977844}, {"text": "accuracy", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9988986253738403}]}, {"text": "However the accuracy on OOV's jumps 3.5% to 35.08% compared to just 0.7% for Model 4.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9998480081558228}]}, {"text": "Performance for both clustering techniques was quite similar, with the WordNet taxonomical features being slightly more useful, especially for OOV's.", "labels": [], "entities": [{"text": "WordNet taxonomical", "start_pos": 71, "end_pos": 90, "type": "DATASET", "confidence": 0.9537332355976105}]}, {"text": "One possible explanation for this is that overall, the coverage of both techniques is similar, but for rarer words, the MI clustering can be inconsistent due to lack of data (for an example, see.2: the word newsstand is a member of a cluster of words that appear to be commodities), whereas the WordNet clustering remains consistent even for rare words.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 295, "end_pos": 302, "type": "DATASET", "confidence": 0.9106308221817017}]}, {"text": "It seems reasonable to expect, however, that the automatic method would do better if trained on more data.", "labels": [], "entities": []}, {"text": "Furthermore, all uses of words can be covered by automatic clustering, whereas for example, the common use of the word apple as a company name is beyond the scope of WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 166, "end_pos": 173, "type": "DATASET", "confidence": 0.9551694989204407}]}, {"text": "In Model 7 we combined the best lexical dependency feature set (Model 4) with the best clustering feature set (Model 6) to investigate the amount of information overlap existing between the feature sets.", "labels": [], "entities": []}, {"text": "Models 4 and 6 improved the baseline performance by 0.8% and 1.3% respectively.", "labels": [], "entities": []}, {"text": "In combination, accuracy was increased by 2.3%, 0.2% more than the sum of the component models' gains.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9997832179069519}]}, {"text": "This is very encouraging and indicates that these models provide independent information, with virtually all of the benefit from both models manifesting itself in the combined model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Tagging accuracy (%), '+' being shorthand for \"Baseline +\", 'c.i.' denotes the confidence  interval of the mean at a 95% significance level, calculated using bootstrap resampling.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9136651158332825}]}]}