{"title": [{"text": "Automatic Identification of Pro and Con Reasons in Online Reviews", "labels": [], "entities": [{"text": "Automatic Identification of Pro and Con Reasons", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.7465145715645382}]}], "abstractContent": [{"text": "In this paper, we present a system that automatically extracts the pros and cons from online reviews.", "labels": [], "entities": []}, {"text": "Although many approaches have been developed for extracting opinions from text, our focus here is on extracting the reasons of the opinions, which may themselves be in the form of either factor opinion.", "labels": [], "entities": []}, {"text": "Leveraging online review sites with author-generated pros and cons, we propose a system for aligning the pros and cons to their sentences in review texts.", "labels": [], "entities": []}, {"text": "A maximum en-tropy model is then trained on the resulting labeled set to subsequently extract pros and cons from online review sites that do not explicitly provide them.", "labels": [], "entities": []}, {"text": "Our experimental results show that our resulting system identifies pros and cons with 66% precision and 76% recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 90, "end_pos": 99, "type": "METRIC", "confidence": 0.9995840191841125}, {"text": "recall", "start_pos": 108, "end_pos": 114, "type": "METRIC", "confidence": 0.9991776347160339}]}], "introductionContent": [{"text": "Many opinions are being expressed on the Web in such settings as product reviews, personal blogs, and newsgroup message boards.", "labels": [], "entities": []}, {"text": "People increasingly participate to express their opinions online.", "labels": [], "entities": []}, {"text": "This trend has raised many interesting and challenging research topics such as subjectivity detection, semantic orientation classification, and review classification.", "labels": [], "entities": [{"text": "subjectivity detection", "start_pos": 79, "end_pos": 101, "type": "TASK", "confidence": 0.7452978193759918}, {"text": "semantic orientation classification", "start_pos": 103, "end_pos": 138, "type": "TASK", "confidence": 0.8466753562291464}, {"text": "review classification", "start_pos": 144, "end_pos": 165, "type": "TASK", "confidence": 0.7613508999347687}]}, {"text": "Subjectivity detection is the task of identifying subjective words, expressions, and sentences.", "labels": [], "entities": [{"text": "Subjectivity detection", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9535030722618103}]}, {"text": "Identifying subjectivity helps separate opinions from fact, which maybe useful in question answering, summarization, etc.", "labels": [], "entities": [{"text": "question answering", "start_pos": 82, "end_pos": 100, "type": "TASK", "confidence": 0.8422890901565552}, {"text": "summarization", "start_pos": 102, "end_pos": 115, "type": "TASK", "confidence": 0.9807783365249634}]}, {"text": "Semantic orientation classification is a task of determining positive or negative sentiment of words ().", "labels": [], "entities": [{"text": "Semantic orientation classification", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.847319503625234}]}, {"text": "Sentiment of phrases and sentences has also been studied in ().", "labels": [], "entities": [{"text": "Sentiment of phrases and sentences", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8931881070137024}]}, {"text": "Document level sentiment classification is mostly applied to reviews, where systems assign a positive or negative sentiment fora whole review document (.", "labels": [], "entities": [{"text": "Document level sentiment classification", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.7903188765048981}]}, {"text": "Building on this work, more sophisticated problems in the opinion domain have been studied by many researchers.", "labels": [], "entities": []}, {"text": "() identified the holder (source) of opinions expressed in sentences using various techniques.", "labels": [], "entities": [{"text": "identified the holder (source) of opinions expressed in sentences", "start_pos": 3, "end_pos": 68, "type": "TASK", "confidence": 0.6391339925202456}]}, {"text": "() focused on the strength of opinion clauses, finding strong and weak opinions.) presented a system that aggregates and quantifies degree assessment of opinions scattered throughout web pages.", "labels": [], "entities": []}, {"text": "Beyond document level sentiment classification in online product reviews, () concentrated on mining and summarizing reviews by extracting opinion sentences regarding product features.", "labels": [], "entities": [{"text": "document level sentiment classification", "start_pos": 7, "end_pos": 46, "type": "TASK", "confidence": 0.7415147572755814}, {"text": "summarizing reviews", "start_pos": 104, "end_pos": 123, "type": "TASK", "confidence": 0.8898613452911377}]}, {"text": "In this paper, we focus on another challenging yet critical problem of opinion analysis, identifying reasons for opinions, especially for opinions in online product reviews.", "labels": [], "entities": [{"text": "opinion analysis", "start_pos": 71, "end_pos": 87, "type": "TASK", "confidence": 0.7380422800779343}]}, {"text": "The opinion reason identification problem in online reviews seeks to answer the question \"What are the reasons that the author of this review likes or dislikes the product?\"", "labels": [], "entities": [{"text": "opinion reason identification", "start_pos": 4, "end_pos": 33, "type": "TASK", "confidence": 0.7021983861923218}]}, {"text": "For example, in hotel reviews, information such as \"found 189 positive reviews and 65 negative reviews\" may not fully satisfy the information needs of different users.", "labels": [], "entities": []}, {"text": "More useful information would be \"This hotel is great for families with young infants\" or \"Elevators are grouped according to floors, which makes the wait short\".", "labels": [], "entities": []}, {"text": "This work differs in important ways from studies in () and ().", "labels": [], "entities": []}, {"text": "These approaches extract features of products and identify sentences that contain opinions about those features by using opinion words and phrases.", "labels": [], "entities": []}, {"text": "Here, we focus on extracting pros and cons which include not only sentences that contain opinion-bearing expressions about products and features but also sentences with reasons why an author of a review writes the review.", "labels": [], "entities": []}, {"text": "Following are examples identified by our system.", "labels": [], "entities": []}], "datasetContent": [{"text": "We collected two different domains of reviews from epinions.com: product reviews and restaurant reviews.", "labels": [], "entities": []}, {"text": "As for the product reviews, we collected 3241 reviews The purpose of selecting one of electronics products and restaurants as topics of reviews for our study is to test our approach in two extremely different situations.", "labels": [], "entities": []}, {"text": "Reasons why consumers like or dislike a product in electronics' reviews are mostly about specific and tangible features.", "labels": [], "entities": []}, {"text": "Also, there are somewhat a fixed set of features of a specific type of product, for example, ease of use, durability, battery life, photo quality, and shutter lag for digital cameras.", "labels": [], "entities": [{"text": "ease", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.9922010898590088}]}, {"text": "Consequently, we can expect that reasons in electronics' reviews may share those product feature words and words that describe aspects of features such as short or long for battery life.", "labels": [], "entities": []}, {"text": "This fact might make the reason identification task easy.", "labels": [], "entities": [{"text": "reason identification task", "start_pos": 25, "end_pos": 51, "type": "TASK", "confidence": 0.9541846712430319}]}, {"text": "On the other hand, restaurant reviewers talk about very diverse aspects and abstract features as reasons.", "labels": [], "entities": []}, {"text": "For example, reasons such as \"You feel like you are in a train station or a busy amusement park that is ill-staffed to meet demand!\", \"preferential treatment given to large groups\", and \"they don't offer salads of any kind\" are hard to predict.", "labels": [], "entities": []}, {"text": "Also, they seem rarely share common keyword features.", "labels": [], "entities": []}, {"text": "We first automatically labeled each sentence in those reviews collected from each domain with the features described in Section 3.1.", "labels": [], "entities": []}, {"text": "We divided the data for training and testing.", "labels": [], "entities": []}, {"text": "We then trained our model using the training set and tested it to see if the system can successfully label sentences in the test set.", "labels": [], "entities": []}, {"text": "From the database 4 in complaints.com, we searched for the same topics of reviews as Dataset 1: 59 complaints reviews about mp3 players and 322 reviews about restaurants . We tested our system on this dataset and compare the results against human judges' annotation results.", "labels": [], "entities": []}, {"text": "Subsection 5.2 reports the evaluation results.", "labels": [], "entities": []}, {"text": "We describe two goals in our experiments in this section.", "labels": [], "entities": []}, {"text": "The first is to investigate how well our pro and con detection model with different feature combinations performs on the data we collected from epinions.com.", "labels": [], "entities": [{"text": "pro and con detection", "start_pos": 41, "end_pos": 62, "type": "TASK", "confidence": 0.6995028704404831}]}, {"text": "The second is to see how well the trained model performs on new data from a different source, complaint.com.", "labels": [], "entities": []}, {"text": "For both datasets, we carried out two separate sets of experiments, for the domains of mp3 players and restaurant reviews.", "labels": [], "entities": []}, {"text": "We divided data into 80% for training, 10% for development, and 10% for test for our experiments.", "labels": [], "entities": []}, {"text": "Identification step: show pros and cons sentences identification results of our system for mp3 player and restaurant reviews respectively.", "labels": [], "entities": [{"text": "Identification", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.8865430951118469}]}, {"text": "The first column indicates which combination of features was used for our model (see for the meaning of Op, Lex, and Pos feature categories).", "labels": [], "entities": []}, {"text": "We measure the performance with accuracy (Acc), precision (Prec), recall (Recl), and F-score 6 . The baseline system assigned all sentences as reason and achieved 57.75% and 54.82% of accuracy.", "labels": [], "entities": [{"text": "accuracy (Acc)", "start_pos": 32, "end_pos": 46, "type": "METRIC", "confidence": 0.8504523932933807}, {"text": "precision (Prec)", "start_pos": 48, "end_pos": 64, "type": "METRIC", "confidence": 0.8554206788539886}, {"text": "recall (Recl)", "start_pos": 66, "end_pos": 79, "type": "METRIC", "confidence": 0.9310292303562164}, {"text": "F-score", "start_pos": 85, "end_pos": 92, "type": "METRIC", "confidence": 0.9992839694023132}, {"text": "accuracy", "start_pos": 184, "end_pos": 192, "type": "METRIC", "confidence": 0.9985711574554443}]}, {"text": "The system performed well when it only used lexical features in mp3 player reviews (76.27% of accuracy in Lex), whereas it performed well with the combination of lexical and opinion features in restaurant reviews (Lex+Op row in).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9975026249885559}]}, {"text": "It was very interesting to see that the system achieved a very low score when it only used opinion word features.", "labels": [], "entities": []}, {"text": "We can interpret this phenomenon as supporting our hypothesis that pro and con sentences in reviews are often purely At the time), there were total 42593 complaint reviews available in the database.", "labels": [], "entities": []}, {"text": "Average numbers of sentences in a complaint is 19.57 for mp3 player reviews and 21.38 for restaurant reviews.", "labels": [], "entities": []}, {"text": "We calculated F-score by factual.", "labels": [], "entities": [{"text": "F-score", "start_pos": 14, "end_pos": 21, "type": "METRIC", "confidence": 0.9980010390281677}]}, {"text": "However, opinion features improved both precision and recall when combined with lexical features in restaurant reviews.", "labels": [], "entities": [{"text": "precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9994274377822876}, {"text": "recall", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.999019980430603}]}, {"text": "It was also interesting that experiments on mp3 players reviews achieved mostly higher scores than restaurants.", "labels": [], "entities": []}, {"text": "Like the observation we described in Subsection 4.1, frequently mentioned keywords of product features (e.g. durability) may have helped performance, especially with lexical features.", "labels": [], "entities": []}, {"text": "Another interesting observation is that the positional features that helped in topic sentence identification did not help much for our task.", "labels": [], "entities": [{"text": "topic sentence identification", "start_pos": 79, "end_pos": 108, "type": "TASK", "confidence": 0.8360400994618734}]}, {"text": "Classification step: show the system results of the pro and con classification task.", "labels": [], "entities": [{"text": "Classification", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9305448532104492}, {"text": "pro and con classification task", "start_pos": 52, "end_pos": 83, "type": "TASK", "confidence": 0.6859368860721589}]}, {"text": "The baseline system marked all sentences as pros and achieved 53.87% and 50.71% accuracy for each domain.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9990098476409912}]}, {"text": "All features performed better than the baseline but the results are not as good as in the identification task.", "labels": [], "entities": [{"text": "identification task", "start_pos": 90, "end_pos": 109, "type": "TASK", "confidence": 0.8826577365398407}]}, {"text": "Unlike the identification task, opinion words by themselves achieved the best accuracy in both mp3 player and restaurant domains.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9985775947570801}]}, {"text": "We think opinion words played more important roles in classifying pros and cons than identifying them.", "labels": [], "entities": []}, {"text": "Position features helped recognizing con sentences in mp3 player reviews.", "labels": [], "entities": [{"text": "recognizing con sentences in mp3 player reviews", "start_pos": 25, "end_pos": 72, "type": "TASK", "confidence": 0.7740589465413775}]}, {"text": "This subsection reports the evaluation results of our system on Dataset 2.", "labels": [], "entities": []}, {"text": "Since Dataset 2 from complaints.com has no training data, we trained a system on Dataset 1 and applied it to Dataset 2.", "labels": [], "entities": [{"text": "Dataset 2 from complaints.com", "start_pos": 6, "end_pos": 35, "type": "DATASET", "confidence": 0.8705615550279617}]}, {"text": "A tough question, however, is how to evaluate the system results.", "labels": [], "entities": []}, {"text": "Since it seemed impossible to evaluate the system without involving a human judge, we annotated a small set of data manually for evaluation purposes.", "labels": [], "entities": []}, {"text": "Gold Standard Annotation: Four humans annotated 3 sets of test sets: Testset 1 with 5 complaints (73 sentences), Testset 2 with 7 complaints (105 sentences), and Testset 3 with 6 complaints (85 sentences).", "labels": [], "entities": []}, {"text": "Testset 1 and 2 are from mp3 player complaints and Testset 3 is from restaurant reviews.", "labels": [], "entities": []}, {"text": "Annotators marked sentences if they describe specific reasons of the complaint.", "labels": [], "entities": []}, {"text": "Each test set was annotated by 2 humans.", "labels": [], "entities": []}, {"text": "The average pair-wise human agreement was 82.1% . System Performance: Like the human annotators, our system also labeled reason sentences.", "labels": [], "entities": [{"text": "agreement", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.7769469022750854}]}, {"text": "Since our goal is to identify reason sentences in complaints, we applied a system modeled as in the identification phase described in Subsection 3.2 instead of the classification phase 8 . reports the accuracy, precision, and recall of the system on each test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 201, "end_pos": 209, "type": "METRIC", "confidence": 0.9994300007820129}, {"text": "precision", "start_pos": 211, "end_pos": 220, "type": "METRIC", "confidence": 0.9981727600097656}, {"text": "recall", "start_pos": 226, "end_pos": 232, "type": "METRIC", "confidence": 0.999413013458252}]}, {"text": "We calculated numbers in each A and B column by assuming each annotator's answers separately as a gold standard.", "labels": [], "entities": []}, {"text": "In, accuracies indicate the agreement between the system and human annotators.", "labels": [], "entities": []}, {"text": "The average accuracy 68.0% is comparable with the pair-wise human agreement 82.1% even if there is still a lot of room for improvement . It was interesting to see that Testset 3, which was from restaurant complaints, achieved higher accuracy and recall than the other test sets from mp3 player complaints, suggesting that it would be interesting to further investigate the performance The kappa value was 0.63.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9963845014572144}, {"text": "accuracy", "start_pos": 233, "end_pos": 241, "type": "METRIC", "confidence": 0.9988300204277039}, {"text": "recall", "start_pos": 246, "end_pos": 252, "type": "METRIC", "confidence": 0.99872225522995}, {"text": "kappa", "start_pos": 389, "end_pos": 394, "type": "METRIC", "confidence": 0.9481129050254822}]}, {"text": "In complaints reviews, we believe that it is more important to identify reason sentences than to classify because most reasons in complaints are likely to be cons.", "labels": [], "entities": []}, {"text": "The baseline system which assigned the majority class to each sentence achieved 59.9% of average accuracy. of reason identification in various other review domains such as travel and beauty products in future work.", "labels": [], "entities": [{"text": "accuracy. of reason identification", "start_pos": 97, "end_pos": 131, "type": "TASK", "confidence": 0.7840428650379181}]}, {"text": "Also, even though we were somewhat able to measure reason sentence identification in complaint reviews, we agree that we need more data annotation for more precise evaluation.", "labels": [], "entities": [{"text": "reason sentence identification", "start_pos": 51, "end_pos": 81, "type": "TASK", "confidence": 0.6185286045074463}]}, {"text": "Finally, the followings are examples of sentences that our system identified as reasons of complaints.", "labels": [], "entities": []}, {"text": "(1) Unfortunately, I find that I am no longer comfortable in your establishment because of the unprofessional, rude, obnoxious, and unsanitary treatment from the employees.", "labels": [], "entities": []}, {"text": "As we can see from the examples, our system was able to detect con sentences which contained opinion-bearing expressions such as in (1), (2), and (3) as well as reason sentences that mostly described mere facts as in and (5).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Pros and cons sentences identification  results on mp3 player reviews.", "labels": [], "entities": [{"text": "Pros and cons sentences identification", "start_pos": 10, "end_pos": 48, "type": "TASK", "confidence": 0.5907585263252259}, {"text": "mp3 player reviews", "start_pos": 61, "end_pos": 79, "type": "DATASET", "confidence": 0.8660115003585815}]}, {"text": " Table 4: Reason sentence identification results  on restaurant reviews.", "labels": [], "entities": [{"text": "Reason sentence identification", "start_pos": 10, "end_pos": 40, "type": "TASK", "confidence": 0.6044841408729553}]}, {"text": " Table 6: Pros and cons sentences classification results for restaurant reviews.", "labels": [], "entities": [{"text": "Pros and cons sentences classification", "start_pos": 10, "end_pos": 48, "type": "TASK", "confidence": 0.5992356956005096}]}]}