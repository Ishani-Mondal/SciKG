{"title": [{"text": "Factorizing Complex Models: A Case Study in Mention Detection", "labels": [], "entities": [{"text": "Mention Detection", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.8874380588531494}]}], "abstractContent": [{"text": "As natural language understanding research advances towards deeper knowledge modeling, the tasks become more and more complex: we are interested in more nu-anced word characteristics, more linguistic properties, deeper semantic and syntactic features.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 3, "end_pos": 33, "type": "TASK", "confidence": 0.6460711856683096}]}, {"text": "One such example, explored in this article, is the mention detection and recognition task in the Automatic Content Extraction project, with the goal of identifying named, nominal or pronominal references to real-world entities-mentions-and labeling them with three types of information: entity type, entity subtype and mention type.", "labels": [], "entities": [{"text": "mention detection and recognition", "start_pos": 51, "end_pos": 84, "type": "TASK", "confidence": 0.7722329497337341}, {"text": "Automatic Content Extraction project", "start_pos": 97, "end_pos": 133, "type": "TASK", "confidence": 0.6708063632249832}]}, {"text": "In this article, we investigate three methods of assigning these related tags and compare them on several data sets.", "labels": [], "entities": []}, {"text": "A system based on the methods presented in this article participated and ranked very competitively in the ACE'04 evaluation.", "labels": [], "entities": [{"text": "ACE'04 evaluation", "start_pos": 106, "end_pos": 123, "type": "DATASET", "confidence": 0.8608427941799164}]}], "introductionContent": [{"text": "Information extraction is a crucial step toward understanding and processing natural language data, its goal being to identify and categorize important information conveyed in a discourse.", "labels": [], "entities": [{"text": "Information extraction", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8372986614704132}]}, {"text": "Examples of information extraction tasks are identification of the actors and the objects in written text, the detection and classification of the relations among them, and the events they participate in.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 12, "end_pos": 34, "type": "TASK", "confidence": 0.7672441601753235}, {"text": "detection and classification of the relations", "start_pos": 111, "end_pos": 156, "type": "TASK", "confidence": 0.7264039069414139}]}, {"text": "These tasks have applications in, among other fields, summarization, information retrieval, data mining, question answering, and language understanding.", "labels": [], "entities": [{"text": "summarization", "start_pos": 54, "end_pos": 67, "type": "TASK", "confidence": 0.9934803247451782}, {"text": "information retrieval", "start_pos": 69, "end_pos": 90, "type": "TASK", "confidence": 0.8234229981899261}, {"text": "data mining", "start_pos": 92, "end_pos": 103, "type": "TASK", "confidence": 0.8676780164241791}, {"text": "question answering", "start_pos": 105, "end_pos": 123, "type": "TASK", "confidence": 0.9220227301120758}, {"text": "language understanding", "start_pos": 129, "end_pos": 151, "type": "TASK", "confidence": 0.7911471724510193}]}, {"text": "One of the basic tasks of information extraction is the mention detection task.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 26, "end_pos": 48, "type": "TASK", "confidence": 0.8230108022689819}, {"text": "mention detection task", "start_pos": 56, "end_pos": 78, "type": "TASK", "confidence": 0.9057002862294515}]}, {"text": "This task is very similar to named entity recognition (NER), as the objects of interest represent very similar concepts.", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 29, "end_pos": 59, "type": "TASK", "confidence": 0.8005299617846807}]}, {"text": "The main difference is that the latter will identify, however, only named references, while mention detection seeks named, nominal and pronominal references.", "labels": [], "entities": [{"text": "mention detection", "start_pos": 92, "end_pos": 109, "type": "TASK", "confidence": 0.7806637585163116}]}, {"text": "In this paper, we will call the identified references mentions -using the ACE nomenclature -to differentiate them from entities which are the real-world objects (the actual person, location, etc) to which the mentions are referring to . Historically, the goal of the NER task was to find named references to entities and quantity references -time, money).", "labels": [], "entities": [{"text": "NER task", "start_pos": 267, "end_pos": 275, "type": "TASK", "confidence": 0.9293690621852875}]}, {"text": "In recent years, Automatic Content Extraction evaluation) expanded the task to also identify nominal and pronominal references, and to group the mentions into sets referring to the same entity, making the task more complicated, as it requires a co-reference module.", "labels": [], "entities": [{"text": "Automatic Content Extraction evaluation", "start_pos": 17, "end_pos": 56, "type": "TASK", "confidence": 0.7159692570567131}]}, {"text": "The set of identified properties has also been extended to include the mention type of a reference (whether it is named, nominal or pronominal), its subtype (a more specific type dependent on the main entity type), and its genericity (whether the entity points to a specific entity, or a generic one 2 ), besides the customary main entity type.", "labels": [], "entities": []}, {"text": "To our knowledge, little research has been done in the natural language processing context or otherwise on investigating the specific problem of how such multiple labels are best assigned.", "labels": [], "entities": []}, {"text": "This article compares three methods for such an assignment.", "labels": [], "entities": []}, {"text": "The simplest model which can be considered for the task is to create anatomic tag by \"gluing\" together the sub-task labels and considering the new label atomic.", "labels": [], "entities": []}, {"text": "This method transforms the problem into a regular sequence classification task, similar to part-of-speech tagging, text chunking, and named entity recognition tasks.", "labels": [], "entities": [{"text": "regular sequence classification task", "start_pos": 42, "end_pos": 78, "type": "TASK", "confidence": 0.7221012115478516}, {"text": "part-of-speech tagging", "start_pos": 91, "end_pos": 113, "type": "TASK", "confidence": 0.7265193611383438}, {"text": "text chunking", "start_pos": 115, "end_pos": 128, "type": "TASK", "confidence": 0.7533212006092072}, {"text": "named entity recognition", "start_pos": 134, "end_pos": 158, "type": "TASK", "confidence": 0.65528937180837}]}, {"text": "We call this model the all-in-one model.", "labels": [], "entities": []}, {"text": "The immediate drawback of this model is that it creates a large classification space (the cross-product of the sub-task classification spaces) and that, during decoding, partially similar classifications will compete instead of cooperate -more details are presented in Section 3.1.", "labels": [], "entities": []}, {"text": "Despite (or maybe due to) its relative simplicity, this model obtained good results in several instances in the past, for POS tagging in morphologically rich languages and mention detection (.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 122, "end_pos": 133, "type": "TASK", "confidence": 0.9312914609909058}, {"text": "mention detection", "start_pos": 172, "end_pos": 189, "type": "TASK", "confidence": 0.7626112699508667}]}, {"text": "At the opposite end of classification methodology space, one can use a cascade model, which performs the sub-tasks sequentially in a predefined order.", "labels": [], "entities": [{"text": "classification methodology", "start_pos": 23, "end_pos": 49, "type": "TASK", "confidence": 0.8917829394340515}]}, {"text": "Under such a model, described in Section 3.3, the user will build separate models for each subtask.", "labels": [], "entities": []}, {"text": "For instance, it could first identify the mention boundaries, then assign the entity type, subtype, and mention level information.", "labels": [], "entities": []}, {"text": "Such a model has the immediate advantage of having smaller classification spaces, with the drawback that it requires a specific model invocation path.", "labels": [], "entities": []}, {"text": "In between the two extremes, one can use a joint model, which models the classification space in the same way as the all-in-one model, but where the classifications are not atomic.", "labels": [], "entities": []}, {"text": "This system incorporates information about sub-model parts, such as whether the current word starts an entity (of any type), or whether the word is part of a nominal mention.", "labels": [], "entities": []}, {"text": "The paper presents a novel contrastive analysis of these three models, comparing them on several datasets in three languages selected from the ACE 2003 and 2004 evaluations.", "labels": [], "entities": [{"text": "ACE 2003 and 2004 evaluations", "start_pos": 143, "end_pos": 172, "type": "DATASET", "confidence": 0.8692691922187805}]}, {"text": "The methods described here are independent of the underlying classifiers, and can be used with any sequence classifiers.", "labels": [], "entities": []}, {"text": "All experiments in this article use our in-house implementation of a maximum entropy classifier), which we selected because of its flexibility of integrating arbitrary types of features.", "labels": [], "entities": []}, {"text": "While we agree that the particular choice of classifier will undoubtedly introduce some classifier bias, we want to point out that the described procedures have more to do with the organization of the search space, and will have an impact, one way or another, on most sequence classifiers, including conditional random field classifiers.", "labels": [], "entities": []}, {"text": "The paper is organized as follows: Section 2 describes the multi-task classification problem and prior work, Section 3.3 presents and contrasts the three meta-classification models.", "labels": [], "entities": [{"text": "multi-task classification", "start_pos": 59, "end_pos": 84, "type": "TASK", "confidence": 0.7521423697471619}]}, {"text": "Section 4 outlines the experimental setup and the obtained results, and Section 5 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "All the experiments in this section are run on the ACE 2003 and 2004 data sets, in all the three languages covered: Arabic, Chinese, and English.", "labels": [], "entities": [{"text": "ACE 2003 and 2004 data sets", "start_pos": 51, "end_pos": 78, "type": "DATASET", "confidence": 0.9742239713668823}]}, {"text": "Since the evaluation test set is not publicly available, we have split the publicly available data into a 80%/20% data split.", "labels": [], "entities": []}, {"text": "To facilitate future comparisons with work presented here, and to simulate a realistic scenario, the splits are created based on article dates: the test data is selected as the last 20% of the data in chronological order.", "labels": [], "entities": []}, {"text": "This way, the documents in the training and test data sets do not overlap in time, and the ones in the test data are posterior to the ones in the training data.", "labels": [], "entities": []}, {"text": "presents the number of documents in the training/test datasets for the three languages.", "labels": [], "entities": []}, {"text": "11 For instance, the full label B-PER is consistent with the partial label B, but not with O or I.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Datasets size (number of documents)", "labels": [], "entities": []}, {"text": " Table 3: Experimental results: F-measure on the  full label", "labels": [], "entities": [{"text": "F-measure", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9940937161445618}]}, {"text": " Table 4: F-measure results on entity type only", "labels": [], "entities": [{"text": "F-measure", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9923615455627441}]}]}