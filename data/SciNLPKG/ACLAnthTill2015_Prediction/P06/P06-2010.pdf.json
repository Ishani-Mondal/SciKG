{"title": [{"text": "A Hybrid Convolution Tree Kernel for Semantic Role Labeling", "labels": [], "entities": [{"text": "Semantic Role Labeling", "start_pos": 37, "end_pos": 59, "type": "TASK", "confidence": 0.6901606520016988}]}], "abstractContent": [{"text": "A hybrid convolution tree kernel is proposed in this paper to effectively model syntactic structures for semantic role labeling (SRL).", "labels": [], "entities": [{"text": "semantic role labeling (SRL)", "start_pos": 105, "end_pos": 133, "type": "TASK", "confidence": 0.7731837928295135}]}, {"text": "The hybrid kernel consists of two individual convolution kernels: a Path kernel, which captures predicate-argument link features, and a Constituent Structure kernel, which captures the syntactic structure features of arguments.", "labels": [], "entities": []}, {"text": "Evaluation on the datasets of CoNLL-2005 SRL shared task shows that the novel hybrid convolution tree kernel out-performs the previous tree kernels.", "labels": [], "entities": [{"text": "CoNLL-2005 SRL shared task", "start_pos": 30, "end_pos": 56, "type": "DATASET", "confidence": 0.8641406148672104}]}, {"text": "We also combine our new hybrid tree kernel based method with the standard rich flat feature based method.", "labels": [], "entities": []}, {"text": "The experimental results show that the combinational method can get better performance than each of them individually.", "labels": [], "entities": [{"text": "combinational", "start_pos": 39, "end_pos": 52, "type": "TASK", "confidence": 0.9599416851997375}]}], "introductionContent": [{"text": "In the last few years there has been increasing interest in Semantic Role Labeling (SRL).", "labels": [], "entities": [{"text": "Semantic Role Labeling (SRL)", "start_pos": 60, "end_pos": 88, "type": "TASK", "confidence": 0.8285893897215525}]}, {"text": "It is currently a well defined task with a substantial body of work and comparative evaluation.", "labels": [], "entities": []}, {"text": "Given a sentence, the task consists of analyzing the propositions expressed by some target verbs and some constituents of the sentence.", "labels": [], "entities": []}, {"text": "In particular, for each target verb (predicate) all the constituents in the sentence which fill a semantic role (argument) of the verb have to be recognized.", "labels": [], "entities": []}, {"text": "shows an example of a semantic role labeling annotation in PropBank ().", "labels": [], "entities": [{"text": "semantic role labeling annotation", "start_pos": 22, "end_pos": 55, "type": "TASK", "confidence": 0.7016079127788544}, {"text": "PropBank", "start_pos": 59, "end_pos": 67, "type": "DATASET", "confidence": 0.9498551487922668}]}, {"text": "The PropBank defines 6 main arguments, Arg0 is the Agent, Arg1 is Patient, etc.", "labels": [], "entities": [{"text": "Arg0", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.988554060459137}, {"text": "Arg1", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.9795239567756653}]}, {"text": "ArgMmay indicate adjunct arguments, such as Locative, Temporal.", "labels": [], "entities": []}, {"text": "Many researchers () use feature-based methods, where a flat feature vector is usually used to represent a predicate-argument structure.", "labels": [], "entities": []}, {"text": "However, it's hard for this kind of representation method to explicitly describe syntactic structure information by a vector of flat features.", "labels": [], "entities": []}, {"text": "As an alternative, convolution tree kernel methods) provide an elegant kernel-based solution to implicitly explore tree structure features by directly computing the similarity between two trees.", "labels": [], "entities": []}, {"text": "In addition, some machine learning algorithms with dual form, such as Perceptron and Support Vector Machines (SVM)), which do not need know the exact presentation of objects and only need compute their kernel functions during the process of learning and prediction.", "labels": [], "entities": []}, {"text": "They can be well used as learning algorithms in the kernel-based methods.", "labels": [], "entities": []}, {"text": "They are named kernel machines.", "labels": [], "entities": []}, {"text": "In this paper, we decompose the Moschitti (2004)'s predicate-argument feature (PAF) kernel into a Path kernel and a Constituent Structure ker-nel, and then compose them into a hybrid convolution tree kernel.", "labels": [], "entities": []}, {"text": "Our hybrid kernel method using Voted Perceptron kernel machine outperforms the PAF kernel in the development sets of CoNLL-2005 SRL shared task.", "labels": [], "entities": [{"text": "Voted Perceptron kernel machine", "start_pos": 31, "end_pos": 62, "type": "DATASET", "confidence": 0.8290035277605057}, {"text": "CoNLL-2005 SRL shared task", "start_pos": 117, "end_pos": 143, "type": "DATASET", "confidence": 0.9033098220825195}]}, {"text": "In addition, the final composing kernel between hybrid convolution tree kernel and standard features' polynomial kernel outperforms each of them individually.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organized as follows: In Section 2 we review the previous work.", "labels": [], "entities": []}, {"text": "In Section 3 we illustrate the state of the art feature-based method for SRL.", "labels": [], "entities": [{"text": "SRL", "start_pos": 73, "end_pos": 76, "type": "TASK", "confidence": 0.9934332370758057}]}, {"text": "Section 4 discusses our method.", "labels": [], "entities": []}, {"text": "Section 5 shows the experimental results.", "labels": [], "entities": []}, {"text": "We conclude our work in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "The aim of our experiments is to verify the effectiveness of our hybrid convolution tree kernel and and its combination with the standard flat features.", "labels": [], "entities": []}, {"text": "The system is evaluated with respect to precision, recall, and F \u03b2=1 of the predicted arguments.", "labels": [], "entities": [{"text": "precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9997418522834778}, {"text": "recall", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.999763548374176}, {"text": "F \u03b2", "start_pos": 63, "end_pos": 66, "type": "METRIC", "confidence": 0.9782586991786957}]}, {"text": "P recision (p) is the proportion of arguments predicted by a system which are correct.", "labels": [], "entities": [{"text": "P recision (p)", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.9445711731910705}]}, {"text": "Recall (r) is the proportion of correct arguments which are predicted by a system.", "labels": [], "entities": [{"text": "Recall (r)", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9712636470794678}]}, {"text": "F \u03b2=1 computes the harmonic mean of precision and recall, which is the final measure to evaluate the performances of systems.", "labels": [], "entities": [{"text": "precision", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9991751313209534}, {"text": "recall", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.9989897608757019}]}, {"text": "It is formulated as: F \u03b2=1 = 2pr/(p + r).", "labels": [], "entities": [{"text": "F \u03b2", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.9791276752948761}]}, {"text": "srl-eval.pl 2 is the official program of the CoNLL-2005 SRL shared task to evaluate a system performance.", "labels": [], "entities": [{"text": "CoNLL-2005 SRL shared task", "start_pos": 45, "end_pos": 71, "type": "DATASET", "confidence": 0.8057558685541153}]}, {"text": "In order to speedup the training process, in the following experiments, we ONLY use WSJ sections 02-05 as training data.", "labels": [], "entities": [{"text": "ONLY", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9794924259185791}, {"text": "WSJ sections 02-05", "start_pos": 84, "end_pos": 102, "type": "DATASET", "confidence": 0.853656272093455}]}, {"text": "The same as, we also set the \u00b5 = 0.4 in the computation of convolution tree kernels.", "labels": [], "entities": [{"text": "\u00b5", "start_pos": 29, "end_pos": 30, "type": "METRIC", "confidence": 0.9749841094017029}]}, {"text": "In order to study the impact of \u03bb in hybrid convolution tree kernel in Eq.", "labels": [], "entities": []}, {"text": "1, we only use the hybrid kernel between K path and K cs . The performance curve on development set changing with \u03bb is shown in.", "labels": [], "entities": []}, {"text": "The performance curve shows that when \u03bb = 0.5, the hybrid convolution tree kernel gets the best performance.", "labels": [], "entities": []}, {"text": "Either the Path kernel (\u03bb = 1, F \u03b2=1 = 61.26) or the Constituent Structure kernel (\u03bb = 0, F \u03b2=1 = 54.91) cannot perform better than the hybrid one.", "labels": [], "entities": []}, {"text": "It suggests that the two individual kernels are complementary to each other.", "labels": [], "entities": []}, {"text": "In addition, the Path kernel performs much better than the Constituent Structure kernel.", "labels": [], "entities": []}, {"text": "It indicates that the predicate-constituent related features are more effective than the constituent features for SRL.", "labels": [], "entities": [{"text": "SRL", "start_pos": 114, "end_pos": 117, "type": "TASK", "confidence": 0.9347273707389832}]}, {"text": "compares the performance comparison among our Hybrid convolution tree kernel, Moschitti (2004)'s PAF kernel, standard flat features with Linear kernels, and Poly kernel (d = 2).", "labels": [], "entities": []}, {"text": "We can see that our hybrid convolution tree kernel outperforms the PAF kernel.", "labels": [], "entities": []}, {"text": "It empirically demonstrates that the weight linear combination in our hybrid kernel is more effective than PAF kernel for SRL.", "labels": [], "entities": [{"text": "SRL", "start_pos": 122, "end_pos": 125, "type": "TASK", "confidence": 0.9438977241516113}]}, {"text": "However, our hybrid kernel still performs worse than the standard feature based system.", "labels": [], "entities": []}, {"text": "This is simple because our kernel only use the syntactic structure information while the feature-based method use a large number of hand-craft diverse features, from word, POS, syntax and semantics, NER, etc.", "labels": [], "entities": []}, {"text": "The standard features with polynomial kernel gets the best performance.", "labels": [], "entities": []}, {"text": "The reason is that the arbitrary binary combination among features implicated by the polynomial kernel is useful to SRL.", "labels": [], "entities": [{"text": "SRL", "start_pos": 116, "end_pos": 119, "type": "TASK", "confidence": 0.9716995358467102}]}, {"text": "We believe that combining the two methods can perform better.", "labels": [], "entities": []}, {"text": "In order to make full use of the syntactic information and the standard flat features, we present a composite kernel between hybrid kernel where 0 \u2264 \u03b3 \u2264 1.", "labels": [], "entities": []}, {"text": "The performance curve changing with \u03b3 in Eq.", "labels": [], "entities": [{"text": "Eq", "start_pos": 41, "end_pos": 43, "type": "DATASET", "confidence": 0.8073358535766602}]}, {"text": "2 on development set is shown in.", "labels": [], "entities": [{"text": "development set", "start_pos": 5, "end_pos": 20, "type": "DATASET", "confidence": 0.8368954062461853}]}, {"text": "We can see that when \u03b3 = 0.5, the system achieves the best performance and F \u03b2=1 = 70.78.", "labels": [], "entities": [{"text": "F \u03b2", "start_pos": 75, "end_pos": 78, "type": "METRIC", "confidence": 0.9539943933486938}]}, {"text": "It's statistically significant improvement (\u03c7 2 test with p = 0.1) than only using the standard features with the polynomial kernel (\u03b3 = 0, F \u03b2=1 = 70.25) and much higher than only using the hybrid convolution tree kernel (\u03b3 = 1, F \u03b2=1 = 66.01).", "labels": [], "entities": []}, {"text": "The main reason is that the convolution tree kernel can represent more general syntactic features than standard flat features, and the standard flat features include the features that the convolution tree kernel cannot represent, such as Voice, SubCat.", "labels": [], "entities": []}, {"text": "The two kind features are complementary to each other.", "labels": [], "entities": []}, {"text": "Finally, we train the composite method using the above setting (Eq. 2 with when \u03b3 = 0.5) on the entire training set.", "labels": [], "entities": []}, {"text": "The final performance is shown in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Counts on the data set", "labels": [], "entities": []}, {"text": " Table 3: Performance (F \u03b2=1 ) comparison among  various kernels", "labels": [], "entities": [{"text": "F \u03b2", "start_pos": 23, "end_pos": 26, "type": "METRIC", "confidence": 0.9896017611026764}]}, {"text": " Table 4: Overall results (top) and detailed results  on the WSJ test (bottom).", "labels": [], "entities": [{"text": "WSJ test", "start_pos": 61, "end_pos": 69, "type": "DATASET", "confidence": 0.8141472339630127}]}]}