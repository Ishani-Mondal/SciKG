{"title": [{"text": "Extracting Parallel Sub-Sentential Fragments from Non-Parallel Corpora", "labels": [], "entities": [{"text": "Extracting Parallel Sub-Sentential Fragments from Non-Parallel Corpora", "start_pos": 0, "end_pos": 70, "type": "TASK", "confidence": 0.8325131024633136}]}], "abstractContent": [{"text": "We present a novel method for extracting parallel sub-sentential fragments from comparable, non-parallel bilingual corpora.", "labels": [], "entities": []}, {"text": "By analyzing potentially similar sentence pairs using a signal processing-inspired approach, we detect which segments of the source sentence are translated into segments in the target sentence, and which are not.", "labels": [], "entities": []}, {"text": "This method enables us to extract useful machine translation training data even from very non-parallel corpora , which contain no parallel sentence pairs.", "labels": [], "entities": [{"text": "machine translation training", "start_pos": 41, "end_pos": 69, "type": "TASK", "confidence": 0.8217591047286987}]}, {"text": "We evaluate the quality of the extracted data by showing that it improves the performance of a state-of-the-art statistical machine translation system.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 112, "end_pos": 143, "type": "TASK", "confidence": 0.6554818252722422}]}], "introductionContent": [{"text": "The high-level architecture of our parallel fragment extraction system is presented in.", "labels": [], "entities": [{"text": "parallel fragment extraction", "start_pos": 35, "end_pos": 63, "type": "TASK", "confidence": 0.6067652801672617}]}, {"text": "The first step of the pipeline identifies document pairs that are similar (and therefore more likely to contain parallel data), using the Lemur information retrieval toolkit 1; each document in the source language is translated word-for-word and turned into a query, which is run against the collection of target language documents.", "labels": [], "entities": []}, {"text": "The top 20 results are retrieved and paired with the query document.", "labels": [], "entities": []}, {"text": "We then take all sentence pairs from these document pairs and run them through the second step in the pipeline, the candidate selection filter.", "labels": [], "entities": []}, {"text": "This step discards pairs which have very few words that are translations of each other.", "labels": [], "entities": []}, {"text": "To all remaining sentence pairs we apply the fragment detection method (described in Section 2.3), which produces the output of the system.", "labels": [], "entities": [{"text": "fragment detection", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.7450507581233978}]}, {"text": "We use two probabilistic lexicons, learned au- The first one, GIZA-Lex, is obtained by running the GIZA++ 2 implementation of the IBM word alignment models () on the initial parallel corpus.", "labels": [], "entities": [{"text": "IBM word alignment", "start_pos": 130, "end_pos": 148, "type": "TASK", "confidence": 0.5244219700495402}]}, {"text": "One of the characteristics of this lexicon is that each source word is associated with many possible translations.", "labels": [], "entities": []}, {"text": "Although most of its high-probability entries are good translations, there area lot of entries (of non-negligible probability) where the two words are at most related.", "labels": [], "entities": []}, {"text": "As an example, in our GIZA-Lex lexicon, each source word has an average of 12 possible translations.", "labels": [], "entities": [{"text": "GIZA-Lex lexicon", "start_pos": 22, "end_pos": 38, "type": "DATASET", "confidence": 0.9054379463195801}]}, {"text": "This characteristic is useful for the first two stages of the extraction pipeline, which are not intended to be very precise.", "labels": [], "entities": []}, {"text": "Their purpose is to accept most of the existing parallel data, and not too much of the non-parallel data; using such a lexicon helps achieve this purpose.", "labels": [], "entities": []}, {"text": "For the last stage, however, precision is paramount.", "labels": [], "entities": [{"text": "precision", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9987462759017944}]}, {"text": "We found empirically that when using GIZA-Lex, the incorrect correspondences that it contains seriously impact the quality of our results; we therefore need a cleaner lexicon.", "labels": [], "entities": [{"text": "GIZA-Lex", "start_pos": 37, "end_pos": 45, "type": "DATASET", "confidence": 0.8137096166610718}]}, {"text": "In addition, since we want to distinguish between source words that have a translation on the target side and words that do not, we also need a measure of the probability that two words are not translations of each other.", "labels": [], "entities": []}, {"text": "All these are part of our second lexicon, LLR-Lex, which we present in detail in Section 2.2.", "labels": [], "entities": [{"text": "LLR-Lex", "start_pos": 42, "end_pos": 49, "type": "DATASET", "confidence": 0.9020358920097351}]}, {"text": "Subsequently, in Section 2.3, we present our algorithm for detecting parallel sub-sentential fragments.", "labels": [], "entities": [{"text": "detecting parallel sub-sentential fragments", "start_pos": 59, "end_pos": 102, "type": "TASK", "confidence": 0.8057510405778885}]}], "datasetContent": [{"text": "In our experiments, we compare our fragment extraction method (which we call FragmentExtract) with the sentence extraction approach of Munteanu and Marcu (2005) (SentenceExtract).", "labels": [], "entities": [{"text": "fragment extraction", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.7361146211624146}, {"text": "FragmentExtract", "start_pos": 77, "end_pos": 92, "type": "DATASET", "confidence": 0.8523473143577576}, {"text": "sentence extraction", "start_pos": 103, "end_pos": 122, "type": "TASK", "confidence": 0.7856635451316833}]}, {"text": "All extracted datasets are evaluated by using them as additional MT training data and measuring their impact on the performance of the MT system.", "labels": [], "entities": [{"text": "MT training", "start_pos": 65, "end_pos": 76, "type": "TASK", "confidence": 0.8812973499298096}, {"text": "MT", "start_pos": 135, "end_pos": 137, "type": "TASK", "confidence": 0.9829866886138916}]}, {"text": "On each of our comparable corpora, and using each of our initial parallel corpora, we apply both the fragment extraction and the sentence extraction method of.", "labels": [], "entities": [{"text": "fragment extraction", "start_pos": 101, "end_pos": 120, "type": "TASK", "confidence": 0.6882668435573578}, {"text": "sentence extraction", "start_pos": 129, "end_pos": 148, "type": "TASK", "confidence": 0.7167612314224243}]}, {"text": "In order to evaluate the importance of the LLRLex lexicon, we also performed fragment extraction experiments that do not use this lexicon, but only GIZA-Lex.", "labels": [], "entities": [{"text": "LLRLex lexicon", "start_pos": 43, "end_pos": 57, "type": "DATASET", "confidence": 0.8242767453193665}, {"text": "fragment extraction", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.7968172132968903}, {"text": "GIZA-Lex", "start_pos": 148, "end_pos": 156, "type": "DATASET", "confidence": 0.5536311268806458}]}, {"text": "Thus, for each initial parallel corpus and each comparable corpus, we extract three datasets: FragmentExtract, SentenceExtract, and Fragment-noLLR.", "labels": [], "entities": []}, {"text": "The sizes of the extracted datasets, measured in million English tokens, are presented in.: Sizes of the extracted datasets.", "labels": [], "entities": []}], "tableCaptions": []}