{"title": [{"text": "Sub-sentential Alignment Using Substring Co-Occurrence Counts", "labels": [], "entities": [{"text": "Sub-sentential Alignment", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.4920203238725662}]}], "abstractContent": [{"text": "In this paper, we will present an efficient method to compute the co-occurrence counts of any pair of substring in a parallel corpus, and an algorithm that make use of these counts to create sub-sentential alignments on such a corpus.", "labels": [], "entities": []}, {"text": "This algorithm has the advantage of being as general as possible regarding the segmentation of text.", "labels": [], "entities": []}], "introductionContent": [{"text": "An interesting and important problem in the Statistical Machine Translation (SMT) domain is the creation of sub-sentential alignment in a parallel corpus (a bilingual corpus already aligned at the sentence level).", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 44, "end_pos": 81, "type": "TASK", "confidence": 0.8419540921847025}]}, {"text": "These alignments can later be used to, for example, train SMT systems or extract bilingual lexicons.", "labels": [], "entities": [{"text": "SMT", "start_pos": 58, "end_pos": 61, "type": "TASK", "confidence": 0.9655728340148926}]}, {"text": "Many algorithms have already been proposed for sub-sentential alignment.", "labels": [], "entities": [{"text": "sub-sentential alignment", "start_pos": 47, "end_pos": 71, "type": "TASK", "confidence": 0.7036339938640594}]}, {"text": "Some of them focus on word-to-word alignment ( or).", "labels": [], "entities": [{"text": "word-to-word alignment", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.7599334716796875}]}, {"text": "Others allow the generation of phrase-level alignments, such as (), () or (Zhang, Vogel,.", "labels": [], "entities": []}, {"text": "However, with the exception of Marcu and Wong, these phrase-level alignment algorithms still place their analyses at the word level; whether by first creating a word-toword alignment or by computing correlation coefficients between pairs of individual words.", "labels": [], "entities": [{"text": "phrase-level alignment", "start_pos": 53, "end_pos": 75, "type": "TASK", "confidence": 0.7829687595367432}]}, {"text": "This is, in our opinion, a limitation of these algorithms; mainly because it makes them rely heavily on our capacity to segment a sentence in words.", "labels": [], "entities": []}, {"text": "And defining what a word is is not as easy as it might seem.", "labels": [], "entities": [{"text": "defining what a word is", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.9273741960525512}]}, {"text": "In peculiar, in many Asians writings systems (Japanese, Chinese or Thai, for example), there is not a special symbol to delimit words (such as the blank inmost nonAsian writing systems).", "labels": [], "entities": []}, {"text": "Current systems usually workaround this problem by using a segmentation tool to pre-process the data.", "labels": [], "entities": []}, {"text": "There are however two major disadvantages: -These tools usually need a lot of linguistic knowledge, such as lexical dictionaries and hand-crafted segmentation rules.", "labels": [], "entities": []}, {"text": "So using them somehow reduces the \"purity\" and universality of the statistical approach.", "labels": [], "entities": [{"text": "purity", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9911251664161682}]}, {"text": "-These tools are not perfect.", "labels": [], "entities": []}, {"text": "They tend to be very dependent on the domain of the text they are used with.", "labels": [], "entities": []}, {"text": "Besides, they cannot take advantage of the fact that there exist a translation of the sentence in another language.", "labels": [], "entities": []}, {"text": "(Xu,) have overcome part of these objections by using multiple segmentations of a Chinese sentence and letting a SMT system choose the best one, as well as creating a segmentation lexicon dictionary by considering every Chinese character to be a word in itself and then creating a phrase alignment.", "labels": [], "entities": [{"text": "SMT", "start_pos": 113, "end_pos": 116, "type": "TASK", "confidence": 0.984977126121521}]}, {"text": "However, it is probable that this technique would meet much more difficulties with Thai, for example (whose characters, unlike Chinese, bear no specific sense) or even Japanese (which use both ideograms and phonetic characters).", "labels": [], "entities": []}, {"text": "Besides, even for more \"computer-friendly\" languages, relying too much on typographic words may not be the best way to create an alignment.", "labels": [], "entities": []}, {"text": "For example, the translation of a set phrase may contain no word that is a translation of the individual words of this set phrase.", "labels": [], "entities": []}, {"text": "And one could consider languages such as German, which tend to merge words that are in relation in a single typographic word.", "labels": [], "entities": []}, {"text": "For such languages, it could be a good thing to be able to create alignment at an even more basic level than the typographic words.", "labels": [], "entities": []}, {"text": "These thoughts are the main motivations for the development of the alignment algorithm we will expose in this paper.", "labels": [], "entities": [{"text": "alignment", "start_pos": 67, "end_pos": 76, "type": "TASK", "confidence": 0.9628633260726929}]}, {"text": "Its main advantage is that it can be applied whatever is the smallest unit of text we want to consider: typographic word or single character.", "labels": [], "entities": []}, {"text": "And even when working at the character level, it can use larger sequence of characters to create correct alignments.", "labels": [], "entities": []}, {"text": "The problem of the segmentation and of the alignment will be resolved simultaneously: a sentence and its translation will mutually induce a segmentation on one another.", "labels": [], "entities": []}, {"text": "Another advantage of this algorithm is that it is purely statistical: it will not require any information other than the parallel corpus we want to align.", "labels": [], "entities": []}, {"text": "It should be noted here that the phrase-level joint-probability model presented in (Marcu and Wong) can pretend to have the same qualities.", "labels": [], "entities": []}, {"text": "However, it was only applied to word-segmented texts by its authors.", "labels": [], "entities": []}, {"text": "Making use of the EM training, it is also much more complex than our approach.", "labels": [], "entities": []}, {"text": "Before describing our algorithm, we will explain in detail a method for extracting the cooccurrence counts of any substring in a parallel corpus.", "labels": [], "entities": []}, {"text": "Such co-occurrence counts are important to our method, but difficult to compute or store in the case of big corpora.", "labels": [], "entities": []}], "datasetContent": [{"text": "We will now test the computational practicality of our method.", "labels": [], "entities": []}, {"text": "For this evaluation, we will consider the English-Japanese BTEC corpus (170,000 bi-sentences, 12MB), and the EnglishFrench Europarl corpus (688,000 bi-sentences, 180 MB).", "labels": [], "entities": [{"text": "English-Japanese BTEC corpus", "start_pos": 42, "end_pos": 70, "type": "DATASET", "confidence": 0.6376448770364126}, {"text": "EnglishFrench Europarl corpus", "start_pos": 109, "end_pos": 138, "type": "DATASET", "confidence": 0.9481282631556193}]}, {"text": "We also want to apply our algorithm to western languages at the character level.", "labels": [], "entities": []}, {"text": "However, working at a character level multiply the size of the suffix array by about 5, and increase the size of the cached vectors as well.", "labels": [], "entities": []}, {"text": "So, because of memory limitations, we extracted a smaller corpus from the Europarl one (100,000 bi-sentences, 20MB) for experimenting on characters substrings.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 74, "end_pos": 82, "type": "DATASET", "confidence": 0.9881243109703064}]}, {"text": "The base elements we will choose for our substrings will be: word/characters for the BTEC, word/word for the bigger EuroParl, and word/characters for the smaller EuroParl.", "labels": [], "entities": [{"text": "BTEC", "start_pos": 85, "end_pos": 89, "type": "DATASET", "confidence": 0.9845116138458252}, {"text": "EuroParl", "start_pos": 116, "end_pos": 124, "type": "DATASET", "confidence": 0.9914406538009644}, {"text": "EuroParl", "start_pos": 162, "end_pos": 170, "type": "DATASET", "confidence": 0.992363452911377}]}, {"text": "We computed the co-occurrence counts of every substrings pair in a bi-sentence for the 100 first bisentences of every corpus, on a 2.5GHz x86 computer.", "labels": [], "entities": []}, {"text": "We give the average figures for different corpora and caching strategies.", "labels": [], "entities": []}, {"text": "These results are good enough and show that the algorithm we are going to introduce is not computationally impracticable.", "labels": [], "entities": []}, {"text": "The cache allows an interesting trade-off between the performances and the used memory.", "labels": [], "entities": []}, {"text": "We note that the proportional speedup depends on the corpus used.", "labels": [], "entities": []}, {"text": "We did not investigate this point, but the different sizes of corpora (inducing different average occurrence vectors sizes), and the differences in the frequency distribution of words and characters are probably the main factors.", "labels": [], "entities": []}, {"text": "3 Sub-sentential alignment  Although we made some tests to confirm that computation time did not prevent our algorithm to work with bigger corpus such as the EuroParl corpus, we have until now limited deeper experiments to the Japanese-English BTEC Corpus.", "labels": [], "entities": [{"text": "EuroParl corpus", "start_pos": 158, "end_pos": 173, "type": "DATASET", "confidence": 0.9917809367179871}, {"text": "Japanese-English BTEC Corpus", "start_pos": 227, "end_pos": 255, "type": "DATASET", "confidence": 0.840022087097168}]}, {"text": "That is why we will only present results for this corpus.", "labels": [], "entities": []}, {"text": "For comparison, we re-implemented the ISA (Integrated Segmentation Alignment) algorithm described in.", "labels": [], "entities": [{"text": "Integrated Segmentation Alignment)", "start_pos": 43, "end_pos": 77, "type": "TASK", "confidence": 0.7105454877018929}]}, {"text": "This algorithm is interesting because it is somehow similar to our own approach, in that it can be seen as a generalization of Melamed's Competitive Linking Algorithm.", "labels": [], "entities": []}, {"text": "It is also fairly easy to implement.", "labels": [], "entities": []}, {"text": "A comparison with the joint probability model of Marcu and Wong (which can also work at the phrase/substring level) would have also been very interesting, but the difficulty of implementing and adapting the algorithm made us delay the experiment.", "labels": [], "entities": []}, {"text": "After trying different settings, we chose to use chi-square statistic as the correlation coefficient for the ISA algorithm, and the dice coefficient for our own algorithm.", "labels": [], "entities": [{"text": "correlation", "start_pos": 77, "end_pos": 88, "type": "METRIC", "confidence": 0.9815996289253235}]}, {"text": "ISA settings as well as the \"alignment size penalties\" of our algorithm were also tuned to give the best results possible with our test set.", "labels": [], "entities": [{"text": "alignment size penalties", "start_pos": 29, "end_pos": 53, "type": "METRIC", "confidence": 0.708361824353536}]}, {"text": "For our algorithm, we considered word-substrings for English and characters substrings for Japanese.", "labels": [], "entities": []}, {"text": "For the ISA algorithm, we pre-segmented the Japanese corpus, but also tried to apply it directly to Japanese by considering characters as words.", "labels": [], "entities": []}, {"text": "Estimating the quality of an alignment is not an easy thing.", "labels": [], "entities": []}, {"text": "We tried to compute a precision and a recall score in the following manner.", "labels": [], "entities": [{"text": "precision", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9990832805633545}, {"text": "recall score", "start_pos": 38, "end_pos": 50, "type": "METRIC", "confidence": 0.9847360253334045}]}], "tableCaptions": []}