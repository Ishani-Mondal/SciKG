{"title": [{"text": "Exploring the Potential of Intractable Parsers", "labels": [], "entities": []}], "abstractContent": [{"text": "We revisit the idea of history-based parsing , and present a history-based parsing framework that strives to be simple, general , and flexible.", "labels": [], "entities": [{"text": "history-based parsing", "start_pos": 23, "end_pos": 44, "type": "TASK", "confidence": 0.6231084913015366}]}, {"text": "We also provide a de-coder for this probability model that is linear-space, optimal, and anytime.", "labels": [], "entities": []}, {"text": "A parser based on this framework, when evaluated on Section 23 of the Penn Tree-bank, compares favorably with other state-of-the-art approaches, in terms of both accuracy and speed.", "labels": [], "entities": [{"text": "Section 23 of the Penn Tree-bank", "start_pos": 52, "end_pos": 84, "type": "DATASET", "confidence": 0.904534767071406}, {"text": "accuracy", "start_pos": 162, "end_pos": 170, "type": "METRIC", "confidence": 0.9992819428443909}, {"text": "speed", "start_pos": 175, "end_pos": 180, "type": "METRIC", "confidence": 0.9695237874984741}]}], "introductionContent": [{"text": "Much of the current research into probabilistic parsing is founded on probabilistic contextfree grammars (PCFGs).", "labels": [], "entities": [{"text": "probabilistic parsing", "start_pos": 34, "end_pos": 55, "type": "TASK", "confidence": 0.6857163608074188}]}, {"text": "For instance, consider the parse tree in.", "labels": [], "entities": []}, {"text": "One way to decompose this parse tree is to view it as a sequence of applications of CFG rules.", "labels": [], "entities": []}, {"text": "For this particular tree, we could view it as the application of rule \"NP \u2192 NP PP,\" followed by rule \"NP \u2192 DT NN,\" followed by rule \"DT \u2192 that,\" and so forth.", "labels": [], "entities": []}, {"text": "Hence instead of analyzing P (tree), we deal with the more modular: Obviously this joint distribution is just as difficult to assess and compute with as P (tree).", "labels": [], "entities": []}, {"text": "However there exist cubic-time dynamic programming algorithms to find the most likely parse if we assume that all CFG rule applications are marginally independent of one another.", "labels": [], "entities": []}, {"text": "The problem, of course, with this simplification is that although it is computationally attractive, it is usually too strong of an independence assumption.", "labels": [], "entities": []}, {"text": "To mitigate this loss of context, without sacrificing algorithmic tractability, typically researchers annotate the nodes of the parse tree with contextual information.", "labels": [], "entities": []}, {"text": "A simple example is the annotation of nodes with their parent labels.", "labels": [], "entities": []}, {"text": "The choice of which annotations to use is one of the main features that distinguish parsers based on this approach.", "labels": [], "entities": []}, {"text": "Generally, this approach has proven quite effective in producing English phrase-structure grammar parsers that perform well on the Penn Treebank.", "labels": [], "entities": [{"text": "phrase-structure grammar parsers", "start_pos": 73, "end_pos": 105, "type": "TASK", "confidence": 0.6725895901521047}, {"text": "Penn Treebank", "start_pos": 131, "end_pos": 144, "type": "DATASET", "confidence": 0.995805025100708}]}, {"text": "One drawback of this approach is its inflexibility.", "labels": [], "entities": []}, {"text": "Because we are adding probabilistic context by changing the data itself, we make our data increasingly sparse as we add features.", "labels": [], "entities": []}, {"text": "Thus we are constrained from adding too many features, because at some point we will not have enough data to sustain them.", "labels": [], "entities": []}, {"text": "We must strike a delicate balance between how much context we want to include versus how much we dare to partition our data set.", "labels": [], "entities": []}, {"text": "The major alternative to PCFG-based approaches are so-called history-based parsers ().", "labels": [], "entities": []}, {"text": "These parsers differ from PCFG parsers in that they incorporate context by using a more complex probability model, rather than by modifying the data itself.", "labels": [], "entities": []}, {"text": "The tradeoff to using a more powerful probabilistic model is that one can no longer employ dynamic programming to find the most probable parse.", "labels": [], "entities": []}, {"text": "Thus one trades assurances of polynomial running time for greater modeling flexibility.", "labels": [], "entities": []}, {"text": "There are two canonical parsers that fall into this category: the decision-tree parser of, and the maximum-entropy parser of.", "labels": [], "entities": []}, {"text": "Both showed decent results on parsing the Penn Treebank, but in the decade since these papers were published, history-based parsers have been largely ignored by the research community in favor of PCFG-based approaches.", "labels": [], "entities": [{"text": "parsing", "start_pos": 30, "end_pos": 37, "type": "TASK", "confidence": 0.9651375412940979}, {"text": "Penn Treebank", "start_pos": 42, "end_pos": 55, "type": "DATASET", "confidence": 0.8997685015201569}]}, {"text": "There are several reasons why this maybe.", "labels": [], "entities": []}, {"text": "First is naturally the matter of time efficiency.", "labels": [], "entities": []}, {"text": "Magerman reports decent parsing times, but for the purposes of efficiency, must restrict his results to sentences of length 40 or less.", "labels": [], "entities": [{"text": "parsing", "start_pos": 24, "end_pos": 31, "type": "TASK", "confidence": 0.965539813041687}]}, {"text": "Furthermore, his twophase stack decoder is a bit complicated and is acknowledged to require too much memory to handle certain sentences.", "labels": [], "entities": []}, {"text": "Ratnaparkhi is vague about the running time performance of his parser, stating that it is \"observed linear-time,\" but in any event, provides only a heuristic, not a complete algorithm.", "labels": [], "entities": []}, {"text": "Next is the matter of flexibility.", "labels": [], "entities": []}, {"text": "The main advantage of abandoning PCFGs is the opportunity to have a more flexible and adaptable probabilistic parsing model.", "labels": [], "entities": []}, {"text": "Unfortunately, both Magerman and Ratnaparkhi's models are rather specific and complicated.", "labels": [], "entities": []}, {"text": "Ratnaparkhi's, for instance, consists of the interleaved sequence of four different types of tree construction operations.", "labels": [], "entities": []}, {"text": "Furthermore, both are inextricably tied to the learning procedure that they employ (decision trees for Magerman, maximum entropy for Ratnaparkhi).", "labels": [], "entities": []}, {"text": "In this work, our goal is to revisit history-based parsers, and provide a general-purpose framework that is (a) simple, (b) fast, (c) space-efficient and (d) easily adaptable to new domains.", "labels": [], "entities": []}, {"text": "As a method of evaluation, we use this framework with a very simple set of features to see how well it performs (both in terms of accuracy and running time) on the Penn Treebank.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 130, "end_pos": 138, "type": "METRIC", "confidence": 0.9988735318183899}, {"text": "Penn Treebank", "start_pos": 164, "end_pos": 177, "type": "DATASET", "confidence": 0.9953053295612335}]}, {"text": "The overarching goal is to develop a history-based hierarchical labeling framework that is viable not only for parsing, but for other application areas that current rely on dynamic programming, like phrase-based machine translation.", "labels": [], "entities": [{"text": "parsing", "start_pos": 111, "end_pos": 118, "type": "TASK", "confidence": 0.9712920188903809}, {"text": "phrase-based machine translation", "start_pos": 199, "end_pos": 231, "type": "TASK", "confidence": 0.6598828236262003}]}], "datasetContent": [{"text": "We employed a familiar experimental set-up.", "labels": [], "entities": []}, {"text": "For training, we used sections 2-21 of the WSJ section of the Penn treebank.", "labels": [], "entities": [{"text": "WSJ section of the Penn treebank", "start_pos": 43, "end_pos": 75, "type": "DATASET", "confidence": 0.9071241517861685}]}, {"text": "As a development set, we used the first 20 files of section 22, and then saved section 23 for testing the final model.", "labels": [], "entities": []}, {"text": "One unconventional preprocessing step was taken.", "labels": [], "entities": []}, {"text": "Namely, for the entire treebank, we compressed all unary chains into a single node, labeled with the label of the node furthest from the root.", "labels": [], "entities": []}, {"text": "We did so in order to simplify our experiments, since the framework outlined in this paper allows only one label per labeling scheme per span.", "labels": [], "entities": []}, {"text": "Thus by avoiding unary chains, we avoid the need for many labeling schemes or more complicated compound labels (labels like \"NP-NN\").", "labels": [], "entities": []}, {"text": "Since our goal here was not to create a parsing tool but rather to explore the viability of this approach, this seemed a fair concession.", "labels": [], "entities": []}, {"text": "It should be noted that it is indeed possible to create a fully general parser using our framework (for instance, by using the above idea of compound labels for unary chains).", "labels": [], "entities": []}, {"text": "The main difficulty with this compromise is that it renders the familiar metrics of labeled precision and labeled recall incomparable with previous work (i.e. the LP of a set of candidate parses with respect to the unmodified test set differs from the LP with respect to the preprocessed test set).", "labels": [], "entities": [{"text": "precision", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.8183037042617798}, {"text": "recall", "start_pos": 114, "end_pos": 120, "type": "METRIC", "confidence": 0.9300404787063599}]}, {"text": "This would be a major problem, were it not for the existence of other metrics which measure only the quality of a parser's recursive decomposition of a sentence.", "labels": [], "entities": []}, {"text": "Fortunately, such metrics do exist, thus we used cross-bracketing statistics as the basic measure of quality for our parser.", "labels": [], "entities": []}, {"text": "The crossbracketing score of a set of candidate parses with respect to the unmodified test set is identical to the cross-bracketing score with respect to the preprocessed test set, hence our preprocessing causes no comparability problems as viewed by this metric.", "labels": [], "entities": []}, {"text": "For our parsing model, we used an HLP H = \ud97b\udf59L, <, A, F, P\ud97b\udf59 with the following parameters.", "labels": [], "entities": [{"text": "parsing", "start_pos": 8, "end_pos": 15, "type": "TASK", "confidence": 0.9739081859588623}]}, {"text": "L consisted of three labeling schemes: the set L wd of word labels, the set L pt of preterminal labels, and the set L nt of nonterminal labels.", "labels": [], "entities": []}, {"text": "The order < of the model variables was the unique order such that for all suitable integers i, j, k, l: (1) is strictly shorter than span (k, l) or they have the same length and integer i is less than integer k.", "labels": [], "entities": []}, {"text": "For auto-assignment function A, we essentially used the function in, modified so that it automatically assigned null to model variables L wd ij and L pt ij for i \ud97b\udf59 = j (i.e. no preterminal or word tagging of internal nodes), and to model variables L nt ii (i.e. no nonterminal tagging of leaves, rendered unnecessary by our preprocessing step).", "labels": [], "entities": [{"text": "word tagging of internal nodes", "start_pos": 192, "end_pos": 222, "type": "TASK", "confidence": 0.8120990633964539}]}, {"text": "Rather than incorporate part-of-speech tagging into the search process, we opted to pretag the sentences of our development and test sets with an off-the-shelf tagger, namely the Brill tagger.", "labels": [], "entities": [{"text": "Brill tagger", "start_pos": 179, "end_pos": 191, "type": "DATASET", "confidence": 0.899560421705246}]}, {"text": "Thus the object of our computation was HLPDECODE(H, n, w), where n was the length of the sentence, and partial assignment w specified the word and PT labels of the leaves.", "labels": [], "entities": [{"text": "HLPDECODE", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9520336985588074}]}, {"text": "Given this partial assignment, the job of HLPDECODE was to find the most probable assignment of model variables S ij and L nt ij for 1 \u2264 i < j \u2264 n.", "labels": [], "entities": []}, {"text": "The two probability models, PS and P nt , were trained in the manner described in Section 4.", "labels": [], "entities": []}, {"text": "Two decisions needed to be made: which features to use and which learning technique to employ.", "labels": [], "entities": []}, {"text": "As for the learning technique, we used maximum entropy models, specifically the implementation called MegaM provided by Hal Daume).", "labels": [], "entities": []}, {"text": "For PS , we needed features  that would be relevant to deciding whether a given span (i, j) should be considered a constituent.", "labels": [], "entities": [{"text": "PS", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.9782770276069641}]}, {"text": "The basic building blocks we used are depicted in.", "labels": [], "entities": []}, {"text": "A few words of explanation are in order.", "labels": [], "entities": []}, {"text": "By label(k), we mean the highest nonterminal label so far assigned that covers word k, or if such a label does not yet exist, then the preterminal label of k (recall that our model order was bottom-up).", "labels": [], "entities": []}, {"text": "By category(k), we mean the category of the preterminal label of word k (given a coarser, hand-made categorization of preterminal labels that grouped all noun tags into one category, all verb tags into another, etc.).", "labels": [], "entities": []}, {"text": "By signature(k, m), where k \u2264 m, we mean the sequence \ud97b\udf59label(k), label(k + 1), ..., label(m)\ud97b\udf59, from which all consecutive sequences of identical labels are compressed into a single label.", "labels": [], "entities": []}, {"text": "For instance, \ud97b\udf59IN, NP, NP, VP, VP \ud97b\udf59 would become \ud97b\udf59IN, NP, VP \ud97b\udf59.", "labels": [], "entities": []}, {"text": "Ad-hoc conjunctions of these basic binary features were used as features for our probability model PS . In total, approximately 800,000 such conjunctions were used.", "labels": [], "entities": []}, {"text": "For P nt , we needed features that would be relevant to deciding which nonterminal label to give to a given constituent span.", "labels": [], "entities": []}, {"text": "For this somewhat simpler task, we used a subset of the basic features used for PS , shown in bold in.", "labels": [], "entities": [{"text": "PS", "start_pos": 80, "end_pos": 82, "type": "TASK", "confidence": 0.9216171503067017}]}, {"text": "Adhoc conjunctions of these boldface binary features were used as features for our probability model P nt . In total, approximately 100,000 such conjunctions were used.", "labels": [], "entities": []}, {"text": "As mentioned earlier, we used cross-bracketing statistics as our basis of comparision.", "labels": [], "entities": []}, {"text": "These results as shown in.", "labels": [], "entities": []}, {"text": "CB denotes the average cross-bracketing, i.e. the overall percentage of candidate constituents that properly overlap with a constituent in the gold parse.", "labels": [], "entities": [{"text": "CB", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9049496054649353}]}, {"text": "0CB denotes the percentage of sentences in the test set that exhibit no cross-bracketing.", "labels": [], "entities": []}, {"text": "With a simple feature set, we manage to obtain performance comparable to the unlexicalized PCFG parser of on the set of sentences of length 40 or less.", "labels": [], "entities": []}, {"text": "On the subset of Section 23 consisting of sentences of length 100 or less, our parser slightly outperforms their results in terms of average cross-bracketing.", "labels": [], "entities": []}, {"text": "Interestingly, our parser has a lower percentage of sentences exhibiting no cross bracketing.", "labels": [], "entities": []}, {"text": "To reconcile this result with the superior overall cross-bracketing score, it would appear that when our parser does make bracketing errors, the errors tend to be less severe.", "labels": [], "entities": []}, {"text": "The surprise was how quickly the parser performed.", "labels": [], "entities": []}, {"text": "Despite its exponential worst-case time bounds, the search space turned out to be quite conducive to depth-first branch-and-bound pruning.", "labels": [], "entities": []}, {"text": "Using an unoptimized Java implementation on a 4x Opteron 848 with 16GB of RAM, the parser required (on average) less than 0.26 seconds per sentence to optimally parse the subset of Section 23 comprised of sentences of 40 words or less.", "labels": [], "entities": []}, {"text": "It required an average of 0.48 seconds per sentence to optimally parse the sentences of 100 words or less (an average of less than 3.5 seconds per sentence for those sentences of length 41-100).", "labels": [], "entities": []}, {"text": "As noted earlier, the parser requires space linear in the size of the sentence.", "labels": [], "entities": []}], "tableCaptions": []}