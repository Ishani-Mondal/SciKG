{"title": [{"text": "Word Alignment for Languages with Scarce Resources Using Bilingual Corpora of Other Language Pairs", "labels": [], "entities": [{"text": "Word Alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7078882157802582}]}], "abstractContent": [{"text": "This paper proposes an approach to improve word alignment for languages with scarce resources using bilingual corpora of other language pairs.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 43, "end_pos": 57, "type": "TASK", "confidence": 0.7611417770385742}]}, {"text": "To perform word alignment between languages L1 and L2, we introduce a third language L3.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 11, "end_pos": 25, "type": "TASK", "confidence": 0.758129894733429}]}, {"text": "Although only small amounts of bilingual data are available for the desired language pair L1-L2, large-scale bilingual corpora in L1-L3 and L2-L3 are available.", "labels": [], "entities": []}, {"text": "Based on these two additional corpora and with L3 as the pivot language, we build a word alignment model for L1 and L2.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 84, "end_pos": 98, "type": "TASK", "confidence": 0.6943399459123611}]}, {"text": "This approach can build a word alignment model for two languages even if no bilingual corpus is available in this language pair.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 26, "end_pos": 40, "type": "TASK", "confidence": 0.7545967102050781}]}, {"text": "In addition, we build another word alignment model for L1 and L2 using the small L1-L2 bilingual corpus.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 30, "end_pos": 44, "type": "TASK", "confidence": 0.7225020974874496}]}, {"text": "Then we interpolate the above two models to further improve word alignment between L1 and L2.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 60, "end_pos": 74, "type": "TASK", "confidence": 0.7155617475509644}]}, {"text": "Experimental results indicate a relative error rate reduction of 21.30% as compared with the method only using the small bilingual corpus in L1 and L2.", "labels": [], "entities": [{"text": "error rate", "start_pos": 41, "end_pos": 51, "type": "METRIC", "confidence": 0.9195386469364166}]}], "introductionContent": [{"text": "Word alignment was first proposed as an intermediate result of statistical machine translation ().", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.76589035987854}, {"text": "statistical machine translation", "start_pos": 63, "end_pos": 94, "type": "TASK", "confidence": 0.6221581796805064}]}, {"text": "Many researchers build alignment links with bilingual corpora).", "labels": [], "entities": []}, {"text": "In order to achieve satisfactory results, all of these methods require a large-scale bilingual corpus for training.", "labels": [], "entities": []}, {"text": "When the large-scale bilingual corpus is unavailable, some researchers acquired class-based alignment rules with existing dictionaries to improve word alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 146, "end_pos": 160, "type": "TASK", "confidence": 0.7665036916732788}]}, {"text": "used a large-scale bilingual corpus in general domain to improve domain-specific word alignment when only a small-scale domainspecific bilingual corpus is available.", "labels": [], "entities": [{"text": "domain-specific word alignment", "start_pos": 65, "end_pos": 95, "type": "TASK", "confidence": 0.6703896621863047}]}, {"text": "This paper proposes an approach to improve word alignment for languages with scarce resources using bilingual corpora of other language pairs.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 43, "end_pos": 57, "type": "TASK", "confidence": 0.7611417770385742}]}, {"text": "To perform word alignment between languages L1 and L2, we introduce a third language L3 as the pivot language.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 11, "end_pos": 25, "type": "TASK", "confidence": 0.7516935765743256}]}, {"text": "Although only small amounts of bilingual data are available for the desired language pair L1-L2, large-scale bilingual corpora in L1-L3 and L2-L3 are available.", "labels": [], "entities": []}, {"text": "Using these two additional bilingual corpora, we train two word alignment models for language pairs L1-L3 and L2-L3, respectively.", "labels": [], "entities": []}, {"text": "And then, with L3 as a pivot language, we can build a word alignment model for L1 and L2 based on the above two models.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 54, "end_pos": 68, "type": "TASK", "confidence": 0.73375004529953}]}, {"text": "Here, we call this model an induced model.", "labels": [], "entities": []}, {"text": "With this induced model, we perform word alignment between languages L1 and L2 even if no parallel corpus is available for this language pair.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 36, "end_pos": 50, "type": "TASK", "confidence": 0.7668253183364868}]}, {"text": "In addition, using the small bilingual corpus in L1 and L2, we train another word alignment model for this language pair.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 77, "end_pos": 91, "type": "TASK", "confidence": 0.6895836889743805}]}, {"text": "Here, we call this model an original model.", "labels": [], "entities": []}, {"text": "An interpolated model can be built by interpolating the induced model and the original model.", "labels": [], "entities": []}, {"text": "As a case study, this paper uses English as the pivot language to improve word alignment between Chinese and Japanese.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 74, "end_pos": 88, "type": "TASK", "confidence": 0.7201782017946243}]}, {"text": "Experimental results show that the induced model performs better than the original model trained on the small Chinese-Japanese corpus.", "labels": [], "entities": []}, {"text": "And the interpolated model further improves the word alignment results, achieving a relative error rate reduction of 21.30% as compared with results produced by the original model.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 48, "end_pos": 62, "type": "TASK", "confidence": 0.7737194001674652}, {"text": "relative error rate reduction", "start_pos": 84, "end_pos": 113, "type": "METRIC", "confidence": 0.8135532885789871}]}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 discusses the related work.", "labels": [], "entities": []}, {"text": "Section 3 introduces the statistical word alignment models.", "labels": [], "entities": [{"text": "statistical word alignment", "start_pos": 25, "end_pos": 51, "type": "TASK", "confidence": 0.6312027871608734}]}, {"text": "Section 4 describes the parameter estimation method using bilingual corpora of other language pairs.", "labels": [], "entities": []}, {"text": "Section 5 presents the interpolation model.", "labels": [], "entities": []}, {"text": "Section 6 reports the experimental results.", "labels": [], "entities": []}, {"text": "Finally, we conclude and present the future work in section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we compare different word alignment methods for Chinese-Japanese alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 38, "end_pos": 52, "type": "TASK", "confidence": 0.704110249876976}, {"text": "Chinese-Japanese alignment", "start_pos": 65, "end_pos": 91, "type": "TASK", "confidence": 0.7070831060409546}]}, {"text": "The \"Original\" method uses the original model trained with the small Chinese-Japanese corpus.", "labels": [], "entities": []}, {"text": "The \"Basic Induced\" method uses the induced model that employs the basic translation probability without introducing cross-language word similarity.", "labels": [], "entities": []}, {"text": "The \"Advanced Induced\" method uses the induced model that introduces the cross-language word similarity into the calculation of the translation probability.", "labels": [], "entities": []}, {"text": "The \"Interpolated\" method uses the interpolation of the word alignment models in the \"Advanced Induced\" and \"Original\" methods.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 56, "end_pos": 70, "type": "TASK", "confidence": 0.7033641189336777}]}, {"text": "We use the same metrics as described in, which is similar to those in.", "labels": [], "entities": []}, {"text": "The difference lies in that took all alignment links assure links.", "labels": [], "entities": []}, {"text": "If we use to represent the set of alignment links identified by the proposed methods and to denote the reference alignment set, the methods to calculate the precision, recall, f-measure, and alignment error rate (AER) are shown in equations (18), (19),, and (21), respectively.", "labels": [], "entities": [{"text": "precision", "start_pos": 157, "end_pos": 166, "type": "METRIC", "confidence": 0.9995172023773193}, {"text": "recall", "start_pos": 168, "end_pos": 174, "type": "METRIC", "confidence": 0.9953135251998901}, {"text": "f-measure", "start_pos": 176, "end_pos": 185, "type": "METRIC", "confidence": 0.8981044292449951}, {"text": "alignment error rate (AER)", "start_pos": 191, "end_pos": 217, "type": "METRIC", "confidence": 0.9383781651655833}]}, {"text": "It can be seen that the higher the f-measure is, the lower the alignment error rate is.", "labels": [], "entities": [{"text": "alignment error rate", "start_pos": 63, "end_pos": 83, "type": "METRIC", "confidence": 0.892730712890625}]}, {"text": "Thus, we will only show precision, recall and AER scores in the evaluation results.", "labels": [], "entities": [{"text": "precision", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9997468590736389}, {"text": "recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9992197751998901}, {"text": "AER", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.9994314312934875}]}, {"text": "We use the held-out data described in section 6.1 to set the interpolation weights in section 5.", "labels": [], "entities": []}, {"text": "t \u03bb is set to 0.3, n \u03bb is set to 0.1, d3 \u03bb for model 3 is set to 0.5, and d4 \u03bb for model 4 is set to 0.1.", "labels": [], "entities": []}, {"text": "With these parameters, we get the lowest alignment error rate on the held-out data.", "labels": [], "entities": [{"text": "alignment error rate", "start_pos": 41, "end_pos": 61, "type": "METRIC", "confidence": 0.8941588401794434}]}, {"text": "For each method described above, we perform bi-directional (source to target and target to source) word alignment and obtain two alignment results.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 99, "end_pos": 113, "type": "TASK", "confidence": 0.7307337671518326}]}, {"text": "Based on the two results, we get a result using \"refined\" combination as described in).", "labels": [], "entities": []}, {"text": "Thus, all of the results reported here describe the results of the \"refined\" combination.", "labels": [], "entities": []}, {"text": "For model training, we use the GIZA++ toolkit 3 .  The evaluation results on the testing data are shown in table 2.", "labels": [], "entities": [{"text": "model training", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.80403071641922}, {"text": "GIZA++ toolkit", "start_pos": 31, "end_pos": 45, "type": "DATASET", "confidence": 0.8679405450820923}]}, {"text": "From the results, it can be seen that both of the two induced models perform better than the \"Original\" method that only uses the limited Chinese-Japanese sentence pairs.", "labels": [], "entities": []}, {"text": "The \"Advanced Induced\" method achieves a relative error rate reduction of 10.41% as compared with the \"Original\" method.", "labels": [], "entities": [{"text": "error rate reduction", "start_pos": 50, "end_pos": 70, "type": "METRIC", "confidence": 0.9453242023785909}]}, {"text": "Thus, with the ChineseEnglish corpus and the English-Japanese corpus, we can achieve a good word alignment results even if no Chinese-Japanese parallel corpus is available.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 92, "end_pos": 106, "type": "TASK", "confidence": 0.6845304667949677}]}, {"text": "After introducing the cross-language word similarity into the translation probability, the \"Advanced Induced\" method achieves a relative error rate reduction of 7.40% as compared with the \"Basic Induced\" method.", "labels": [], "entities": [{"text": "error rate reduction", "start_pos": 137, "end_pos": 157, "type": "METRIC", "confidence": 0.9176058570543925}]}, {"text": "It indicates that cross-language word similarity is effective in the calculation of the translation probability.", "labels": [], "entities": []}, {"text": "Moreover, the \"interpolated\" method further improves the result, which achieves relative error rate reductions of 12.51% and 21.30% as compared with the \"Advanced Induced\" method and the \"Original\" method.", "labels": [], "entities": [{"text": "error rate", "start_pos": 89, "end_pos": 99, "type": "METRIC", "confidence": 0.9278493225574493}]}], "tableCaptions": [{"text": " Table 1. Statistics for Training Data", "labels": [], "entities": []}, {"text": " Table 2. Word Alignment Results", "labels": [], "entities": [{"text": "Word Alignment", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.7117661684751511}]}]}