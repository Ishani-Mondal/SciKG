{"title": [{"text": "Soft Syntactic Constraints for Word Alignment through Discriminative Training", "labels": [], "entities": [{"text": "Word Alignment", "start_pos": 31, "end_pos": 45, "type": "TASK", "confidence": 0.7842020988464355}]}], "abstractContent": [{"text": "Word alignment methods can gain valuable guidance by ensuring that their alignments maintain cohesion with respect to the phrases specified by a monolingual dependency tree.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.6891153901815414}]}, {"text": "However, this hard constraint can also rule out correct alignments, and its utility decreases as alignment models become more complex.", "labels": [], "entities": []}, {"text": "We use a publicly available structured output SVM to create a max-margin syntactic aligner with a soft cohesion constraint.", "labels": [], "entities": []}, {"text": "The resulting aligner is the first, to our knowledge, to use a discriminative learning method to train an ITG bitext parser.", "labels": [], "entities": [{"text": "ITG bitext parser", "start_pos": 106, "end_pos": 123, "type": "TASK", "confidence": 0.6981966296831766}]}], "introductionContent": [{"text": "Given a parallel sentence pair, or bitext, bilingual word alignment finds word-to-word connections across languages.", "labels": [], "entities": [{"text": "bilingual word alignment", "start_pos": 43, "end_pos": 67, "type": "TASK", "confidence": 0.6341977914174398}]}, {"text": "Originally introduced as a byproduct of training statistical translation models in (, word alignment has become the first step in training most statistical translation systems, and alignments are useful to a host of other tasks.", "labels": [], "entities": [{"text": "statistical translation", "start_pos": 49, "end_pos": 72, "type": "TASK", "confidence": 0.6443095207214355}, {"text": "word alignment", "start_pos": 86, "end_pos": 100, "type": "TASK", "confidence": 0.7797495722770691}, {"text": "statistical translation", "start_pos": 144, "end_pos": 167, "type": "TASK", "confidence": 0.682562530040741}]}, {"text": "The dominant IBM alignment models) use minimal linguistic intuitions: sentences are treated as flat strings.", "labels": [], "entities": []}, {"text": "These carefully designed generative models are difficult to extend, and have resisted the incorporation of intuitively useful features, such as morphology.", "labels": [], "entities": []}, {"text": "There have been many attempts to incorporate syntax into alignment; we will not present a complete list here.", "labels": [], "entities": []}, {"text": "Some methods parse two flat strings at once using a bitext grammar).", "labels": [], "entities": []}, {"text": "Others parse one of the two strings before alignment begins, and align the resulting tree to the remaining string).", "labels": [], "entities": []}, {"text": "The statistical models associated with syntactic aligners tend to be very different from their IBM counterparts.", "labels": [], "entities": []}, {"text": "They model operations that are meaningful at a syntax level, like re-ordering children, but ignore features that have proven useful in IBM models, such as the preference to align words with similar positions, and the HMM preference for links to appear near one another (.", "labels": [], "entities": []}, {"text": "Recently, discriminative learning technology for structured output spaces has enabled several discriminative word alignment solutions ().", "labels": [], "entities": [{"text": "word alignment", "start_pos": 109, "end_pos": 123, "type": "TASK", "confidence": 0.7592663168907166}]}, {"text": "Discriminative learning allows easy incorporation of any feature one might have access to during the alignment search.", "labels": [], "entities": []}, {"text": "Because the features are handled so easily, discriminative methods use features that are not tied directly to the search: the search and the model become decoupled.", "labels": [], "entities": []}, {"text": "In this work, we view synchronous parsing only as a vehicle to expose syntactic features to a discriminative model.", "labels": [], "entities": []}, {"text": "This allows us to include the constraints that would usually be imposed by a tree-to-string alignment method as a feature in our model, creating a powerful soft constraint.", "labels": [], "entities": []}, {"text": "We add our syntactic features to an already strong flat-string discriminative solution, and we show that they provide new information resulting in improved alignments.", "labels": [], "entities": []}], "datasetContent": [{"text": "The first tests the dependency-augmented ITG described in Section 3.2 as an aligner with hard cohesion constraints.", "labels": [], "entities": []}, {"text": "The second tests our discriminative ITG with soft cohesion constraints against two strong baselines.", "labels": [], "entities": []}, {"text": "We conduct our experiments using French-English Hansard data.", "labels": [], "entities": [{"text": "French-English Hansard data", "start_pos": 33, "end_pos": 60, "type": "DATASET", "confidence": 0.8476995229721069}]}, {"text": "Our \u03c6 2 scores, link probabilities and word frequency counts are determined using a sentence-aligned bitext consisting of 50K sentence pairs.", "labels": [], "entities": [{"text": "\u03c6 2 scores", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.8821449677149454}, {"text": "word frequency counts", "start_pos": 39, "end_pos": 60, "type": "METRIC", "confidence": 0.6857362488905588}]}, {"text": "Our training set for the discriminative aligners is the first 100 sentence pairs from the FrenchEnglish gold standard provided for the 2003 WPT workshop).", "labels": [], "entities": [{"text": "FrenchEnglish gold standard provided for the 2003 WPT workshop", "start_pos": 90, "end_pos": 152, "type": "DATASET", "confidence": 0.7329449653625488}]}, {"text": "For evaluation we compare to the remaining 347 gold standard pairs using the alignment evaluation metrics: precision, recall and alignment error rate or AER.", "labels": [], "entities": [{"text": "precision", "start_pos": 107, "end_pos": 116, "type": "METRIC", "confidence": 0.9996569156646729}, {"text": "recall", "start_pos": 118, "end_pos": 124, "type": "METRIC", "confidence": 0.9940884113311768}, {"text": "alignment error rate", "start_pos": 129, "end_pos": 149, "type": "METRIC", "confidence": 0.8876825968424479}, {"text": "AER", "start_pos": 153, "end_pos": 156, "type": "METRIC", "confidence": 0.9615949392318726}]}, {"text": "SVM learning parameters are tuned using the 37-pair development set provided with this data.", "labels": [], "entities": []}, {"text": "English dependency trees are provided by.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The effect of hard cohesion constraints on  a simple unsupervised link score.", "labels": [], "entities": []}, {"text": " Table 2: The performance of SVM-trained align- ers with various degrees of cohesion constraint.", "labels": [], "entities": []}]}