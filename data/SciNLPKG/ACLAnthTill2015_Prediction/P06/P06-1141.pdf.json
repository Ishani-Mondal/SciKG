{"title": [{"text": "An Effective Two-Stage Model for Exploiting Non-Local Dependencies in Named Entity Recognition", "labels": [], "entities": [{"text": "Named Entity Recognition", "start_pos": 70, "end_pos": 94, "type": "TASK", "confidence": 0.6588826278845469}]}], "abstractContent": [{"text": "This paper shows that a simple two-stage approach to handle non-local dependencies in Named Entity Recognition (NER) can outperform existing approaches that handle non-local dependencies, while being much more computationally efficient.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 86, "end_pos": 116, "type": "TASK", "confidence": 0.8156596223513285}]}, {"text": "NER systems typically use sequence models for tractable inference, but this makes them unable to capture the long distance structure present in text.", "labels": [], "entities": []}, {"text": "We use a Conditional Random Field (CRF) based NER system using local features to make predictions and then train another CRF which uses both local information and features extracted from the output of the first CRF.", "labels": [], "entities": []}, {"text": "Using features capturing non-local dependencies from the same document, our approach yields a 12.6% relative error reduction on the F1 score, over state-of-the-art NER systems using local-information alone, when compared to the 9.3% relative error reduction offered by the best systems that exploit non-local information.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.9846619963645935}]}, {"text": "Our approach also makes it easy to incorporate non-local information from other documents in the test corpus, and this gives us a 13.3% error reduction over NER systems using local-information alone.", "labels": [], "entities": [{"text": "error", "start_pos": 136, "end_pos": 141, "type": "METRIC", "confidence": 0.962012767791748}]}, {"text": "Additionally , our running time for inference is just the inference time of two sequential CRFs, which is much less than that of other more complicated approaches that directly model the dependencies and do approximate inference.", "labels": [], "entities": []}], "introductionContent": [{"text": "Named entity recognition (NER) seeks to locate and classify atomic elements in unstructured text into predefined entities such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc.", "labels": [], "entities": [{"text": "Named entity recognition (NER) seeks to locate and classify atomic elements in unstructured text into predefined entities such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc", "start_pos": 0, "end_pos": 245, "type": "Description", "confidence": 0.7588618653161185}]}, {"text": "A particular problem for Named Entity Recognition(NER) systems is to exploit the presence of useful information regarding labels assigned at along distance from a given entity.", "labels": [], "entities": [{"text": "Named Entity Recognition(NER)", "start_pos": 25, "end_pos": 54, "type": "TASK", "confidence": 0.8123397380113602}]}, {"text": "An example is the label-consistency constraint that if our text has two occurrences of New York separated by other tokens, we would want our learner to encourage both these entities to get the same label.", "labels": [], "entities": []}, {"text": "Most statistical models currently used for Named Entity Recognition, use sequence models and thereby capture local structure.", "labels": [], "entities": [{"text": "Named Entity Recognition", "start_pos": 43, "end_pos": 67, "type": "TASK", "confidence": 0.8384186426798502}]}, {"text": "Hidden Markov Models (HMMs)), Conditional Markov Models (CMMs)), and Conditional Random Fields (CRFs) () have been successfully employed in NER and other information extraction tasks.", "labels": [], "entities": [{"text": "NER", "start_pos": 140, "end_pos": 143, "type": "TASK", "confidence": 0.9907316565513611}, {"text": "information extraction tasks", "start_pos": 154, "end_pos": 182, "type": "TASK", "confidence": 0.8183363278706869}]}, {"text": "All these models encode the Markov property i.e. labels directly depend only on the labels assigned to a small window around them.", "labels": [], "entities": []}, {"text": "These models exploit this property for tractable computation as this allows the Forward-Backward, Viterbi and Clique Calibration algorithms to become tractable.", "labels": [], "entities": []}, {"text": "Although this constraint is essential to make exact inference tractable, it makes us unable to exploit the non-local structure present in natural language.", "labels": [], "entities": []}, {"text": "Label consistency is an example of a non-local dependency important in NER.", "labels": [], "entities": [{"text": "NER", "start_pos": 71, "end_pos": 74, "type": "TASK", "confidence": 0.8530403971672058}]}, {"text": "Apart from label consistency between the same token sequences, we would also like to exploit richer sources of dependencies between similar token sequences.", "labels": [], "entities": []}, {"text": "For example, as shown in, we would want it to encourage Einstein to be labeled \"Person\" if there is strong evidence that Albert Einstein should be labeled \"Person\".", "labels": [], "entities": []}, {"text": "Sequence models unfortu- nately cannot model this due to their Markovian assumption.", "labels": [], "entities": []}, {"text": "Recent approaches attempting to capture nonlocal dependencies model the non-local dependencies directly, and use approximate inference algorithms, since exact inference is in general, not tractable for graphs with non-local structure.", "labels": [], "entities": []}, {"text": "define a Relational Markov Network (RMN) which explicitly models long-distance dependencies, and use it to represent relations between entities.", "labels": [], "entities": []}, {"text": "augment a sequential CRF with skip-edges i.e. edges between different occurrences of a token, in a document.", "labels": [], "entities": []}, {"text": "Both these approaches use loopy belief propagation) for approximate inference.", "labels": [], "entities": []}, {"text": "hand-set penalties for inconsistency in entity labeling at different occurrences in the text, based on some statistics from training data.", "labels": [], "entities": [{"text": "entity labeling", "start_pos": 40, "end_pos": 55, "type": "TASK", "confidence": 0.6834581196308136}]}, {"text": "They then employ Gibbs sampling for dealing with their local feature weights and their non-local penalties to do approximate inference.", "labels": [], "entities": []}, {"text": "We present a simple two-stage approach where our second CRF uses features derived from the output of the first CRF.", "labels": [], "entities": []}, {"text": "This gives us the advantage of defining a rich set of features to model non-local dependencies, and also eliminates the need to do approximate inference, since we do not explicitly capture the non-local dependencies in a single model, like the more complex existing approaches.", "labels": [], "entities": []}, {"text": "This also enables us to do inference efficiently since our inference time is merely the inference time of two sequential CRF's; in contrast reported an increase in running time by a factor of 30 over the sequential CRF, with their Gibbs sampling approximate inference.", "labels": [], "entities": []}, {"text": "In all, our approach is simpler, yields higher F1 scores, and is also much more computationally efficient than existing approaches modeling nonlocal dependencies.", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9835377037525177}]}], "datasetContent": [{"text": "We test the effectiveness of our technique on the CoNLL 2003 English named entity recognition dataset downloadable from http://cnts.uia.ac.be/conll2003/ner/.", "labels": [], "entities": [{"text": "CoNLL 2003 English named entity recognition dataset downloadable", "start_pos": 50, "end_pos": 114, "type": "DATASET", "confidence": 0.9015939831733704}]}, {"text": "The data comprises Reuters newswire articles annotated with four entity types: person (PER), location (LOC), organization (ORG), and miscellaneous (MISC).", "labels": [], "entities": [{"text": "Reuters newswire", "start_pos": 19, "end_pos": 35, "type": "DATASET", "confidence": 0.9139888286590576}]}, {"text": "The data is separated into a training set, a development set (testa), and a test set (testb).", "labels": [], "entities": []}, {"text": "The training set contains 945 documents, and approximately 203,000 tokens and the test set has 231 documents and approximately 46,000 tokens.", "labels": [], "entities": []}, {"text": "Performance on this task is evaluated by measuring the precision and recall of annotated entities (and not tokens), combined into an F1 score.", "labels": [], "entities": [{"text": "precision", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.9993231296539307}, {"text": "recall", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.9950590133666992}, {"text": "F1 score", "start_pos": 133, "end_pos": 141, "type": "METRIC", "confidence": 0.969939649105072}]}, {"text": "There is no partial credit for labeling part of an entity sequence correctly; an incorrect entity boundary is penalized as both a false positive and as a false negative.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Table showing the number of pairs of different occurrences of the same token sequence, where one occurrence is given  a certain label and the other occurrence is given a certain label. We show these counts both within documents, as well as over  the whole corpus. As we would expect, most pairs of the same entity sequence are labeled the same(i.e. the diagonal has most  of the density) at both the document and corpus levels. These statistics are from the CoNLL 2003 English training set.", "labels": [], "entities": [{"text": "CoNLL 2003 English training set", "start_pos": 468, "end_pos": 499, "type": "DATASET", "confidence": 0.9793149590492248}]}, {"text": " Table 2: Table showing the number of (token sequence, token subsequence) pairs where the token sequence is assigned a certain  entity label, and the token subsequence is assigned a certain entity label. We show these counts both within documents, as well  as over the whole corpus. Rows correspond to sequences, and columns to subsequences. These statistics are from the CoNLL  2003 English training set.", "labels": [], "entities": [{"text": "CoNLL  2003 English training set", "start_pos": 372, "end_pos": 404, "type": "DATASET", "confidence": 0.980880880355835}]}]}