{"title": [{"text": "An End-to-End Discriminative Approach to Machine Translation", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.7593742609024048}]}], "abstractContent": [{"text": "We present a perceptron-style discriminative approach to machine translation in which large feature sets can be exploited.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 57, "end_pos": 76, "type": "TASK", "confidence": 0.7971454560756683}]}, {"text": "Unlike discriminative rerank-ing approaches, our system can take advantage of learned features in all stages of decoding.", "labels": [], "entities": []}, {"text": "We first discuss several challenges to error-driven discrim-inative approaches.", "labels": [], "entities": []}, {"text": "In particular, we explore different ways of updating parameters given a training example.", "labels": [], "entities": []}, {"text": "We find that making frequent but smaller updates is preferable to making fewer but larger updates.", "labels": [], "entities": []}, {"text": "Then, we discuss an array of features and show both how they quantitatively increase BLEU score and how they qualitatively interact on specific examples.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 85, "end_pos": 95, "type": "METRIC", "confidence": 0.9801613688468933}]}, {"text": "One particular feature we investigate is a novel way to introduce learning into the initial phrase extraction process, which has previously been entirely heuristic.", "labels": [], "entities": [{"text": "phrase extraction process", "start_pos": 92, "end_pos": 117, "type": "TASK", "confidence": 0.7787405351797739}]}], "introductionContent": [{"text": "The generative, noisy-channel paradigm has historically served as the foundation for most of the work in statistical machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 105, "end_pos": 136, "type": "TASK", "confidence": 0.7350475589434305}]}, {"text": "At the same time, discriminative methods have provided substantial improvements over generative models on a wide range of NLP tasks.", "labels": [], "entities": [{"text": "generative", "start_pos": 85, "end_pos": 95, "type": "TASK", "confidence": 0.9666141271591187}]}, {"text": "They allow one to easily encode domain knowledge in the form of features.", "labels": [], "entities": []}, {"text": "Moreover, parameters are tuned to directly minimize error rather than to maximize joint likelihood, which may not correspond well to the task objective.", "labels": [], "entities": [{"text": "joint likelihood", "start_pos": 82, "end_pos": 98, "type": "METRIC", "confidence": 0.8535257577896118}]}, {"text": "In this paper, we present an end-to-end discriminative approach to machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.8177749514579773}]}, {"text": "The proposed system is phrase-based, as in, but uses an online perceptron training scheme to learn model parameters.", "labels": [], "entities": []}, {"text": "Unlike minimum error rate training, our system is able to exploit large numbers of specific features in the same manner as static reranking systems).", "labels": [], "entities": []}, {"text": "However, unlike static rerankers, our system does not rely on a baseline translation system.", "labels": [], "entities": []}, {"text": "Instead, it updates based on its own n-best lists.", "labels": [], "entities": []}, {"text": "As parameter estimates improve, the system produces better nbest lists, which can in turn enable better updates in future training iterations.", "labels": [], "entities": []}, {"text": "In this paper, we focus on two aspects of the problem of discriminative translation: the inherent difficulty of learning from reference translations, and the challenge of engineering effective features for this task.", "labels": [], "entities": [{"text": "discriminative translation", "start_pos": 57, "end_pos": 83, "type": "TASK", "confidence": 0.6179968416690826}]}, {"text": "Discriminative learning from reference translations is inherently problematic because standard discriminative methods need to know which outputs are correct and which are not.", "labels": [], "entities": [{"text": "Discriminative learning from reference translations", "start_pos": 0, "end_pos": 51, "type": "TASK", "confidence": 0.897733736038208}]}, {"text": "However, a proposed translation that differs from a reference translation need not be incorrect.", "labels": [], "entities": []}, {"text": "It may differ in word choice, literalness, or style, yet be fully acceptable.", "labels": [], "entities": [{"text": "style", "start_pos": 46, "end_pos": 51, "type": "METRIC", "confidence": 0.951608419418335}]}, {"text": "Pushing our system to avoid such alternate translations is undesirable.", "labels": [], "entities": []}, {"text": "On the other hand, even if a system produces a reference translation, it may do so by abusing the hidden structure (sentence segmentation and alignment).", "labels": [], "entities": [{"text": "sentence segmentation", "start_pos": 116, "end_pos": 137, "type": "TASK", "confidence": 0.7341905683279037}]}, {"text": "We can therefore never be entirely sure whether or not a proposed output is safe to update towards.", "labels": [], "entities": []}, {"text": "We discuss this issue in detail in Section 5, where we show that conservative updates (which push the system towards a local variant of the current prediction) are more effective than more aggressive updates (which try to directly update towards the reference).", "labels": [], "entities": []}, {"text": "The second major contribution of this work is an investigation of an array of features for our model.", "labels": [], "entities": []}, {"text": "We show how our features quantitatively increase BLEU score, as well as how they qualitatively interact on specific examples.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 49, "end_pos": 59, "type": "METRIC", "confidence": 0.9755223393440247}]}, {"text": "We first consider learning weights for individual phrases and part-of-speech patterns, showing gains from each.", "labels": [], "entities": []}, {"text": "We then present a novel way to parameterize and introduce learning into the initial phrase extraction process.", "labels": [], "entities": [{"text": "phrase extraction process", "start_pos": 84, "end_pos": 109, "type": "TASK", "confidence": 0.7737231155236562}]}, {"text": "In particular, we introduce alignment constellation features, which allow us to weight phrases based on the word alignment pattern that led to their extraction.", "labels": [], "entities": []}, {"text": "This kind of feature provides a potential way to initially extract phrases more aggressively and then later downweight undesirable patterns, essentially learning a weighted extraction heuristic.", "labels": [], "entities": []}, {"text": "Finally, we use POS features to parameterize a distortion model in a limited distortion decoder ().", "labels": [], "entities": []}, {"text": "We show that overall, BLEU score increases from 28.4 to 29.6 on French-English.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 22, "end_pos": 32, "type": "METRIC", "confidence": 0.963286966085434}]}], "datasetContent": [{"text": "Our experiments were done on the French-English portion of the Europarl corpus We split the data into three sets according to.", "labels": [], "entities": [{"text": "French-English portion of the Europarl corpus", "start_pos": 33, "end_pos": 78, "type": "DATASET", "confidence": 0.6533887485663096}]}, {"text": "TRAIN served two purposes: it was used to construct the features, and the 5-15 length sentences were used for tuning the parameters of those features.", "labels": [], "entities": [{"text": "TRAIN", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.8250879645347595}]}, {"text": "DEV, which consisted of the first 1K length 5-15 sentences in 2002, was used to evaluate the performance of the system as we developed it.", "labels": [], "entities": [{"text": "DEV", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8392000198364258}]}, {"text": "Note that the DEV set was not used to tune any parameters; tuning was done exclusively on TRAIN.", "labels": [], "entities": [{"text": "DEV set", "start_pos": 14, "end_pos": 21, "type": "DATASET", "confidence": 0.8662949800491333}, {"text": "TRAIN", "start_pos": 90, "end_pos": 95, "type": "DATASET", "confidence": 0.8598884344100952}]}, {"text": "At the end we ran our models once on TEST to get final numbers.", "labels": [], "entities": [{"text": "TEST", "start_pos": 37, "end_pos": 41, "type": "DATASET", "confidence": 0.7371498942375183}]}], "tableCaptions": [{"text": " Table 2: Comparison of BLEU scores between dif- ferent updating strategies for the monotonic and  limited distortion decoders on DEV.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.9991596937179565}, {"text": "DEV", "start_pos": 130, "end_pos": 133, "type": "DATASET", "confidence": 0.9634965062141418}]}, {"text": " Table 3: Main results on our system with differ- ent feature sets compared to minimum error-rate  trained Pharaoh.", "labels": [], "entities": []}, {"text": " Table 4: DEV BLEU score increase resulting from  adding constellation features.", "labels": [], "entities": [{"text": "DEV", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.5667358040809631}, {"text": "BLEU score increase", "start_pos": 14, "end_pos": 33, "type": "METRIC", "confidence": 0.9374380906422933}]}]}