{"title": [], "abstractContent": [{"text": "In this paper, we compare the performance of a state-of-the-art statistical parser (Bikel, 2004) in parsing written and spoken language and in generating sub-categorization cues from written and spoken language.", "labels": [], "entities": [{"text": "parsing written and spoken language", "start_pos": 100, "end_pos": 135, "type": "TASK", "confidence": 0.848557424545288}]}, {"text": "Although Bikel's parser achieves a higher accuracy for parsing written language, it achieves a higher accuracy when extracting subcategorization cues from spoken language.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9980277419090271}, {"text": "parsing written language", "start_pos": 55, "end_pos": 79, "type": "TASK", "confidence": 0.8938352664311727}, {"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9977471232414246}, {"text": "extracting subcategorization cues from spoken language", "start_pos": 116, "end_pos": 170, "type": "TASK", "confidence": 0.7582162121931711}]}, {"text": "Our experiments also show that current technology for extracting subcategorization frames initially designed for written texts works equally well for spoken language.", "labels": [], "entities": []}, {"text": "Additionally , we explore the utility of punctuation in helping parsing and extraction of subcategorization cues.", "labels": [], "entities": [{"text": "parsing and extraction of subcategorization cues", "start_pos": 64, "end_pos": 112, "type": "TASK", "confidence": 0.7344600607951483}]}, {"text": "Our experiments show that punctuation is of little help in parsing spoken language and extracting subcategorization cues from spoken language.", "labels": [], "entities": [{"text": "parsing spoken language", "start_pos": 59, "end_pos": 82, "type": "TASK", "confidence": 0.8903993169466654}]}, {"text": "This indicates that there is no need to add punctuation in transcribing spoken corpora simply in order to help parsers.", "labels": [], "entities": []}], "introductionContent": [{"text": "Robust statistical syntactic parsers, made possible by new statistical techniques) and by the availability of large, hand-annotated training corpora such as WSJ ( and Switchboard (), have had a major impact on the field of natural language processing.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 157, "end_pos": 160, "type": "DATASET", "confidence": 0.8609758019447327}, {"text": "natural language processing", "start_pos": 223, "end_pos": 250, "type": "TASK", "confidence": 0.6431374053160349}]}, {"text": "There are many ways to make use of parsers' output.", "labels": [], "entities": []}, {"text": "One particular form of data that can be extracted from parses is information about subcategorization.", "labels": [], "entities": []}, {"text": "Subcategorization data comes in two forms: subcategorization frame (SCF) and subcategorization cue (SCC).", "labels": [], "entities": []}, {"text": "SCFs differ from SCCs in that SCFs contain only arguments while SCCs contain both arguments and adjuncts.", "labels": [], "entities": []}, {"text": "Both SCFs and SCCs have been crucial to NLP tasks.", "labels": [], "entities": [{"text": "NLP tasks", "start_pos": 40, "end_pos": 49, "type": "TASK", "confidence": 0.8850571811199188}]}, {"text": "For example, SCFs have been used for verb disambiguation and classification) and SCCs for semantic role labeling ().", "labels": [], "entities": [{"text": "verb disambiguation and classification", "start_pos": 37, "end_pos": 75, "type": "TASK", "confidence": 0.7741791158914566}, {"text": "semantic role labeling", "start_pos": 90, "end_pos": 112, "type": "TASK", "confidence": 0.671723743279775}]}, {"text": "Current technology for automatically acquiring subcategorization data from corpora usually relies on statistical parsers to generate SCCs.", "labels": [], "entities": []}, {"text": "While great efforts have been made in parsing written texts and extracting subcategorization data from written texts, spoken corpora have received little attention.", "labels": [], "entities": [{"text": "parsing written texts", "start_pos": 38, "end_pos": 59, "type": "TASK", "confidence": 0.877850075562795}]}, {"text": "This is understandable given that spoken language poses several challenges that are absent in written texts, including disfluency, uncertainty about utterance segmentation and lack of punctuation.", "labels": [], "entities": [{"text": "utterance segmentation", "start_pos": 149, "end_pos": 171, "type": "TASK", "confidence": 0.7086269706487656}]}, {"text": "have suggested that there are substantial subcategorization differences between written corpora and spoken corpora.", "labels": [], "entities": []}, {"text": "For example, while written corpora show a much higher percentage of passive structures, spoken corpora usually have a higher percentage of zero-anaphora constructions.", "labels": [], "entities": []}, {"text": "We believe that subcategorization data derived from spoken language, if of acceptable quality, would be of more value to NLP tasks involving a syntactic analysis of spoken language.", "labels": [], "entities": []}, {"text": "We do not show this here.", "labels": [], "entities": []}, {"text": "The goals of this study are as follows: 1.", "labels": [], "entities": []}, {"text": "Test the performance of Bikel's parser in parsing written and spoken language.", "labels": [], "entities": [{"text": "parsing written and spoken language", "start_pos": 42, "end_pos": 77, "type": "TASK", "confidence": 0.8739301919937134}]}, {"text": "2. Compare the accuracy level of SCCs generated from parsed written and spoken lan-guage.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9994133710861206}]}, {"text": "We hope that such a comparison will shed some light on the feasibility of acquiring subcategorization data from spoken language using the current SCF acquisition technology initially designed for written language.", "labels": [], "entities": [{"text": "SCF acquisition", "start_pos": 146, "end_pos": 161, "type": "TASK", "confidence": 0.8989420831203461}]}, {"text": "3. Apply our SCF extraction system () to spoken and written language separately and compare the accuracy achieved for the acquired SCFs from spoken and written language.", "labels": [], "entities": [{"text": "SCF extraction", "start_pos": 13, "end_pos": 27, "type": "TASK", "confidence": 0.957354873418808}, {"text": "accuracy", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.9993966817855835}]}], "datasetContent": [{"text": "Three models will be investigated for parsing and extracting SCCs from the parser's output: 1.", "labels": [], "entities": [{"text": "parsing and extracting SCCs", "start_pos": 38, "end_pos": 65, "type": "TASK", "confidence": 0.8158260583877563}]}, {"text": "punc: leaving punctuation in both training and test data.", "labels": [], "entities": [{"text": "punc", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.892377495765686}]}, {"text": "2. no-punc: removing punctuation from both training and test data.", "labels": [], "entities": []}, {"text": "3. punc-no-punc: removing punctuation from only the test data.", "labels": [], "entities": []}, {"text": "Following the convention in the parsing community, for written language, we selected sections 02-21 of WSJ as training data and section 23 as test data.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 103, "end_pos": 106, "type": "DATASET", "confidence": 0.922269344329834}]}, {"text": "For spoken language, we designated section 2 and 3 of Switchboard as training data and files of sw4004 to sw4135 of section 4 as test data).", "labels": [], "entities": [{"text": "Switchboard", "start_pos": 54, "end_pos": 65, "type": "DATASET", "confidence": 0.8761235475540161}]}, {"text": "Since we are also interested in extracting SCCs from the parser's output, label clause type desired SCCs gerundive (NP)-GERUND S small clause NP-NP, (NP)-ADJP control (NP)-INF-to control (NP)-INF-wh-to SBAR with a complementizer (NP)-S-wh, (NP)-S-that without a complementizer (NP)-S-that: SCCs for different clauses we eliminated from the two test corpora all sentences that do not contain verbs.", "labels": [], "entities": [{"text": "GERUND", "start_pos": 120, "end_pos": 126, "type": "METRIC", "confidence": 0.8875636458396912}]}, {"text": "Our experiments proceed in the following three steps: 1.", "labels": [], "entities": []}, {"text": "Tag test data using the POS-tagger described in Ratnaparkhi (1996).", "labels": [], "entities": []}, {"text": "2. Parse the POS-tagged data using Bikel's parser.", "labels": [], "entities": []}, {"text": "3. Extract SCCs from the parser's output.", "labels": [], "entities": []}, {"text": "The extractor we built first locates each verb in the parser's output and then identifies the syntactic categories of all its sisters and combines them into an SCC.", "labels": [], "entities": []}, {"text": "However, there are cases where the extractor has more work to do.", "labels": [], "entities": []}, {"text": "\u2022 Finite and Infinite Clauses: In the Penn Treebank, Sand SBAR are used to label different types of clauses, obscuring too much detail about the internal structure of each clause.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 38, "end_pos": 51, "type": "DATASET", "confidence": 0.9953303039073944}]}, {"text": "Our extractor is designed to identify the internal structure of different types of clause, as shown in.", "labels": [], "entities": []}, {"text": "\u2022 Passive Structures: As noted above,) have noticed that written language tends to have a much higher percentage of passive structures than spoken language.", "labels": [], "entities": []}, {"text": "Our extractor is also designed to identify passive structures from the parser's output.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results of parsing and extraction of SCCs", "labels": [], "entities": [{"text": "parsing", "start_pos": 21, "end_pos": 28, "type": "TASK", "confidence": 0.9856837391853333}, {"text": "SCCs", "start_pos": 47, "end_pos": 51, "type": "TASK", "confidence": 0.45960161089897156}]}, {"text": " Table 3: The algorithm for computing shared de- pendents", "labels": [], "entities": [{"text": "computing shared de- pendents", "start_pos": 28, "end_pos": 57, "type": "TASK", "confidence": 0.6137201726436615}]}, {"text": " Table 4: An example of computing the number of  shared dependents", "labels": [], "entities": []}, {"text": " Table 5: SCCs and correct SCFs for introduce", "labels": [], "entities": []}, {"text": " Table 6: Training data for WC and SC", "labels": [], "entities": [{"text": "WC", "start_pos": 28, "end_pos": 30, "type": "TASK", "confidence": 0.6443040370941162}]}, {"text": " Table 7: Type precision and recall and F-measure", "labels": [], "entities": [{"text": "precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.8646354079246521}, {"text": "recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9997140765190125}, {"text": "F-measure", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9953006505966187}]}, {"text": " Table 7. As  shown in", "labels": [], "entities": []}]}