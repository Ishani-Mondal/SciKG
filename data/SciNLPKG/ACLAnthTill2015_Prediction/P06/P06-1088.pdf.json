{"title": [], "abstractContent": [{"text": "With performance above 97% accuracy for newspaper text, part of speech (POS) tagging might be considered a solved problem.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9987217783927917}, {"text": "part of speech (POS) tagging", "start_pos": 56, "end_pos": 84, "type": "TASK", "confidence": 0.6392834782600403}]}, {"text": "Previous studies have shown that allowing the parser to resolve POS tag ambiguity does not improve performance.", "labels": [], "entities": []}, {"text": "However, for grammar formalisms which use more fine-grained grammatical categories , for example TAG and CCG, tagging accuracy is much lower.", "labels": [], "entities": [{"text": "tagging", "start_pos": 110, "end_pos": 117, "type": "TASK", "confidence": 0.940326452255249}, {"text": "accuracy", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.9755673408508301}]}, {"text": "In fact, for these formalisms, premature ambiguity resolution makes parsing infeasible.", "labels": [], "entities": []}, {"text": "We describe a multi-tagging approach which maintains a suitable level of lexical category ambiguity for accurate and efficient CCG parsing.", "labels": [], "entities": [{"text": "CCG parsing", "start_pos": 127, "end_pos": 138, "type": "TASK", "confidence": 0.701066642999649}]}, {"text": "We extend this multi-tagging approach to the POS level to overcome errors introduced by automatically assigned POS tags.", "labels": [], "entities": []}, {"text": "Although POS tagging accuracy seems high, maintaining some POS tag ambiguity in the language processing pipeline results in more accurate CCG supertagging.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 9, "end_pos": 20, "type": "TASK", "confidence": 0.8117215037345886}, {"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9809045195579529}]}], "introductionContent": [{"text": "State-of-the-art part of speech (POS) tagging accuracy is now above 97% for newspaper text.", "labels": [], "entities": [{"text": "State-of-the-art part of speech (POS) tagging", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.5459174700081348}, {"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.979367196559906}]}, {"text": "One possible conclusion from the POS tagging literature is that accuracy is approaching the limit, and any remaining improvement is within the noise of the Penn Treebank training data.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 33, "end_pos": 44, "type": "TASK", "confidence": 0.6919158697128296}, {"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9995585083961487}, {"text": "Penn Treebank training data", "start_pos": 156, "end_pos": 183, "type": "DATASET", "confidence": 0.9914143085479736}]}, {"text": "So why should we continue to work on the POS tagging problem?", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 41, "end_pos": 52, "type": "TASK", "confidence": 0.8734315037727356}]}, {"text": "Here we give two reasons.", "labels": [], "entities": []}, {"text": "First, for lexicalized grammar formalisms such as TAG and CCG, the tagging problem is much harder.", "labels": [], "entities": [{"text": "tagging", "start_pos": 67, "end_pos": 74, "type": "TASK", "confidence": 0.9766801595687866}]}, {"text": "Second, any errors in POS tagger output, even at 97% acuracy, can have a significant impact on components further down the language processing pipeline.", "labels": [], "entities": [{"text": "POS tagger output", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.8037761052449545}]}, {"text": "In previous work we have shown that using automatically assigned, rather than gold standard, POS tags reduces the accuracy of our CCG parser by almost 2% in dependency F-score.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9994314312934875}, {"text": "F-score", "start_pos": 168, "end_pos": 175, "type": "METRIC", "confidence": 0.9367763996124268}]}, {"text": "CCG supertagging is much harder than POS tagging because the CCG tag set consists of finegrained lexical categories, resulting in a larger tag set -over 400 CCG lexical categories compared with 45 Penn Treebank POS tags.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 37, "end_pos": 48, "type": "TASK", "confidence": 0.8290636241436005}, {"text": "Penn Treebank POS tags", "start_pos": 197, "end_pos": 219, "type": "DATASET", "confidence": 0.9564269334077835}]}, {"text": "In fact, using a state-of-the-art tagger as a front end to a CCG parser makes accurate parsing infeasible because of the high supertagging error rate.", "labels": [], "entities": []}, {"text": "Our solution is to use multi-tagging, in which a CCG supertagger can potentially assign more than one lexical category to a word.", "labels": [], "entities": []}, {"text": "In this paper we significantly improve our earlier approach) by adapting the forward-backward algorithm to a Maximum Entropy tagger, which is used to calculate a probability distribution over lexical categories for each word.", "labels": [], "entities": []}, {"text": "This distribution is used to assign one or more categories to each word (.", "labels": [], "entities": []}, {"text": "We report large increases inaccuracy over single-tagging at only a small cost in increased ambiguity.", "labels": [], "entities": []}, {"text": "A further contribution of the paper is to also use multi-tagging for the POS tags, and to maintain some POS ambiguity in the language processing pipeline.", "labels": [], "entities": []}, {"text": "In particular, since POS tags are important features for the supertagger, we investigate how supertagging accuracy can be improved by not prematurely committing to a POS tag decision.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9934282898902893}]}, {"text": "Our results first demonstrate that a surprising in-crease in POS tagging accuracy can be achieved with only a tiny increase in ambiguity; and second that maintaining some POS ambiguity can significantly improve the accuracy of the supertagger.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 61, "end_pos": 72, "type": "TASK", "confidence": 0.7772205173969269}, {"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9464008808135986}, {"text": "accuracy", "start_pos": 215, "end_pos": 223, "type": "METRIC", "confidence": 0.9988688826560974}]}, {"text": "The parser uses the CCG lexical categories to build syntactic structure, and the POS tags are used by the supertagger and parser as part of their statisical models.", "labels": [], "entities": []}, {"text": "We show that using a multitagger for supertagging results in an effective preprocessor for CCG parsing, and that using a multitagger for POS tagging results in more accurate CCG supertagging.", "labels": [], "entities": [{"text": "CCG parsing", "start_pos": 91, "end_pos": 102, "type": "TASK", "confidence": 0.8257469832897186}, {"text": "POS tagging", "start_pos": 137, "end_pos": 148, "type": "TASK", "confidence": 0.8183894455432892}]}], "datasetContent": [{"text": "We performed several sets of experiments for POS tagging and CCG supertagging to explore the trade-off between ambiguity and tagging accuracy.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 45, "end_pos": 56, "type": "TASK", "confidence": 0.769030749797821}, {"text": "accuracy", "start_pos": 133, "end_pos": 141, "type": "METRIC", "confidence": 0.958817183971405}]}, {"text": "For both POS tagging and supertagging we varied the average number of tags assigned to each word, to see whether it is possible to significantly increase tagging accuracy with only a small increase in ambiguity.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 9, "end_pos": 20, "type": "TASK", "confidence": 0.8530943989753723}, {"text": "accuracy", "start_pos": 162, "end_pos": 170, "type": "METRIC", "confidence": 0.9808529019355774}]}, {"text": "For CCG supertagging, we also compared multi-tagging approaches, with a fixed category ambiguity of 1.4 categories per word.", "labels": [], "entities": []}, {"text": "All of the experiments used Section 02-21 of CCGbank as training data, Section 00 as development data and Section 23 as final test data.", "labels": [], "entities": [{"text": "CCGbank", "start_pos": 45, "end_pos": 52, "type": "DATASET", "confidence": 0.9027959108352661}]}, {"text": "We evaluate both per-word tag accuracy and sentence accuracy, which is the percentage of sentences for which every word is tagged correctly.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9258414506912231}, {"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.7594764232635498}]}, {"text": "For the multi-tagging results we consider the word to be tagged correctly if the correct tag appears in the set of tags assigned to the word.", "labels": [], "entities": []}, {"text": "gives an upper bound on accuracy if the maximum ambiguity is allowed.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9994373917579651}]}, {"text": "This involves setting the \u03b2 value to 0, so all feasible tags are assigned.", "labels": [], "entities": []}, {"text": "Note that the performance gain is only 1.6% in sentence accuracy, compared with the previous row, at the cost of a large increase in ambiguity.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9621865749359131}]}, {"text": "We have experimented with four different approaches to passing multiple POS tags as features through to the supertagger.", "labels": [], "entities": []}, {"text": "For the later experiments, this required the existing binary-valued framework to be extended to support real values.", "labels": [], "entities": []}, {"text": "The level of POS tag ambiguity was varied between 1.05 and 1.3 POS tags per word on average.", "labels": [], "entities": [{"text": "POS tag ambiguity", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.6311402519543966}]}, {"text": "These results are shown in.", "labels": [], "entities": []}, {"text": "The first approach is to treat the multiple POS tags as binary features (bin).", "labels": [], "entities": []}, {"text": "This simply involves adding the multiple POS tags for each word in both the training and test data.", "labels": [], "entities": []}, {"text": "Every assigned POS tag is treated as a separate feature and considered equally important regardless of its uncertainty.", "labels": [], "entities": []}, {"text": "Here we see a minor increase in performance over the original supertagger at the lower levels of POS ambiguity.", "labels": [], "entities": []}, {"text": "However, as the POS ambiguity is increased, the performance of the binary-valued features decreases and is eventually worse than the original supertagger.", "labels": [], "entities": []}, {"text": "This is because at the lowest levels of ambiguity the extra POS tags can be treated as being of similar reliability.", "labels": [], "entities": []}, {"text": "However, at higher levels of ambiguity many POS tags are added which are unreliable and should not be trusted equally.", "labels": [], "entities": []}, {"text": "The second approach (split) uses real-valued features to model some degree of uncertainty in the POS tags, dividing the POS tag probability mass evenly among the alternatives.", "labels": [], "entities": []}, {"text": "This has the effect of giving smaller feature values to tags where many alternative tags have been assigned.", "labels": [], "entities": []}, {"text": "This produces similar results to the binary-valued features, again performing best at low levels of ambiguity.", "labels": [], "entities": []}, {"text": "The third approach (invrank) is to use the inverse rank of each POS tag as a real-valued feature.", "labels": [], "entities": []}, {"text": "The inverse rank is the reciprocal of the tag's rank ordered by decreasing probability.", "labels": [], "entities": [{"text": "inverse rank", "start_pos": 4, "end_pos": 16, "type": "METRIC", "confidence": 0.9597312211990356}]}, {"text": "This method assumes the POS tagger correctly orders the alternative tags, but does not rely on the probability assigned to each tag.", "labels": [], "entities": []}, {"text": "Overall, invrank performs worse than split.", "labels": [], "entities": [{"text": "split", "start_pos": 37, "end_pos": 42, "type": "METRIC", "confidence": 0.7908079028129578}]}, {"text": "The final and best approach is to use the probabilities assigned to each alternative tag as realvalued features:: Multi-POS supertagging on Section 00 with different levels of POS ambiguity and using different approaches to POS feature encoding.", "labels": [], "entities": [{"text": "POS feature encoding", "start_pos": 224, "end_pos": 244, "type": "TASK", "confidence": 0.7400514682133993}]}, {"text": "shows our best performance figures for the multi-POS supertagger, against the previously described method using both gold standard and automatically assigned POS tags.", "labels": [], "entities": []}, {"text": "uses the Section 23 test data to demonstrate the improvement in supertagging when moving from single-tagging (single) to simple multi-tagging (noseq); from simple multitagging to the full forward-backward algorithm (fwdbwd); and finally when using the probabilities of multiply-assigned POS tags as features (MULTI-POS column).", "labels": [], "entities": [{"text": "Section 23 test data", "start_pos": 9, "end_pos": 29, "type": "DATASET", "confidence": 0.9162114262580872}, {"text": "MULTI-POS column", "start_pos": 309, "end_pos": 325, "type": "METRIC", "confidence": 0.6614314317703247}]}, {"text": "All of these multi-tagging experiments use an ambiguity level of 1.4 categories per word and the last result uses POS tag ambiguity of 1.1 tags per word.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Supertagging accuracy on Section 00 us- ing different approaches with multi-tagger ambi- guity fixed at 1.4 categories per word.", "labels": [], "entities": [{"text": "Supertagging", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.9254598617553711}, {"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.974621593952179}]}, {"text": " Table 3: Supertagging accuracy on Section 00 for  different levels of ambiguity.", "labels": [], "entities": [{"text": "Supertagging", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.7085288763046265}, {"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9617197513580322}, {"text": "Section 00", "start_pos": 35, "end_pos": 45, "type": "DATASET", "confidence": 0.6825492084026337}]}, {"text": " Table 4: Multi-POS supertagging on Section 00  with different levels of POS ambiguity and using  different approaches to POS feature encoding.", "labels": [], "entities": [{"text": "POS feature encoding", "start_pos": 122, "end_pos": 142, "type": "TASK", "confidence": 0.6423863271872202}]}, {"text": " Table 5: Best multi-POS supertagging accuracy on  Section 00 using POS ambiguity of 1.1 and the  probability real-valued features.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9944608211517334}, {"text": "Section 00", "start_pos": 51, "end_pos": 61, "type": "DATASET", "confidence": 0.8524857759475708}]}, {"text": " Table 6: Final supertagging results on Section 23.", "labels": [], "entities": [{"text": "Section 23", "start_pos": 40, "end_pos": 50, "type": "DATASET", "confidence": 0.803925096988678}]}]}