{"title": [{"text": "Annealing Structural Bias in Multilingual Weighted Grammar Induction *", "labels": [], "entities": [{"text": "Annealing Structural Bias", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7935638825098673}, {"text": "Multilingual Weighted Grammar Induction", "start_pos": 29, "end_pos": 68, "type": "TASK", "confidence": 0.6247545704245567}]}], "abstractContent": [{"text": "We first show how a structural locality bias can improve the accuracy of state-of-the-art dependency grammar induction models trained by EM from unannotated examples (Klein and Manning, 2004).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9979763627052307}]}, {"text": "Next, by annealing the free parameter that controls this bias, we achieve further improvements.", "labels": [], "entities": []}, {"text": "We then describe an alternative kind of structural bias, toward \"broken\" hypotheses consisting of partial structures over segmented sentences, and show a similar pattern of improvement.", "labels": [], "entities": []}, {"text": "We relate this approach to contrastive estimation (Smith and Eisner, 2005a), apply the latter to grammar induction in six languages, and show that our new approach improves accuracy by 1-17% (absolute) over CE (and 8-30% over EM), achieving to our knowledge the best results on this task to date.", "labels": [], "entities": [{"text": "contrastive estimation", "start_pos": 27, "end_pos": 49, "type": "TASK", "confidence": 0.7951575219631195}, {"text": "grammar induction", "start_pos": 97, "end_pos": 114, "type": "TASK", "confidence": 0.7377879321575165}, {"text": "accuracy", "start_pos": 173, "end_pos": 181, "type": "METRIC", "confidence": 0.9989726543426514}]}, {"text": "Our method, structural annealing, is a general technique with broad applicability to hidden-structure discovery problems.", "labels": [], "entities": [{"text": "hidden-structure discovery", "start_pos": 85, "end_pos": 111, "type": "TASK", "confidence": 0.7312658131122589}]}], "introductionContent": [{"text": "Inducing a weighted context-free grammar from flat text is a hard problem.", "labels": [], "entities": []}, {"text": "A common starting point for weighted grammar induction is the Expectation-Maximization (EM) algorithm).", "labels": [], "entities": [{"text": "weighted grammar induction", "start_pos": 28, "end_pos": 54, "type": "TASK", "confidence": 0.7121057311693827}]}, {"text": "EM's mediocre performance reflects two problems.", "labels": [], "entities": [{"text": "EM", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.8119263648986816}]}, {"text": "First, it seeks to maximize likelihood, but a grammar that makes the training data likely does not necessarily assign a linguistically defensible syntactic structure.", "labels": [], "entities": [{"text": "likelihood", "start_pos": 28, "end_pos": 38, "type": "METRIC", "confidence": 0.9353786110877991}]}, {"text": "Second, the likelihood surface is not globally concave, and learners such as the EM algorithm can get trapped on local maxima.", "labels": [], "entities": []}, {"text": "We seek hereto capitalize on the intuition that, at least early in learning, the learner should search primarily for string-local structure, because most structure is local.", "labels": [], "entities": []}, {"text": "1 By penalizing dependencies between two words that are farther apart in the string, we obtain consistent improvements inaccuracy of the learned model ( \u00a73).", "labels": [], "entities": []}, {"text": "We then explore how gradually changing \u03b4 overtime affects learning ( \u00a74): we start outwith a * This work was supported by a Fannie and John Hertz Foundation fellowship to the first author and NSF ITR grant IIS-0313193 to the second author.", "labels": [], "entities": [{"text": "NSF ITR grant IIS-0313193", "start_pos": 192, "end_pos": 217, "type": "DATASET", "confidence": 0.5422429740428925}]}, {"text": "The views expressed are not necessarily endorsed by the sponsors.", "labels": [], "entities": []}, {"text": "We thank three anonymous COLING-ACL reviewers for comments.", "labels": [], "entities": []}, {"text": "To be concrete, in the corpora tested here, 95% of dependency links cover \u2264 4 words (English, Bulgarian, Portuguese), \u2264 5 words (German, Turkish), \u2264 6 words strong preference for short dependencies, then relax the preference.", "labels": [], "entities": []}, {"text": "The new approach, structural annealing, often gives superior performance.", "labels": [], "entities": []}, {"text": "An alternative structural bias is explored in \u00a75.", "labels": [], "entities": []}, {"text": "This approach views a sentence as a sequence of one or more yields of separate, independent trees.", "labels": [], "entities": []}, {"text": "The points of segmentation area hidden variable, and during learning all possible segmentations are entertained probabilistically.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 14, "end_pos": 26, "type": "TASK", "confidence": 0.9672213196754456}]}, {"text": "This allows the learner to accept hypotheses that explain the sentences as independent pieces.", "labels": [], "entities": []}, {"text": "In \u00a76 we briefly review contrastive estimation, relating it to the new method, and show its performance alone and when augmented with structural bias.", "labels": [], "entities": [{"text": "contrastive estimation", "start_pos": 24, "end_pos": 46, "type": "TASK", "confidence": 0.8293889760971069}]}], "datasetContent": [{"text": "For (language, N) pairs where CE was effective, we trained models using CE with a fixed-\u03b2 segmentation model.", "labels": [], "entities": []}, {"text": "Across conditions (\u03b2 \u2208 [\u22121, 1]), these models performed very badly, hypothesizing extremely local parse trees: typically over 90% of dependencies were length 1 and pointed in the same direction, compared with the 60-70% length-1 rate seen in gold standards.", "labels": [], "entities": []}, {"text": "To understand why, consider that the CE goal is to maximize the score of a sentence and all its segmentations while minimizing the scores of neighborhood sentences and their segmentations.", "labels": [], "entities": [{"text": "CE", "start_pos": 37, "end_pos": 39, "type": "METRIC", "confidence": 0.8403177261352539}]}, {"text": "An ngram model can accomplish this, since the same n-grams are present in all segmentations of x, and (some) different n-grams appear in N(x) (for LENGTH and DELETEORTRANSPOSE1).", "labels": [], "entities": [{"text": "LENGTH", "start_pos": 147, "end_pos": 153, "type": "METRIC", "confidence": 0.9816930294036865}, {"text": "DELETEORTRANSPOSE1", "start_pos": 158, "end_pos": 176, "type": "METRIC", "confidence": 0.748364269733429}]}, {"text": "A bigram-like model that favors monotone branching, then, is not a bad choice fora CE learner that must account for segmentations of x and N(x).", "labels": [], "entities": []}, {"text": "Why doesn't CE without segmentation resort to n-gram-like models?", "labels": [], "entities": []}, {"text": "Inspection of models trained using the standard CE method (no segmentation) with transposition-based neighborhoods TRANS-POSE1 and DELETEORTRANSPOSE1 did have high rates of length-1 dependencies, while the poorly-performing DELETE1 models found low length-1 rates.", "labels": [], "entities": []}, {"text": "This suggests that a bias toward locality (\"n-gram-ness\") is built into the former neighborhoods, and may partly explain why CE works when it does.", "labels": [], "entities": []}, {"text": "We achieved a similar locality bias in the likelihood framework when we broadened the hypothesis space, but doing sounder CE over-focuses the model on local structures.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Baseline performance of EM-trained dependency  parsing models: F1 on non-$ attachments in test data, with  various model selection conditions (3 initializers \u00d7 6 smooth- ing values). The languages are listed in decreasing order by  the training set size. Experimental details can be found in the  appendix.", "labels": [], "entities": [{"text": "EM-trained dependency  parsing", "start_pos": 34, "end_pos": 64, "type": "TASK", "confidence": 0.7818840543429056}, {"text": "F1", "start_pos": 73, "end_pos": 75, "type": "METRIC", "confidence": 0.9990468621253967}]}, {"text": " Table 2: Performance of CE on test data, for different neigh- borhoods and with different levels of regularization. Bold- face marks scores better than EM-trained models selected the  same way (Table 1). The score is the F1 measure on non-$  attachments.", "labels": [], "entities": [{"text": "CE", "start_pos": 25, "end_pos": 27, "type": "METRIC", "confidence": 0.9411579966545105}, {"text": "F1", "start_pos": 222, "end_pos": 224, "type": "METRIC", "confidence": 0.9990196228027344}]}, {"text": " Table 3: Summary comparing models trained in a variety of ways with some relevant hyperparameters. Supervised model  selection was applied in all cases, including EM (see the appendix). Boldface marks the best performance overall and trials  that this performance did not significantly surpass under a sign test (i.e., p < 0.05). The score is the F1 measure on non-$  attachments. The fixed \u03b4 + CE condition was tested only for languages where CE improved over EM.", "labels": [], "entities": [{"text": "F1", "start_pos": 348, "end_pos": 350, "type": "METRIC", "confidence": 0.9986080527305603}, {"text": "CE", "start_pos": 396, "end_pos": 398, "type": "METRIC", "confidence": 0.6955535411834717}, {"text": "CE", "start_pos": 445, "end_pos": 447, "type": "METRIC", "confidence": 0.9835475087165833}]}]}