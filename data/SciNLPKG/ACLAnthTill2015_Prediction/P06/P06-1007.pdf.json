{"title": [{"text": "A Finite-State Model of Human Sentence Processing", "labels": [], "entities": [{"text": "Sentence Processing", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.743767112493515}]}], "abstractContent": [{"text": "It has previously been assumed in the psycholinguistic literature that finite-state models of language are crucially limited in their explanatory power by the locality of the probability distribution and the narrow scope of information used by the model.", "labels": [], "entities": []}, {"text": "We show that a simple computational model (a bigram part-of-speech tag-ger based on the design used by Corley and Crocker (2000)) makes correct predictions on processing difficulty observed in a wide range of empirical sentence processing data.", "labels": [], "entities": []}, {"text": "We use two modes of evaluation: one that relies on comparison with a control sentence, paralleling practice inhuman studies; another that measures probability drop in the disambiguating region of the sentence.", "labels": [], "entities": []}, {"text": "Both are surprisingly good indicators of the processing difficulty of garden-path sentences.", "labels": [], "entities": []}, {"text": "The sentences tested are drawn from published sources and systematically explore five different types of ambiguity: previous studies have been narrower in scope and smaller in scale.", "labels": [], "entities": []}, {"text": "We do not deny the limitations of finite-state models, but argue that our results show that their usefulness has been underestimated.", "labels": [], "entities": []}], "introductionContent": [{"text": "The main purpose of the current study is to investigate the extent to which a probabilistic part-ofspeech (POS) tagger can correctly model human sentence processing data.", "labels": [], "entities": []}, {"text": "Syntactically ambiguous sentences have been studied in great depth in psycholinguistics because the pattern of ambiguity resolution provides a window onto the human sentence processing mechanism (HSPM).", "labels": [], "entities": []}, {"text": "Prima facie it seems unlikely that such a tagger will be adequate, because almost all previous researchers have assumed, following standard linguistic theory, that a formally adequate account of recursive syntactic structure is an essential component of any model of the behaviour.", "labels": [], "entities": []}, {"text": "In this study, we tested a bigram POS tagger on different types of structural ambiguities and (as a sanity check) to the well-known asymmetry of subject and object relative clause processing.", "labels": [], "entities": [{"text": "subject and object relative clause processing", "start_pos": 145, "end_pos": 190, "type": "TASK", "confidence": 0.6608646959066391}]}, {"text": "Theoretically, the garden-path effect is defined as processing difficulty caused by reanalysis.", "labels": [], "entities": []}, {"text": "Empirically, it is attested as comparatively slower reading time or longer eye fixation at a disambiguating region in an ambiguous sentence compared to its control sentences.", "labels": [], "entities": []}, {"text": "That is, the garden-path effect detected in many human studies, in fact, is measured through a \"comparative\" method.", "labels": [], "entities": []}, {"text": "This characteristic of the sentence processing research design is reconstructed in the current study using a probabilistic POS tagging system.", "labels": [], "entities": [{"text": "sentence processing research", "start_pos": 27, "end_pos": 55, "type": "TASK", "confidence": 0.8470969796180725}, {"text": "POS tagging", "start_pos": 123, "end_pos": 134, "type": "TASK", "confidence": 0.7246645092964172}]}, {"text": "Under the assumption that larger probability decrease indicates slower reading time, the test results suggest that the probabilistic POS tagging system can predict reading time penalties at the disambiguating region of garden-path sentences compared to that of non-garden-path sentences (i.e. control sentences).", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 133, "end_pos": 144, "type": "TASK", "confidence": 0.7358736991882324}]}], "datasetContent": [{"text": "In the current study, Corley and Crocker's model is further tested on a wider range of so-called structural ambiguity types.", "labels": [], "entities": []}, {"text": "A Hidden Markov Model POS tagger based on bigrams was used.", "labels": [], "entities": []}, {"text": "We made our own implementation to be sure of getting as close as possible to the design of.", "labels": [], "entities": []}, {"text": "Given a word string, w 0 , w 1 , \u00b7 \u00b7 \u00b7 , w n , the tagger calculates the probability of every possible tag path, t 0 , \u00b7 \u00b7 \u00b7 , tn . Under the Markov assumption, the joint probability of the given word sequence and each possible POS sequence can be approximated as a product of conditional probability and transition probability as shown in (1).", "labels": [], "entities": []}, {"text": "Using the Viterbi algorithm, the tagger finds the most likely POS sequence fora given word string as shown in (2).", "labels": [], "entities": []}, {"text": "(2) arg max P (t 0 , t 1 , \u00b7 \u00b7 \u00b7 , tn |w 0 , w 1 , \u00b7 \u00b7 \u00b7 , w n , \u00b5).", "labels": [], "entities": []}, {"text": "This is known technology, see, but the particular use we make of it is unusual.", "labels": [], "entities": []}, {"text": "The tagger takes a word string as an input, outputs the most likely POS sequence and the final probability.", "labels": [], "entities": []}, {"text": "Additionally, it presents accumulated probability at each word break and probability re-ranking, if any.", "labels": [], "entities": []}, {"text": "Note that the running probability at the beginning of a sentence will be 1, and will keep decreasing at each word break since it is a product of conditional probabilities.", "labels": [], "entities": []}, {"text": "We tested the predictability of the model on empirical reading data with the probability decrease and the presence or absence of probability reranking.", "labels": [], "entities": []}, {"text": "Adopting the standard experimental design used inhuman sentence processing studies, where word-by-word reading time or eye-fixation time is compared between an experimental sentence and its control sentence, this study compares probability at each word break between a pair of sentences.", "labels": [], "entities": []}, {"text": "Comparatively faster or larger drop of probability is expected to be a good indicator of comparative processing difficulty.", "labels": [], "entities": []}, {"text": "Probability reranking, which is a simplified model of the reanalysis process assumed in many human studies, is also tested as another indicator of garden-path effect.", "labels": [], "entities": []}, {"text": "Given a word string, all the possible POS sequences compete with each other based on their probability.", "labels": [], "entities": []}, {"text": "Probability re-ranking occurs when an initially dispreferred POS sub-sequence becomes the preferred candidate later in the parse, because it fits in better with later words.", "labels": [], "entities": [{"text": "Probability re-ranking", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6565755605697632}]}, {"text": "The model parameters, P (w i |t i ) and P (t i |t i\u22121 ), are estimated from a small section (970,995 tokens,47,831 distinct words) of the British National Corpus (BNC), which is a 100 million-word collection of British English, both written and spoken, developed by Oxford University Press.", "labels": [], "entities": [{"text": "British National Corpus (BNC)", "start_pos": 138, "end_pos": 167, "type": "DATASET", "confidence": 0.9667029281457266}, {"text": "Oxford University Press", "start_pos": 266, "end_pos": 289, "type": "DATASET", "confidence": 0.8878329992294312}]}, {"text": "The BNC was chosen for training the model because it is a POS-annotated corpus, which allows supervised training.", "labels": [], "entities": [{"text": "BNC", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.8902479410171509}]}, {"text": "In the implementation we use log probabilities to avoid underflow, and we report log probabilities in the sequel.", "labels": [], "entities": []}], "tableCaptions": []}