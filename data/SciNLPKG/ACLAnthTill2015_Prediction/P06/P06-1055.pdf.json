{"title": [{"text": "Learning Accurate, Compact, and Interpretable Tree Annotation", "labels": [], "entities": []}], "abstractContent": [{"text": "We present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank.", "labels": [], "entities": []}, {"text": "Starting with a simple X-bar grammar, we learn anew grammar whose non-terminals are subsymbols of the original nontermi-nals.", "labels": [], "entities": []}, {"text": "In contrast with previous work, we are able to split various terminals to different degrees, as appropriate to the actual complexity in the data.", "labels": [], "entities": []}, {"text": "Our grammars automatically learn the kinds of linguistic distinctions exhibited in previous work on manual tree annotation.", "labels": [], "entities": []}, {"text": "On the other hand, our grammars are much more compact and substantially more accurate than previous work on automatic annotation.", "labels": [], "entities": []}, {"text": "Despite its simplicity, our best grammar achieves an F1 of 90.2% on the Penn Treebank, higher than fully lexicalized systems.", "labels": [], "entities": [{"text": "F1", "start_pos": 53, "end_pos": 55, "type": "METRIC", "confidence": 0.9995957016944885}, {"text": "Penn Treebank", "start_pos": 72, "end_pos": 85, "type": "DATASET", "confidence": 0.9962050914764404}]}], "introductionContent": [{"text": "Probabilistic context-free grammars (PCFGs) underlie most high-performance parsers in one way or another).", "labels": [], "entities": []}, {"text": "However, as demonstrated in and, a PCFG which simply takes the empirical rules and probabilities off of a treebank does not perform well.", "labels": [], "entities": []}, {"text": "This naive grammar is a poor one because its context-freedom assumptions are too strong in some places (e.g. it assumes that subject and object NPs share the same distribution) and too weak in others (e.g. it assumes that long rewrites are not decomposable into smaller steps).", "labels": [], "entities": []}, {"text": "Therefore, a variety of techniques have been developed to both enrich and generalize the naive grammar, ranging from simple tree annotation and symbol splitting to full lexicalization and intricate smoothing.", "labels": [], "entities": [{"text": "symbol splitting", "start_pos": 144, "end_pos": 160, "type": "TASK", "confidence": 0.7983426749706268}]}, {"text": "In this paper, we investigate the learning of a grammar consistent with a treebank at the level of evaluation symbols (such as NP, VP, etc.) but split based on the likelihood of the training trees.", "labels": [], "entities": []}, {"text": "addressed this question from a linguistic perspective, starting with a Markov grammar and manually splitting symbols in response to observed linguistic trends in the data.", "labels": [], "entities": []}, {"text": "For example, the symbol NP might be split into the subsymbol NP\u02c6S in subject position and the subsymbol NP\u02c6VP in object position.", "labels": [], "entities": []}, {"text": "Recently, and also exhibited an automatic approach in which each symbol is split into a fixed number of subsymbols.", "labels": [], "entities": []}, {"text": "For example, NP would be split into NP-1 through NP-8.", "labels": [], "entities": []}, {"text": "Their exciting result was that, while grammars quickly grew too large to be managed, a 16-subsymbol induced grammar reached the parsing performance of's manual grammar.", "labels": [], "entities": []}, {"text": "Other work has also investigated aspects of automatic grammar refinement; for example, learn annotations such as head rules in a constrained declarative language for tree-adjoining grammars.", "labels": [], "entities": [{"text": "automatic grammar refinement", "start_pos": 44, "end_pos": 72, "type": "TASK", "confidence": 0.6657490531603495}]}, {"text": "We present a method that combines the strengths of both manual and automatic approaches while addressing some of their common shortcomings.", "labels": [], "entities": []}, {"text": "Like and, we induce splits in a fully automatic fashion.", "labels": [], "entities": []}, {"text": "However, we use a more sophisticated split-and-merge approach that allocates subsymbols adaptively where they are most effective, like a linguist would.", "labels": [], "entities": []}, {"text": "The grammars recover patterns like those discussed in, heavily articulating complex and frequent categories like NP and VP while barely splitting rare or simple ones (see Section 3 for an empirical analysis).", "labels": [], "entities": []}, {"text": "Empirically, hierarchical splitting increases the accuracy and lowers the variance of the learned grammars.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9995924830436707}]}, {"text": "Another contribution is that, unlike previous work, we investigate smoothed models, allowing us to split grammars more heavily before running into the oversplitting effect discussed in, where data fragmentation outweighs increased expressivity.", "labels": [], "entities": []}, {"text": "Our method is capable of learning grammars of substantially smaller size and higher accuracy than previous grammar refinement work, starting from a simpler initial grammar.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9960247278213501}]}, {"text": "For example, even beginning with an X-bar grammar (see Section 1.1) with 98 symbols, our best grammar, using 1043 symbols, achieves a test set F 1 of 90.2%.", "labels": [], "entities": [{"text": "F 1", "start_pos": 143, "end_pos": 146, "type": "METRIC", "confidence": 0.9353621900081635}]}, {"text": "This is a 27% reduction in error and a significant reduction in size 1 over the most accurate gram- mar in.", "labels": [], "entities": [{"text": "error", "start_pos": 27, "end_pos": 32, "type": "METRIC", "confidence": 0.9961761236190796}, {"text": "size 1", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9464266896247864}, {"text": "gram- mar", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.7966105143229166}]}, {"text": "Our grammar's accuracy was higher than fully lexicalized systems, including the maximum-entropy inspired parser of Charniak and Johnson (2005).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9995400905609131}]}], "datasetContent": [{"text": "We ran our experiments on the Wall Street Journal (WSJ) portion of the Penn Treebank using the standard setup: we trained on sections 2 to 21, and we used section 1 as a validation set for tuning model hyperparameters.", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ) portion of the Penn Treebank", "start_pos": 30, "end_pos": 84, "type": "DATASET", "confidence": 0.9453146024183794}]}, {"text": "Section 22 was used as development set for intermediate results.", "labels": [], "entities": []}, {"text": "All of section 23 was reserved for the final test.", "labels": [], "entities": []}, {"text": "We used the EVALB parseval reference implementation, available from Sekine and Collins (1997), for scoring.", "labels": [], "entities": [{"text": "scoring", "start_pos": 99, "end_pos": 106, "type": "TASK", "confidence": 0.9511380791664124}]}, {"text": "All reported development set results are averages over four runs.", "labels": [], "entities": []}, {"text": "For the final test we selected the grammar that performed best on the development set.", "labels": [], "entities": []}, {"text": "Our experiments are based on a completely unannotated X-bar style grammar, obtained directly from the Penn Treebank by the binarization procedure shown in.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 102, "end_pos": 115, "type": "DATASET", "confidence": 0.9957755208015442}]}, {"text": "For each local tree rooted at an evaluation nonterminal X, we introduce a cascade of new nodes labeled X so that each has two children.", "labels": [], "entities": []}, {"text": "Rather than experiment with head-outward binarization as in, we simply used a left branching binarization; contains a comparison showing that the differences between binarizations are small.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The most frequent three words in the subcategories of several part-of-speech tags.", "labels": [], "entities": []}, {"text": " Table 4: Comparison of our results with those of others.", "labels": [], "entities": []}]}