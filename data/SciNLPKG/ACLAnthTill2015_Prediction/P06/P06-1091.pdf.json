{"title": [{"text": "A Discriminative Global Training Algorithm for Statistical MT", "labels": [], "entities": [{"text": "MT", "start_pos": 59, "end_pos": 61, "type": "TASK", "confidence": 0.7297856211662292}]}], "abstractContent": [{"text": "This paper presents a novel training algorithm fora linearly-scored block sequence translation model.", "labels": [], "entities": [{"text": "block sequence translation", "start_pos": 68, "end_pos": 94, "type": "TASK", "confidence": 0.6372886697451273}]}, {"text": "The key component is anew procedure to directly optimize the global scoring function used by a SMT decoder.", "labels": [], "entities": [{"text": "SMT decoder", "start_pos": 95, "end_pos": 106, "type": "TASK", "confidence": 0.922746330499649}]}, {"text": "No translation, language, or distortion model probabilities are used as in earlier work on SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 91, "end_pos": 94, "type": "TASK", "confidence": 0.9903096556663513}]}, {"text": "Therefore our method, which employs less domain specific knowledge, is both simpler and more extensible than previous approaches.", "labels": [], "entities": []}, {"text": "Moreover, the training procedure treats the decoder as a black-box, and thus can be used to optimize any decoding scheme.", "labels": [], "entities": []}, {"text": "The training algorithm is evaluated on a standard Arabic-English translation task.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper presents a view of phrase-based SMT as a sequential process that generates block orientation sequences.", "labels": [], "entities": [{"text": "SMT", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.7690346837043762}]}, {"text": "A block is a pair of phrases which are translations of each other.", "labels": [], "entities": []}, {"text": "For example, shows an Arabic-English translation example that uses four blocks.", "labels": [], "entities": []}, {"text": "During decoding, we view translation as a block segmentation process, where the input sentence is segmented from left to right and the target sentence is generated from bottom to top, one block at a time.", "labels": [], "entities": []}, {"text": "A monotone block sequence is generated except for the possibility to handle some local phrase re-ordering.", "labels": [], "entities": []}, {"text": "In this local re-ordering model () a block with orientation \u00a1 is generated relative to its predecessor block \u00a3 \u00a2 . During decoding, we maximize the score quence \u0086 is generated under the restriction that the concatenated source phrases of the blocks g yield the input sentence.", "labels": [], "entities": []}, {"text": "In modeling a block sequence, we emphasize adjacent block neighbors that have right or left orientation, since in the current experiments only local block swapping is handled (neutral orientation is used for 'detached' blocks as described in ().", "labels": [], "entities": []}, {"text": "This paper focuses on the discriminative training of the weight vector p used in Eq.", "labels": [], "entities": []}, {"text": "1. The decoding process is decomposed into local decision steps based on Eq.", "labels": [], "entities": []}, {"text": "1, but the model is trained in a global setting as shown below.", "labels": [], "entities": []}, {"text": "The advantage of this approach is that it can easily handle tens of millions of features, e.g. up to \u0087 \u0089 \u0088 million features for the experiments in this paper.", "labels": [], "entities": []}, {"text": "Moreover, under this view, SMT becomes quite similar to sequential natural language annotation problems such as part-of-speech tagging and shallow parsing, and the novel training algorithm presented in this paper is actually most similar to work on training algorithms presented for these task, e.g. the on-line training algorithm presented in) and the perceptron training algorithm presented in).", "labels": [], "entities": [{"text": "SMT", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.9940424561500549}, {"text": "part-of-speech tagging", "start_pos": 112, "end_pos": 134, "type": "TASK", "confidence": 0.7273095995187759}, {"text": "shallow parsing", "start_pos": 139, "end_pos": 154, "type": "TASK", "confidence": 0.7182872593402863}]}, {"text": "The current approach does not use specialized probability features as in) in any stage during decoder parameter training.", "labels": [], "entities": []}, {"text": "Such probability features include language model, translation or distortion probabilities, which are commonly used in current SMT approaches . We are able to achieve comparable performance to ().", "labels": [], "entities": [{"text": "SMT", "start_pos": 126, "end_pos": 129, "type": "TASK", "confidence": 0.9913764595985413}]}, {"text": "The novel algorithm differs computationally from earlier work in discriminative training algorithms for SMT  The paper is structured as follows: Section 2 presents the baseline block sequence model and the feature representation.", "labels": [], "entities": [{"text": "SMT", "start_pos": 104, "end_pos": 107, "type": "TASK", "confidence": 0.9928330183029175}]}, {"text": "Section 3 presents the discriminative training algorithm that learns a good global ranking function used during decoding.", "labels": [], "entities": []}, {"text": "Section 4 presents results on a standard Arabic-English translation task.", "labels": [], "entities": [{"text": "Arabic-English translation task", "start_pos": 41, "end_pos": 72, "type": "TASK", "confidence": 0.747442384560903}]}, {"text": "Finally, some discussion and future work is presented in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We applied the novel discriminative training approach to a standard Arabic-to-English translation task.", "labels": [], "entities": [{"text": "Arabic-to-English translation task", "start_pos": 68, "end_pos": 102, "type": "TASK", "confidence": 0.6902063091595968}]}, {"text": "The block set is generated using a phrase-pair selection algorithm similar to (), which includes some heuristic filtering to mal statement here.", "labels": [], "entities": [{"text": "phrase-pair selection", "start_pos": 35, "end_pos": 56, "type": "TASK", "confidence": 0.7101258337497711}]}, {"text": "A detailed theoretical investigation of the method will be given in a journal paper.", "labels": [], "entities": []}, {"text": "increase \u00d8 phrase translation accuracy.", "labels": [], "entities": [{"text": "\u00d8 phrase translation", "start_pos": 9, "end_pos": 29, "type": "TASK", "confidence": 0.5468060870965322}, {"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.964749276638031}]}, {"text": "Blocks that occur only once in the training data are included as well.", "labels": [], "entities": []}], "tableCaptions": []}