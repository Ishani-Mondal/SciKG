{"title": [], "abstractContent": [{"text": "We introduce an error mining technique for automatically detecting errors in resources that are used in parsing systems.", "labels": [], "entities": [{"text": "error mining", "start_pos": 16, "end_pos": 28, "type": "TASK", "confidence": 0.7196057736873627}]}, {"text": "We applied this technique on parsing results produced on several million words by two distinct parsing systems, which share the syntactic lexicon and the pre-parsing processing chain.", "labels": [], "entities": [{"text": "parsing results produced", "start_pos": 29, "end_pos": 53, "type": "TASK", "confidence": 0.87975545724233}]}, {"text": "We were thus able to identify missing and erroneous information in these resources.", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural language parsing is a hard task, partly because of the complexity and the volume of information that have to betaken into account about words and syntactic constructions.", "labels": [], "entities": [{"text": "Natural language parsing", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.6974042256673177}]}, {"text": "However, it is necessary to have access to such information, stored in resources such as lexica and grammars, and to try and minimize the amount of missing and erroneous information in these resources.", "labels": [], "entities": []}, {"text": "To achieve this, the use of these resources at a largescale in parsers is a very promising approach), and in particular the analysis of situations that lead to a parsing failure: one can learn from one's own mistakes.", "labels": [], "entities": []}, {"text": "We introduce a probabilistic model that allows to identify forms and form bigrams that maybe the source of errors, thanks to a corpus of parsed sentences.", "labels": [], "entities": []}, {"text": "In order to facilitate the exploitation of forms and form bigrams detected by the model, and in particular to identify causes of errors, we have developed a visualization environment.", "labels": [], "entities": []}, {"text": "The whole system has been tested on parsing results produced for several multi-million-word corpora and with two different parsers for French, namely SXLFG and FRMG.", "labels": [], "entities": [{"text": "FRMG", "start_pos": 160, "end_pos": 164, "type": "METRIC", "confidence": 0.5480250716209412}]}, {"text": "However, the error mining technique which is the topic of this paper is fully system-and language-independent.", "labels": [], "entities": [{"text": "error mining", "start_pos": 13, "end_pos": 25, "type": "TASK", "confidence": 0.7709698677062988}]}, {"text": "It could be applied without any change on parsing results produced by any system working on any language.", "labels": [], "entities": []}, {"text": "The only information that is needed is a boolean value for each sentence which indicates if it has been successfully parsed or not.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to validate our approach, we applied these principles to look for error causes in parsing results given by two deep parsing systems for French, FRMG and SXLFG, on large corpora.", "labels": [], "entities": [{"text": "parsing", "start_pos": 91, "end_pos": 98, "type": "TASK", "confidence": 0.9711153507232666}, {"text": "FRMG", "start_pos": 153, "end_pos": 157, "type": "DATASET", "confidence": 0.832139790058136}]}], "tableCaptions": [{"text": " Table 1: General information on corpora and parsing results", "labels": [], "entities": []}, {"text": " Table 2: Suspicious forms repartition for MD/FRMG", "labels": [], "entities": [{"text": "Suspicious forms repartition", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.867059330145518}, {"text": "MD/FRMG", "start_pos": 43, "end_pos": 50, "type": "TASK", "confidence": 0.4863994022210439}]}, {"text": " Table 3: Analysis of the 10 best-ranked forms (ranked according to M f = S f \u00b7 ln |O f |)", "labels": [], "entities": []}, {"text": " Table 4: The 10 best-ranked suspicious forms, according the the M f measure, as computed by different  algorithms: ours (this paper), a standard maximum entropy algorithm (maxent) and van Noord's rate  err(f ) (global).", "labels": [], "entities": [{"text": "M f measure", "start_pos": 65, "end_pos": 76, "type": "METRIC", "confidence": 0.7470961113770803}, {"text": "rate  err(f )", "start_pos": 197, "end_pos": 210, "type": "METRIC", "confidence": 0.8378116250038147}]}, {"text": " Table 5: Best ranked form bigrams (forms ranked inbetween are not shown; ranked according to M f =  S f \u00b7 ln |O f |). These results have been computed on a subset of the MD corpus (60,000 sentences).", "labels": [], "entities": [{"text": "MD corpus", "start_pos": 171, "end_pos": 180, "type": "DATASET", "confidence": 0.8316650390625}]}]}