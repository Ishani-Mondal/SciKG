{"title": [{"text": "LexNet: A Graphical Environment for Graph-Based NLP", "labels": [], "entities": []}], "abstractContent": [{"text": "This interactive presentation describes LexNet, a graphical environment for graph-based NLP developed at the University of Michigan.", "labels": [], "entities": []}, {"text": "LexNet includes LexRank (for text summarization), biased LexRank (for passage retrieval), and TUMBL (for binary classification).", "labels": [], "entities": [{"text": "LexNet", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.960553765296936}, {"text": "LexRank", "start_pos": 16, "end_pos": 23, "type": "DATASET", "confidence": 0.8825616240501404}, {"text": "text summarization", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.7453541159629822}, {"text": "passage retrieval", "start_pos": 70, "end_pos": 87, "type": "TASK", "confidence": 0.7892506718635559}]}, {"text": "All tools in the collection are based on random walks on lexical graphs, that is graphs where different NLP objects (e.g., sentences or phrases) are represented as nodes linked by edges proportional to the lexical similarity between the two nodes.", "labels": [], "entities": []}, {"text": "We will demonstrate these tools on a variety of NLP tasks including summarization, question answering, and prepositional phrase attachment.", "labels": [], "entities": [{"text": "summarization", "start_pos": 68, "end_pos": 81, "type": "TASK", "confidence": 0.9856805801391602}, {"text": "question answering", "start_pos": 83, "end_pos": 101, "type": "TASK", "confidence": 0.8393646478652954}, {"text": "prepositional phrase attachment", "start_pos": 107, "end_pos": 138, "type": "TASK", "confidence": 0.6184321045875549}]}], "introductionContent": [{"text": "We will present a series of graph-based tools fora variety of NLP tasks such as text summarization, passage retrieval, prepositional phrase attachment, and binary classification in general.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 80, "end_pos": 98, "type": "TASK", "confidence": 0.7820967733860016}, {"text": "passage retrieval", "start_pos": 100, "end_pos": 117, "type": "TASK", "confidence": 0.8788752853870392}, {"text": "prepositional phrase attachment", "start_pos": 119, "end_pos": 150, "type": "TASK", "confidence": 0.60886549949646}, {"text": "binary classification", "start_pos": 156, "end_pos": 177, "type": "TASK", "confidence": 0.7236410230398178}]}, {"text": "Recently proposed graph-based methods;;) are particularly well suited for transductive learning).", "labels": [], "entities": []}, {"text": "Transductive learning is based on the idea) that instead of splitting a learning problem into two possibly harder problems, namely induction and deduction, one can build a model that covers both labeled and unlabeled data.", "labels": [], "entities": [{"text": "Transductive learning", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8970387578010559}]}, {"text": "Unlabeled data are abundant as well as significantly cheaper than labeled data in a variety of natural language applications.", "labels": [], "entities": []}, {"text": "Parsing and machine translation both offer examples of this relationship, with unparsed text from the Web and untranslated texts being computationally less costly.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.7203742563724518}]}, {"text": "These can then be used to supplement manually translated and aligned corpora.", "labels": [], "entities": []}, {"text": "Hence transductive methods are of great potential for NLP problems and, as a result, LexNet includes a number of transductive methods.", "labels": [], "entities": [{"text": "LexNet", "start_pos": 85, "end_pos": 91, "type": "DATASET", "confidence": 0.9723083972930908}]}], "datasetContent": [], "tableCaptions": []}