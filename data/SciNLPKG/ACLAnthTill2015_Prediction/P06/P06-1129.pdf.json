{"title": [{"text": "Exploring Distributional Similarity Based Models for Query Spelling Correction", "labels": [], "entities": [{"text": "Query Spelling Correction", "start_pos": 53, "end_pos": 78, "type": "TASK", "confidence": 0.7844280004501343}]}], "abstractContent": [{"text": "A query speller is crucial to search engine in improving web search relevance.", "labels": [], "entities": []}, {"text": "This paper describes novel methods for use of distributional similarity estimated from query logs in learning improved query spelling correction models.", "labels": [], "entities": [{"text": "query spelling correction", "start_pos": 119, "end_pos": 144, "type": "TASK", "confidence": 0.6518832345803579}]}, {"text": "The key to our methods is the property of dis-tributional similarity between two terms: it is high between a frequently occurring misspelling and its correction, and low between two irrelevant terms only with similar spellings.", "labels": [], "entities": []}, {"text": "We present two models that are able to take advantage of this property.", "labels": [], "entities": []}, {"text": "Experimental results demonstrate that the distributional similarity based models can significantly outper-form their baseline systems in the web query spelling correction task.", "labels": [], "entities": [{"text": "web query spelling correction task", "start_pos": 141, "end_pos": 175, "type": "TASK", "confidence": 0.6851540625095367}]}], "introductionContent": [{"text": "Investigations into query log data reveal that more than 10% of queries sent to search engines contain misspelled terms ().", "labels": [], "entities": []}, {"text": "Such statistics indicate that a good query speller is crucial to search engine in improving web search relevance, because there is little opportunity that a search engine can retrieve many relevant contents with misspelled terms.", "labels": [], "entities": []}, {"text": "The problem of designing a spelling correction program for web search queries, however, poses special technical challenges and cannot be well solved by general purpose spelling correction methods.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.8776853382587433}, {"text": "spelling correction", "start_pos": 168, "end_pos": 187, "type": "TASK", "confidence": 0.7308925837278366}]}, {"text": "discussed in detail specialties and difficulties of a query spellchecker, and illustrated why the existing methods could notwork for query spelling correction.", "labels": [], "entities": [{"text": "query spelling correction", "start_pos": 133, "end_pos": 158, "type": "TASK", "confidence": 0.783822238445282}]}, {"text": "They also identified that no single evidence, either a conventional spelling lexicon or term frequency in the query logs, can serve as criteria for validate queries.", "labels": [], "entities": []}, {"text": "To address these challenges, we concentrate on the problem of learning improved query spelling correction model by integrating distributional similarity information automatically derived from query logs.", "labels": [], "entities": [{"text": "query spelling correction", "start_pos": 80, "end_pos": 105, "type": "TASK", "confidence": 0.658169815937678}]}, {"text": "The key contribution of our work is identifying that we can successfully use the evidence of distributional similarity to achieve better spelling correction accuracy.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 137, "end_pos": 156, "type": "TASK", "confidence": 0.9120091795921326}, {"text": "accuracy", "start_pos": 157, "end_pos": 165, "type": "METRIC", "confidence": 0.8207575082778931}]}, {"text": "We present two methods that are able to take advantage of distributional similarity information.", "labels": [], "entities": []}, {"text": "The first method extends a string edit-based error model with confusion probabilities within a generative source channel model.", "labels": [], "entities": []}, {"text": "The second method explores the effectiveness of our approach within a discriminative maximum entropy model framework by integrating distributional similarity-based features.", "labels": [], "entities": []}, {"text": "Experimental results demonstrate that both methods can significantly outperform their baseline systems in the spelling correction task for web search queries.", "labels": [], "entities": [{"text": "spelling correction task", "start_pos": 110, "end_pos": 134, "type": "TASK", "confidence": 0.9162123401959738}]}, {"text": "The rest of the paper is structured as follows: after a brief overview of the related work in Section 2, we discuss the motivations for our approach, and describe two methods that can make use of distributional similarity information in Section 3.", "labels": [], "entities": []}, {"text": "Experiments and results are presented in Section 4.", "labels": [], "entities": []}, {"text": "The last section contains summaries and outlines promising future work.", "labels": [], "entities": [{"text": "summaries", "start_pos": 26, "end_pos": 35, "type": "TASK", "confidence": 0.9228768348693848}]}], "datasetContent": [{"text": "We randomly sampled 7,000 queries from daily query logs of MSN Search and they were manually labeled by two annotators.", "labels": [], "entities": [{"text": "MSN Search", "start_pos": 59, "end_pos": 69, "type": "DATASET", "confidence": 0.9098837673664093}]}, {"text": "For each query identified to contain spelling errors, corrections were given by the annotators independently.", "labels": [], "entities": []}, {"text": "From the annotation results that both annotators agreed upon 3,061 queries were extracted, which were further divided into a test set containing 1,031 queries and a training set containing 2,030 queries.", "labels": [], "entities": []}, {"text": "In the test set there are 171 queries identified containing spelling errors with an error rate of 16.6%.", "labels": [], "entities": [{"text": "error rate", "start_pos": 84, "end_pos": 94, "type": "METRIC", "confidence": 0.9862869679927826}]}, {"text": "The numbers on the training set is 312 and 15.3%, respectively.", "labels": [], "entities": [{"text": "training set", "start_pos": 19, "end_pos": 31, "type": "DATASET", "confidence": 0.6938197314739227}]}, {"text": "The average length of queries on training set is 2.8 terms and on test set it is 2.6.", "labels": [], "entities": []}, {"text": "In our experiments, a term bigram model is used as the source model.", "labels": [], "entities": []}, {"text": "The bigram model is trained with query log data of MSN Search during the period from October 2004 to June 2005.", "labels": [], "entities": [{"text": "MSN Search", "start_pos": 51, "end_pos": 61, "type": "DATASET", "confidence": 0.8988477885723114}]}, {"text": "Correction candidates are generated from a term base extracted from the same set of query logs.", "labels": [], "entities": []}, {"text": "For each of the experiments, the performance is evaluated by the following metrics: Accuracy: The number of correct outputs generated by the system divided by the total number of queries in the test set; Recall: The number of correct suggestions for misspelled queries generated by the system divided by the total number of misspelled queries in the test set; Precision: The number of correct suggestions for misspelled queries generated by the system divided by the total number of suggestions made by the system.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9990423321723938}, {"text": "Recall", "start_pos": 204, "end_pos": 210, "type": "METRIC", "confidence": 0.9980027079582214}, {"text": "Precision", "start_pos": 360, "end_pos": 369, "type": "METRIC", "confidence": 0.9668824076652527}]}], "tableCaptions": [{"text": " Table 1. Statistics of two word pairs  with similar spellings", "labels": [], "entities": []}, {"text": " Table 2. Performance results for different models", "labels": [], "entities": []}]}