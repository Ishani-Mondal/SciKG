{"title": [], "abstractContent": [{"text": "We present an approach to pronoun resolution based on syntactic paths.", "labels": [], "entities": [{"text": "pronoun resolution", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.7650666534900665}]}, {"text": "Through a simple bootstrapping procedure, we learn the likelihood of coreference between a pronoun and a candidate noun based on the path in the parse tree between the two entities.", "labels": [], "entities": []}, {"text": "This path information enables us to handle previously challenging resolution instances, and also robustly addresses traditional syntactic coreference constraints.", "labels": [], "entities": []}, {"text": "Highly coreferent paths also allow mining of precise probabilistic gender/number information.", "labels": [], "entities": []}, {"text": "We combine statistical knowledge with well known features in a Support Vector Machine pronoun resolution classifier.", "labels": [], "entities": [{"text": "Support Vector Machine pronoun resolution classifier", "start_pos": 63, "end_pos": 115, "type": "TASK", "confidence": 0.5911508103211721}]}, {"text": "Significant gains in performance are observed on several datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Pronoun resolution is a difficult but vital part of the overall coreference resolution task.", "labels": [], "entities": [{"text": "Pronoun resolution", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.856465071439743}, {"text": "coreference resolution", "start_pos": 64, "end_pos": 86, "type": "TASK", "confidence": 0.9643069505691528}]}, {"text": "In each of the following sentences, a pronoun resolution system must determine what the pronoun his refers to: (1) John needs his friend.", "labels": [], "entities": [{"text": "pronoun resolution", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.7760887145996094}]}, {"text": "(2) John needs his support.", "labels": [], "entities": []}, {"text": "In (1), John and his corefer.", "labels": [], "entities": []}, {"text": "In, his refers to some other, perhaps previously evoked entity.", "labels": [], "entities": []}, {"text": "Traditional pronoun resolution systems are not designed to distinguish between these cases.", "labels": [], "entities": [{"text": "pronoun resolution", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.7551952004432678}]}, {"text": "They lack the specific world knowledge required in the second instance -the knowledge that a person does not usually explicitly need his own support.", "labels": [], "entities": []}, {"text": "We collect statistical path-coreference information from a large, automatically-parsed corpus to address this limitation.", "labels": [], "entities": []}, {"text": "A dependency path is defined as the sequence of dependency links between two potentially coreferent entities in a parse tree.", "labels": [], "entities": []}, {"text": "A path does not include the terminal entities; for example, \"John needs his support\" and \"He needs their support\" have the same syntactic path.", "labels": [], "entities": []}, {"text": "Our algorithm determines that the dependency path linking the Noun and pronoun is very likely to connect coreferent entities for the path \"Noun needs pronoun's friend,\" while it is rarely coreferent for the path \"Noun needs pronoun's support.\"", "labels": [], "entities": []}, {"text": "This likelihood can be learned by simply counting how often we see a given path in text with an initial Noun and a final pronoun that are from the same/different gender/number classes.", "labels": [], "entities": []}, {"text": "Cases such as \"John needs her support\" or \"They need his support\" are much more frequent in text than cases where the subject noun and pronoun terminals agree in gender/number.", "labels": [], "entities": []}, {"text": "When there is agreement, the terminal nouns are likely to be coreferent.", "labels": [], "entities": []}, {"text": "When they disagree, they refer to different entities.", "labels": [], "entities": []}, {"text": "After a sufficient number of occurrences of agreement or disagreement, there is a strong statistical indication of whether the path is coreferent (terminal nouns tend to refer to the same entity) or non-coreferent (nouns refer to different entities).", "labels": [], "entities": []}, {"text": "We show that including path coreference information enables significant performance gains on three third-person pronoun resolution experiments.", "labels": [], "entities": [{"text": "path coreference information", "start_pos": 23, "end_pos": 51, "type": "TASK", "confidence": 0.759413500626882}, {"text": "third-person pronoun resolution", "start_pos": 99, "end_pos": 130, "type": "TASK", "confidence": 0.7448196609814962}]}, {"text": "We also show that coreferent paths can provide the seed information for bootstrapping other, even more important information, such as the gender/number of noun phrases.", "labels": [], "entities": []}], "datasetContent": [{"text": "The noun-pronoun path coreference can be used directly as a feature in a pronoun resolution system.", "labels": [], "entities": [{"text": "pronoun resolution", "start_pos": 73, "end_pos": 91, "type": "TASK", "confidence": 0.7515497505664825}]}, {"text": "However, path coreference is undefined for cases where there is no path between the pronoun and the candidate noun -for example, when the candidate is in the previous sentence.", "labels": [], "entities": [{"text": "path coreference", "start_pos": 9, "end_pos": 25, "type": "TASK", "confidence": 0.6933801025152206}]}, {"text": "Therefore, rather than using path coreference directly, we have features that are true if C(p) is above or below certain thresholds.", "labels": [], "entities": []}, {"text": "The features are thus set when coreference between the pronoun and candidate noun is likely (a coreferent path) or unlikely (a non-coreferent path).", "labels": [], "entities": [{"text": "coreference between the pronoun and candidate noun", "start_pos": 31, "end_pos": 81, "type": "TASK", "confidence": 0.7635581919125148}]}, {"text": "We now evaluate the utility of path coreference within a state-of-the-art machine-learned resolution system for third-person pronouns with nominal antecedents.", "labels": [], "entities": [{"text": "path coreference", "start_pos": 31, "end_pos": 47, "type": "TASK", "confidence": 0.6908269375562668}]}, {"text": "A standard set of features is used along with the bootstrapped gender/number, semantic compatibility, and path coreference information.", "labels": [], "entities": []}, {"text": "We refer to these features as our \"probabilistic features\" (Prob.", "labels": [], "entities": []}, {"text": "Features) and run experiments using the full system trained and tested with each absent, in turn).", "labels": [], "entities": []}, {"text": "We have 29 features in total, including measures of candidate distance, frequency, grammatical role, and different kinds of parallelism between the pronoun and the candidate noun.", "labels": [], "entities": []}, {"text": "Several reliable features are used as hard constraints, removing candidates before consideration by the scoring algorithm.", "labels": [], "entities": []}, {"text": "All of the parsing, noun-phrase identification, and named-entity recognition are done automatically with Minipar.", "labels": [], "entities": [{"text": "parsing", "start_pos": 11, "end_pos": 18, "type": "TASK", "confidence": 0.9684140682220459}, {"text": "noun-phrase identification", "start_pos": 20, "end_pos": 46, "type": "TASK", "confidence": 0.8374543488025665}, {"text": "named-entity recognition", "start_pos": 52, "end_pos": 76, "type": "TASK", "confidence": 0.7103532701730728}, {"text": "Minipar", "start_pos": 105, "end_pos": 112, "type": "DATASET", "confidence": 0.9669069051742554}]}, {"text": "Candidate antecedents are considered in the current and previous sentence only.", "labels": [], "entities": []}, {"text": "We use SVM light) to learn a linear-kernel classifier on pairwise examples in the training set.", "labels": [], "entities": []}, {"text": "When resolving pronouns, we select the candidate with the farthest positive distance from the SVM classification hyperplane.", "labels": [], "entities": [{"text": "SVM classification hyperplane", "start_pos": 94, "end_pos": 123, "type": "DATASET", "confidence": 0.7579988241195679}]}, {"text": "Our training set is the anaphora-annotated portion of the American National Corpus (ANC) used in, containing 1270 anaphoric pronouns 4 . We test on the ANC Test set (1291 instances) also used in (highest resolution accuracy reported: 73.3%), the anaphoralabelled portion of AQUAINT used in Cherry and Bergsma (2005) (1078 instances, highest accuracy: 71.4%), and the anaphoric pronoun subset of the MUC7 (1997) coreference evaluation formal test set (169 instances, highest precision of 62.1 reported on all pronouns in ().", "labels": [], "entities": [{"text": "American National Corpus (ANC)", "start_pos": 58, "end_pos": 88, "type": "DATASET", "confidence": 0.960230549176534}, {"text": "ANC Test set", "start_pos": 152, "end_pos": 164, "type": "DATASET", "confidence": 0.8719324072202047}, {"text": "resolution accuracy", "start_pos": 204, "end_pos": 223, "type": "METRIC", "confidence": 0.8066847622394562}, {"text": "accuracy", "start_pos": 341, "end_pos": 349, "type": "METRIC", "confidence": 0.9432004690170288}, {"text": "MUC7 (1997) coreference evaluation formal test set", "start_pos": 399, "end_pos": 449, "type": "DATASET", "confidence": 0.8659829099973043}, {"text": "precision", "start_pos": 474, "end_pos": 483, "type": "METRIC", "confidence": 0.9032739996910095}]}, {"text": "These particular corpora were chosen so we could test our approach using the same data as comparable machine-learned systems exploiting probabilistic information sources.", "labels": [], "entities": []}, {"text": "Parameters were set using cross-validation on the training set; test sets were used only once to obtain the final performance values.", "labels": [], "entities": [{"text": "Parameters", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9679363965988159}]}, {"text": "Evaluation Metric: We report results in terms of accuracy: Of all the anaphoric pronouns in the test set, the proportion we resolve correctly.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.999122679233551}]}], "tableCaptions": [{"text": " Table 3: Gender classification performance (%)", "labels": [], "entities": [{"text": "Gender classification", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.8404677510261536}]}, {"text": " Table 4: Example gender/number probability (%)", "labels": [], "entities": [{"text": "gender/number probability", "start_pos": 18, "end_pos": 43, "type": "METRIC", "confidence": 0.6818551495671272}]}, {"text": " Table 5: Resolution accuracy (%)", "labels": [], "entities": [{"text": "Resolution", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9912370443344116}, {"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9267212152481079}]}]}