{"title": [], "abstractContent": [{"text": "This paper investigates a machine learning approach for temporally ordering and anchoring events in natural language texts.", "labels": [], "entities": [{"text": "temporally ordering and anchoring events in natural language texts", "start_pos": 56, "end_pos": 122, "type": "TASK", "confidence": 0.7661855551931593}]}, {"text": "To address data sparseness, we used temporal reasoning as an over-sampling method to dramatically expand the amount of training data, resulting in predictive accuracy on link labeling as high as 93% using a Maximum Entropy classifier on human annotated data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 158, "end_pos": 166, "type": "METRIC", "confidence": 0.9836152195930481}]}, {"text": "This method compared favorably against a series of increasingly sophisticated base-lines involving expansion of rules derived from human intuitions.", "labels": [], "entities": []}], "introductionContent": [{"text": "The growing interest in practical NLP applications such as question-answering and text summarization places increasing demands on the processing of temporal information.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 82, "end_pos": 100, "type": "TASK", "confidence": 0.7319270372390747}]}, {"text": "In multidocument summarization of news articles, it can be useful to know the relative order of events so as to merge and present information from multiple news sources correctly.", "labels": [], "entities": [{"text": "multidocument summarization of news articles", "start_pos": 3, "end_pos": 47, "type": "TASK", "confidence": 0.7827498316764832}]}, {"text": "In questionanswering, one would like to be able to ask when an event occurs, or what events occurred prior to a particular event.", "labels": [], "entities": []}, {"text": "A wealth of prior research by,,,,,, and others, has explored the different knowledge sources used in inferring the temporal ordering of events, including temporal adverbials, tense, aspect, rhetorical relations, pragmatic conventions, and background knowledge.", "labels": [], "entities": []}, {"text": "For example, the narrative convention of events being described in the order in which they occur is followed in (1), but overridden by means of a discourse relation, Explanation in (2).", "labels": [], "entities": []}, {"text": "(1) Max stood up.", "labels": [], "entities": []}, {"text": "(2) Max fell.", "labels": [], "entities": [{"text": "Max", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.9117878675460815}]}, {"text": "In addition to discourse relations, which often require inferences based on world knowledge, the ordering decisions humans carryout appear to involve a variety of knowledge sources, including tense and grammatical aspect (3a), lexical aspect (3b), and temporal adverbials (3c): (3a) Max entered the room.", "labels": [], "entities": []}, {"text": "He had drunk a lot of wine.", "labels": [], "entities": []}, {"text": "(3b) Max entered the room.", "labels": [], "entities": []}, {"text": "Mary was seated behind the desk.", "labels": [], "entities": []}, {"text": "(3c) The company announced Tuesday that third-quarter sales had fallen.", "labels": [], "entities": []}, {"text": "Clearly, substantial linguistic processing maybe required fora system to make these inferences, and world knowledge is hard to make available to a domain-independent program.", "labels": [], "entities": []}, {"text": "An important strategy in this area is of course the development of annotated corpora than can facilitate the machine learning of such ordering inferences.", "labels": [], "entities": []}, {"text": "This paper 1 investigates a machine learning approach for temporally ordering events in natural language texts.", "labels": [], "entities": [{"text": "temporally ordering events in natural language texts", "start_pos": 58, "end_pos": 110, "type": "TASK", "confidence": 0.6799500286579132}]}, {"text": "In Section 2, we describe the annotation scheme and annotated corpora, and the challenges posed by them.", "labels": [], "entities": []}, {"text": "A basic learning approach is described in Section 3.", "labels": [], "entities": []}, {"text": "To address data sparseness, we used temporal reasoning as an over-sampling method to dramatically expand the amount of training data.", "labels": [], "entities": []}, {"text": "As we will discuss in Section 5, there are no standard algorithms for making these inferences that we can compare against.", "labels": [], "entities": []}, {"text": "We believe strongly that in such situations, it's worthwhile for computational linguists to devote consider-able effort to developing insightful baselines.", "labels": [], "entities": []}, {"text": "Our work is, accordingly, evaluated in comparison against four baselines: (i) the usual majority class statistical baseline, shown along with each result, (ii) a more sophisticated baseline that uses hand-coded rules (Section 4.1), (iii) a hybrid baseline based on hand-coded rules expanded with Google-induced rules (Section 4.2), and (iv) a machine learning version that learns from imperfect annotation produced by (ii) (Section 4.3).", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2. Machine learning results using unclosed and closed data", "labels": [], "entities": []}, {"text": " Table 3. Machine Learning from subsamples of the closed data", "labels": [], "entities": []}, {"text": " Table 4. Accuracy of 'Intuition' Derived Baselines", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9942242503166199}]}]}