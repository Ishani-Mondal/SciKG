{"title": [{"text": "Question Answering with Lexical Chains Propagating Verb Arguments", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7779044806957245}]}], "abstractContent": [{"text": "This paper describes an algorithm for propagating verb arguments along lexical chains consisting of WordNet relations.", "labels": [], "entities": []}, {"text": "The algorithm creates verb argument structures using VerbNet syntactic patterns.", "labels": [], "entities": []}, {"text": "In order to increase the coverage , a larger set of verb senses were automatically associated with the existing patterns from VerbNet.", "labels": [], "entities": [{"text": "VerbNet", "start_pos": 126, "end_pos": 133, "type": "DATASET", "confidence": 0.9461252093315125}]}, {"text": "The algorithm is used in an in-house Question Answering system for re-ranking the set of candidate answers.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.7082759737968445}]}, {"text": "Tests on factoid questions from TREC 2004 indicate that the algorithm improved the system performance by 2.4%.", "labels": [], "entities": [{"text": "TREC 2004", "start_pos": 32, "end_pos": 41, "type": "DATASET", "confidence": 0.7901190221309662}]}], "introductionContent": [{"text": "In Question Answering the correct answer can be formulated with different but related words than the question.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 3, "end_pos": 21, "type": "TASK", "confidence": 0.7644822895526886}]}, {"text": "Connecting the words in the question with the words in the candidate answer is not enough to recognize the correct answer.", "labels": [], "entities": []}, {"text": "For example the following question from TREC 2004: Q: (boxer Floyd Patterson) Who did he beat to win the title? has the following wrong answer: WA: He saw Ingemar Johanson knockdown Floyd Patterson seven times therein winning the heavyweight title.", "labels": [], "entities": [{"text": "TREC 2004", "start_pos": 40, "end_pos": 49, "type": "DATASET", "confidence": 0.7512316703796387}, {"text": "WA", "start_pos": 144, "end_pos": 146, "type": "METRIC", "confidence": 0.9929008483886719}]}, {"text": "Although the above sentence contains the words Floyd, Patterson, win, title, and the verb beat can be connected to the verb knockdown using lexical chains from WordNet, this sentence does not answer the question because the verb arguments are in the wrong position.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 160, "end_pos": 167, "type": "DATASET", "confidence": 0.959679365158081}]}, {"text": "The proposed answer describes Floyd Patterson as being the object/patient of the beating event while in the question he is the subject/agent of the similar event.", "labels": [], "entities": []}, {"text": "Therefore the selection of the correct answer from a list of candidate answers requires the check of additional constraints including the match of verb arguments.", "labels": [], "entities": []}, {"text": "Previous approaches to answer ranking, used syntactic partial matching, syntactic and semantic relations and logic forms for selecting the correct answer from a set of candidate answers.) used an algorithm for partial matching of syntactic structures.", "labels": [], "entities": [{"text": "answer ranking", "start_pos": 23, "end_pos": 37, "type": "TASK", "confidence": 0.8999971747398376}, {"text": "syntactic partial matching", "start_pos": 44, "end_pos": 70, "type": "TASK", "confidence": 0.7063935001691183}]}, {"text": "For lexical variations they used a dependency based thesaurus of similar words.) used an algorithm to compute the similarity between dependency relation paths from a parse tree to rank the candidate answers.) used Discourse Representation Structures (DRS) resembling logic forms and semantic relations to represent questions and answers and then computed a score \"indicating how well DRSs match each other\".) transformed the question and the candidate answers into logic forms and used a logic prover to determine if the candidate answer logic form (ALF) entails the question logic form(QLF).", "labels": [], "entities": []}, {"text": "Continuing this work) built a logic prover for Question Answering.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.8168989419937134}]}, {"text": "The logic prover uses a relaxation module that is used iteratively if the proof fails at the price of decreasing the score of the proof.", "labels": [], "entities": []}, {"text": "This logic prover was improved with temporal context detection ().", "labels": [], "entities": [{"text": "temporal context detection", "start_pos": 36, "end_pos": 62, "type": "TASK", "confidence": 0.6198646426200867}]}, {"text": "All these approaches superficially addressed verb lexical variations.", "labels": [], "entities": []}, {"text": "Similar meanings can be expressed using different verbs that use the same arguments in different positions.", "labels": [], "entities": []}, {"text": "For example the sentence: John bought a cowboy hat for $50 can be reformulated as: John paid $50 fora cowboy hat.", "labels": [], "entities": []}, {"text": "The verb buy entails the verb pay however the arguments a cowboy hat and $50 have different position around the verb.", "labels": [], "entities": []}, {"text": "This paper describes the approach for propagating the arguments from one verb to another using lexical chains derived using WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 124, "end_pos": 131, "type": "DATASET", "confidence": 0.9559653997421265}]}, {"text": "The algorithm uses verb argument structures created from VerbNet syntactic patterns.", "labels": [], "entities": []}, {"text": "Section 2 presents VerbNet syntactic patterns and the machine learning approach used to increase the coverage of verb senses.", "labels": [], "entities": []}, {"text": "Section 3 describes the algorithms for propagating verb arguments.", "labels": [], "entities": []}, {"text": "Section 4 presents the results and the final section 5 draws the conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "The algorithm for propagating verb arguments was used to improve performance of an in-house Question Answering system ().", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 92, "end_pos": 110, "type": "TASK", "confidence": 0.6992639452219009}]}, {"text": "This improvement comes from a better matching between a question and the sentences containing the correct answer.", "labels": [], "entities": []}, {"text": "Integration of this algorithm into the Question Answering system requires 3 steps: (1) creation of structures containing verb arguments for the questions and its possible answers, (2) derivation of lexical chains between the two structures and propagation of the arguments along lexical chains, (3) measuring the similarity between the propagated structures and the structures from the question and re-ranking of the candidate answers based on similarity scores.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.8083834648132324}]}, {"text": "Structures containing predicate arguments are created for all the verbs in the question and all verbs in each possible answer.", "labels": [], "entities": []}, {"text": "The QA system takes care of coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 28, "end_pos": 50, "type": "TASK", "confidence": 0.9648653268814087}]}, {"text": "Argument structures are created for verbs in both active and passive voice.", "labels": [], "entities": []}, {"text": "If the verb is in passive voice, then its arguments are normalized to active voice.", "labels": [], "entities": []}, {"text": "The subject phrase of the verb in passive voice represents its object and the noun phrase inside prepositional phrase with preposition \"by\" becomes its subject.", "labels": [], "entities": []}, {"text": "Special attention is given to di-transitive verbs.", "labels": [], "entities": []}, {"text": "If in passive voice, the subject phrase can represent either the direct objector indirect object.", "labels": [], "entities": []}, {"text": "The distinction is made in the following way: if the verb in passive voice has a direct object then the subject represents the indirect object (beneficiary), otherwise the subject represents direct object.", "labels": [], "entities": []}, {"text": "All the other arguments are treated in the same way as in the active voice case.", "labels": [], "entities": []}, {"text": "After the structures are created from a candidate answer and a question, lexical chains are created between their heads.", "labels": [], "entities": []}, {"text": "Because lexical chains link two word senses, the heads need to be disambiguated.", "labels": [], "entities": []}, {"text": "Before searching for lexical chains, the heads could be already partially disambiguated, because only a restricted number of senses of the head verb can have the VerbNet syntactic pattern matching the input text.", "labels": [], "entities": []}, {"text": "An additional semantic disambiguation can take place before deriving lexical chains.", "labels": [], "entities": []}, {"text": "The verbs from the answer and question can also be disambiguated by selecting the best lexical chain between them.", "labels": [], "entities": []}, {"text": "This was the approach used in our experiment.", "labels": [], "entities": []}, {"text": "The algorithm propagating verb arguments was tested on a set of 106 pairs of phrases with similar meaning for which argument structures could be built.", "labels": [], "entities": []}, {"text": "These phrases were selected from pairs of questions and their correct answers from the set of factoid questions in TREC 2004 and also from the pairs of scenarios and hypotheses from first edition of PASCAL RTE Challenge ().", "labels": [], "entities": [{"text": "TREC 2004", "start_pos": 115, "end_pos": 124, "type": "DATASET", "confidence": 0.8394636809825897}, {"text": "PASCAL RTE Challenge", "start_pos": 199, "end_pos": 219, "type": "DATASET", "confidence": 0.6084985534350077}]}, {"text": "The columns in the table correspond to the following cases: a) how many cases the algorithm propagated all the arguments; b) how many cases the algorithm propagated one argument; c) home many cases the algorithm did not propagate any argument; using top 5, 20, 50 lexical chains.", "labels": [], "entities": []}, {"text": "The purpose of the algorithm for propagating predicate arguments is to measure the similarity between the sentences for which the argument structures have been built.", "labels": [], "entities": []}, {"text": "This similarity can be computed by comparing the target argument structure with the propagated argument structure.", "labels": [], "entities": []}, {"text": "The similarity score is computed in the following way: if , except for the subject that has a contribution if matched of 2/(N+1).", "labels": [], "entities": [{"text": "similarity score", "start_pos": 4, "end_pos": 20, "type": "METRIC", "confidence": 0.9824801683425903}]}, {"text": "The propagated pattern is compared with the target pattern and the score is computed by summing up the contributions of all matched arguments.", "labels": [], "entities": []}, {"text": "The set of factoid questions in TREC 2004 has 230 questions.", "labels": [], "entities": [{"text": "TREC 2004", "start_pos": 32, "end_pos": 41, "type": "DATASET", "confidence": 0.7479306161403656}]}, {"text": "Lexical chains containing the restricted set of relations that propagate verb arguments were found for 33 questions, linking verbs in those questions to verbs in their correct answer.", "labels": [], "entities": []}, {"text": "This is the maximum number of questions on which the algorithm for propagating syntactic constraints can have an impact without using other knowledge.", "labels": [], "entities": []}, {"text": "The algorithm for propagating verb argument could be applied on 15 of these questions.", "labels": [], "entities": []}, {"text": "shows the improvement of the Question Answering system when the first 20 or 50 answers returned by factoid strategy are re-ranked according to similarity scores between argument structures.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.768980085849762}]}, {"text": "The performance of the question answering system was measured using Mean Reciprocal Rank (MRR).: The impact of the algorithm for propagating predicate arguments over the question answering system", "labels": [], "entities": [{"text": "question answering", "start_pos": 23, "end_pos": 41, "type": "TASK", "confidence": 0.838102400302887}, {"text": "Mean Reciprocal Rank (MRR).", "start_pos": 68, "end_pos": 95, "type": "METRIC", "confidence": 0.9675011038780212}]}], "tableCaptions": [{"text": " Table 2: Performance of learning verb senses for  several syntactic patterns", "labels": [], "entities": []}, {"text": " Table 4: The weight assigned to each relation", "labels": [], "entities": []}, {"text": " Table 5: Some of the weights assigned to pair of  relations", "labels": [], "entities": []}]}