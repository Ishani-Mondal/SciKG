{"title": [{"text": "Stochastic Iterative Alignment for Machine Translation Evaluation", "labels": [], "entities": [{"text": "Stochastic Iterative Alignment", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.6468874514102936}, {"text": "Machine Translation Evaluation", "start_pos": 35, "end_pos": 65, "type": "TASK", "confidence": 0.854779859383901}]}], "abstractContent": [{"text": "A number of metrics for automatic evaluation of machine translation have been proposed in recent years, with some met-rics focusing on measuring the adequacy of MT output, and other metrics focus-ing on fluency.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.7278479039669037}, {"text": "MT", "start_pos": 161, "end_pos": 163, "type": "TASK", "confidence": 0.9461735486984253}]}, {"text": "Adequacy-oriented met-rics such as BLEU measure n-gram overlap of MT outputs and their references, but do not represent sentence-level information.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9709288477897644}, {"text": "MT outputs", "start_pos": 66, "end_pos": 76, "type": "TASK", "confidence": 0.878050833940506}]}, {"text": "In contrast, fluency-oriented metrics such as ROUGE-W compute longest common subsequences, but ignore words not aligned by the LCS.", "labels": [], "entities": []}, {"text": "We propose a metric based on stochastic iterative string alignment (SIA), which aims to combine the strengths of both approaches.", "labels": [], "entities": [{"text": "stochastic iterative string alignment (SIA)", "start_pos": 29, "end_pos": 72, "type": "TASK", "confidence": 0.731119189943586}]}, {"text": "We compare SIA with existing metrics, and find that it outperforms them in overall evaluation , and works specially well in fluency evaluation.", "labels": [], "entities": [{"text": "SIA", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.9349143505096436}]}], "introductionContent": [{"text": "Evaluation has long been a stumbling block in the development of machine translation systems, due to the simple fact that there are many correct translations fora given sentence.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.7320820093154907}]}, {"text": "Human evaluation of system output is costly in both time and money, leading to the rise of automatic evaluation metrics in recent years.", "labels": [], "entities": []}, {"text": "In the 2003 Johns Hopkins Workshop on Speech and Language Engineering, experiments on MT evaluation showed that BLEU and NIST do not correlate well with human judgments at the sentence level, even when they correlate well overlarge test sets.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 86, "end_pos": 99, "type": "TASK", "confidence": 0.9752535820007324}, {"text": "BLEU", "start_pos": 112, "end_pos": 116, "type": "METRIC", "confidence": 0.9976565837860107}]}, {"text": "also pointed out that due to the limited references for every MT output, using the overlapping ratio of n-grams longer than 2 did not improve sentence level evaluation performance of BLEU.", "labels": [], "entities": [{"text": "MT output", "start_pos": 62, "end_pos": 71, "type": "TASK", "confidence": 0.8685410916805267}, {"text": "BLEU", "start_pos": 183, "end_pos": 187, "type": "METRIC", "confidence": 0.9426670074462891}]}, {"text": "The problem leads to an even worse result in BLEU'S fluency evaluation, which is supposed to rely on the long ngrams.", "labels": [], "entities": [{"text": "BLEU'S fluency evaluation", "start_pos": 45, "end_pos": 70, "type": "METRIC", "confidence": 0.8882286747296652}]}, {"text": "In order to improve sentence-level evaluation performance, several metrics have been proposed, including ROUGE-W, ROUGE-S () and METEOR ().", "labels": [], "entities": [{"text": "ROUGE-W", "start_pos": 105, "end_pos": 112, "type": "METRIC", "confidence": 0.9811040163040161}, {"text": "ROUGE-S", "start_pos": 114, "end_pos": 121, "type": "METRIC", "confidence": 0.9571415781974792}, {"text": "METEOR", "start_pos": 129, "end_pos": 135, "type": "METRIC", "confidence": 0.9244170784950256}]}, {"text": "ROUGE-W differs from BLEU and NIST in that it doesn't require the common sequence between MT output and the references to be consecutive, and thus longer common sequences can be found.", "labels": [], "entities": [{"text": "ROUGE-W", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.6160593628883362}, {"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9822587966918945}, {"text": "NIST", "start_pos": 30, "end_pos": 34, "type": "DATASET", "confidence": 0.922222912311554}]}, {"text": "There is a problem with loose-sequencebased metrics: the words outside the longest common sequence are not considered in the metric, even if they appear both in MT output and the reference.", "labels": [], "entities": []}, {"text": "ROUGE-S is meant to alleviate this problem by computing the common skipped bigrams instead of the LCS.", "labels": [], "entities": [{"text": "ROUGE-S", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.5527684092521667}]}, {"text": "But the price ROUGE-S pays is falling back to the shorter sequences and losing the advantage of long common sequences.", "labels": [], "entities": [{"text": "ROUGE-S", "start_pos": 14, "end_pos": 21, "type": "METRIC", "confidence": 0.8405348062515259}]}, {"text": "METEOR is essentially a unigram based metric, which prefers the monotonic word alignment between MT output and the references by penalizing crossing word alignments.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.5309633612632751}]}, {"text": "There are two problems with METEOR.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.5373871922492981}]}, {"text": "First, it doesn't consider gaps in the aligned words, which is an important feature for evaluating the sentence fluency; second, it cannot use multiple references simultaneously.", "labels": [], "entities": []}, {"text": "1 ROUGE and METEOR both use WordNet and Porter Stemmer to increase the chance of the MT output words matching the reference words.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 2, "end_pos": 7, "type": "METRIC", "confidence": 0.9550005197525024}, {"text": "METEOR", "start_pos": 12, "end_pos": 18, "type": "METRIC", "confidence": 0.9542242288589478}, {"text": "WordNet", "start_pos": 28, "end_pos": 35, "type": "DATASET", "confidence": 0.9720182418823242}, {"text": "MT output words", "start_pos": 85, "end_pos": 100, "type": "TASK", "confidence": 0.8535189429918925}]}, {"text": "Such morphological processing and synonym extraction tools are available for English, but are not always available for other languages.", "labels": [], "entities": [{"text": "synonym extraction", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.93830806016922}]}, {"text": "In order to take advantage of loose-sequence-based metrics and avoid the problems in ROUGE and METEOR, we propose anew metric SIA, which is based on loose sequence alignment but enhanced with the following features: 1 METEOR and ROUGE both compute the score based on the best reference \u2022 Computing the string alignment score based on the gaps in the common sequence.", "labels": [], "entities": []}, {"text": "Though ROUGE-W also takes into consider the gaps in the common sequence between the MT output and the reference by giving more credits to the n-grams in the common sequence, our method is more flexible in that not only do the strict n-grams get more credits, but also the tighter sequences.", "labels": [], "entities": [{"text": "ROUGE-W", "start_pos": 7, "end_pos": 14, "type": "METRIC", "confidence": 0.8909329771995544}]}, {"text": "For the purpose of increasing hitting chance of MT outputs in references, we use a stochastic word matching in the string alignment instead of WORD-STEM and WORD-NET used in METEOR and ROUGE.", "labels": [], "entities": [{"text": "MT outputs", "start_pos": 48, "end_pos": 58, "type": "TASK", "confidence": 0.9197249114513397}]}, {"text": "Instead of using exact matching, we use a soft matching based on the similarity between two words, which is trained in a bilingual corpus.", "labels": [], "entities": []}, {"text": "The corpus is aligned in the word level using IBM.", "labels": [], "entities": [{"text": "IBM", "start_pos": 46, "end_pos": 49, "type": "DATASET", "confidence": 0.843662679195404}]}, {"text": "Stochastic word matching is a uniform replacement for both morphological processing and synonym matching.", "labels": [], "entities": [{"text": "Stochastic word matching", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.6463795304298401}, {"text": "synonym matching", "start_pos": 88, "end_pos": 104, "type": "TASK", "confidence": 0.8577605485916138}]}, {"text": "More importantly, it can be easily adapted for different kinds of languages, as long as there are bilingual parallel corpora available (which is always true for statistical machine translation).", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 161, "end_pos": 192, "type": "TASK", "confidence": 0.6309501230716705}]}, {"text": "In this scheme, the string alignment will be continued until there are no more co-occuring words to be found between the MT output and anyone of the references.", "labels": [], "entities": []}, {"text": "In this way, every co-occuring word between the MT output and the references can be considered and contribute to the final score, and multiple references can be used simultaneously.", "labels": [], "entities": [{"text": "MT", "start_pos": 48, "end_pos": 50, "type": "TASK", "confidence": 0.8160514831542969}]}, {"text": "The remainder of the paper is organized as follows: section 2 gives a recap of BLEU, ROUGE-W and METEOR; section 3 describes the three components of SIA; section 4 compares the performance of different metrics based on experimental results; section 5 presents our conclusion.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.9989720582962036}, {"text": "ROUGE-W", "start_pos": 85, "end_pos": 92, "type": "METRIC", "confidence": 0.9943976402282715}, {"text": "METEOR", "start_pos": 97, "end_pos": 103, "type": "METRIC", "confidence": 0.9873225092887878}, {"text": "SIA", "start_pos": 149, "end_pos": 152, "type": "TASK", "confidence": 0.8963354229927063}]}], "datasetContent": [{"text": "We introduce three techniques to allow more sensitive scores to be computed.", "labels": [], "entities": []}, {"text": "Evaluation experiments were conducted to compare the performance of different metrics including BLEU, ROUGE, METEOR and SIA.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 96, "end_pos": 100, "type": "METRIC", "confidence": 0.9985842704772949}, {"text": "ROUGE", "start_pos": 102, "end_pos": 107, "type": "METRIC", "confidence": 0.9942814111709595}, {"text": "METEOR", "start_pos": 109, "end_pos": 115, "type": "METRIC", "confidence": 0.970109760761261}]}, {"text": "The test data for the experiments are from the MT evaluation workshop at ACL05.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 47, "end_pos": 60, "type": "TASK", "confidence": 0.7803219556808472}, {"text": "ACL05", "start_pos": 73, "end_pos": 78, "type": "DATASET", "confidence": 0.582861602306366}]}, {"text": "There are seven sets of MT outputs (E09 E11 E12 E14 E15 E17 E22), all of which contain 919 English sentences.", "labels": [], "entities": [{"text": "MT", "start_pos": 24, "end_pos": 26, "type": "TASK", "confidence": 0.9526585340499878}, {"text": "E09 E11 E12 E14 E15 E17 E22", "start_pos": 36, "end_pos": 63, "type": "DATASET", "confidence": 0.6842369181769234}]}, {"text": "These sentences are the translation of the same Chinese input generated by seven different MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 91, "end_pos": 93, "type": "TASK", "confidence": 0.9655194878578186}]}, {"text": "The fluency and adequacy of each sentence are manually ranked from 1 to 5.", "labels": [], "entities": []}, {"text": "For each MT output, there are two sets of human scores available, and  we randomly choose one as the score used in the experiments.", "labels": [], "entities": [{"text": "MT", "start_pos": 9, "end_pos": 11, "type": "TASK", "confidence": 0.9834438562393188}]}, {"text": "The human overall scores are calculated as the arithmetic means of the human fluency scores and adequacy scores.", "labels": [], "entities": []}, {"text": "There are four sets of human translations (E01, E02, E03, E04) serving as references for those MT outputs.", "labels": [], "entities": [{"text": "MT outputs", "start_pos": 95, "end_pos": 105, "type": "TASK", "confidence": 0.9111138582229614}]}, {"text": "The MT outputs and reference sentences are transformed to lowercase.", "labels": [], "entities": [{"text": "MT outputs", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.8547248840332031}]}, {"text": "Our experiments are carried out as follows: automatic metrics are used to evaluate the MT outputs based on the four sets of references, and the Pearson's correlation coefficient of the automatic scores and the human scores is computed to see how well they agree.", "labels": [], "entities": [{"text": "MT", "start_pos": 87, "end_pos": 89, "type": "TASK", "confidence": 0.9828699231147766}, {"text": "Pearson's correlation coefficient", "start_pos": 144, "end_pos": 177, "type": "METRIC", "confidence": 0.9164314270019531}]}], "tableCaptions": [{"text": " Table 2: Sentence level evaluation results of  BLEU, ROUGE, METEOR and SIA", "labels": [], "entities": [{"text": "BLEU", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.9985417127609253}, {"text": "ROUGE", "start_pos": 54, "end_pos": 59, "type": "METRIC", "confidence": 0.9911692142486572}, {"text": "METEOR", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9280217885971069}, {"text": "SIA", "start_pos": 72, "end_pos": 75, "type": "TASK", "confidence": 0.3845541775226593}]}, {"text": " Table 1: Sentence level evaluation results of BLEU and ROUGE-W", "labels": [], "entities": [{"text": "BLEU", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.9972684383392334}, {"text": "ROUGE-W", "start_pos": 56, "end_pos": 63, "type": "METRIC", "confidence": 0.9462496042251587}]}, {"text": " Table 3: 95% significance intervals for sentence- level fluency evaluation", "labels": [], "entities": [{"text": "sentence- level fluency evaluation", "start_pos": 41, "end_pos": 75, "type": "TASK", "confidence": 0.6246067523956299}]}, {"text": " Table 4: 95% significance intervals for sentence- level adequacy evaluation", "labels": [], "entities": []}, {"text": " Table 6. SIA uses  the same decay factor as in the sentence-level eval- uation. Its system-level score is computed as the  arithmetic mean of the sentence level scores, and", "labels": [], "entities": []}, {"text": " Table 5: 95% significance intervals for sentence- level overall evaluation", "labels": [], "entities": [{"text": "sentence- level overall evaluation", "start_pos": 41, "end_pos": 75, "type": "TASK", "confidence": 0.661070853471756}]}, {"text": " Table 7: Results of different components in SIA", "labels": [], "entities": [{"text": "SIA", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.9727246165275574}]}, {"text": " Table 8: Results of SIA working with Porter-Stem  and WordNet", "labels": [], "entities": [{"text": "SIA", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.9421230554580688}, {"text": "WordNet", "start_pos": 55, "end_pos": 62, "type": "DATASET", "confidence": 0.7911408543586731}]}, {"text": " Table 6: Results of BLEU, ROUGE, METEOR and SIA in system level evaluation", "labels": [], "entities": [{"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9992271661758423}, {"text": "ROUGE", "start_pos": 27, "end_pos": 32, "type": "METRIC", "confidence": 0.9966849684715271}, {"text": "METEOR", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9714977145195007}, {"text": "SIA", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.7051214575767517}]}]}