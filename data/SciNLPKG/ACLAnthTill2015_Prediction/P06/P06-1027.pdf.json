{"title": [{"text": "Semi-Supervised Conditional Random Fields for Improved Sequence Segmentation and Labeling", "labels": [], "entities": [{"text": "Sequence Segmentation", "start_pos": 55, "end_pos": 76, "type": "TASK", "confidence": 0.7742572128772736}, {"text": "Labeling", "start_pos": 81, "end_pos": 89, "type": "TASK", "confidence": 0.48248767852783203}]}], "abstractContent": [{"text": "We present anew semi-supervised training procedure for conditional random fields (CRFs) that can be used to train sequence segmentors and labelers from a combination of labeled and unlabeled training data.", "labels": [], "entities": []}, {"text": "Our approach is based on extending the minimum entropy regularization framework to the structured prediction case, yielding a training objective that combines unlabeled conditional entropy with labeled conditional likelihood.", "labels": [], "entities": []}, {"text": "Although the training objective is no longer concave, it can still be used to improve an initial model (e.g. obtained from supervised training) by iterative ascent.", "labels": [], "entities": []}, {"text": "We apply our new training algorithm to the problem of identifying gene and protein mentions in biological texts, and show that incorporating unlabeled data improves the performance of the supervised CRF in this case.", "labels": [], "entities": [{"text": "identifying gene and protein mentions in biological texts", "start_pos": 54, "end_pos": 111, "type": "TASK", "confidence": 0.8136742115020752}]}], "introductionContent": [{"text": "Semi-supervised learning is often touted as one of the most natural forms of training for language processing tasks, since unlabeled data is so plentiful whereas labeled data is usually quite limited or expensive to obtain.", "labels": [], "entities": [{"text": "language processing tasks", "start_pos": 90, "end_pos": 115, "type": "TASK", "confidence": 0.7849218646685282}]}, {"text": "The attractiveness of semisupervised learning for language tasks is further heightened by the fact that the models learned are large and complex, and generally even thousands of labeled examples can only sparsely cover the parameter space.", "labels": [], "entities": []}, {"text": "Moreover, in complex structured prediction tasks, such as parsing or sequence modeling (part-of-speech tagging, word segmentation, named entity recognition, and so on), it is considerably more difficult to obtain labeled training data than for classification tasks (such as document classification), since hand-labeling individual words and word boundaries is much harder than assigning text-level class labels.", "labels": [], "entities": [{"text": "parsing or sequence modeling", "start_pos": 58, "end_pos": 86, "type": "TASK", "confidence": 0.6750283688306808}, {"text": "part-of-speech tagging", "start_pos": 88, "end_pos": 110, "type": "TASK", "confidence": 0.7302113473415375}, {"text": "word segmentation, named entity recognition", "start_pos": 112, "end_pos": 155, "type": "TASK", "confidence": 0.6456421911716461}, {"text": "document classification)", "start_pos": 274, "end_pos": 298, "type": "TASK", "confidence": 0.809680163860321}]}, {"text": "Many approaches have been proposed for semisupervised learning in the past, including: generative models, self-learning, cotraining, informationtheoretic regularization (Corduneanu and Jaakkola 2006;, and graphbased transductive methods (;).", "labels": [], "entities": []}, {"text": "Unfortunately, these techniques have been developed primarily for single class label classification problems, or class label classification with a structured input (;).", "labels": [], "entities": [{"text": "single class label classification", "start_pos": 66, "end_pos": 99, "type": "TASK", "confidence": 0.6172979474067688}, {"text": "class label classification", "start_pos": 113, "end_pos": 139, "type": "TASK", "confidence": 0.6374766131242117}]}, {"text": "Although still highly desirable, semi-supervised learning for structured classification problems like sequence segmentation and labeling have not been as widely studied as in the other semi-supervised settings mentioned above, with the sole exception of generative models.", "labels": [], "entities": [{"text": "sequence segmentation", "start_pos": 102, "end_pos": 123, "type": "TASK", "confidence": 0.6911915689706802}]}, {"text": "With generative models, it is natural to include unlabeled data using an expectation-maximization approach.", "labels": [], "entities": []}, {"text": "However, generative models generally do not achieve the same accuracy as discriminatively trained models, and therefore it is preferable to focus on discriminative approaches.", "labels": [], "entities": [{"text": "generative", "start_pos": 9, "end_pos": 19, "type": "TASK", "confidence": 0.9726276397705078}, {"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9982763528823853}]}, {"text": "Unfortunately, it is far from obvious how unlabeled training data can be naturally incorporated into a discriminative training criterion.", "labels": [], "entities": []}, {"text": "For example, unlabeled data simply cancels from the objective if one attempts to use a traditional conditional likelihood criterion.", "labels": [], "entities": []}, {"text": "Nevertheless, recent progress has been made on incorporating unlabeled data in discriminative training procedures.", "labels": [], "entities": []}, {"text": "For example, dependencies can be introduced between the labels of nearby instances and thereby have an effect on training ().", "labels": [], "entities": []}, {"text": "These models are trained to encourage nearby data points to have the same class label, and they can obtain impressive accuracy using a very small amount of labeled data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.9986428618431091}]}, {"text": "However, since they model pairwise similarities among data points, most of these approaches require joint inference over the whole data set attest time, which is not practical for large data sets.", "labels": [], "entities": []}, {"text": "In this paper, we propose anew semi-supervised training method for conditional random fields (CRFs) that incorporates both labeled and unlabeled sequence data to estimate a discriminative structured predictor.", "labels": [], "entities": []}, {"text": "CRFs area flexible and powerful model for structured predictors based on undirected graphical models that have been globally conditioned on a set of input covariates.", "labels": [], "entities": []}, {"text": "CRFs have proved to be particularly useful for sequence segmentation and labeling tasks, since, as conditional models of the labels given inputs, they relax the independence assumptions made by traditional generative models like hidden Markov models.", "labels": [], "entities": [{"text": "sequence segmentation", "start_pos": 47, "end_pos": 68, "type": "TASK", "confidence": 0.7289236634969711}, {"text": "labeling tasks", "start_pos": 73, "end_pos": 87, "type": "TASK", "confidence": 0.8930439949035645}]}, {"text": "As such, CRFs provide additional flexibility for using arbitrary overlapping features of the input sequence to define a structured conditional model over the output sequence, while maintaining two advantages: first, efficient dynamic program can be used for inference in both classification and training, and second, the training objective is concave in the model parameters, which permits global optimization.", "labels": [], "entities": []}, {"text": "To obtain anew semi-supervised training algorithm for CRFs, we extend the minimum entropy regularization framework of to structured predictors.", "labels": [], "entities": []}, {"text": "The resulting objective combines the likelihood of the CRF on labeled training data with its conditional entropy on unlabeled training data.", "labels": [], "entities": []}, {"text": "Unfortunately, the maximization objective is no longer concave, but we can still use it to effectively improve an initial supervised model.", "labels": [], "entities": []}, {"text": "To develop an effective training procedure, we first show how the derivative of the new objective can be computed from the covariance matrix of the features on the unlabeled data (combined with the labeled conditional likelihood).", "labels": [], "entities": []}, {"text": "This relationship facilitates the development of an efficient dynamic programming for computing the gradient, and thereby allows us to perform efficient iterative ascent for training.", "labels": [], "entities": []}, {"text": "We apply our new training technique to the problem of sequence labeling and segmentation, and demonstrate it specifically on the problem of identifying gene and protein mentions in biological texts.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 54, "end_pos": 71, "type": "TASK", "confidence": 0.6829407215118408}, {"text": "identifying gene and protein mentions in biological texts", "start_pos": 140, "end_pos": 197, "type": "TASK", "confidence": 0.8241131901741028}]}, {"text": "Our results show the advantage of semi-supervised learning over the standard supervised algorithm.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Performance of the semi-supervised CRFs obtained on the held-out sets", "labels": [], "entities": []}, {"text": " Table 2: Performance of the semi-supervised CRFs trained by using unlabeled sets", "labels": [], "entities": []}]}