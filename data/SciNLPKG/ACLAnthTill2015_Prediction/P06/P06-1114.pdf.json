{"title": [{"text": "Methods for Using Textual Entailment in Open-Domain Question Answering", "labels": [], "entities": [{"text": "Textual Entailment in Open-Domain Question Answering", "start_pos": 18, "end_pos": 70, "type": "TASK", "confidence": 0.5998362501462301}]}], "abstractContent": [{"text": "Work on the semantics of questions has argued that the relation between a question and its answer(s) can be cast in terms of logical entailment.", "labels": [], "entities": []}, {"text": "In this paper, we demonstrate how computational systems designed to recognize textual entailment can be used to enhance the accuracy of current open-domain automatic question answering (Q/A) systems.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9975729584693909}, {"text": "open-domain automatic question answering (Q/A)", "start_pos": 144, "end_pos": 190, "type": "TASK", "confidence": 0.7978626688321432}]}, {"text": "In our experiments , we show that when textual entail-ment information is used to either filter or rank answers returned by a Q/A system, accuracy can be increased by as much as 20% overall.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 138, "end_pos": 146, "type": "METRIC", "confidence": 0.9988194108009338}]}], "introductionContent": [{"text": "Open-Domain Question Answering (Q/A) systems return a textual expression, identified from avast document collection, as a response to a question asked in natural language.", "labels": [], "entities": [{"text": "Open-Domain Question Answering (Q/A)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7755946107208729}]}, {"text": "In the quest for producing accurate answers, the open-domain Q/A problem has been cast as: (1) a pipeline of linguistic processes pertaining to the processing of questions, relevant passages and candidate answers, interconnected by several types of lexicosemantic feedback (cf. (); (2) a combination of language processes that transform questions and candidate answers in logic representations such that reasoning systems can select the correct answer based on their proofs (cf. (); (3) a noisy-channel model which selects the most likely answer to a question (cf.); or (4) a constraint satisfaction problem, where sets of auxiliary questions are used to provide more information and better constrain the answers to individual questions (cf. ().", "labels": [], "entities": []}, {"text": "While different in their approach, each of these frameworks seeks to approximate the forms of semantic inference that will allow them to identify valid textual answers to natural language questions.", "labels": [], "entities": []}, {"text": "Recently, the task of automatically recognizing one form of semantic inference -textual entailment -has received much attention from groups participating in the PASCAL Recognizing Textual Entailment (RTE) Challenges ( ).", "labels": [], "entities": [{"text": "automatically recognizing one form of semantic inference -textual entailment", "start_pos": 22, "end_pos": 98, "type": "TASK", "confidence": 0.5936325877904892}, {"text": "PASCAL Recognizing Textual Entailment (RTE) Challenges", "start_pos": 161, "end_pos": 215, "type": "TASK", "confidence": 0.7304697297513485}]}, {"text": "As currently defined, the RTE task requires systems to determine whether, given two text fragments, the meaning of one text could be reasonably inferred, or textually entailed, from the meaning of the other text.", "labels": [], "entities": [{"text": "RTE task", "start_pos": 26, "end_pos": 34, "type": "TASK", "confidence": 0.9208079278469086}]}, {"text": "We believe that systems developed specifically for this task can provide current question-answering systems with valuable semantic information that can be leveraged to identify exact answers from ranked lists of candidate answers.", "labels": [], "entities": []}, {"text": "By replacing the pairs of texts evaluated in the RTE Challenge with combinations of questions and candidate answers, we expect that textual entailment could provide yet another mechanism for approximating the types of inference needed in order answer questions accurately.", "labels": [], "entities": [{"text": "RTE Challenge", "start_pos": 49, "end_pos": 62, "type": "TASK", "confidence": 0.5546716302633286}]}, {"text": "In this paper, we present three different methods for incorporating systems for textual entailment into the traditional Q/A architecture employed by many current systems.", "labels": [], "entities": [{"text": "textual entailment", "start_pos": 80, "end_pos": 98, "type": "TASK", "confidence": 0.7007918655872345}]}, {"text": "Our experimental results indicate that (even at their current level of performance) textual entailment systems can substantially improve the accuracy of Q/A, even when no other form of semantic inference is employed.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 141, "end_pos": 149, "type": "METRIC", "confidence": 0.9987921118736267}]}, {"text": "The remainder of the paper is organized as fol-  lows.", "labels": [], "entities": []}, {"text": "Section 2 describes the three methods of using textual entailment in open-domain question answering that we have identified, while Section 3 presents the textual entailment system we have used.", "labels": [], "entities": [{"text": "question answering", "start_pos": 81, "end_pos": 99, "type": "TASK", "confidence": 0.6844239085912704}]}, {"text": "Section 4 details our experimental methods and our evaluation results.", "labels": [], "entities": []}, {"text": "Finally, Section 5 provides a discussion of our findings, and Section 6 summarizes our conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe results from four sets of experiments designed to explore how textual entailment information can be used to enhance the quality of automatic Q/A systems.", "labels": [], "entities": []}, {"text": "We show that by incorporating features from TE into a Q/A system which employs no other form of textual inference, we can improve accuracy by more than 20% over a baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 130, "end_pos": 138, "type": "METRIC", "confidence": 0.9989619255065918}]}, {"text": "We conducted our evaluations on a set of 500 factoid questions selected randomly from questions previously evaluated during the annual TREC Q/A evaluations.", "labels": [], "entities": [{"text": "TREC Q/A evaluations", "start_pos": 135, "end_pos": 155, "type": "TASK", "confidence": 0.649606454372406}]}, {"text": "2 Of these 500 questions, 335 (67.0%) were automatically assigned an answer type from our system's answer type hierarchy ; the remaining 165 (33.0%) questions were classified as having an unknown answer type.", "labels": [], "entities": []}, {"text": "In order to provide a baseline for our experiments, we ran aversion of our Q/A system, known as FERRET (, that does not make use of textual entailment information when identifying answers to questions.", "labels": [], "entities": [{"text": "FERRET", "start_pos": 96, "end_pos": 102, "type": "METRIC", "confidence": 0.9953468441963196}]}, {"text": "Results from this baseline are presented in  The performance of the TE system described in Section 3 was first evaluated in the 2006 PAS-CAL RTE Challenge.", "labels": [], "entities": [{"text": "TE", "start_pos": 68, "end_pos": 70, "type": "TASK", "confidence": 0.7225658893585205}, {"text": "PAS-CAL RTE Challenge", "start_pos": 133, "end_pos": 154, "type": "DATASET", "confidence": 0.6250033378601074}]}, {"text": "In this task, systems were tasked with determining whether the meaning of a sentence (referred to as a hypothesis) could be reasonably inferred from the meaning of another sentence (known as a text).", "labels": [], "entities": []}, {"text": "Four types of sentence pairs were evaluated in the 2006 RTE Challenge, including: pairs derived from the output of (1) automatic question-answering (QA) systems, (2) information extraction systems (IE), (3) information retrieval (IR) systems, and (4) multidocument summarization (SUM) systems.", "labels": [], "entities": [{"text": "RTE Challenge", "start_pos": 56, "end_pos": 69, "type": "TASK", "confidence": 0.514827162027359}, {"text": "information extraction systems (IE)", "start_pos": 166, "end_pos": 201, "type": "TASK", "confidence": 0.7919122775395712}, {"text": "information retrieval (IR)", "start_pos": 207, "end_pos": 233, "type": "TASK", "confidence": 0.8110106468200684}, {"text": "multidocument summarization (SUM)", "start_pos": 251, "end_pos": 284, "type": "TASK", "confidence": 0.798752635717392}]}, {"text": "The accuracy of our TE system across these four tasks is presented in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9995056390762329}, {"text": "TE", "start_pos": 20, "end_pos": 22, "type": "TASK", "confidence": 0.6237554550170898}]}, {"text": "In previous work (), we have found that the type and amount of training data available to our TE system significantly (p < 0.05) impacted its performance on the 2006 RTE Test Set.", "labels": [], "entities": [{"text": "2006 RTE Test Set", "start_pos": 161, "end_pos": 178, "type": "DATASET", "confidence": 0.8075776770710945}]}, {"text": "When our system was trained on the training corpora described in Section 3.3, the overall accuracy of the system increased by more than 10%, from 65.25% to 75.38%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9996926784515381}]}, {"text": "In order to provide training data that replicated the task of recognizing entailment between a question and an answer, we assembled a corpus of 5000 question-answer pairs selected from answers that our baseline Q/A system returned in response to anew set of 1000 questions selected from the TREC test sets.", "labels": [], "entities": [{"text": "recognizing entailment between a question and an answer", "start_pos": 62, "end_pos": 117, "type": "TASK", "confidence": 0.8837244510650635}, {"text": "TREC test sets", "start_pos": 291, "end_pos": 305, "type": "DATASET", "confidence": 0.9177152713139852}]}, {"text": "2500 positive training examples were created from answers identified by human annotators to be correct answers to a question, while 2500 negative examples were created by pairing questions with incorrect answers returned by the Q/A system.", "labels": [], "entities": []}, {"text": "After training our TE system on this corpus, we performed the following four experiments: Method 1.", "labels": [], "entities": []}, {"text": "In the first experiment, the ranked lists of answers produced by the Q/A system were submitted to the TE system for validation.", "labels": [], "entities": [{"text": "TE", "start_pos": 102, "end_pos": 104, "type": "METRIC", "confidence": 0.7402297854423523}]}, {"text": "Under this method, answers that were not entailed by the question were removed from consideration; the top-ranked entailed answer was then returned as the system's answer to the question.", "labels": [], "entities": []}, {"text": "Results from this method are presented in.", "labels": [], "entities": []}, {"text": "In this experiment, entailment information was used to rank passages returned by the PR module.", "labels": [], "entities": []}, {"text": "After an initial relevance ranking was determined from the PR engine, the top 50 passages were paired with the original question and were submitted to the TE system.", "labels": [], "entities": [{"text": "PR", "start_pos": 59, "end_pos": 61, "type": "TASK", "confidence": 0.899002730846405}, {"text": "TE", "start_pos": 155, "end_pos": 157, "type": "METRIC", "confidence": 0.6314444541931152}]}, {"text": "Passages were re-ranked using the entailment judgment and the entailment confidence computed for each pair and then submitted to the AP module.", "labels": [], "entities": [{"text": "AP module", "start_pos": 133, "end_pos": 142, "type": "DATASET", "confidence": 0.8158683180809021}]}, {"text": "Features derived from the entailment confidence were then combined with the keyword-and relation-based features described in () in order to produce a final ranking of candidate answers.", "labels": [], "entities": []}, {"text": "Results from this method are presented in.", "labels": [], "entities": []}, {"text": "In the third experiment, TE was used to select AGQs that were entailed by the question submitted to the Q/A system.", "labels": [], "entities": [{"text": "TE", "start_pos": 25, "end_pos": 27, "type": "METRIC", "confidence": 0.99826580286026}]}, {"text": "Here, AutoQUAB was used to generate questions for the top 50 candidate answers identified by the system.", "labels": [], "entities": []}, {"text": "When at least one of the top 50 AGQs were entailed by the original question, the answer passage associated with the top-ranked entailed question was returned as the answer.", "labels": [], "entities": []}, {"text": "When none of the top 50 AGQs were entailed by the question, questionanswer pairs were re-ranked based on the entailment confidence, and the top-ranked answer was returned.", "labels": [], "entities": []}, {"text": "Results for both of these conditions are presented in.", "labels": [], "entities": []}, {"text": "Finally, we found that the best results could be obtained by combining aspects of each of these three strategies.", "labels": [], "entities": []}, {"text": "Under this approach, candidate answers were initially ranked using features derived from entailment classifications performed between (1) the original question and each candidate answer and (2) the original question and the AGQ generated from each candidate answer.", "labels": [], "entities": []}, {"text": "Once a ranking was established, answers that were not judged to be entailed by the question were also removed from final ranking.", "labels": [], "entities": []}, {"text": "Results from this hybrid method are provided in", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Re-ranking of answers by Method 1.  Method 2.", "labels": [], "entities": [{"text": "Re-ranking of answers", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.7406394680341085}]}, {"text": " Table 4: Performance of Alignment Classifier", "labels": [], "entities": []}, {"text": " Table 7: Q/A Accuracy without TE", "labels": [], "entities": [{"text": "Q/A", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.7026833494504293}, {"text": "Accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.7721098065376282}, {"text": "TE", "start_pos": 31, "end_pos": 33, "type": "METRIC", "confidence": 0.9479995369911194}]}, {"text": " Table 8: Accuracy on the 2006 RTE Test Set", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9990804195404053}, {"text": "2006 RTE Test Set", "start_pos": 26, "end_pos": 43, "type": "DATASET", "confidence": 0.8717345297336578}]}, {"text": " Table 9: Q/A Performance with TE", "labels": [], "entities": [{"text": "A", "start_pos": 12, "end_pos": 13, "type": "METRIC", "confidence": 0.5324263572692871}, {"text": "TE", "start_pos": 31, "end_pos": 33, "type": "METRIC", "confidence": 0.9719165563583374}]}]}