{"title": [{"text": "Unsupervised Topic Identification by Integrating Linguistic and Visual Information Based on Hidden Markov Models", "labels": [], "entities": [{"text": "Topic Identification", "start_pos": 13, "end_pos": 33, "type": "TASK", "confidence": 0.7573496997356415}]}], "abstractContent": [{"text": "This paper presents an unsupervised topic identification method integrating linguistic and visual information based on Hidden Markov Models (HMMs).", "labels": [], "entities": [{"text": "topic identification", "start_pos": 36, "end_pos": 56, "type": "TASK", "confidence": 0.8504548668861389}]}, {"text": "We employ HMMs for topic identification, wherein a state corresponds to a topic and various features including linguistic, visual and audio information are observed.", "labels": [], "entities": [{"text": "topic identification", "start_pos": 19, "end_pos": 39, "type": "TASK", "confidence": 0.8740021884441376}]}, {"text": "Our experiments on two kinds of cooking TV programs show the effectiveness of our proposed method.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recent years have seen the rapid increase of multimedia contents with the continuing advance of information technology.", "labels": [], "entities": []}, {"text": "To make the best use of multimedia contents, it is necessary to segment them into meaningful segments and annotate them.", "labels": [], "entities": []}, {"text": "Because manual annotation is extremely expensive and time consuming, automatic annotation technique is required.", "labels": [], "entities": []}, {"text": "In the field of video analysis, there have been a number of studies on shot analysis for video retrieval or summarization (highlight extraction) using Hidden Markov Models (HMMs) (e.g.,; Q.).", "labels": [], "entities": [{"text": "video analysis", "start_pos": 16, "end_pos": 30, "type": "TASK", "confidence": 0.7709816992282867}, {"text": "summarization (highlight extraction", "start_pos": 108, "end_pos": 143, "type": "TASK", "confidence": 0.6118979677557945}]}, {"text": "These studies first segmented videos into shots, within which the camera motion is continuous, and extracted features such as color histograms and motion vectors.", "labels": [], "entities": []}, {"text": "Then, they classified the shots based on HMMs into several classes (for baseball sports video, for example, pitch view, running overview or audience view).", "labels": [], "entities": []}, {"text": "In these studies, to achieve high accuracy, they relied on handmade domain-specific knowledge or trained HMMs with manually labeled data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9934822916984558}]}, {"text": "Therefore, they cannot be easily extended to new domains on a large scale.", "labels": [], "entities": []}, {"text": "In addition, although linguistic information, such as narration, speech of characters, and commentary, is intuitively useful for shot analysis, it is not utilized by many of the previous studies.", "labels": [], "entities": [{"text": "shot analysis", "start_pos": 129, "end_pos": 142, "type": "TASK", "confidence": 0.8505940139293671}]}, {"text": "Although some studies attempted to utilize linguistic information (;, it was just keywords.", "labels": [], "entities": []}, {"text": "In the field of Natural Language Processing, Barzilay and Lee have recently proposed a probabilistic content model for representing topics and topic shifts ().", "labels": [], "entities": []}, {"text": "This content model is based on HMMs wherein a state corresponds to a topic and generates sentences relevant to that topic according to a state-specific language model, which are learned from raw texts via analysis of word distribution patterns.", "labels": [], "entities": []}, {"text": "In this paper, we describe an unsupervised topic identification method integrating linguistic and visual information using HMMs.", "labels": [], "entities": [{"text": "topic identification", "start_pos": 43, "end_pos": 63, "type": "TASK", "confidence": 0.8594479560852051}]}, {"text": "Among several types of videos, in which instruction videos (howto videos) about sports, cooking, D.I.Y., and others are the most valuable, we focus on cooking TV programs.", "labels": [], "entities": []}, {"text": "In an example shown in, preparation, sauteing, and dishing up are automatically labeled in sequence.", "labels": [], "entities": []}, {"text": "Identified topics lead to video segmentation and can be utilized for video summarization.", "labels": [], "entities": [{"text": "video segmentation", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.6969040483236313}, {"text": "video summarization", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.643547534942627}]}, {"text": "Inspired by Barzilay's work, we employ HMMs for topic identification, wherein a state corresponds to a topic, like preparation and frying, and various features, which include visual and audio information as well as linguistic information (instructor's utterances), are observed.", "labels": [], "entities": [{"text": "topic identification", "start_pos": 48, "end_pos": 68, "type": "TASK", "confidence": 0.736018106341362}]}, {"text": "This study considers a clause as an unit of analysis and the following eight topics as a set of states: preparation, sauteing, frying, baking, simmering, boiling, dishing up, steaming.", "labels": [], "entities": []}, {"text": "word distribution can be learned from raw texts, their model cannot utilize discourse features, such as cue phrases and lexical chains.", "labels": [], "entities": [{"text": "word distribution", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7441391348838806}]}, {"text": "We incorporate domain-independent discourse features such as cue phrases, noun/verb chaining, which indicate topic change/persistence, into the domain-specific word distribution.", "labels": [], "entities": [{"text": "noun/verb chaining", "start_pos": 74, "end_pos": 92, "type": "TASK", "confidence": 0.6549757570028305}]}, {"text": "Our main claim is that we utilize visual and audio information to achieve robust topic identification.", "labels": [], "entities": [{"text": "topic identification", "start_pos": 81, "end_pos": 101, "type": "TASK", "confidence": 0.8178061842918396}]}, {"text": "As for visual information, we can utilize background color distribution of the image.", "labels": [], "entities": []}, {"text": "For example, frying and boiling are usually performed on a gas range and preparation and dishing up are usually performed on a cutting board.", "labels": [], "entities": []}, {"text": "This information can bean aid to topic identification.", "labels": [], "entities": [{"text": "topic identification", "start_pos": 33, "end_pos": 53, "type": "TASK", "confidence": 0.9280471205711365}]}, {"text": "As for audio information, silence can be utilized as a clue to a topic shift.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted the experiment of the topic identification.", "labels": [], "entities": [{"text": "topic identification", "start_pos": 35, "end_pos": 55, "type": "TASK", "confidence": 0.8786317110061646}]}, {"text": "We first trained HMM parameters for each program, and then applied the trained model to five videos each, in which, we manually assigned appropriate topics to clauses.", "labels": [], "entities": []}, {"text": "The unit of evaluation was a clause.", "labels": [], "entities": []}, {"text": "The accuracy was improved by integrating linguistic and visual information compared to using linguistic / visual information alone.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9994916915893555}]}, {"text": "(Note that \"visual information\" uses pseudo-labeled data.)", "labels": [], "entities": []}, {"text": "In addition, the accuracy was improved by using various discourse features.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9993841648101807}]}, {"text": "The reason why silence did not contribute to accuracy improvement is supposed to be that closed captions and video streams were not synchronized precisely due to time lagging of closed captions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9985505938529968}]}, {"text": "To deal with this problem, an automatic closed caption alignment technique () will be applied or automatic speech recognition will be used as texts instead of closed captions with the advance of speech recognition technology.", "labels": [], "entities": [{"text": "closed caption alignment", "start_pos": 40, "end_pos": 64, "type": "TASK", "confidence": 0.651748408873876}, {"text": "speech recognition", "start_pos": 107, "end_pos": 125, "type": "TASK", "confidence": 0.7354215979576111}, {"text": "speech recognition", "start_pos": 195, "end_pos": 213, "type": "TASK", "confidence": 0.7092609703540802}]}, {"text": "illustrates an improved example by adding visual information.", "labels": [], "entities": []}, {"text": "In the case of using only linguistic information, this topic was rec- ognized as sauteing, but this topic was actually preparation, which referred to the next topic.", "labels": [], "entities": []}, {"text": "By using the visual information that background color was white, this topic was correctly recognized as preparation.", "labels": [], "entities": []}, {"text": "We conducted another experiment to demonstrate the validity of several linguistic processes, such as utterance-type recognition and word sense disambiguation with case frames, for extracting linguistic information from closed captions described in Section 3.1.1.", "labels": [], "entities": [{"text": "utterance-type recognition", "start_pos": 101, "end_pos": 127, "type": "TASK", "confidence": 0.7339513450860977}, {"text": "word sense disambiguation", "start_pos": 132, "end_pos": 157, "type": "TASK", "confidence": 0.6276177068551382}, {"text": "extracting linguistic information from closed captions", "start_pos": 180, "end_pos": 234, "type": "TASK", "confidence": 0.7962779800097147}]}, {"text": "We compared our method to three methods: a method that does not perform word sense disambiguation with case frames (w/o cf), a method that does not perform utterancetype recognition for extracting actions (uses all utterance-type texts) (w/o utype), a method, in which a sentence is emitted according to a statespecific language model (bigram) as Barzilay and Lee adopted (bigram).", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 72, "end_pos": 97, "type": "TASK", "confidence": 0.6751230657100677}, {"text": "utterancetype recognition", "start_pos": 156, "end_pos": 181, "type": "TASK", "confidence": 0.7200711518526077}]}, {"text": "gives the experimental result, which demonstrates our method is appropriate.", "labels": [], "entities": []}, {"text": "One cause of errors in topic identification is that some case frames are incorrectly constructed.", "labels": [], "entities": [{"text": "topic identification", "start_pos": 23, "end_pos": 43, "type": "TASK", "confidence": 0.9684892892837524}]}, {"text": "For example, kiru:1 (cut) contains \" (cut a vegetable)\" and \" (drain oil)\".", "labels": [], "entities": []}, {"text": "This leads to incorrect parameter training.", "labels": [], "entities": []}, {"text": "Other cause is that some verbs are assigned to an inaccurate case frame by the failure of case analysis.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 5: Experimental result of topic identification.", "labels": [], "entities": [{"text": "topic identification", "start_pos": 33, "end_pos": 53, "type": "TASK", "confidence": 0.9240391254425049}]}, {"text": " Table 4: Characteristics of the two cooking pro- grams we used for our experiments.", "labels": [], "entities": []}, {"text": " Table 6: Results of the experiment that compares our method to three methods.", "labels": [], "entities": []}]}