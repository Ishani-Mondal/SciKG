{"title": [{"text": "MT Evaluation: Human-like vs. Human Acceptable", "labels": [], "entities": [{"text": "MT Evaluation", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.7293381690979004}, {"text": "Acceptable", "start_pos": 36, "end_pos": 46, "type": "METRIC", "confidence": 0.8700779676437378}]}], "abstractContent": [{"text": "We present a comparative study on Machine Translation Evaluation according to two different criteria: Human Likeness and Human Acceptability.", "labels": [], "entities": [{"text": "Machine Translation Evaluation", "start_pos": 34, "end_pos": 64, "type": "TASK", "confidence": 0.9075288573900858}, {"text": "Acceptability", "start_pos": 127, "end_pos": 140, "type": "METRIC", "confidence": 0.9199987053871155}]}, {"text": "We provide empirical evidence that there is a relationship between these two kinds of evaluation: Human Likeness implies Human Acceptability but the reverse is not true.", "labels": [], "entities": [{"text": "Acceptability", "start_pos": 127, "end_pos": 140, "type": "METRIC", "confidence": 0.916834831237793}]}, {"text": "From the point of view of automatic evaluation this implies that metrics based on Human Likeness are more reliable for system tuning.", "labels": [], "entities": [{"text": "system tuning", "start_pos": 119, "end_pos": 132, "type": "TASK", "confidence": 0.7777446508407593}]}, {"text": "Our results also show that current evaluation metrics are not always able to distinguish between automatic and human translations.", "labels": [], "entities": []}, {"text": "In order to improve the descriptive power of current metrics we propose the use of additional syntax-based met-rics, and metric combinations inside the QARLA Framework.", "labels": [], "entities": [{"text": "QARLA Framework", "start_pos": 152, "end_pos": 167, "type": "DATASET", "confidence": 0.932557225227356}]}], "introductionContent": [{"text": "Current approaches to Automatic Machine Translation (MT) Evaluation are mostly based on metrics which determine the quality of a given translation according to its similarity to a given set of reference translations.", "labels": [], "entities": [{"text": "Automatic Machine Translation (MT) Evaluation", "start_pos": 22, "end_pos": 67, "type": "TASK", "confidence": 0.8265688376767295}]}, {"text": "The commonly accepted criterion that defines the quality of an evaluation metric is its level of correlation with human evaluators.", "labels": [], "entities": []}, {"text": "High levels of correlation (Pearson over 0.9) have been attained at the system level (.", "labels": [], "entities": [{"text": "correlation", "start_pos": 15, "end_pos": 26, "type": "METRIC", "confidence": 0.9975836277008057}, {"text": "Pearson", "start_pos": 28, "end_pos": 35, "type": "METRIC", "confidence": 0.9762340188026428}]}, {"text": "But this is an average effect: the degree of correlation achieved at the sentence level, crucial for an accurate error analysis, is much lower.", "labels": [], "entities": [{"text": "correlation", "start_pos": 45, "end_pos": 56, "type": "METRIC", "confidence": 0.9472565054893494}]}, {"text": "We argue that there is two main reasons that explain this fact: Firstly, current MT evaluation metrics are based on shallow features.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 81, "end_pos": 94, "type": "TASK", "confidence": 0.9124340116977692}]}, {"text": "Most metrics work only at the lexical level.", "labels": [], "entities": []}, {"text": "However, natural languages are rich and ambiguous, allowing for many possible different ways of expressing the same idea.", "labels": [], "entities": []}, {"text": "In order to capture this flexibility, these metrics would require a combinatorial number of reference translations, when indeed inmost cases only a single reference is available.", "labels": [], "entities": []}, {"text": "Therefore, metrics with higher descriptive power are required.", "labels": [], "entities": []}, {"text": "Secondly, there exists, indeed, two different evaluation criteria: (i) Human Acceptability, i.e., to what extent an automatic translation could be considered acceptable by humans; and (ii) Human Likeness, i.e., to what extent an automatic translation could have been generated by a human translator.", "labels": [], "entities": [{"text": "Acceptability", "start_pos": 77, "end_pos": 90, "type": "METRIC", "confidence": 0.8911870121955872}]}, {"text": "Most approaches to automatic MT evaluation implicitly assume that both criteria should lead to the same results; but this assumption has not been proved empirically or even discussed.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 29, "end_pos": 42, "type": "TASK", "confidence": 0.9680642783641815}]}, {"text": "In this work, we analyze this issue through empirical evidence.", "labels": [], "entities": []}, {"text": "First, in Section 2, we investigate to what extent current evaluation metrics are able to distinguish between human and automatic translations (Human Likeness).", "labels": [], "entities": []}, {"text": "As individual metrics do not capture such distinction well, in Section 3 we study how to improve the descriptive power of current metrics by means of metric combinations inside the QARLA Framework (), including anew family of metrics based on syntactic criteria.", "labels": [], "entities": [{"text": "QARLA Framework", "start_pos": 181, "end_pos": 196, "type": "DATASET", "confidence": 0.9404605627059937}]}, {"text": "Second, we claim that the two evaluation criteria (Human Acceptability and Human Likeness) are indeed of a different nature, and may lead to different results (Section 4).", "labels": [], "entities": [{"text": "Acceptability", "start_pos": 57, "end_pos": 70, "type": "METRIC", "confidence": 0.8419601321220398}]}, {"text": "However, translations exhibiting a high level of Human Likeness obtain good results inhuman judges.", "labels": [], "entities": []}, {"text": "Therefore, automatic evaluation metrics based on similarity to references should be optimized \u00a4 over their capacity to represent Human Likeness.", "labels": [], "entities": []}, {"text": "See conclusions in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "The ideal test set to study this dichotomy inside the QARLA Framework would consist of a large number of human references per sentence, and automatic outputs generated by heterogeneous MT systems.", "labels": [], "entities": [{"text": "QARLA Framework", "start_pos": 54, "end_pos": 69, "type": "DATASET", "confidence": 0.9293250143527985}]}, {"text": "Our main assumption is that if an evaluation metric is able to characterize human translations, then, human references should be closer to each other than automatic translations to other human references.", "labels": [], "entities": [{"text": "characterize human translations", "start_pos": 63, "end_pos": 94, "type": "TASK", "confidence": 0.8687379360198975}]}, {"text": "Based on this assumption we introduce two measures which analyze the descriptive power of evaluation metrics from diferent points of view.", "labels": [], "entities": []}], "tableCaptions": []}