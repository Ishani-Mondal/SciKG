{"title": [{"text": "Hybrid Parsing: Using Probabilistic Models as Predictors fora Symbolic Parser", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper we investigate the benefit of stochastic predictor components for the parsing quality which can be obtained with a rule-based dependency grammar.", "labels": [], "entities": [{"text": "parsing", "start_pos": 84, "end_pos": 91, "type": "TASK", "confidence": 0.9667330980300903}]}, {"text": "By including a chunker, a supertagger, a PP at-tacher, and a fast probabilistic parser we were able to improve upon the baseline by 3.2%, bringing the overall labelled accuracy to 91.1% on the German NEGRA corpus.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 168, "end_pos": 176, "type": "METRIC", "confidence": 0.8146147727966309}, {"text": "German NEGRA corpus", "start_pos": 193, "end_pos": 212, "type": "DATASET", "confidence": 0.8675075769424438}]}, {"text": "We attribute the successful integration to the ability of the underlying grammar model to combine uncertain evidence in a soft manner, thus avoiding the problem of error propagation.", "labels": [], "entities": [{"text": "error propagation", "start_pos": 164, "end_pos": 181, "type": "TASK", "confidence": 0.7167639285326004}]}], "introductionContent": [{"text": "There seems to bean upper limit for the level of quality that can be achieved by a parser if it is confined to information drawn from a single source.", "labels": [], "entities": []}, {"text": "Stochastic parsers for English trained on the Penn Treebank have peaked their performance around 90%).", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 46, "end_pos": 59, "type": "DATASET", "confidence": 0.9864416122436523}]}, {"text": "Parsing of German seems to be even harder and parsers trained on the NEGRA corpus or an enriched version of it still perform considerably worse.", "labels": [], "entities": [{"text": "Parsing of German", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9058598081270853}, {"text": "NEGRA corpus", "start_pos": 69, "end_pos": 81, "type": "DATASET", "confidence": 0.9400895535945892}]}, {"text": "On the other hand, a great number of shallow components like taggers, chunkers, supertaggers, as well as general or specialized attachment predictors have been developed that might provide additional information to further improve the quality of a parser's output, as long as their contributions are in some sense complementory.", "labels": [], "entities": []}, {"text": "Despite these prospects, such possibilities have rarely been investigated so far.", "labels": [], "entities": []}, {"text": "To estimate the degree to which the desired synergy between heterogeneous knowledge sources can be achieved, we have established an experimental framework for syntactic analysis which allows us to plugin a wide variety of external predictor components, and to integrate their contributions as additional evidence in the general decision-making on the optimal structural interpretation.", "labels": [], "entities": [{"text": "syntactic analysis", "start_pos": 159, "end_pos": 177, "type": "TASK", "confidence": 0.7670570015907288}]}, {"text": "We refer to this approach as hybrid parsing because it combines different kinds of linguistic models, which have been acquired in totally different ways, ranging from manually compiled rule sets to statistically trained components.", "labels": [], "entities": [{"text": "hybrid parsing", "start_pos": 29, "end_pos": 43, "type": "TASK", "confidence": 0.6620862483978271}]}, {"text": "In this paper we investigate the benefit of external predictor components for the parsing quality which can be obtained with a rule-based grammar.", "labels": [], "entities": [{"text": "parsing", "start_pos": 82, "end_pos": 89, "type": "TASK", "confidence": 0.9644431471824646}]}, {"text": "For that purpose we trained a range of predictor components and integrated their output into the parser by means of soft constraints.", "labels": [], "entities": []}, {"text": "Accordingly, the goal of our research was not to extensively optimize the predictor components themselves, but to quantify their contribution to the overall parsing quality.", "labels": [], "entities": []}, {"text": "The results of these experiments not only lead to a better understanding of the utility of the different knowledge sources, but also allow us to derive empirically based priorities for further improving them.", "labels": [], "entities": []}, {"text": "We are able to show that the potential of WCDG for information fusion is strong enough to accomodate even rather unreliable information from a wide range of predictor components.", "labels": [], "entities": [{"text": "information fusion", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.8595950901508331}]}, {"text": "Using this potential we were able to reach a quality level for dependency parsing German which is unprecendented so far.", "labels": [], "entities": [{"text": "dependency parsing German", "start_pos": 63, "end_pos": 88, "type": "TASK", "confidence": 0.8854257067044576}]}], "datasetContent": [{"text": "Since the WCDG parser never fails on typical treebank sentences, and always delivers an analysis that contains exactly one subordination for each word, the common measures of precision, recall and f-score all coincide; all three are summarized as accuracy here.", "labels": [], "entities": [{"text": "precision", "start_pos": 175, "end_pos": 184, "type": "METRIC", "confidence": 0.9995139837265015}, {"text": "recall", "start_pos": 186, "end_pos": 192, "type": "METRIC", "confidence": 0.9990441203117371}, {"text": "f-score", "start_pos": 197, "end_pos": 204, "type": "METRIC", "confidence": 0.9335885643959045}, {"text": "accuracy", "start_pos": 247, "end_pos": 255, "type": "METRIC", "confidence": 0.9991687536239624}]}, {"text": "We measure the structural (i.e. unlabelled) accuracy as the ratio of correctly attached words to all words; the labelled accuracy counts only those words that have the correct regent and also bear the correct label.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9930633902549744}, {"text": "accuracy", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.5056599378585815}]}, {"text": "For comparison with previous work, we used the next-to-last 1,000 sentences of the NEGRA corpus as our test set.", "labels": [], "entities": [{"text": "NEGRA corpus", "start_pos": 83, "end_pos": 95, "type": "DATASET", "confidence": 0.9054677188396454}]}, {"text": "The gold standard used for evaluation was derived from the annotations of the NEGRA treebank (version 2.0) in a semi-automatic procedure.", "labels": [], "entities": [{"text": "NEGRA treebank", "start_pos": 78, "end_pos": 92, "type": "DATASET", "confidence": 0.9697272479534149}]}, {"text": "First, the NEGRA phrase structures were automatically transformed to dependency trees with the DEPSY tool ().", "labels": [], "entities": [{"text": "DEPSY", "start_pos": 95, "end_pos": 100, "type": "DATASET", "confidence": 0.7736321687698364}]}, {"text": "However, before the parsing experiments, the results were manually corrected to (1) take care of systematic inconsistencies between the NEGRA annotations and the WCDG annotations (e.g. for nonprojectivities, which in our case are used only if necessary for an ambiguity free attachment of verbal arguments, relative clauses and coordinations, but not for other types of adjuncts) and (2) to remove inconsistencies with NEGRAs own annotation guidelines (e.g. with regard to elliptical and co-ordinated structures, adverbs and subordinated main clauses.)", "labels": [], "entities": []}, {"text": "To illustrate the consequences of these corrections we report in both kinds of results: those obtained on our WCDG-conform annotations (reannotated) and the others on the raw output of the automatic conversion (trans-formed), although the latter ones introduce a systematic mismatch between the gold standard and the design principles of the grammar.", "labels": [], "entities": []}, {"text": "The experiments 2-5 show the effect of adding the POS tagger and one of the other predictor components to the parser.", "labels": [], "entities": []}, {"text": "The chunk parser yields only a slight improvement of about 0.5% accuracy; this is most probably because the baseline parser (line 1) does not make very many mistakes at this level anyway.", "labels": [], "entities": [{"text": "chunk parser", "start_pos": 4, "end_pos": 16, "type": "TASK", "confidence": 0.6603295207023621}, {"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9991877675056458}]}, {"text": "For instance, the relation type with the highest error rate is prepositional attachment, about which the chunk parser makes no predictions at all.", "labels": [], "entities": [{"text": "error rate", "start_pos": 49, "end_pos": 59, "type": "METRIC", "confidence": 0.9290378391742706}]}, {"text": "In fact, the benefit of the PP component alone (line 3) is much larger even though it predicts only the regents of prepositions.", "labels": [], "entities": []}, {"text": "The two other components make predictions about all types of relations, and yield even bigger benefits.", "labels": [], "entities": []}, {"text": "When more than one other predictor is added to the grammar, the beneft is generally higher than that of either alone, but smaller than the sum of both.", "labels": [], "entities": []}, {"text": "An exception is seen inline 8, where the combination of POS tagging, supertagging and PP prediction fails to better the results of just POS tagging and supertagging (line 4).", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 56, "end_pos": 67, "type": "TASK", "confidence": 0.5830439925193787}, {"text": "PP prediction", "start_pos": 86, "end_pos": 99, "type": "TASK", "confidence": 0.676752895116806}, {"text": "POS tagging", "start_pos": 136, "end_pos": 147, "type": "TASK", "confidence": 0.6162205338478088}]}, {"text": "Individual inspection of the results suggests that the lexicalized information of the PP attacher is often counteracted by the less informed predictions of the supertagger (this was confirmed in preliminary experiments by again inaccuracy when prepositions were exempted from the supertag constraint).", "labels": [], "entities": []}, {"text": "Finally, combining all five predictors results in the highest accuracy of all, improving over the first experiment by 2.8% and 3.2% for structural and labelled accuracy respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9993273019790649}, {"text": "accuracy", "start_pos": 160, "end_pos": 168, "type": "METRIC", "confidence": 0.8711722493171692}]}, {"text": "We see that the introduction of stochastical information into the handwritten language model is generally helpful, although the different predictors contribute different types of information.", "labels": [], "entities": []}, {"text": "The POS tagger and PP attacher capture lexicalized regularities which are genuinely new to the grammar: in effect, they refine the language model of the grammar in places that would be tedious to describe through individual rules.", "labels": [], "entities": []}, {"text": "In contrast, the more global components tend to make the same predictions as the WCDG itself, only explicitly.", "labels": [], "entities": []}, {"text": "This guides the parser so that it tends to check the correct alternative first more often, and has a greater chance of finding the global optimum.", "labels": [], "entities": []}, {"text": "This explains why their addition increases parsing accuracy even when their own accuracy is markedly lower than even the baseline (line 1).", "labels": [], "entities": [{"text": "parsing", "start_pos": 43, "end_pos": 50, "type": "TASK", "confidence": 0.978906512260437}, {"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.955202043056488}, {"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9981358051300049}]}], "tableCaptions": [{"text": " Table 1: Structural/labelled parsing accuracy with  various predictor components.", "labels": [], "entities": [{"text": "Structural/labelled parsing", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.8213684856891632}, {"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.898602306842804}]}]}