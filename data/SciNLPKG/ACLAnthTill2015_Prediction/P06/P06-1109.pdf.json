{"title": [{"text": "An All-Subtrees Approach to Unsupervised Parsing", "labels": [], "entities": [{"text": "Parsing", "start_pos": 41, "end_pos": 48, "type": "TASK", "confidence": 0.40843555331230164}]}], "abstractContent": [{"text": "We investigate generalizations of the all-subtrees \"DOP\" approach to unsupervised parsing.", "labels": [], "entities": []}, {"text": "Unsupervised DOP models assign all possible binary trees to a set of sentences and next use (a large random subset of) all subtrees from these binary trees to compute the most probable parse trees.", "labels": [], "entities": []}, {"text": "We will test both a relative frequency estimator for unsupervised DOP and a maximum likelihood estimator which is known to be statistically consistent.", "labels": [], "entities": [{"text": "maximum likelihood estimator", "start_pos": 76, "end_pos": 104, "type": "METRIC", "confidence": 0.7856654524803162}]}, {"text": "We report state-of-the-art results on English (WSJ), German (NEGRA) and Chinese (CTB) data.", "labels": [], "entities": [{"text": "English (WSJ), German (NEGRA) and Chinese (CTB) data", "start_pos": 38, "end_pos": 90, "type": "DATASET", "confidence": 0.5918322622776031}]}, {"text": "To the best of our knowledge this is the first paper which tests a maximum likelihood estimator for DOP on the Wall Street Journal, leading to the surprising result that an unsupervised parsing model beats a widely used supervised model (a treebank PCFG).", "labels": [], "entities": [{"text": "Wall Street Journal", "start_pos": 111, "end_pos": 130, "type": "DATASET", "confidence": 0.9410685896873474}]}], "introductionContent": [{"text": "The problem of bootstrapping syntactic structure from unlabeled data has regained considerable interest.", "labels": [], "entities": [{"text": "bootstrapping syntactic structure", "start_pos": 15, "end_pos": 48, "type": "TASK", "confidence": 0.8664021293322245}]}, {"text": "While supervised parsers suffer from shortage of hand-annotated data, unsupervised parsers operate with unlabeled raw data of which unlimited quantities are available.", "labels": [], "entities": []}, {"text": "During the last few years there has been steady progress in the field.", "labels": [], "entities": []}, {"text": "Where van Zaanen (2000) achieved 39.2% unlabeled f-score on ATIS word strings, Clark (2001) reports 42.0% on the same data, and obtain 51.2% f-score on ATIS part-of-speech strings using a constituent-context model called CCM.", "labels": [], "entities": []}, {"text": "On Penn Wall Street Journal po-s-strings \u2264 10 (WSJ10), report 71.1% unlabeled f-score with CCM.", "labels": [], "entities": [{"text": "Penn Wall Street Journal", "start_pos": 3, "end_pos": 27, "type": "DATASET", "confidence": 0.9643650501966476}, {"text": "WSJ10", "start_pos": 47, "end_pos": 52, "type": "DATASET", "confidence": 0.7477678060531616}]}, {"text": "And the hybrid approach of, which combines constituency and dependency models, yields 77.6% f-score.", "labels": [], "entities": [{"text": "f-score", "start_pos": 92, "end_pos": 99, "type": "METRIC", "confidence": 0.9842878580093384}]}, {"text": "shows that a further improvement on the WSJ10 can be achieved by an unsupervised generalization of the all-subtrees approach known as.", "labels": [], "entities": [{"text": "WSJ10", "start_pos": 40, "end_pos": 45, "type": "DATASET", "confidence": 0.9397175908088684}]}, {"text": "This unsupervised DOP model, coined U-DOP, first assigns all possible unlabeled binary trees to a set of sentences and next uses all subtrees from (a large subset of) these trees to compute the most probable parse trees.", "labels": [], "entities": []}, {"text": "reports that U-DOP not only outperforms previous unsupervised parsers but that its performance is as good as a binarized supervised parser (i.e. a treebank PCFG) on the WSJ.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 169, "end_pos": 172, "type": "DATASET", "confidence": 0.9713935852050781}]}, {"text": "A possible drawback of U-DOP, however, is the statistical inconsistency of its estimator) which is inherited from the DOP1 model.", "labels": [], "entities": []}, {"text": "That is, even with unlimited training data, U-DOP's estimator is not guaranteed to converge to the correct weight distribution.", "labels": [], "entities": []}, {"text": "argues in favor of a maximum likelihood estimator for DOP which is statistically consistent.", "labels": [], "entities": [{"text": "DOP", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.7137019038200378}]}, {"text": "As it happens, in Bod (2000) we already developed such a DOP model, termed ML-DOP, which reestimates the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization.", "labels": [], "entities": [{"text": "Bod (2000", "start_pos": 18, "end_pos": 27, "type": "DATASET", "confidence": 0.8935353755950928}]}, {"text": "Although crossvalidation is needed to avoid overlearning, ML-DOP outperforms DOP1 on the OVIS corpus.", "labels": [], "entities": [{"text": "OVIS corpus", "start_pos": 89, "end_pos": 100, "type": "DATASET", "confidence": 0.9312401115894318}]}, {"text": "This raises the question whether we can create an unsupervised DOP model which is also statistically consistent.", "labels": [], "entities": []}, {"text": "In this paper we will show that an unsupervised version of ML-DOP can be constructed along the lines of U-DOP.", "labels": [], "entities": []}, {"text": "We will start out by summarizing DOP, U-DOP and ML-DOP, and next create anew unsupervised model called UML-DOP.", "labels": [], "entities": []}, {"text": "We report that UML-DOP not only obtains higher parse accuracy than U-DOP on three different domains, but that it also achieves this with fewer subtrees than U-DOP.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.8403895497322083}]}, {"text": "To the best of our knowledge, this paper presents the first unsupervised parser that outperforms a widely used supervised parser on the WSJ, i.e. a treebank PCFG.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 136, "end_pos": 139, "type": "DATASET", "confidence": 0.9555025100708008}]}, {"text": "We will raise the question whether the end of supervised parsing is insight.", "labels": [], "entities": []}], "datasetContent": [{"text": "To compare UML-DOP to U-DOP, we started outwith the WSJ10 corpus, which contains 7422 sentences \u2264 10 words after removing empty elements and punctuation.", "labels": [], "entities": [{"text": "WSJ10 corpus", "start_pos": 52, "end_pos": 64, "type": "DATASET", "confidence": 0.9867499768733978}]}, {"text": "We used the same evaluation metrics for unlabeled precision (UP) and unlabeled recall (UR) as defined in).", "labels": [], "entities": [{"text": "precision (UP)", "start_pos": 50, "end_pos": 64, "type": "METRIC", "confidence": 0.9188960939645767}, {"text": "recall (UR)", "start_pos": 79, "end_pos": 90, "type": "METRIC", "confidence": 0.9606003314256668}]}, {"text": "Klein's definitions differ slightly from the standard PARSEVAL metrics: multiplicity of brackets is ignored, brackets of span one are ignored and the bracket labels are ignored.", "labels": [], "entities": []}, {"text": "The two metrics of UP and UR are combined by the unlabeled fscore F1 which is defined as the harmonic mean of UP and UR: F1 = 2*UP*UR/(UP+UR).", "labels": [], "entities": [{"text": "F1", "start_pos": 66, "end_pos": 68, "type": "METRIC", "confidence": 0.5245740413665771}, {"text": "F1", "start_pos": 121, "end_pos": 123, "type": "METRIC", "confidence": 0.9967600703239441}]}, {"text": "For the WSJ10, we obtained a binary tree set of 5.68 * 10 5 trees, by extracting the binary trees as described in section 5.", "labels": [], "entities": [{"text": "WSJ10", "start_pos": 8, "end_pos": 13, "type": "DATASET", "confidence": 0.9611732959747314}]}, {"text": "From this binary tree set we sampled 200,000 subtrees for each subtreedepth.", "labels": [], "entities": []}, {"text": "This resulted in a total set of roughly 1.7 * 10 6 subtrees that were reestimated by our maximum-likelihood procedure.", "labels": [], "entities": []}, {"text": "The decrease in cross-entropy became negligible after 14 iterations (for both halfs of WSJ10).", "labels": [], "entities": [{"text": "WSJ10", "start_pos": 87, "end_pos": 92, "type": "DATASET", "confidence": 0.9238750338554382}]}, {"text": "After computing the most probable parse trees, UML-DOP achieved an f-score of 82.9% which is a 20.5% error reduction compared to U-DOP's f-score of 78.5% on the same data).", "labels": [], "entities": [{"text": "f-score", "start_pos": 67, "end_pos": 74, "type": "METRIC", "confidence": 0.9916864037513733}, {"text": "error reduction", "start_pos": 101, "end_pos": 116, "type": "METRIC", "confidence": 0.953260987997055}]}, {"text": "We next tested UML-DOP on two additional domains which were also used in and: the German NEGRA10 () and the Chinese CTB10 ( both containing 2200+ sentences \u2264 10 words after removing punctuation.", "labels": [], "entities": [{"text": "German NEGRA10", "start_pos": 82, "end_pos": 96, "type": "DATASET", "confidence": 0.6865786015987396}]}, {"text": "shows the results of UML-DOP compared to U-DOP, the CCM model by, the DMV dependency learning model by as well as their combined model DMV+CCM.", "labels": [], "entities": []}, {"text": "shows that UML-DOP scores better than U-DOP and Klein and Manning's models in all cases.", "labels": [], "entities": []}, {"text": "It thus pays off to not only use subtrees rather than substrings (as in CCM) but to also reestimate the subtrees' probabilities by a maximum-likelihood procedure rather than using their (smoothed) relative frequencies (as in U-DOP).", "labels": [], "entities": []}, {"text": "Note that UML-DOP achieves these improved results with fewer subtrees than U-DOP, due to UML-DOP's more drastic pruning of subtrees.", "labels": [], "entities": []}, {"text": "It is also noteworthy that UML-DOP, like U-DOP, does not employ a separate class for non-constituents, so-called distituents, while CCM and CCM+DMV do.", "labels": [], "entities": []}, {"text": "(Interestingly, the top 10 most frequently learned constituents by UML-DOP were exactly the same as by U-DOP --see the relevant We were also interested in testing UML-DOP on longer sentences.", "labels": [], "entities": []}, {"text": "We therefore included all WSJ sentences up to 40 words after removing empty elements and punctuation (WSJ40) and again sampled 200,000 subtrees for each depth, using the same method as before.", "labels": [], "entities": [{"text": "WSJ40", "start_pos": 102, "end_pos": 107, "type": "DATASET", "confidence": 0.8934705257415771}]}, {"text": "Furthermore, we compared UML-DOP against a supervised binarized PCFG, i.e. a treebank PCFG whose simple relative frequency estimator corresponds to maximum likelihood (, and which we shall refer to as \"ML-PCFG\".", "labels": [], "entities": []}, {"text": "To this end, we used a random 90%/10% division of WSJ40 into a training set and a test set.", "labels": [], "entities": [{"text": "WSJ40", "start_pos": 50, "end_pos": 55, "type": "DATASET", "confidence": 0.9235948324203491}]}, {"text": "The ML-PCFG had thus access to the Penn WSJ trees in the training set, while UML-DOP had to bootstrap all structure from the flat strings from the training set to next parse the 10% test set --clearly a much more challenging task.", "labels": [], "entities": [{"text": "Penn WSJ trees", "start_pos": 35, "end_pos": 49, "type": "DATASET", "confidence": 0.9530596137046814}]}, {"text": "gives the results in terms of f-scores.", "labels": [], "entities": []}, {"text": "The table shows that UML-DOP scores better than U-DOP, also for WSJ40.", "labels": [], "entities": [{"text": "WSJ40", "start_pos": 64, "end_pos": 69, "type": "DATASET", "confidence": 0.944866955280304}]}, {"text": "Our results on WSJ10 are somewhat lower than in table 1 due to the use of a smaller training set of 90% of the data.", "labels": [], "entities": [{"text": "WSJ10", "start_pos": 15, "end_pos": 20, "type": "DATASET", "confidence": 0.8906451463699341}]}, {"text": "But the most surprising result is that UML-DOP's f-score is higher than the supervised binarized treebank PCFG (ML-PCFG) for both WSJ10 and WSJ40.", "labels": [], "entities": [{"text": "WSJ10", "start_pos": 130, "end_pos": 135, "type": "DATASET", "confidence": 0.9444242715835571}, {"text": "WSJ40", "start_pos": 140, "end_pos": 145, "type": "DATASET", "confidence": 0.8288432955741882}]}, {"text": "In order to check whether this difference is statistically significant, we additionally tested on 10 different 90/10 divisions of the WSJ40 (which were the same splits as in Bod 2006).", "labels": [], "entities": [{"text": "WSJ40", "start_pos": 134, "end_pos": 139, "type": "DATASET", "confidence": 0.9730976819992065}, {"text": "Bod 2006", "start_pos": 174, "end_pos": 182, "type": "DATASET", "confidence": 0.9669321179389954}]}, {"text": "For these splits, UML-DOP achieved an average f-score of 66.9%, while ML-PCFG obtained an average f-score of 64.7%.", "labels": [], "entities": [{"text": "UML-DOP", "start_pos": 18, "end_pos": 25, "type": "DATASET", "confidence": 0.7946072816848755}, {"text": "f-score", "start_pos": 46, "end_pos": 53, "type": "METRIC", "confidence": 0.9831461310386658}, {"text": "ML-PCFG", "start_pos": 70, "end_pos": 77, "type": "DATASET", "confidence": 0.7455014586448669}]}, {"text": "The difference inaccuracy between UML-DOP and ML-PCFG was statistically significant according to paired t-testing ( p\u22640.05).", "labels": [], "entities": []}, {"text": "To the best of our knowledge this means that we have shown for the first time that an unsupervised parsing model (UML-DOP) outperforms a widely used supervised parsing model (a treebank PCFG) on the WSJ40.", "labels": [], "entities": [{"text": "WSJ40", "start_pos": 199, "end_pos": 204, "type": "DATASET", "confidence": 0.9851024150848389}]}, {"text": "To be sure, the unbinarized version of the treebank PCFG obtains 89.0% average f-score on WSJ10 and 72.3% average f-score on WSJ40.", "labels": [], "entities": [{"text": "treebank PCFG", "start_pos": 43, "end_pos": 56, "type": "DATASET", "confidence": 0.839379221200943}, {"text": "WSJ10", "start_pos": 90, "end_pos": 95, "type": "DATASET", "confidence": 0.9866956472396851}, {"text": "WSJ40", "start_pos": 125, "end_pos": 130, "type": "DATASET", "confidence": 0.9946280121803284}]}, {"text": "Remember that the Penn Treebank annotations are often exceedingly flat, and many branches have arity larger than two.", "labels": [], "entities": [{"text": "Penn Treebank annotations", "start_pos": 18, "end_pos": 43, "type": "DATASET", "confidence": 0.9902034004529318}]}, {"text": "It would be interesting to see how UML-DOP performs if we also accept ternary (and wider) branches --though the total number of possible trees that can be assigned to strings would then further explode.", "labels": [], "entities": []}, {"text": "UML-DOP's performance still remains behind that of supervised (binarized) DOP parsers, such as DOP1, which achieved 81.9% average fscore on the 10 WSJ40 splits, and ML-DOP, which performed slightly better with 82.1% average fscore.", "labels": [], "entities": [{"text": "WSJ40 splits", "start_pos": 147, "end_pos": 159, "type": "DATASET", "confidence": 0.8661931157112122}]}, {"text": "And if DOP1 and ML-DOP are not binarized, their average f-scores are respectively 90.1% and 90.5% on WSJ40.", "labels": [], "entities": [{"text": "WSJ40", "start_pos": 101, "end_pos": 106, "type": "DATASET", "confidence": 0.9865108132362366}]}, {"text": "However, DOP1 and ML-DOP heavily depend on annotated data whereas UML-DOP only needs unannotated data.", "labels": [], "entities": []}, {"text": "It would thus be interesting to see how close UML-DOP can get to ML-DOP's performance if we enlarge the amount of training data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. F-scores of UML-DOP compared to  previous models on the same data", "labels": [], "entities": [{"text": "F-scores", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9964564442634583}]}, {"text": " Table 2. F-scores of U-DOP, UML-DOP and a  supervised treebank PCFG (ML-PCFG) for a  random 90/10 split of WSJ10 and WSJ40.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9823134541511536}, {"text": "WSJ10", "start_pos": 108, "end_pos": 113, "type": "DATASET", "confidence": 0.9216609597206116}, {"text": "WSJ40", "start_pos": 118, "end_pos": 123, "type": "DATASET", "confidence": 0.7846512198448181}]}]}