{"title": [{"text": "Combining Statistical and Knowledge-based Spoken Language Understanding in Conditional Models", "labels": [], "entities": [{"text": "Spoken Language Understanding", "start_pos": 42, "end_pos": 71, "type": "TASK", "confidence": 0.6281294922033945}]}], "abstractContent": [{"text": "Spoken Language Understanding (SLU) addresses the problem of extracting semantic meaning conveyed in an utterance.", "labels": [], "entities": [{"text": "Spoken Language Understanding (SLU)", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8697148561477661}, {"text": "extracting semantic meaning conveyed in an utterance", "start_pos": 61, "end_pos": 113, "type": "TASK", "confidence": 0.8547634056636265}]}, {"text": "The traditional knowledge-based approach to this problem is very expensive-it requires joint expertise in natural language processing and speech recognition, and best practices in language engineering for every new domain.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 138, "end_pos": 156, "type": "TASK", "confidence": 0.705817386507988}]}, {"text": "On the other hand, a statistical learning approach needs a large amount of annotated data for model training, which is seldom available in practical applications outside of large research labs.", "labels": [], "entities": [{"text": "model training", "start_pos": 94, "end_pos": 108, "type": "TASK", "confidence": 0.8018391132354736}]}, {"text": "A generative HMM/CFG composite model, which integrates easy-to-obtain domain knowledge into a data-driven statistical learning framework, has previously been introduced to reduce data requirement.", "labels": [], "entities": [{"text": "generative HMM/CFG", "start_pos": 2, "end_pos": 20, "type": "TASK", "confidence": 0.6568253636360168}]}, {"text": "The major contribution of this paper is the investigation of integrating prior knowledge and statistical learning in a conditional model framework.", "labels": [], "entities": []}, {"text": "We also study and compare conditional random fields (CRFs) with perceptron learning for SLU.", "labels": [], "entities": []}, {"text": "Experimental results show that the conditional models achieve more than 20% relative reduction in slot error rate over the HMM/CFG model, which had already achieved an SLU accuracy at the same level as the best results reported on the ATIS data.", "labels": [], "entities": [{"text": "slot error rate", "start_pos": 98, "end_pos": 113, "type": "METRIC", "confidence": 0.8047653039296468}, {"text": "accuracy", "start_pos": 172, "end_pos": 180, "type": "METRIC", "confidence": 0.7435522079467773}, {"text": "ATIS data", "start_pos": 235, "end_pos": 244, "type": "DATASET", "confidence": 0.9681979715824127}]}], "introductionContent": [{"text": "Spoken Language Understanding (SLU) addresses the problem of extracting meaning conveyed in an utterance.", "labels": [], "entities": [{"text": "Spoken Language Understanding (SLU)", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8752818902333578}, {"text": "extracting meaning conveyed in an utterance", "start_pos": 61, "end_pos": 104, "type": "TASK", "confidence": 0.8052799999713898}]}, {"text": "Traditionally, the problem is solved with a knowledge-based approach, which requires joint expertise in natural language processing and speech recognition, and best practices in language engineering for every new domain.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 136, "end_pos": 154, "type": "TASK", "confidence": 0.7099369466304779}]}, {"text": "In the past decade many statistical learning approaches have been proposed, most of which exploit generative models, as surveyed in ().", "labels": [], "entities": []}, {"text": "While the data-driven approach addresses the difficulties in knowledge engineering, it requires a large amount of labeled data for model training, which is seldom available in practical applications outside of large research labs.", "labels": [], "entities": [{"text": "knowledge engineering", "start_pos": 61, "end_pos": 82, "type": "TASK", "confidence": 0.7103089690208435}]}, {"text": "To alleviate the problem, a generative HMM/CFG composite model has previously been introduced ().", "labels": [], "entities": [{"text": "generative HMM/CFG", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.7506627589464188}]}, {"text": "It integrates a knowledge-based approach into a statistical learning framework, utilizing prior knowledge to compensate for the dearth of training data.", "labels": [], "entities": []}, {"text": "In the ATIS evaluation, this model achieves the same level of understanding accuracy (5.3% error rate on standard ATIS evaluation) as the best system (5.5% error rate), which is a semantic parsing system based on a manually developed grammar.", "labels": [], "entities": [{"text": "ATIS evaluation", "start_pos": 7, "end_pos": 22, "type": "DATASET", "confidence": 0.7136367857456207}, {"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9311747550964355}, {"text": "semantic parsing", "start_pos": 180, "end_pos": 196, "type": "TASK", "confidence": 0.7433440983295441}]}, {"text": "Discriminative training has been widely used for acoustic modeling in speech recognition; Juang,).", "labels": [], "entities": [{"text": "acoustic modeling", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.7177298963069916}, {"text": "speech recognition", "start_pos": 70, "end_pos": 88, "type": "TASK", "confidence": 0.7821926176548004}]}, {"text": "Most of the methods use the same generative model framework, exploit the same features, and apply discriminative training for parameter optimization.", "labels": [], "entities": [{"text": "parameter optimization", "start_pos": 126, "end_pos": 148, "type": "TASK", "confidence": 0.7329496741294861}]}, {"text": "Along the same lines, we have recently exploited conditional models by directly porting the HMM/CFG model to Hidden Conditional Random Fields (HCRFs)), but failed to obtain any improvement.", "labels": [], "entities": []}, {"text": "This is mainly due to the vast parameter space, with the parameters settling at local optima.", "labels": [], "entities": []}, {"text": "We then simplified the original model structure by removing the hidden variables, and introduced a number of important overlapping and non-homogeneous features.", "labels": [], "entities": []}, {"text": "The resulting Conditional Random Fields (CRFs)) yielded a 21% relative improvement in SLU accuracy.", "labels": [], "entities": [{"text": "SLU", "start_pos": 86, "end_pos": 89, "type": "TASK", "confidence": 0.8322189450263977}, {"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9493093490600586}]}, {"text": "We also applied a much simpler perceptron learning algorithm on the conditional model and observed improved SLU accuracy as well.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.7953540682792664}]}, {"text": "In this paper, we will first introduce the generative HMM/CFG composite model, then discuss the problem of directly porting the model to HCRFs, and finally introduce the CRFs and the features that obtain the best SLU result on ATIS test data.", "labels": [], "entities": [{"text": "generative HMM/CFG", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.6957834810018539}, {"text": "ATIS test data", "start_pos": 227, "end_pos": 241, "type": "DATASET", "confidence": 0.9433753887812296}]}, {"text": "We compare the CRF and perceptron training performances on the task.", "labels": [], "entities": []}], "datasetContent": [{"text": "Since the objective function is convex, the optimization algorithm does not make any significant difference on SLU accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.9357941746711731}]}, {"text": "We trained the model with SGD.", "labels": [], "entities": [{"text": "SGD", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.5235814452171326}]}, {"text": "Other optimization algorithm like Stochastic Meta-Decent (Vishwanathan,) can be used to speedup the convergence.", "labels": [], "entities": []}, {"text": "The training stopping criterion is cross-validated with the development set.", "labels": [], "entities": []}, {"text": "shows the number of new parameters and the slot error rate (SER) on the test data, after each new feature set is introduced.", "labels": [], "entities": [{"text": "slot error rate (SER)", "start_pos": 43, "end_pos": 64, "type": "METRIC", "confidence": 0.9659588932991028}]}, {"text": "The new features improve the prediction of slot identities and reduce the SER by 21%, relative to the generative HMM/CFG composite model.", "labels": [], "entities": [{"text": "SER", "start_pos": 74, "end_pos": 77, "type": "METRIC", "confidence": 0.9982008934020996}]}, {"text": "The figures below show in detail the impact of the n-gram, previous-slot context and chunk coverage features.", "labels": [], "entities": []}, {"text": "The chunk coverage feature has three settings: 0 stands for no chunk coverage features; 1 for chunk coverage features for preamble words only; and 2 for both words and slot boundaries.", "labels": [], "entities": [{"text": "chunk coverage", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.772375077009201}]}, {"text": "shows the impact of the order of ngram features.", "labels": [], "entities": []}, {"text": "Zero-order means no lexical features for preamble states are included.", "labels": [], "entities": []}, {"text": "As the figure illustrates, the inclusion of CFG rules for slot filler states and domain-specific knowledge about command priors and slot transitions have already produced a reasonable SER under 15%.", "labels": [], "entities": [{"text": "SER", "start_pos": 184, "end_pos": 187, "type": "METRIC", "confidence": 0.9980261921882629}]}, {"text": "Unigram features for preamble states cut the error by more than 50%, while the impact of bigram features is not consistent --it yields a small positive or negative difference depending on other experimental parameter settings.", "labels": [], "entities": [{"text": "error", "start_pos": 45, "end_pos": 50, "type": "METRIC", "confidence": 0.9779128432273865}]}, {"text": "The window size for the previous-slot context features is 2.", "labels": [], "entities": []}, {"text": "shows the impact of the CFG chunk coverage feature.", "labels": [], "entities": [{"text": "CFG chunk coverage", "start_pos": 24, "end_pos": 42, "type": "DATASET", "confidence": 0.887357751528422}]}, {"text": "Coverage for both preamble words and slot boundaries help improve the SLU accuracy.", "labels": [], "entities": [{"text": "SLU", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.8106241822242737}, {"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.956057608127594}]}, {"text": "shows the impact of the window size for the previous-slot context feature.", "labels": [], "entities": []}, {"text": "Here, 0 means that the previous-slot context feature is not used.", "labels": [], "entities": []}, {"text": "When the window size is k, the k words in front of the longest previous CFG-covered word sequence are included as the previous-slot unigram context features.", "labels": [], "entities": [{"text": "CFG-covered", "start_pos": 72, "end_pos": 83, "type": "DATASET", "confidence": 0.9666319489479065}]}, {"text": "As the figure illustrates, this feature significantly reduces SER, while the window size does not make any significant difference.", "labels": [], "entities": [{"text": "SER", "start_pos": 62, "end_pos": 65, "type": "METRIC", "confidence": 0.9989644289016724}]}, {"text": "compares the perceptron and CRF training algorithms, using chunk coverage features for both preamble words and slot boundaries, with which the best accuracy results are achieved.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 148, "end_pos": 156, "type": "METRIC", "confidence": 0.9988251328468323}]}, {"text": "Both improve upon the 5% baseline SER from the generative HMM/CFG model.", "labels": [], "entities": [{"text": "SER", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.975176215171814}, {"text": "generative HMM/CFG", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.7997112721204758}]}, {"text": "CRF training outperforms the perceptron inmost settings, except for the one with unigram features for preamble states and with window size 1 --the model with the fewest parameters.", "labels": [], "entities": []}, {"text": "One possible explanation is as follows.", "labels": [], "entities": []}, {"text": "The objective function in CRFs is a convex function, and so SGD can find the single global optimum for it.", "labels": [], "entities": []}, {"text": "In contrast, the objective function for the perceptron, which is the difference between two convex functions, is not convex.", "labels": [], "entities": []}, {"text": "The gradient ascent approach in perceptron training is hence more likely to settle on a local optimum as the model becomes more complicated.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Number of additional parameters and the", "labels": [], "entities": [{"text": "Number", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9678521752357483}]}]}