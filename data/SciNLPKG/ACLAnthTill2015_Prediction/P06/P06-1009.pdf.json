{"title": [{"text": "Discriminative Word Alignment with Conditional Random Fields", "labels": [], "entities": [{"text": "Discriminative Word Alignment", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6372421284516653}]}], "abstractContent": [{"text": "In this paper we present a novel approach for inducing word alignments from sentence aligned data.", "labels": [], "entities": [{"text": "word alignments from sentence aligned", "start_pos": 55, "end_pos": 92, "type": "TASK", "confidence": 0.7671325087547303}]}, {"text": "We use a Conditional Random Field (CRF), a discrimina-tive model, which is estimated on a small supervised training set.", "labels": [], "entities": []}, {"text": "The CRF is conditioned on both the source and target texts, and thus allows for the use of arbitrary and overlapping features over these data.", "labels": [], "entities": []}, {"text": "Moreover, the CRF has efficient training and decoding processes which both find globally optimal solutions.", "labels": [], "entities": []}, {"text": "We apply this alignment model to both French-English and Romanian-English language pairs.", "labels": [], "entities": []}, {"text": "We show how a large number of highly predictive features can be easily incorporated into the CRF, and demonstrate that even with only a few hundred word-aligned training sentences, our model improves over the current state-of-the-art with alignment error rates of 5.29 and 25.8 for the two tasks respectively.", "labels": [], "entities": [{"text": "alignment error", "start_pos": 239, "end_pos": 254, "type": "METRIC", "confidence": 0.8759935200214386}]}], "introductionContent": [{"text": "Modern phrase based statistical machine translation (SMT) systems usually break the translation task into two phases.", "labels": [], "entities": [{"text": "phrase based statistical machine translation (SMT)", "start_pos": 7, "end_pos": 57, "type": "TASK", "confidence": 0.7060497775673866}]}, {"text": "The first phase induces word alignments over a sentence-aligned bilingual corpus, and the second phase uses statistics over these predicted word alignments to decode (translate) novel sentences.", "labels": [], "entities": []}, {"text": "This paper deals with the first of these tasks: word alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 48, "end_pos": 62, "type": "TASK", "confidence": 0.8042031228542328}]}, {"text": "Most current SMT systems () use a generative model for word alignment such as the freely available GIZA++ (, an implementation of the IBM alignment models ().", "labels": [], "entities": [{"text": "SMT", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.9904187321662903}, {"text": "word alignment", "start_pos": 55, "end_pos": 69, "type": "TASK", "confidence": 0.7964622676372528}]}, {"text": "These models treat word alignment as a hidden process, and maximise the probability of the observed (e, f ) sentence pairs 1 using the expectation maximisation (EM) algorithm.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 19, "end_pos": 33, "type": "TASK", "confidence": 0.7891648709774017}]}, {"text": "After the maximisation process is complete, the word alignments are set to maximum posterior predictions of the model.", "labels": [], "entities": []}, {"text": "While GIZA++ gives good results when trained on large sentence aligned corpora, its generative models have a number of limitations.", "labels": [], "entities": []}, {"text": "Firstly, they impose strong independence assumptions between features, making it very difficult to incorporate non-independent features over the sentence pairs.", "labels": [], "entities": []}, {"text": "For instance, as well as detecting that a source word is aligned to a given target word, we would also like to encode syntactic and lexical features of the word pair, such as their partsof-speech, affixes, lemmas, etc.", "labels": [], "entities": []}, {"text": "Features such as these would allow for more effective use of sparse data and result in a model which is more robust in the presence of unseen words.", "labels": [], "entities": []}, {"text": "Adding these non-independent features to a generative model requires that the features' inter-dependence be modelled explicitly, which often complicates the model (eg.).", "labels": [], "entities": []}, {"text": "Secondly, the later IBM models, such as Model 4, have to resort to heuristic search techniques to approximate forward-backward and Viterbi inference, which sacrifice optimality for tractability.", "labels": [], "entities": []}, {"text": "This paper presents an alternative discriminative method for word alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 61, "end_pos": 75, "type": "TASK", "confidence": 0.8001740276813507}]}, {"text": "We use a conditional random field (CRF) sequence model, which allows for globally optimal training and decoding ().", "labels": [], "entities": []}, {"text": "The inference algo-rithms are tractable and efficient, thereby avoiding the need for heuristics.", "labels": [], "entities": []}, {"text": "The CRF is conditioned on both the source and target sentences, and therefore supports large sets of diverse and overlapping features.", "labels": [], "entities": []}, {"text": "Furthermore, the model allows regularisation using a prior over the parameters, a very effective and simple method for limiting over-fitting.", "labels": [], "entities": []}, {"text": "We use a similar graphical structure to the directed hidden Markov model (HMM) from GIZA++ (.", "labels": [], "entities": []}, {"text": "This models one-to-many alignments, where each target word is aligned with zero or more source words.", "labels": [], "entities": []}, {"text": "Many-to-many alignments are recoverable using the standard techniques for superimposing predicted alignments in both translation directions.", "labels": [], "entities": []}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents CRFs for word alignment, describing their form and their inference techniques.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 28, "end_pos": 42, "type": "TASK", "confidence": 0.7869861125946045}]}, {"text": "The features of our model are presented in Section 3, and experimental results for word aligning both French-English and Romanian-English sentences are given in Section 4.", "labels": [], "entities": []}, {"text": "Section 5 presents related work, and we describe future work in Section 6.", "labels": [], "entities": []}, {"text": "Finally, we conclude in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have applied our model to two publicly available word aligned corpora.", "labels": [], "entities": []}, {"text": "The first is the English-French Hansards corpus, which consists of 1.1 million aligned sentences and 484 wordaligned sentences.", "labels": [], "entities": [{"text": "Hansards corpus", "start_pos": 32, "end_pos": 47, "type": "DATASET", "confidence": 0.861206978559494}]}, {"text": "This data set was used for the 2003 NAACL shared task, where the word-aligned sentences were split into a 37 sentence trial set and a 447 sentence testing set.", "labels": [], "entities": [{"text": "NAACL shared task", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.6000109513600668}]}, {"text": "Unlike the unsupervised entrants in the 2003 task, we require word-aligned training data, and therefore must cannibalise the test set for this purpose.", "labels": [], "entities": []}, {"text": "We follow by using the first 100 test sentences for training and the remaining 347 for testing.", "labels": [], "entities": []}, {"text": "This means that our results should not be directly compared to those entrants, other than in an approximate manner.", "labels": [], "entities": []}, {"text": "We used the original 37 sentence trial set for feature engineering and for fitting a Gaussian prior.", "labels": [], "entities": [{"text": "feature engineering", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.8717920780181885}]}, {"text": "The word aligned data are annotated with both sure (S) and possible (P ) alignments (S \u2286 P ; Och and Ney), where the possible alignments indicate ambiguous or idiomatic alignments.", "labels": [], "entities": []}, {"text": "We measure the performance of our model using alignment error rate (AER), which is defined as: where A is the set of predicted alignments.", "labels": [], "entities": [{"text": "alignment error rate (AER)", "start_pos": 46, "end_pos": 72, "type": "METRIC", "confidence": 0.9422695438067118}]}, {"text": "The second data set is the Romanian-English parallel corpus from the 2005 ACL shared task).", "labels": [], "entities": [{"text": "Romanian-English parallel corpus from the 2005 ACL shared task", "start_pos": 27, "end_pos": 89, "type": "DATASET", "confidence": 0.8405994375546774}]}, {"text": "This consists of approximately 50,000 aligned sentences and 448 wordaligned sentences, which are split into a 248 sentence trial set and a 200 sentence test set.", "labels": [], "entities": []}, {"text": "We used these as our training and test sets, respectively.", "labels": [], "entities": []}, {"text": "For parameter tuning, we used the 17 sentence trial set from the Romanian-English corpus in the 2003 NAACL task.", "labels": [], "entities": [{"text": "Romanian-English corpus", "start_pos": 65, "end_pos": 88, "type": "DATASET", "confidence": 0.7501278221607208}, {"text": "NAACL task", "start_pos": 101, "end_pos": 111, "type": "DATASET", "confidence": 0.7105750739574432}]}, {"text": "For this task we have used the same test data as the competition entrants, and therefore can directly compare our results.", "labels": [], "entities": []}, {"text": "The word alignments in this corpus were only annotated with sure (S) alignments, and therefore the AER is equivalent to the F 1 score.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.6802371591329575}, {"text": "AER", "start_pos": 99, "end_pos": 102, "type": "METRIC", "confidence": 0.9993324875831604}, {"text": "F 1 score", "start_pos": 124, "end_pos": 133, "type": "METRIC", "confidence": 0.9768696427345276}]}, {"text": "In the shared task it was found that models which were trained on only the first four letters of each word obtained superior results to those using the full words).", "labels": [], "entities": []}, {"text": "We observed the same result with our model on the trial set and thus have only used the first four letters when training the Dice and Model 1 translation probabilities.", "labels": [], "entities": []}, {"text": "show the results when all feature types are employed on both language pairs.", "labels": [], "entities": []}, {"text": "We report the results for both translation directions and when combined using the refined and intersection methods.", "labels": [], "entities": [{"text": "translation", "start_pos": 31, "end_pos": 42, "type": "TASK", "confidence": 0.9592976570129395}]}, {"text": "The Model 4 results are from GIZA++ with the default parameters and the training data lowercased.", "labels": [], "entities": []}, {"text": "For Romanian, Model 4 was trained using the first four letters of each word.", "labels": [], "entities": []}, {"text": "The Romanian results are close to the best reported result of 26.10 from the ACL shared task).", "labels": [], "entities": [{"text": "ACL shared task", "start_pos": 77, "end_pos": 92, "type": "DATASET", "confidence": 0.8279616634051005}]}, {"text": "This result was from a system based on Model 4 plus additional parameters such as a dictionary.", "labels": [], "entities": []}, {"text": "The standard Model 4 implementation in the shared task achieved a result of 31.65, while when only the first 4 letters of each word were used it achieved 28.80.", "labels": [], "entities": []}, {"text": "5 shows the effect of removing each of the feature types in turn from the full model.", "labels": [], "entities": []}, {"text": "The most useful features are the Dice and Model 1 values which allow the model to incorporate translation probabilities from the large sentence aligned corpora.", "labels": [], "entities": []}, {"text": "This is to be expected as the amount of word aligned data are extremely small, and therefore the model can only estimate translation probabilities for only a fraction of the lexicon.", "labels": [], "entities": []}, {"text": "We would expect the dependence on sentence aligned data to decrease as more word aligned data becomes available.", "labels": [], "entities": []}, {"text": "The effect of removing the Markov features can be seen from comparing and (b).", "labels": [], "entities": []}, {"text": "The model has learnt to prefer alignments that follow the diagonal, thus alignments such as 3 \u2194 three and prestation \u2194 provision are found, and missalignments such as de \u2194 of, which lie well off the diagonal, are avoided.", "labels": [], "entities": []}, {"text": "The differing utility of the alignment word pair feature between the two tasks is probably a result of the different proportions of word-to sentencealigned data.", "labels": [], "entities": []}, {"text": "For the French data, where a very large lexicon can be estimated from the million sentence alignments, the sparse word pairs learnt on the word aligned sentences appear to lead to overfitting.", "labels": [], "entities": [{"text": "French data", "start_pos": 8, "end_pos": 19, "type": "DATASET", "confidence": 0.8920798897743225}]}, {"text": "In contrast, for Romanian, where more word alignments are used to learn the translation pair features and much less sentence aligned data are available, these features have a significant impact on the model.", "labels": [], "entities": []}, {"text": "Suprisingly the orthographic features actually worsen the performance in the tasks (incidentally, these features help the trial set).", "labels": [], "entities": []}, {"text": "Our explanation is that the other features (eg. Model 1) already adequately model these correspondences, and therefore the orthographic fea-  tures do not add much additional modelling power.", "labels": [], "entities": []}, {"text": "We expect that with further careful feature engineering, and a larger trial set, these orthographic features could be much improved.", "labels": [], "entities": []}, {"text": "The Romanian-English language pair appears to offer a more difficult modelling problem than the French-English pair.", "labels": [], "entities": []}, {"text": "With both the translation score features (Dice and Model 1) removed -the sentence aligned data are not used -the AER of the Romanian is more than twice that of the French, despite employing more word aligned data.", "labels": [], "entities": [{"text": "AER", "start_pos": 113, "end_pos": 116, "type": "METRIC", "confidence": 0.9993700385093689}]}, {"text": "This could be caused by the lack of possible (P) alignment markup in the Romanian data, which provide a boost in AER on the French data set, rewarding what would otherwise be considered errors.", "labels": [], "entities": [{"text": "Romanian data", "start_pos": 73, "end_pos": 86, "type": "DATASET", "confidence": 0.81771120429039}, {"text": "AER", "start_pos": 113, "end_pos": 116, "type": "METRIC", "confidence": 0.9996415376663208}, {"text": "French data set", "start_pos": 124, "end_pos": 139, "type": "DATASET", "confidence": 0.9815932909647623}]}, {"text": "Interestingly, without any features derived from the sentence aligned corpus, our model achieves performance equivalent to Model 3 trained on the full corpus.", "labels": [], "entities": []}, {"text": "This is a particularly strong result, indicating that this method is ideal for data-impoverished alignment tasks.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Results on the Hansard data using all features", "labels": [], "entities": [{"text": "Hansard data", "start_pos": 25, "end_pos": 37, "type": "DATASET", "confidence": 0.9853221476078033}]}, {"text": " Table 2. Results on the Romanian data using all fea- tures", "labels": [], "entities": [{"text": "Romanian data", "start_pos": 25, "end_pos": 38, "type": "DATASET", "confidence": 0.8870986998081207}]}, {"text": " Table 3. The resulting AERs after removing individual  groups of features from the full model.", "labels": [], "entities": [{"text": "AERs", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.901541531085968}]}, {"text": " Table 4. Results using features from Model 4 bi- directional alignments, training with and without the  possible (P) alignments.", "labels": [], "entities": []}, {"text": " Table 5. 10-fold cross-validation results, with and with- out Model 4 features.", "labels": [], "entities": []}]}