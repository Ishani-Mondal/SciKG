{"title": [{"text": "The Role of Information Retrieval in Answering Complex Questions", "labels": [], "entities": [{"text": "Information Retrieval", "start_pos": 12, "end_pos": 33, "type": "TASK", "confidence": 0.729196235537529}, {"text": "Answering Complex Questions", "start_pos": 37, "end_pos": 64, "type": "TASK", "confidence": 0.9259698192278544}]}], "abstractContent": [{"text": "This paper explores the role of information retrieval in answering \"relationship\" questions, anew class complex information needs formally introduced in TREC 2005.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 32, "end_pos": 53, "type": "TASK", "confidence": 0.6992572396993637}, {"text": "answering \"relationship\" questions", "start_pos": 57, "end_pos": 91, "type": "TASK", "confidence": 0.8131901025772095}, {"text": "TREC 2005", "start_pos": 153, "end_pos": 162, "type": "DATASET", "confidence": 0.8454267680644989}]}, {"text": "Since information retrieval is often an integral component of many question answering strategies, it is important to understand the impact of different term-based techniques.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 6, "end_pos": 27, "type": "TASK", "confidence": 0.7622848451137543}, {"text": "question answering", "start_pos": 67, "end_pos": 85, "type": "TASK", "confidence": 0.8063554167747498}]}, {"text": "Within a framework of sentence retrieval, we examine three factors that contribute to question answering performance: the use of different retrieval engines, relevance (both at the document and sentence level), and redundancy.", "labels": [], "entities": [{"text": "sentence retrieval", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.7170546650886536}, {"text": "question answering", "start_pos": 86, "end_pos": 104, "type": "TASK", "confidence": 0.7073686420917511}]}, {"text": "Results point out the limitations of purely term-based methods to this challenging task.", "labels": [], "entities": []}, {"text": "Nevertheless, IR-based techniques provide a strong baseline on top of which more sophisticated language processing techniques can be deployed.", "labels": [], "entities": []}], "introductionContent": [{"text": "The field of question answering arose from the recognition that the document does not occupy a privileged position in the space of information objects as the most ideal unit of retrieval.", "labels": [], "entities": [{"text": "question answering", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.8748558759689331}]}, {"text": "Indeed, for certain types of information needs, sub-document segments are preferred-an example is answers to factoid questions such as \"Who won the Nobel Prize for literature in 1972?\"", "labels": [], "entities": [{"text": "Who won the Nobel Prize for literature in 1972?\"", "start_pos": 136, "end_pos": 184, "type": "TASK", "confidence": 0.7283797670494426}]}, {"text": "By leveraging sophisticated language processing capabilities, factoid question answering systems are able to pinpoint the exact span of text that directly satisfies an information need.", "labels": [], "entities": [{"text": "factoid question answering", "start_pos": 62, "end_pos": 88, "type": "TASK", "confidence": 0.7904600699742635}]}, {"text": "Nevertheless, IR engines remain integral components of question answering systems, primarily as a source of candidate documents that are subsequently analyzed in greater detail.", "labels": [], "entities": [{"text": "IR engines", "start_pos": 14, "end_pos": 24, "type": "TASK", "confidence": 0.9178448617458344}, {"text": "question answering", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.8958459794521332}]}, {"text": "Although this two-stage architecture was initially conceived as an expedient to overcome the computational processing bottleneck associated with more sophisticated but slower language processing technology, it has worked quite well in practice.", "labels": [], "entities": []}, {"text": "The architecture has since evolved into a widely-accepted paradigm for building working systems).", "labels": [], "entities": []}, {"text": "Due to the reliance of QA systems on IR technology, the relationship between them is an important area of study.", "labels": [], "entities": []}, {"text": "For example, how sensitive is answer extraction performance to the initial quality of the result set?", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.8449600040912628}]}, {"text": "Does better document retrieval necessarily translate into more accurate answer extraction?", "labels": [], "entities": [{"text": "document retrieval", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.7060925960540771}, {"text": "answer extraction", "start_pos": 72, "end_pos": 89, "type": "TASK", "confidence": 0.8892515897750854}]}, {"text": "These answers cannot be solely determined from first principles, but must be addressed through empirical experiments.", "labels": [], "entities": []}, {"text": "Indeed, a number of works have specifically examined the effects of information retrieval on question answering, including a dedicated workshop at SIGIR 2004 ().", "labels": [], "entities": [{"text": "question answering", "start_pos": 93, "end_pos": 111, "type": "TASK", "confidence": 0.8403438329696655}, {"text": "SIGIR 2004", "start_pos": 147, "end_pos": 157, "type": "DATASET", "confidence": 0.6708786189556122}]}, {"text": "More recently, the importance of document retrieval has prompted NIST to introduce a document ranking subtask inside the TREC 2005 QA track.", "labels": [], "entities": [{"text": "document retrieval", "start_pos": 33, "end_pos": 51, "type": "TASK", "confidence": 0.7375662624835968}, {"text": "NIST", "start_pos": 65, "end_pos": 69, "type": "DATASET", "confidence": 0.9018218517303467}, {"text": "TREC 2005 QA track", "start_pos": 121, "end_pos": 139, "type": "DATASET", "confidence": 0.9123235493898392}]}, {"text": "However, the connection between QA and IR has mostly been explored in the context of factoid questions such as \"Who shot Abraham Lincoln?\", which represent only a small fraction of all information needs.", "labels": [], "entities": [{"text": "IR", "start_pos": 39, "end_pos": 41, "type": "TASK", "confidence": 0.9141897559165955}, {"text": "Who shot Abraham Lincoln?\"", "start_pos": 112, "end_pos": 138, "type": "TASK", "confidence": 0.6137340903282166}]}, {"text": "In contrast to factoid questions, which can be answered by short phrases found within an individual document, there is a large class of questions whose answers require synthesis of information from multiple sources.", "labels": [], "entities": []}, {"text": "The socalled definition/other questions at recent TREC evaluations) serve as good examples: \"good answers\" to these questions include in-  teresting \"nuggets\" about a particular person, organization, entity, or event.", "labels": [], "entities": []}, {"text": "No single document can provide a complete answer, and hence systems must integrate information from multiple sources; cf..", "labels": [], "entities": []}, {"text": "This work focuses on so-called relationship questions, which represent anew and underexplored area in question answering.", "labels": [], "entities": [{"text": "question answering", "start_pos": 102, "end_pos": 120, "type": "TASK", "confidence": 0.8288622200489044}]}, {"text": "Although they require systems to extract information nuggets from multiple documents (just like definition/other questions), relationship questions demand a different approach (see Section 2).", "labels": [], "entities": []}, {"text": "This paper explores the role of information retrieval in answering such questions, focusing primarily on three aspects: document retrieval performance, term-based measures of relevance, and term-based approaches to reducing redundancy.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 32, "end_pos": 53, "type": "TASK", "confidence": 0.7355815470218658}, {"text": "document retrieval", "start_pos": 120, "end_pos": 138, "type": "TASK", "confidence": 0.68074531853199}]}, {"text": "The overall goal is to push the limits of information retrieval technology and provide strong baselines against which linguistic processing capabilities can be compared.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 42, "end_pos": 63, "type": "TASK", "confidence": 0.7721876800060272}]}, {"text": "The rest of this paper is organized as follows: Section 2 provides an overview of relationship questions.", "labels": [], "entities": []}, {"text": "Section 3 describes experiments focused on document retrieval performance.", "labels": [], "entities": [{"text": "document retrieval", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.752023309469223}]}, {"text": "An approach to answering relationship questions based on sentence retrieval is discussed in Section 4.", "labels": [], "entities": [{"text": "sentence retrieval", "start_pos": 57, "end_pos": 75, "type": "TASK", "confidence": 0.652672678232193}]}, {"text": "A simple utility model that incorporates both relevance and redundancy is explored in Section 5.", "labels": [], "entities": []}, {"text": "Before concluding, we discuss the implications of our experimental results in Section 6.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Document retrieval performance, with  and without blind relevance feedback.", "labels": [], "entities": [{"text": "Document retrieval", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.8865993916988373}]}, {"text": " Table 2: Question answering performance at different answer length cutoffs, as measured by POURPRE.", "labels": [], "entities": [{"text": "Question answering", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.9021350741386414}, {"text": "POURPRE", "start_pos": 92, "end_pos": 99, "type": "METRIC", "confidence": 0.9635741710662842}]}, {"text": " Table 3: The effect of using different document retrieval systems on answer quality.", "labels": [], "entities": []}, {"text": " Table 4: Evaluation of different utility settings.", "labels": [], "entities": []}]}