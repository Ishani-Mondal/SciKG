{"title": [{"text": "Valido: a Visual Tool for Validating Sense Annotations", "labels": [], "entities": [{"text": "Validating Sense Annotations", "start_pos": 26, "end_pos": 54, "type": "TASK", "confidence": 0.8509483734766642}]}], "abstractContent": [{"text": "In this paper we present Valido, a tool that supports the difficult task of validating sense choices produced by a set of annota-tors.", "labels": [], "entities": []}, {"text": "The validator can analyse the semantic graphs resulting from each sense choice and decide which sense is more coherent with respect to the structure of the adopted lexicon.", "labels": [], "entities": []}, {"text": "We describe the interface and report an evaluation of the tool in the validation of manual sense annotations.", "labels": [], "entities": []}], "introductionContent": [{"text": "The task of sense annotation consists in the assignment of the appropriate senses to words in context.", "labels": [], "entities": [{"text": "sense annotation", "start_pos": 12, "end_pos": 28, "type": "TASK", "confidence": 0.7084469050168991}]}, {"text": "For each word, the senses are chosen with respect to a sense inventory encoded by a reference dictionary.", "labels": [], "entities": []}, {"text": "The free availability and, as a result, the massive adoption of WordNet largely contributed to its status of de facto standard in the NLP community.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 64, "end_pos": 71, "type": "DATASET", "confidence": 0.9483472108840942}]}, {"text": "Unfortunately, WordNet is a fine-grained resource, which encodes possibly subtle sense distictions.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 15, "end_pos": 22, "type": "DATASET", "confidence": 0.973388671875}]}, {"text": "Several studies report an inter-annotator agreement around 70% when using WordNet as a reference sense inventory.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 74, "end_pos": 81, "type": "DATASET", "confidence": 0.9546273946762085}]}, {"text": "For instance, the agreement in the Open Mind Word Expert project) was 67.3%.", "labels": [], "entities": [{"text": "agreement", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.967004656791687}, {"text": "Open Mind Word Expert project", "start_pos": 35, "end_pos": 64, "type": "DATASET", "confidence": 0.5330453872680664}]}, {"text": "Such a low agreement is only in part due to the inexperience of sense annotators (e.g. volunteers on the web).", "labels": [], "entities": [{"text": "agreement", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9600972533226013}]}, {"text": "Rather, to a large part it is due to the difficulty in making clear which are the real distinctions between close word senses in the WordNet inventory.", "labels": [], "entities": [{"text": "WordNet inventory", "start_pos": 133, "end_pos": 150, "type": "DATASET", "confidence": 0.9480253458023071}]}, {"text": "Adjudicating sense choices, i.e. the task of validating word senses, is therefore critical in building a high-quality data set.", "labels": [], "entities": []}, {"text": "The validation task can be defined as follows: let w be a word in a sentence \u03c3, previously annotated by a set of annotators A = {a 1 , a 2 , ..., an } each providing a sense for w, and let SA = {s 1 , s 2 , ..., s m } \u2286 Senses(w) be the set of senses chosen for w by the annotators in A, where Senses(w) is the set of senses of win the reference inventory (e.g. WordNet).", "labels": [], "entities": []}, {"text": "A validator is asked to validate, that is to adjudicate a sense s \u2208 Senses(w) fora word w over the others.", "labels": [], "entities": []}, {"text": "Notice that sis a word sense for win the sense inventory, but is not necessarily in SA , although it is likely to be.", "labels": [], "entities": []}, {"text": "Also note that the annotators in A can be either human or automatic, depending upon the purpose of the exercise.", "labels": [], "entities": []}], "datasetContent": [{"text": "We briefly report here an experiment on the validation of manual sense annotations with the aid of Valido.", "labels": [], "entities": [{"text": "validation of manual sense annotations", "start_pos": 44, "end_pos": 82, "type": "TASK", "confidence": 0.7980010271072387}]}, {"text": "For more detailed experiments the reader can refer to.", "labels": [], "entities": []}, {"text": "1,000 sentences were uniformly selected from the set of documents in the semantically-tagged SemCor corpus).", "labels": [], "entities": [{"text": "SemCor corpus", "start_pos": 93, "end_pos": 106, "type": "DATASET", "confidence": 0.7428886592388153}]}, {"text": "For each sentence \u03c3 = w 1 w 2 . .", "labels": [], "entities": []}, {"text": "wk annotated in SemCor with the senses s w 1 s w 2 . .", "labels": [], "entities": []}, {"text": "s wk (s w i \u2208 Senses(w i ), i \u2208 {1, 2, . .", "labels": [], "entities": []}, {"text": ", k}), we randomly identified a word w i \u2208 \u03c3, and chose at random a different sense s w i for that word, that is s w i \u2208 Senses(w i ) \\ {s w i }.", "labels": [], "entities": []}, {"text": "In other words, we simulated in vitro a situation in which an annotator provides an appropriate sense and the other selects a different sense.", "labels": [], "entities": []}, {"text": "We applied Valido with policy (\u03b3) to the annotated sentences and evaluated the performance of the approach in suggesting the appropriate choice for the words with disagreement.", "labels": [], "entities": []}, {"text": "The results are reported in for nouns, adjectives, and verbs (we neglected adverbs as very few interconnections can be found for them).", "labels": [], "entities": []}, {"text": "The experiment shows that evidences of inconsistency due to inappropriate annotations are provided with good precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 109, "end_pos": 118, "type": "METRIC", "confidence": 0.9971849322319031}]}, {"text": "The overall F1 measure is 59.18%.", "labels": [], "entities": [{"text": "F1 measure", "start_pos": 12, "end_pos": 22, "type": "METRIC", "confidence": 0.9388828277587891}]}, {"text": "The chance baseline is 50%.", "labels": [], "entities": [{"text": "chance baseline", "start_pos": 4, "end_pos": 19, "type": "METRIC", "confidence": 0.9572867453098297}]}, {"text": "The low recall obtained for verbs, but especially for adjectives, is due to alack of connectivity in the lexical knowledge base, when dealing with connections across different parts of speech.", "labels": [], "entities": [{"text": "recall", "start_pos": 8, "end_pos": 14, "type": "METRIC", "confidence": 0.9988152980804443}]}], "tableCaptions": []}