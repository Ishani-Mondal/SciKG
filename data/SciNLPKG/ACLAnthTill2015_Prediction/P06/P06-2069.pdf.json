{"title": [{"text": "Examining the Content Load of Part of Speech Blocks for Information Retrieval", "labels": [], "entities": [{"text": "Examining the Content Load of Part of Speech Blocks", "start_pos": 0, "end_pos": 51, "type": "TASK", "confidence": 0.7177938156657748}, {"text": "Information Retrieval", "start_pos": 56, "end_pos": 77, "type": "TASK", "confidence": 0.7432141304016113}]}], "abstractContent": [{"text": "We investigate the connection between part of speech (POS) distribution and content in language.", "labels": [], "entities": []}, {"text": "We define POS blocks to be groups of parts of speech.", "labels": [], "entities": []}, {"text": "We hypo-thesise that there exists a directly proportional relation between the frequency of POS blocks and their content salience.", "labels": [], "entities": []}, {"text": "We also hypothesise that the class membership of the parts of speech within such blocks reflects the content load of the blocks, on the basis that open class parts of speech are more content-bearing than closed class parts of speech.", "labels": [], "entities": []}, {"text": "We test these hypotheses in the context of Information Retrieval, by syntactically representing queries, and removing from them content-poor blocks, inline with the aforementioned hypotheses.", "labels": [], "entities": [{"text": "Information Retrieval", "start_pos": 43, "end_pos": 64, "type": "TASK", "confidence": 0.7158263772726059}]}, {"text": "For our first hypothesis, we induce POS distribution information from a corpus , and approximate the probability of occurrence of POS blocks as per two statistical estimators separately.", "labels": [], "entities": []}, {"text": "For our second hypothesis, we use simple heuristics to estimate the content load within POS blocks.", "labels": [], "entities": []}, {"text": "We use the Text REtrieval Conference (TREC) queries of 1999 and 2000 to retrieve documents from the WT2G and WT10G test collections, with five different retrieval strategies.", "labels": [], "entities": [{"text": "Text REtrieval Conference (TREC)", "start_pos": 11, "end_pos": 43, "type": "TASK", "confidence": 0.7984177619218826}, {"text": "WT2G", "start_pos": 100, "end_pos": 104, "type": "DATASET", "confidence": 0.897253155708313}, {"text": "WT10G test collections", "start_pos": 109, "end_pos": 131, "type": "DATASET", "confidence": 0.8776949644088745}]}, {"text": "Experimental outcomes confirm that our hypotheses hold in the context of Information Retrieval.", "labels": [], "entities": [{"text": "Information Retrieval", "start_pos": 73, "end_pos": 94, "type": "TASK", "confidence": 0.7285528928041458}]}], "introductionContent": [{"text": "The task of an Information Retrieval (IR) system is to retrieve documents from a collection, in response to a user need, which is expressed in the form of a query.", "labels": [], "entities": [{"text": "Information Retrieval (IR)", "start_pos": 15, "end_pos": 41, "type": "TASK", "confidence": 0.8696797013282775}]}, {"text": "Very often, this task is realised by indexing the documents in the collection with keyword descriptors.", "labels": [], "entities": []}, {"text": "Retrieval consists in matching the query against the descriptors of the documents, and returning the ones that appear closest, in ranked lists of relevance.", "labels": [], "entities": [{"text": "Retrieval", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.5844798684120178}]}, {"text": "Usually, the keywords that constitute the document descriptors are associated with individual weights, which capture the importance of the keywords to the content of the document.", "labels": [], "entities": []}, {"text": "Such weights, commonly referred to as term weights, can be computed using various term weighting schemes.", "labels": [], "entities": []}, {"text": "Not all words can be used as keyword descriptors.", "labels": [], "entities": []}, {"text": "In fact, a relatively small number of words accounts for most of a document's content.", "labels": [], "entities": []}, {"text": "Function words make 'noisy' index terms, and are usually ignored during the retrieval process.", "labels": [], "entities": []}, {"text": "This is practically realised with the use of stopword lists, which are lists of words to be exempted when indexing the collection and the queries.", "labels": [], "entities": []}, {"text": "The use of stopword lists in IR is a manifestation of a well-known bifurcation in linguistics between open and closed classes of words.", "labels": [], "entities": [{"text": "IR", "start_pos": 29, "end_pos": 31, "type": "TASK", "confidence": 0.9370048642158508}]}, {"text": "In brief, open class words are more content-bearing than closed class words.", "labels": [], "entities": []}, {"text": "Generally, the open class contains parts of speech that are morphologically and semantically flexible, while the closed class contains words that primarily perform linguistic well-formedness functions.", "labels": [], "entities": []}, {"text": "The membership of the closed class is mostly fixed and largely restricted to function words, which are not prone to semantic or morphological alterations.", "labels": [], "entities": []}, {"text": "We define a block of parts of speech (POS block) as a block of fixed length , where is set empirically.", "labels": [], "entities": []}, {"text": "We define POS block tokens as individual instances of POS blocks, and POS block types as distinct POS blocks in a corpus.", "labels": [], "entities": []}, {"text": "The purpose of this paper is to test two hypotheses.", "labels": [], "entities": []}, {"text": "The intuition behind both of these hypotheses is that, just as individual words can be content-rich or content-poor, the same can hold for blocks of parts of speech.", "labels": [], "entities": []}, {"text": "According to our first hypothesis, POS blocks can be categorized as content-rich or content-poor, on the basis of their distribution within a corpus.", "labels": [], "entities": []}, {"text": "Specifically, we hypothesise that the more frequently a POS block occurs in language, the more content it is likely to bear.", "labels": [], "entities": []}, {"text": "According to our second hypothesis, POS blocks can be categorized as content-rich or content-poor, on the basis of the part of speech class membership of their individual components.", "labels": [], "entities": []}, {"text": "Specifically, we hypothesise that the more closed class components found in a POS block, the less content the block is likely to bear.", "labels": [], "entities": []}, {"text": "Both aforementioned hypotheses are evaluated in the context of IR as follows.", "labels": [], "entities": [{"text": "IR", "start_pos": 63, "end_pos": 65, "type": "TASK", "confidence": 0.9806306958198547}]}, {"text": "We observe the distribution of POS blocks in a corpus.", "labels": [], "entities": []}, {"text": "We create a list of POS block types with their respective probabilities of occurrence.", "labels": [], "entities": []}, {"text": "As a first step, to test our first hypothesis, we remove the POS blocks with a low probability of occurrence from each query, on the assumption that these blocks are content-poor.", "labels": [], "entities": []}, {"text": "The decision regarding the threshold of low probability of occurrence is realised empirically.", "labels": [], "entities": []}, {"text": "As a second step, we further remove from each query POS blocks that contain less open class than closed class components, in order to test the validity of our second hypothesis, as an extension of the first hypothesis.", "labels": [], "entities": []}, {"text": "We retrieve documents from two standard IR English test collections, namely WT2G and WT10G.", "labels": [], "entities": [{"text": "IR English test collections", "start_pos": 40, "end_pos": 67, "type": "DATASET", "confidence": 0.7748405486345291}, {"text": "WT2G", "start_pos": 76, "end_pos": 80, "type": "DATASET", "confidence": 0.9712196588516235}, {"text": "WT10G", "start_pos": 85, "end_pos": 90, "type": "DATASET", "confidence": 0.960462212562561}]}, {"text": "Both of these collections are commonly used for retrieval effectiveness evaluations in the Text REtrieval Conference (TREC), and come with sets of queries and query relevance assessments . Query relevance assessments are lists of relevant documents, given a query.", "labels": [], "entities": [{"text": "retrieval effectiveness evaluations", "start_pos": 48, "end_pos": 83, "type": "TASK", "confidence": 0.8159182667732239}, {"text": "Text REtrieval Conference (TREC)", "start_pos": 91, "end_pos": 123, "type": "TASK", "confidence": 0.6375589817762375}]}, {"text": "We retrieve relevant documents using firstly the original queries, secondly the queries produced after step 1, and thirdly the queries produced after step 2.", "labels": [], "entities": []}, {"text": "We use five statistically different term weighting schemes to match the query terms to the document keywords, in order to assess our hypotheses across a range of retrieval techniques.", "labels": [], "entities": []}, {"text": "We associate improvement of retrieval performance with successful noise reduction in the queries.", "labels": [], "entities": []}, {"text": "We assume noise reduction to reflect the correct iden-1 http://trec.nist.gov/ tification of content-poor blocks, inline with our hypotheses.", "labels": [], "entities": []}, {"text": "Section 2 presents related studies in this field.", "labels": [], "entities": []}, {"text": "Section 3 introduces our methodology.", "labels": [], "entities": []}, {"text": "Section 4 presents the experimental settings used to test our hypotheses, and their evaluation outcomes.", "labels": [], "entities": []}, {"text": "Section 5 provides our conclusions and remarks.", "labels": [], "entities": []}], "datasetContent": [{"text": "We present the experiments realised to test the two hypotheses formulated in Section 1.", "labels": [], "entities": []}, {"text": "Section 4.1 presents our experimental settings, and Section 4.2 our evaluation results.", "labels": [], "entities": []}, {"text": "We induce POS blocks from the English language component of the second release of the parallel Europarl corpus(75MB) 2 . We POS tag the corpus using the TreeTagger 3 , which is a probabilistic POS tagger that uses the Penn TreeBank tagset).", "labels": [], "entities": [{"text": "Europarl corpus(75MB) 2", "start_pos": 95, "end_pos": 118, "type": "DATASET", "confidence": 0.9636683960755666}, {"text": "Penn TreeBank tagset", "start_pos": 218, "end_pos": 238, "type": "DATASET", "confidence": 0.9876272678375244}]}, {"text": "Since we are solely interested in a POS analysis, we introduce a stage of tagset simplification, during which, any information on top of surface POS classification is lost.", "labels": [], "entities": [{"text": "POS analysis", "start_pos": 36, "end_pos": 48, "type": "TASK", "confidence": 0.9392734169960022}, {"text": "POS classification", "start_pos": 145, "end_pos": 163, "type": "TASK", "confidence": 0.7215480506420135}]}, {"text": "Practically, this leads to 48 original TreeBank (TB) tag classes being narrowed down to 15 Reduced TreeBank (RTB) tag classes.", "labels": [], "entities": []}, {"text": "Additionally, tag names are shortened into two-letter names, for reasons of computational efficiency.", "labels": [], "entities": []}, {"text": "We consider the TBR tags JJ, FW, NN, and VB as open-class, and the remaining tags as closed class.", "labels": [], "entities": [{"text": "TBR tags JJ", "start_pos": 16, "end_pos": 27, "type": "DATASET", "confidence": 0.6915854811668396}]}, {"text": "We extract 214,398,227 POS block tokens and 19,343 POS block types from the corpus.", "labels": [], "entities": []}, {"text": "We retrieve relevant documents from two standard TREC test collections, namely WT2G (2GB) and WT10G (10GB), from the 1999 and 2000 TREC Web tracks, respectively.", "labels": [], "entities": [{"text": "TREC test collections", "start_pos": 49, "end_pos": 70, "type": "DATASET", "confidence": 0.7985019286473592}, {"text": "WT2G", "start_pos": 79, "end_pos": 83, "type": "DATASET", "confidence": 0.9478753805160522}, {"text": "WT10G", "start_pos": 94, "end_pos": 99, "type": "DATASET", "confidence": 0.924481987953186}, {"text": "TREC Web tracks", "start_pos": 131, "end_pos": 146, "type": "DATASET", "confidence": 0.901424785455068}]}, {"text": "We use the queries 401-450 from the ad-hoc task of the 1999 Web track, for the WT2G test collection, and the queries 451-500 from the ad-hoc task of the 2000 Web track, for the WT10G test collection, with their respective relevance assessments.", "labels": [], "entities": [{"text": "1999 Web track", "start_pos": 55, "end_pos": 69, "type": "DATASET", "confidence": 0.7295325398445129}, {"text": "WT2G test collection", "start_pos": 79, "end_pos": 99, "type": "DATASET", "confidence": 0.9534887075424194}, {"text": "WT10G test collection", "start_pos": 177, "end_pos": 198, "type": "DATASET", "confidence": 0.9601995348930359}]}, {"text": "Each query contains three fields, namely title, description, and narrative.", "labels": [], "entities": []}, {"text": "The title contains keywords describing the information need.", "labels": [], "entities": []}, {"text": "The description expands briefly on the information need.", "labels": [], "entities": []}, {"text": "The narrative part consists of sentences denoting key concepts to be considered or ignored.", "labels": [], "entities": []}, {"text": "We use all three query fields to match query terms to document keyword descriptors, but extract POS blocks only from the narrative field of the queries.", "labels": [], "entities": []}, {"text": "This choice is motivated by the two following reasons.", "labels": [], "entities": []}, {"text": "Firstly, the narrative includes the longest sentences in the whole query.", "labels": [], "entities": []}, {"text": "For our experiments, longer sentences provide better grounds upon which we can test our hypotheses, since the longer a sentence, the more POS blocks we can match within it.", "labels": [], "entities": []}, {"text": "Secondly, the narrative field contains the most noise in the whole query.", "labels": [], "entities": []}, {"text": "Especially when using bag-ofwords term weighting, such as in our evaluation, information on what is not relevant to the query only introduces noise.", "labels": [], "entities": []}, {"text": "Thus, we select the most noisy field of the query to test whether the application of our hypotheses indeed results in the reduction of noise.", "labels": [], "entities": []}, {"text": "During indexing, we remove stopwords, and stem the collections and the queries, using Porter's 4 stemming algorithm.", "labels": [], "entities": []}, {"text": "We use the Terrier IR platform, and apply five different weighting schemes to match query terms to document descriptors.", "labels": [], "entities": []}, {"text": "In IR, term weighting schemes estimate the relevance for the WT10G test collection.", "labels": [], "entities": [{"text": "IR", "start_pos": 3, "end_pos": 5, "type": "TASK", "confidence": 0.9806002974510193}, {"text": "WT10G test collection", "start_pos": 61, "end_pos": 82, "type": "DATASET", "confidence": 0.913298765818278}]}, {"text": "We use default values, instead of tuning the term weighting parameters, because our focus lies in testing our hypotheses, and not in optimising retrieval performance.", "labels": [], "entities": []}, {"text": "If the said parameters are optimised, retrieval performance maybe further improved.", "labels": [], "entities": []}, {"text": "We measure the retrieval performance using the Mean Average Precision (MAP) measure.", "labels": [], "entities": [{"text": "Mean Average Precision (MAP) measure", "start_pos": 47, "end_pos": 83, "type": "METRIC", "confidence": 0.9685475655964443}]}, {"text": "Throughout all experiments, we set POS block length at = 4.", "labels": [], "entities": [{"text": "POS block length", "start_pos": 35, "end_pos": 51, "type": "METRIC", "confidence": 0.5952757398287455}]}, {"text": "We employ Good-Turing and Laplace smoothing, and set the threshold of high probability of occurrence empirically at ) , appear in boldface, while highest \u0096 percentages appear in italics.", "labels": [], "entities": []}, {"text": "Our retrieval baseline consists in testing the performance of each term weighting scheme, with each of the two test collections, using the original queries.", "labels": [], "entities": []}, {"text": "We introduce two retrieval combinations on top of the baseline, which we call POS and POSC.", "labels": [], "entities": []}, {"text": "The POS retrieval experiments, which relate to our first hypothesis, and the POSC retrieval experiments, which relate to our second hypothesis, are described in Section 4.2.1.", "labels": [], "entities": []}, {"text": "Section 4.2.2 presents the assessment of our hypotheses using a performance-boosting retrieval technique, namely query expansion.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 113, "end_pos": 128, "type": "TASK", "confidence": 0.7499275505542755}]}, {"text": "The aim of the POS and POSC experiments is to test our first and second hypotheses, respectively.", "labels": [], "entities": []}, {"text": "Firstly, to test the first hypothesis, namely that there is a direct connection between the removal of low-frequency POS blocks from the queries and noise reduction in the queries, we remove all lowfrequency POS blocks from the narrative field of the queries.", "labels": [], "entities": []}, {"text": "Secondly, to test our second hypothesis as an extension of our first hypothesis, we refilter the queries used in the POS experiments by removing from them POS blocks that contain more closed class than open class tags.", "labels": [], "entities": []}, {"text": "The processes involved in both hypotheses take place prior to the removal of stop words and stemming of the queries.", "labels": [], "entities": []}, {"text": "displays the relevant evaluation results.", "labels": [], "entities": []}, {"text": "Overall, the removal of low-probability POS blocks from the queries (Hypothesis 1 section in) is associated with an improvement in retrieval performance over the baseline inmost cases, which sometimes is statistically significant.", "labels": [], "entities": []}, {"text": "This improvement is quite similar across the two statistical estimators.", "labels": [], "entities": []}, {"text": "Moreover, two interesting patterns emerge.", "labels": [], "entities": []}, {"text": "Firstly, the DFR weighting schemes seem to be divided, performance-wise, between the parametric BB2 and PL2, which are associated with the highest improvement in retrieval performance, and the non-parametric DLH, which is associated with the lowest improvement, or even deterioration in retrieval performance.", "labels": [], "entities": [{"text": "BB2", "start_pos": 96, "end_pos": 99, "type": "METRIC", "confidence": 0.5134776830673218}]}, {"text": "This may indicate that the parameter used in BB2 and PL2 is not optimal, which would explain a low baseline, and thus a very high improvement over it.", "labels": [], "entities": [{"text": "BB2", "start_pos": 45, "end_pos": 48, "type": "DATASET", "confidence": 0.5168444514274597}, {"text": "PL2", "start_pos": 53, "end_pos": 56, "type": "DATASET", "confidence": 0.6858618855476379}]}, {"text": "Secondly, when comparing the improvement in performance related to the WT2G and the WT10G test collections, we observe a more marked improvement in retrieval performance with WT2G than with WT10G.", "labels": [], "entities": [{"text": "WT2G", "start_pos": 71, "end_pos": 75, "type": "DATASET", "confidence": 0.9413773417472839}, {"text": "WT10G test collections", "start_pos": 84, "end_pos": 106, "type": "DATASET", "confidence": 0.9509351452191671}, {"text": "WT2G", "start_pos": 175, "end_pos": 179, "type": "DATASET", "confidence": 0.9171087741851807}, {"text": "WT10G", "start_pos": 190, "end_pos": 195, "type": "DATASET", "confidence": 0.9465839266777039}]}, {"text": "The combination of our two hypotheses (Hypotheses 1+2 section in) is associated with an improvement in retrieval performance over the baseline inmost cases, which sometimes is statistically significant.", "labels": [], "entities": []}, {"text": "This improvement is very similar across the two statistical estimators, namely Good-Turing and Laplace.", "labels": [], "entities": []}, {"text": "When combining hypotheses 1+2, retrieval performance improves more than it did for hypothesis 1 only, for the WT2G test collection, which indicates that our second hypothesis might further reduce the amount of noise in the queries successfully.", "labels": [], "entities": [{"text": "WT2G test collection", "start_pos": 110, "end_pos": 130, "type": "DATASET", "confidence": 0.9168187578519186}]}, {"text": "For the WT10G collection, we object similar results, with the exception of DLH.", "labels": [], "entities": [{"text": "WT10G collection", "start_pos": 8, "end_pos": 24, "type": "DATASET", "confidence": 0.9330919981002808}]}, {"text": "Generally, the improvement in performance associated to the WT2G test collection is more marked than the improvement associated to WT10G.", "labels": [], "entities": [{"text": "WT2G test collection", "start_pos": 60, "end_pos": 80, "type": "DATASET", "confidence": 0.8672017852465311}, {"text": "WT10G", "start_pos": 131, "end_pos": 136, "type": "DATASET", "confidence": 0.9554815292358398}]}, {"text": "To recapitulate on the evaluation outcomes of our two hypotheses, we report an improvement in retrieval performance over the baseline for most, but not all cases, which is sometimes statistically significant.", "labels": [], "entities": []}, {"text": "This maybe indicative of successful noise reduction in the queries, as per our hypotheses.", "labels": [], "entities": []}, {"text": "Also, the difference in the improvement in retrieval performance across the two test collections may suggest that data sparseness affects retrieval performance.", "labels": [], "entities": []}, {"text": "Query expansion (QE) is a performanceboosting technique often used in IR, which consists in extracting the most relevant terms from the top retrieved documents, and in using these terms to expand the initial query.", "labels": [], "entities": [{"text": "Query expansion (QE)", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8451508939266205}, {"text": "IR", "start_pos": 70, "end_pos": 72, "type": "TASK", "confidence": 0.9834609031677246}]}, {"text": "The expanded query is then used to retrieve documents anew.", "labels": [], "entities": []}, {"text": "Query expansion has the distinct property of improving retrieval performance when queries do not contain noise, but harming retrieval performance when queries contain noise, furnishing us with a strong baseline, against which we can measure our hypotheses.", "labels": [], "entities": [{"text": "Query expansion", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8885159194469452}]}, {"text": "We repeat the experiments described in Section 4.2.1 with query expansion.", "labels": [], "entities": []}, {"text": "We use the Bo1 query expansion scheme from the DFR framework.", "labels": [], "entities": [{"text": "DFR framework", "start_pos": 47, "end_pos": 60, "type": "DATASET", "confidence": 0.954238086938858}]}, {"text": "We optimise the query expansion settings, so as to maximise its performance.", "labels": [], "entities": []}, {"text": "This provides us with an even stronger baseline, against which we can compare our proposed technique, which we tune empirically too through the tuning of the threshold . We optimise query expansion on the basis of the corresponding relevance assessments available for the queries and collections employed, by selecting the most relevant terms from the top retrieved documents.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 182, "end_pos": 197, "type": "TASK", "confidence": 0.8386372923851013}]}, {"text": "For the WT2G test collection, the relevant terms / top retrieved documents ratio we use is (i) 20/5 with TF IDF, BM25, and DLH; (ii) 30/5 with PL2; and (iii) 10/5 with BB2.", "labels": [], "entities": [{"text": "WT2G test collection", "start_pos": 8, "end_pos": 28, "type": "DATASET", "confidence": 0.8034087618192037}, {"text": "BM25", "start_pos": 113, "end_pos": 117, "type": "DATASET", "confidence": 0.8435817956924438}, {"text": "BB2", "start_pos": 168, "end_pos": 171, "type": "DATASET", "confidence": 0.9516540765762329}]}, {"text": "For the WT10G collection, the said ratio is (i) 10/5 for TF IDF; (ii) 20/5 for BM25 and DLH; and (iii) 5/5 for PL2 and BB2.", "labels": [], "entities": [{"text": "WT10G collection", "start_pos": 8, "end_pos": 24, "type": "DATASET", "confidence": 0.8267650306224823}, {"text": "BM25", "start_pos": 79, "end_pos": 83, "type": "DATASET", "confidence": 0.8829363584518433}, {"text": "BB2", "start_pos": 119, "end_pos": 122, "type": "DATASET", "confidence": 0.8963398337364197}]}, {"text": "We repeat our POS and POSC retrieval experiments with query expansion.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 54, "end_pos": 69, "type": "TASK", "confidence": 0.7087516486644745}]}, {"text": "displays the relevant evaluation results.", "labels": [], "entities": []}, {"text": "Query expansion has overall improved retrieval performance (compare), for both test collections, with two exceptions, where query expansion has made no difference at all, namely for BB2 and PL2, with the WT10G collection.", "labels": [], "entities": [{"text": "Query expansion", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7769643366336823}, {"text": "WT10G collection", "start_pos": 204, "end_pos": 220, "type": "DATASET", "confidence": 0.9700759947299957}]}, {"text": "The removal of low-probability POS blocks from the queries, as per our first hypothesis, combined with query expansion, is associated with an im-  provement in retrieval performance over the new baseline at all times, which is sometimes statistically significant.", "labels": [], "entities": []}, {"text": "This may indicate that noise has been further reduced in the queries.", "labels": [], "entities": []}, {"text": "Also, the two statistical estimators lead to similar improvements in retrieval performance.", "labels": [], "entities": []}, {"text": "When we compare these results to the ones reported with identical settings but without query expansion, we observe the following.", "labels": [], "entities": []}, {"text": "Firstly, the previously reported division in the DFR weighting schemes, where BB2 and PL2 improved the most from our hypothesised noise reduction in the queries, while DLH improved the least, is no longer valid.", "labels": [], "entities": [{"text": "DFR weighting", "start_pos": 49, "end_pos": 62, "type": "TASK", "confidence": 0.5133309364318848}, {"text": "BB2", "start_pos": 78, "end_pos": 81, "type": "METRIC", "confidence": 0.9452912211418152}]}, {"text": "The improvement in retrieval performance now associated to DLH is similar to the improvement associated with the other weighting schemes.", "labels": [], "entities": []}, {"text": "Secondly, the difference in the retrieval improvement previously observed between the two test collections is now smaller.", "labels": [], "entities": [{"text": "retrieval improvement", "start_pos": 32, "end_pos": 53, "type": "METRIC", "confidence": 0.941327691078186}]}, {"text": "To recapitulate on the evaluation outcomes of our two hypotheses combined with query expansion, we report an improvement in retrieval performance over the baseline at all times, which is sometimes statistically significant.", "labels": [], "entities": []}, {"text": "It appears that the combination of our hypotheses with query expansion tones down previously reported sharp differences in retrieval improvements over the baseline, which maybe indicative of further noise reduction.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Mean Average Precision (MAP) scores of the POS and POSC experiments.  WT2G collection  Hypothesis 1  Hypotheses 1+2  w(t,d) base  POSGT", "labels": [], "entities": [{"text": "Mean Average Precision (MAP) scores", "start_pos": 10, "end_pos": 45, "type": "METRIC", "confidence": 0.9620370268821716}, {"text": "WT2G collection  Hypothesis", "start_pos": 80, "end_pos": 107, "type": "METRIC", "confidence": 0.5727684299151102}, {"text": "POSGT", "start_pos": 140, "end_pos": 145, "type": "TASK", "confidence": 0.30715644359588623}]}, {"text": " Table 3: Mean Average Precision (MAP) scores of the POS and POSC experiments with Query Expan- sion.  WT2G collection  Hypothesis 1  Hypotheses 1+2  w(t,d) base  POSGT", "labels": [], "entities": [{"text": "Mean Average Precision (MAP) scores", "start_pos": 10, "end_pos": 45, "type": "METRIC", "confidence": 0.9632976480892727}, {"text": "WT2G collection", "start_pos": 103, "end_pos": 118, "type": "DATASET", "confidence": 0.7517203688621521}, {"text": "POSGT", "start_pos": 163, "end_pos": 168, "type": "TASK", "confidence": 0.4275241196155548}]}]}