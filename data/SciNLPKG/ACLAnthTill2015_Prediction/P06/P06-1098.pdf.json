{"title": [{"text": "Left-to-Right Target Generation for Hierarchical Phrase-based Translation", "labels": [], "entities": [{"text": "Hierarchical Phrase-based Translation", "start_pos": 36, "end_pos": 73, "type": "TASK", "confidence": 0.806937555472056}]}], "abstractContent": [{"text": "We present a hierarchical phrase-based statistical machine translation in which a target sentence is efficiently generated in left-to-right order.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 39, "end_pos": 70, "type": "TASK", "confidence": 0.6389532188574473}]}, {"text": "The model is a class of synchronous-CFG with a Greibach Normal Form-like structure for the projected production rule: The paired target-side of a production rule takes a phrase prefixed form.", "labels": [], "entities": []}, {"text": "The decoder for the target-normalized form is based on an Early-style top down parser on the source side.", "labels": [], "entities": []}, {"text": "The target-normalized form coupled with our top down parser implies a left-to-right generation of translations which enables us a straightforward integration with ngram language models.", "labels": [], "entities": []}, {"text": "Our model was experimented on a Japanese-to-English newswire translation task, and showed statistically significant performance improvements against a phrase-based translation system.", "labels": [], "entities": [{"text": "Japanese-to-English newswire translation task", "start_pos": 32, "end_pos": 77, "type": "TASK", "confidence": 0.6930135115981102}, {"text": "phrase-based translation", "start_pos": 151, "end_pos": 175, "type": "TASK", "confidence": 0.6848321557044983}]}], "introductionContent": [{"text": "Ina classical statistical machine translation, a foreign language sentence f J 1 = f 1 , f 2 , ...", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 14, "end_pos": 45, "type": "TASK", "confidence": 0.6385843853155772}]}, {"text": "f J is translated into another language, i.e. English, e I 1 = e 1 , e 2 , ..., e I by seeking a maximum likely solution of: The source channel approach in Equation 2 independently decomposes translation knowledge into a translation model and a language model, respectively ().", "labels": [], "entities": []}, {"text": "The former represents the correspondence between two languages and the latter contributes to the fluency of English.", "labels": [], "entities": []}, {"text": "In the state of the art statistical machine translation, the posterior probability Pr(e I 1 | f J 1 ) is directly maximized using a log-linear combination of feature functions: where h m (e I 1 , f J 1 ) is a feature function, such as a ngram language model or a translation model.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 24, "end_pos": 55, "type": "TASK", "confidence": 0.6304726203282675}, {"text": "posterior probability Pr", "start_pos": 61, "end_pos": 85, "type": "METRIC", "confidence": 0.8652581373850504}]}, {"text": "When decoding, the denominator is dropped since it depends only on f J 1 . Feature function scaling factors \u03bb mare optimized based on a maximum likely approach) or on a direct error minimization approach.", "labels": [], "entities": []}, {"text": "This modeling allows the integration of various feature functions depending on the scenario of how a translation is constituted.", "labels": [], "entities": []}, {"text": "A phrase-based translation model is one of the modern approaches which exploits a phrase, a contiguous sequence of words, as a unit of translation ().", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 2, "end_pos": 26, "type": "TASK", "confidence": 0.6951585710048676}]}, {"text": "The idea is based on a word-based source channel modeling of: It assumes that e I 1 is segmented into a sequence of K phrases \u00af e K 1 . Each phrase \u00af e k is transformed into \u00af f k . The translated phrases are reordered to form f J 1 . One of the benefits of the modeling is that the phrase translation unit preserves localized word reordering.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 283, "end_pos": 301, "type": "TASK", "confidence": 0.7180595844984055}]}, {"text": "However, it cannot hypothesize a long-distance reordering required for linguistically divergent language pairs.", "labels": [], "entities": []}, {"text": "For instance, when translating Japanese to English, a Japanese SOV structure has to be reordered to match with an En-glish SVO structure.", "labels": [], "entities": []}, {"text": "Such a sentence-wise movement cannot be realized within the phrase-based modeling.", "labels": [], "entities": []}, {"text": "introduced a hierarchical phrasebased translation model that combined the strength of the phrase-based approach and a synchronous-CFG formalism): A rewrite system initiated from a start symbol which synchronously rewrites paired nonterminals.", "labels": [], "entities": [{"text": "phrasebased translation", "start_pos": 26, "end_pos": 49, "type": "TASK", "confidence": 0.7057763636112213}]}, {"text": "Their translation model is a binarized synchronous-CFG, or a rank-2 of synchronous-CFG, in which the right-hand side of a production rule contains at most two non-terminals.", "labels": [], "entities": []}, {"text": "The form can be regarded as a phrase translation pair with at most two holes instantiated with other phrases.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.7239835411310196}]}, {"text": "The hierarchically combined phrases provide a sort of reordering constraints that is not directly modeled by a phrase-based model.", "labels": [], "entities": []}, {"text": "Rules are induced from a bilingual corpus without linguistic clues first by extracting phrase translation pairs, and then by generalizing extracted phrases with holes.", "labels": [], "entities": []}, {"text": "Even in a phrase-based model, the number of phrases extracted from a bilingual corpus is quadratic to the length of bilingual sentences.", "labels": [], "entities": []}, {"text": "The grammar size for the hierarchical phrase-based model will be further exploded, since there exists numerous combination of inserting holes to each rule.", "labels": [], "entities": []}, {"text": "The spuriously increasing grammar size will be problematic for decoding without certain heuristics, such as a length based thresholding.", "labels": [], "entities": []}, {"text": "The integration with a ngram language model further increases the cost of decoding especially when incorporating a higher order ngram, such as 5-gram.", "labels": [], "entities": []}, {"text": "In the hierarchical phrase-based model, and an inversion transduction grammar (ITG), the problem is resolved by restricting to a binarized form whereat most two non-terminals are allowed in the righthand side.", "labels": [], "entities": []}, {"text": "However, reported that the computational complexity for decoding amounted to O(J 3+3(n\u22121) ) with n-gram even using a hook technique.", "labels": [], "entities": [{"text": "O", "start_pos": 77, "end_pos": 78, "type": "METRIC", "confidence": 0.9954975843429565}]}, {"text": "The complexity lies in memorizing the ngram's context for each constituent.", "labels": [], "entities": []}, {"text": "The order of ngram would be a dominant factor for higher order ngrams.", "labels": [], "entities": []}, {"text": "As an alternative to a binarized form, we present a target-normalized hierarchical phrasebased translation model.", "labels": [], "entities": [{"text": "phrasebased translation", "start_pos": 83, "end_pos": 106, "type": "TASK", "confidence": 0.6207393556833267}]}, {"text": "The model is a class of a hierarchical phrase-based model, but constrained so that the English part of the right-hand side is restricted to a Greibach Normal Form (GNF)-like structure: A contiguous sequence of terminals, or a phrase, is followed by a string of nonterminals.", "labels": [], "entities": []}, {"text": "The target-normalized form reduces the number of rules extracted from a bilingual corpus, but still preserves the strength of the phrase-based approach.", "labels": [], "entities": []}, {"text": "An integration with ngram language model is straightforward, since the model generates a translation in left-to-right order.", "labels": [], "entities": []}, {"text": "Our decoder is based on an Earley-style top down parsing on the foreign language side.", "labels": [], "entities": []}, {"text": "The projected English-side is generated in left-to-right order synchronized with the derivation of the foreign language side.", "labels": [], "entities": []}, {"text": "The decoder's implementation is taken after a decoder for an existing phrase-based model with a simple modification to account for production rules.", "labels": [], "entities": []}, {"text": "Experimental results on a Japanese-toEnglish newswire translation task showed significant improvement against a phrase-based modeling.", "labels": [], "entities": [{"text": "Japanese-toEnglish newswire translation task", "start_pos": 26, "end_pos": 70, "type": "TASK", "confidence": 0.6959817409515381}]}], "datasetContent": [{"text": "The bilingual corpus used for our experiments was obtained from an automatically sentence aligned Japanese/English Yomiuri newspaper corpus consisting of 180K sentence pairs (refer to).", "labels": [], "entities": [{"text": "Japanese/English Yomiuri newspaper corpus", "start_pos": 98, "end_pos": 139, "type": "DATASET", "confidence": 0.6215899537007014}]}, {"text": "From one-toone aligned sentences, 1,500 sentence pairs were sampled fora development set and a test set 1 . Since the bilingual corpus is rather small, especially for the newspaper translation domain, Japanese/English dictionaries consisting of 1.3M entries were added into a training set to alleviate an OOV problem 2 . Word alignments were annotated by a HMM translation model.", "labels": [], "entities": [{"text": "newspaper translation domain", "start_pos": 171, "end_pos": 199, "type": "TASK", "confidence": 0.7342398862044016}, {"text": "Word alignments", "start_pos": 321, "end_pos": 336, "type": "TASK", "confidence": 0.7210514545440674}, {"text": "HMM translation", "start_pos": 357, "end_pos": 372, "type": "TASK", "confidence": 0.6579396277666092}]}, {"text": "After the annotation via Viterbi alignments with refinements, phrases translation pairs and production rules were extracted (refer to).", "labels": [], "entities": []}, {"text": "We performed the rule extraction using the hierarchical phrase-based constraint (Hierarchical) and our proposed target-normalized form with 2 and 3 non-terminals (Normalized-2 and Normalized-3).", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 17, "end_pos": 32, "type": "TASK", "confidence": 0.807706892490387}]}, {"text": "Phrase translation pairs were also extracted for comparison (Phrase).", "labels": [], "entities": []}, {"text": "We did not threshold the extracted phrases or rules by their length.", "labels": [], "entities": []}, {"text": "Table 2 shows that Normalized-2 extracted slightly larger number of rules than those for phrasebased model.", "labels": [], "entities": []}, {"text": "Including three non-terminals did not increase the grammar size.", "labels": [], "entities": []}, {"text": "The hierarchical phrase-based translation model extracts twice as large as our target-normalized formalism.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 17, "end_pos": 41, "type": "TASK", "confidence": 0.5760658532381058}]}, {"text": "The target-normalized form is restrictive in that nonterminals should be consecutive for the Englishside.", "labels": [], "entities": []}, {"text": "This property prohibits spuriously extracted production rules.", "labels": [], "entities": []}, {"text": "Mixed-casing 3-gram/5-gram language models were estimated from LDC English GigaWord 2 together with the 100K English articles of Yomiuri newspaper that were used neither for development nor test sets 3 . We run the decoder for the target-normalized hierarchical phrase-based model consisting of at most two non-terminals, since adding rules with three non-terminals did not increase the grammar size.", "labels": [], "entities": [{"text": "LDC English GigaWord 2", "start_pos": 63, "end_pos": 85, "type": "DATASET", "confidence": 0.9048183262348175}, {"text": "Yomiuri newspaper", "start_pos": 129, "end_pos": 146, "type": "DATASET", "confidence": 0.9467507898807526}]}, {"text": "ITG-constraint simulated phrase-based rules were also included into our grammar.", "labels": [], "entities": [{"text": "ITG-constraint", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.7550258040428162}]}, {"text": "The foreign word span size was thresholded so that at least one non-terminal should span at most 7 words.", "labels": [], "entities": []}, {"text": "Our phrase-based model employed all feature functions for the hierarchical phrase-based system with additional feature functions: \u2022 A distortion model that penalizes the reordering of phrases by the number of words \u2022 Lexicalized reordering models constrain the reordering of phrases whether to favor monotone, swap or discontinuous positions).", "labels": [], "entities": []}, {"text": "The phrase-based decoder's reordering was constrained by ITG-constraints with a window size of The translation results are summarized in.", "labels": [], "entities": []}, {"text": "Two systems were contrasted by 3-gram and 5-gram language models.", "labels": [], "entities": []}, {"text": "Results were evaluated by ngram precision based metrics, BLEU and NIST, on the casing preserved single reference test set.", "labels": [], "entities": [{"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.6209307312965393}, {"text": "BLEU", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9938794374465942}, {"text": "NIST", "start_pos": 66, "end_pos": 70, "type": "DATASET", "confidence": 0.8116170763969421}]}, {"text": "Feature function scaling factors for each system were optimized on BLEU score under the development set using a downhill simplex method.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 67, "end_pos": 77, "type": "METRIC", "confidence": 0.9371359348297119}]}, {"text": "The differences of translation qualities are statistically significant at the 95% confidence level.", "labels": [], "entities": [{"text": "translation", "start_pos": 19, "end_pos": 30, "type": "TASK", "confidence": 0.9215363264083862}]}, {"text": "Although the figures presented in are rather low, we found that Normalized-2 resulted in statistically significant improvement over Phrase.", "labels": [], "entities": [{"text": "Phrase", "start_pos": 132, "end_pos": 138, "type": "DATASET", "confidence": 0.8117461800575256}]}, {"text": "shows some translation results from the test set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Japanese/English news corpus  Japanese  English  train sentence  175,384  dictionary  + 1,329,519  words  8,373,478 7,222,726  vocabulary  297,646  397,592  dev. sentence  1,500  words  47,081  39,117  OOV  45  149  test  sentence  1,500  words  47,033  38,707  OOV  51  127", "labels": [], "entities": [{"text": "Japanese/English news corpus  Japanese  English  train sentence  175,384  dictionary", "start_pos": 10, "end_pos": 94, "type": "DATASET", "confidence": 0.7019610946828668}, {"text": "OOV", "start_pos": 212, "end_pos": 215, "type": "METRIC", "confidence": 0.8658561110496521}, {"text": "OOV", "start_pos": 272, "end_pos": 275, "type": "METRIC", "confidence": 0.951190173625946}]}, {"text": " Table 2:  Phrases/rules extracted from the  Japanese/English bilingual corpus. Figures do not  include phrase-based rules.  # rules/phrases  Phrase  5,433,091  Normalized-2  6,225,630  Normalized-3  6,233,294  Hierarchical  12,824,387", "labels": [], "entities": []}, {"text": " Table 3: Results for the Japanese-to-English  newswire translation task.  BLEU NIST  [%]  Phrase  3-gram  7.14  3.21  5-gram  7.33  3.19  Normalized-2 3-gram 10.00  4.11  5-gram 10.26  4.20", "labels": [], "entities": [{"text": "Japanese-to-English  newswire translation task", "start_pos": 26, "end_pos": 72, "type": "TASK", "confidence": 0.6491971090435982}, {"text": "BLEU", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9992181062698364}, {"text": "NIST", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.6250239014625549}]}, {"text": " Table  3. Two systems were contrasted by 3-gram and 5- gram language models. Results were evaluated by  ngram precision based metrics, BLEU and NIST,  on the casing preserved single reference test set.  Feature function scaling factors for each system  were optimized on BLEU score under the devel- opment set using a downhill simplex method. The  differences of translation qualities are statistically  significant at the 95% confidence level", "labels": [], "entities": [{"text": "BLEU", "start_pos": 136, "end_pos": 140, "type": "METRIC", "confidence": 0.9952298402786255}, {"text": "NIST", "start_pos": 145, "end_pos": 149, "type": "DATASET", "confidence": 0.706790030002594}, {"text": "BLEU", "start_pos": 272, "end_pos": 276, "type": "METRIC", "confidence": 0.9884374737739563}]}]}