{"title": [{"text": "Learning More Effective Dialogue Strategies Using Limited Dialogue Move Features", "labels": [], "entities": []}], "abstractContent": [{"text": "We explore the use of restricted dialogue contexts in reinforcement learning (RL) of effective dialogue strategies for information seeking spoken dialogue systems (e.g. COMMUNICATOR (Walker et al., 2001)).", "labels": [], "entities": [{"text": "reinforcement learning (RL)", "start_pos": 54, "end_pos": 81, "type": "TASK", "confidence": 0.6997241973876953}]}, {"text": "The contexts we use are richer than previous research in this area, e.g. only slot-based information , but are much less complex than the full dialogue \"Information States\" explored in (Henderson et al., 2005), for which tractabe learning is an issue.", "labels": [], "entities": [{"text": "tractabe learning", "start_pos": 221, "end_pos": 238, "type": "TASK", "confidence": 0.8305190205574036}]}, {"text": "We explore how incrementally adding richer features allows learning of more effective dialogue strategies.", "labels": [], "entities": []}, {"text": "We use 2 user simulations learned from COMMUNICATOR data (Walker et al., 2001; Georgila et al., 2005b) to explore the effects of different features on learned dialogue strategies.", "labels": [], "entities": []}, {"text": "Our results show that adding the dialogue moves of the last system and user turns increases the average reward of the automatically learned strategies by 65.9% over the original (hand-coded) COMMUNICATOR systems, and by 7.8% over a base-line RL policy that uses only slot-status features.", "labels": [], "entities": []}, {"text": "We show that the learned strategies exhibit an emergent \"focus switch-ing\" strategy and effective use of the 'give help' action.", "labels": [], "entities": []}], "introductionContent": [{"text": "Reinforcement Learning (RL) applied to the problem of dialogue management attempts to find optimal mappings from dialogue contexts to system actions.", "labels": [], "entities": [{"text": "Reinforcement Learning (RL)", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7769524097442627}, {"text": "dialogue management", "start_pos": 54, "end_pos": 73, "type": "TASK", "confidence": 0.8215818703174591}]}, {"text": "The idea of using Markov Decision Processes (MDPs) and reinforcement learning to design dialogue strategies for dialogue systems was first proposed by).", "labels": [], "entities": []}, {"text": "There, and in subsequent work such as (), only very limited state information was used in strategy learning, based always on the number and status of filled information slots in the application (e.g. departure-city is filled, destination-city is unfilled).", "labels": [], "entities": [{"text": "strategy learning", "start_pos": 90, "end_pos": 107, "type": "TASK", "confidence": 0.8629266321659088}]}, {"text": "This we refer to as low-level contextual information.", "labels": [], "entities": []}, {"text": "Much prior work () concentrated only on specific strategy decisions (e.g. confirmation and initiative strategies), rather than the full problem of what system dialogue move to take next.", "labels": [], "entities": []}, {"text": "The simple strategies learned for low-level definitions of state cannot be sensitive to (sometimes critical) aspects of the dialogue context, such as the user's last dialogue move (DM) (e.g. requesthelp) unless that move directly affects the status of an information slot (e.g. provide-info(destinationcity)).", "labels": [], "entities": []}, {"text": "We refer to additional contextual information such as the system and user's last dialogue moves as high-level contextual information.) learned full strategies with limited 'high-level' information (i.e. the dialogue move(s) of the last user utterance) and only used a stochastic user simulation whose probabilities were supplied via commonsense and intuition, rather than learned from data.", "labels": [], "entities": []}, {"text": "This paper uses data-driven n-gram user simulations () and a richer dialogue context.", "labels": [], "entities": []}, {"text": "On the other hand, increasing the size of the state space for RL has the danger of making the learning problem intractable, and at the very least means that data is more sparse and state approximation methods may need to be used).", "labels": [], "entities": []}, {"text": "To date, the use of very large state spaces relies on a \"hybrid\" supervised/reinforcement learning technique, where the reinforcement learning element has not yet been shown to significantly improve policies over the purely supervised case ().", "labels": [], "entities": []}, {"text": "The extended state spaces that we propose are based on theories of dialogue such as, where which actions a dialogue participant can or should take next are not based solely on the task-state (i.e. in our domain, which slots are filled), but also on wider contextual factors such as a user's dialogue moves or speech acts.", "labels": [], "entities": []}, {"text": "In future work we also intend to use feature selection techniques (e.g. correlation-based feature subset (CFS) evaluation ()) on the COMMUNICATOR data () in order to identify additional context features that it maybe effective to represent in the state.", "labels": [], "entities": [{"text": "COMMUNICATOR data", "start_pos": 133, "end_pos": 150, "type": "DATASET", "confidence": 0.7523735761642456}]}], "datasetContent": [{"text": "Each experiment is executed using the DIPPER Information State Update dialogue manager () (which here is used to track and update dialogue context rather than deciding which actions to take), a Reinforcement Learning program (which determines the next dialogue action to take), and various user simulations.", "labels": [], "entities": []}, {"text": "In sections 2.3 and 2.4 we give more details about the reinforcement learner and user simulations.", "labels": [], "entities": []}, {"text": "First we learned strategies with the 4-gram user simulation and tested with the 5-gram simulation, and then did the reverse.", "labels": [], "entities": []}, {"text": "We experimented with different feature sets, exploring whether better strategies could be learned by adding limited context features.", "labels": [], "entities": []}, {"text": "We used two baselines for comparison: \u2022 The performance of the original COMMUNI-CATOR systems in the data set ().", "labels": [], "entities": []}, {"text": "\u2022 An RL baseline dialogue manager learned using only slot-status features i.e. for each of slots 1 \u2212 4, is the slot empty, filled or confirmed?", "labels": [], "entities": [{"text": "RL baseline dialogue manager", "start_pos": 5, "end_pos": 33, "type": "TASK", "confidence": 0.7756322622299194}]}, {"text": "We then learned two further strategies: \u2022 Strategy 2 (UDM) was learned by adding the user's last dialogue move to the state.", "labels": [], "entities": []}, {"text": "\u2022 Strategy 3 (USDM) was learned by adding both the user and system's last dialogue moves to the state.", "labels": [], "entities": [{"text": "Strategy 3 (USDM)", "start_pos": 2, "end_pos": 19, "type": "METRIC", "confidence": 0.6069192945957184}]}, {"text": "The possible system and user dialogue moves were those given in sections 2.1 and 2.4 respectively, and the reward function was that described in section 2.2.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Testing the learned strategies after 50000 training dialogues, average reward achieved per dia- logue over 1000 test dialogues.", "labels": [], "entities": []}]}