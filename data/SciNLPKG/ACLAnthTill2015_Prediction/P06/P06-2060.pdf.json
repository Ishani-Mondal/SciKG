{"title": [{"text": "Minority Vote: At-Least-N Voting Improves Recall for Extracting Relations", "labels": [], "entities": [{"text": "Voting Improves Recall", "start_pos": 26, "end_pos": 48, "type": "TASK", "confidence": 0.8243366678555807}]}], "abstractContent": [{"text": "Several NLP tasks are characterized by asymmetric data where one class label NONE, signifying the absence of any structure (named entity, coreference, relation , etc.) dominates all other classes.", "labels": [], "entities": []}, {"text": "Classifiers built on such data typically have a higher precision and a lower recall and tend to overproduce the NONE class.", "labels": [], "entities": [{"text": "precision", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.998994767665863}, {"text": "recall", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.9994912147521973}, {"text": "NONE class", "start_pos": 112, "end_pos": 122, "type": "DATASET", "confidence": 0.7833400070667267}]}, {"text": "We present a novel scheme for voting among a committee of classifiers that can significantly boost the recall in such situations.", "labels": [], "entities": [{"text": "recall", "start_pos": 103, "end_pos": 109, "type": "METRIC", "confidence": 0.9986959099769592}]}, {"text": "We demonstrate results showing up to a 16% relative improvement in ACE value for the 2004 ACE relation extraction task for English, Arabic and Chi-nese.", "labels": [], "entities": [{"text": "ACE value", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.9580453634262085}, {"text": "ACE relation extraction task", "start_pos": 90, "end_pos": 118, "type": "TASK", "confidence": 0.7071178108453751}]}], "introductionContent": [{"text": "Statistical classifiers are widely used for diverse NLP applications such as part of speech tagging), chunking (), semantic parsing, named entity extraction), coreference resolution (), relation extraction), etc.", "labels": [], "entities": [{"text": "speech tagging", "start_pos": 85, "end_pos": 99, "type": "TASK", "confidence": 0.7873883247375488}, {"text": "semantic parsing", "start_pos": 115, "end_pos": 131, "type": "TASK", "confidence": 0.7116000205278397}, {"text": "named entity extraction", "start_pos": 133, "end_pos": 156, "type": "TASK", "confidence": 0.652601401011149}, {"text": "coreference resolution", "start_pos": 159, "end_pos": 181, "type": "TASK", "confidence": 0.9358038902282715}, {"text": "relation extraction", "start_pos": 186, "end_pos": 205, "type": "TASK", "confidence": 0.8876710534095764}]}, {"text": "A number of these applications are characterized by a dominance of a NONE class in the training examples.", "labels": [], "entities": []}, {"text": "For example, for coreference resolution, classifiers might classify whether a given pair of mentions are references to the same entity or not.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 17, "end_pos": 39, "type": "TASK", "confidence": 0.9771808683872223}]}, {"text": "In this case, we typically have a lot more examples of mention pairs that are not coreferential (i.e. the NONE class) than otherwise.", "labels": [], "entities": []}, {"text": "Similarly, if a classifier is predicting the presence/absence of a semantic relation between two mentions, there are typically far more examples signifying an absence of a relation.", "labels": [], "entities": [{"text": "predicting the presence/absence of a semantic relation between two mentions", "start_pos": 30, "end_pos": 105, "type": "TASK", "confidence": 0.6761935651302338}]}, {"text": "Classifiers built with asymmetric data dominated by one class (a NONE class donating absence of a relation or coreference or a named entity etc.) can overgenerate the NONE class.", "labels": [], "entities": []}, {"text": "This often results in a unbalanced classifier where precision is higher than recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.9993371367454529}, {"text": "recall", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.9981001019477844}]}, {"text": "In this paper, we present a novel approach for improving the recall of such classifiers by using anew voting scheme from a committee of classifiers.", "labels": [], "entities": [{"text": "recall", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9961203932762146}]}, {"text": "There area plethora of algorithms for combining classifiers (e.g. see ().", "labels": [], "entities": []}, {"text": "A widely used approach is a majority voting scheme, where each classifier in the committee gets a vote and the class with the largest number of votes 'wins' (i.e. the corresponding class is output as the prediction of the committee).", "labels": [], "entities": []}, {"text": "We are interested in improving overall recall and reduce the overproduction of the class NONE.", "labels": [], "entities": [{"text": "recall", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9883516430854797}, {"text": "NONE", "start_pos": 89, "end_pos": 93, "type": "DATASET", "confidence": 0.6993238925933838}]}, {"text": "Our scheme predicts the class label C obtaining the second highest number of votes when NONE gets the highest number of votes, provided C gets at least N votes.", "labels": [], "entities": [{"text": "NONE", "start_pos": 88, "end_pos": 92, "type": "DATASET", "confidence": 0.6836594939231873}]}, {"text": "Thus, we predict a label other than NONE when there is some evidence of the presense of the structure we are looking for (relations, coreference, named entities, etc.) even in the absense of a clear majority.", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "In section 2, we give an overview of the various schemes for combining classifiers.", "labels": [], "entities": []}, {"text": "In section 3, we present our vot-ing algorithm.", "labels": [], "entities": []}, {"text": "In section 4, we describe the ACE relation extraction task.", "labels": [], "entities": [{"text": "ACE relation extraction task", "start_pos": 30, "end_pos": 58, "type": "TASK", "confidence": 0.8747150003910065}]}, {"text": "In section 5, we present empirical results for relation extraction and we discuss our results and conclude in section 6.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.9271437227725983}]}], "datasetContent": [{"text": "In this section, we present results of experiments comparing three different methods of combining classifiers for ACE relation extraction: \u2022 At-Least-N for different values of N, \u2022 Majority Voting, and \u2022 a simple algorithm, called summing, where we add the posterior scores for each class from all the classifiers and select the class with the maximum summed score.", "labels": [], "entities": [{"text": "ACE relation extraction", "start_pos": 114, "end_pos": 137, "type": "TASK", "confidence": 0.9071742097536722}, {"text": "summing", "start_pos": 231, "end_pos": 238, "type": "TASK", "confidence": 0.9881675839424133}]}, {"text": "Since the official ACE evaluation set is not publicly available, to facilitate comparison with our results and for internal testing of our algorithms, for each language (English, Arabic, and Chinese), we summarizes the number of documents and the number of relation mentions in each data set.", "labels": [], "entities": [{"text": "ACE evaluation set", "start_pos": 19, "end_pos": 37, "type": "DATASET", "confidence": 0.8244763414065043}]}, {"text": "The test sets were deliberately chosen to be the most recent 25% of documents in chronological order, since entities and relations in news tend to repeat and random shuffles can greatly reduce the out-of-vocabulary problem.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1 shows the set of relation types, subtypes,  and their frequency counts in the training data for the  2004 ACE evaluation. For training classifiers, the  great paucity of positive training events (where rela- tions exist) compared to the negative events (where", "labels": [], "entities": [{"text": "2004 ACE evaluation", "start_pos": 110, "end_pos": 129, "type": "DATASET", "confidence": 0.6515228549639384}]}, {"text": " Table 2: The Division of LDC annotated data into  training and development test sets.", "labels": [], "entities": [{"text": "Division of LDC annotated", "start_pos": 14, "end_pos": 39, "type": "TASK", "confidence": 0.8191636204719543}]}, {"text": " Table 3: Comparing the best F-measure obtained by  At-Least-N Voting with Majority Voting, Summing  and the single best classifier.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9849869608879089}, {"text": "Summing", "start_pos": 92, "end_pos": 99, "type": "METRIC", "confidence": 0.602641761302948}]}, {"text": " Table 4: Comparing the ACE Value obtained by At- Least-N Voting with the single best classifier for the  operating points used in", "labels": [], "entities": [{"text": "ACE Value", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9479623138904572}]}]}