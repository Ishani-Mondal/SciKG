{"title": [{"text": "A Progressive Feature Selection Algorithm for Ultra Large Feature Spaces", "labels": [], "entities": []}], "abstractContent": [{"text": "Recent developments in statistical modeling of various linguistic phenomena have shown that additional features give consistent performance improvements.", "labels": [], "entities": []}, {"text": "Quite often, improvements are limited by the number of features a system is able to explore.", "labels": [], "entities": []}, {"text": "This paper describes a novel progressive training algorithm that selects features from virtually unlimited feature spaces for conditional maximum entropy (CME) modeling.", "labels": [], "entities": [{"text": "conditional maximum entropy (CME) modeling", "start_pos": 126, "end_pos": 168, "type": "TASK", "confidence": 0.5895994901657104}]}, {"text": "Experimental results in edit region identification demonstrate the benefits of the progressive feature selection (PFS) algorithm: the PFS algorithm maintains the same accuracy performance as previous CME feature selection algorithms (e.g., Zhou et al., 2003) when the same feature spaces are used.", "labels": [], "entities": [{"text": "edit region identification", "start_pos": 24, "end_pos": 50, "type": "TASK", "confidence": 0.6646642883618673}, {"text": "progressive feature selection (PFS)", "start_pos": 83, "end_pos": 118, "type": "TASK", "confidence": 0.7337399423122406}, {"text": "accuracy", "start_pos": 167, "end_pos": 175, "type": "METRIC", "confidence": 0.9979165196418762}, {"text": "CME feature selection", "start_pos": 200, "end_pos": 221, "type": "TASK", "confidence": 0.5952331920464834}]}, {"text": "When additional features and their combinations are used, the PFS gives 17.66% relative improvement over the previously reported best result in edit region identification on Switchboard corpus (Kahn et al., 2005), which leads to a 20% relative error reduction in parsing the Switchboard corpus when gold edits are used as the upper bound.", "labels": [], "entities": [{"text": "PFS", "start_pos": 62, "end_pos": 65, "type": "METRIC", "confidence": 0.48176145553588867}, {"text": "edit region identification", "start_pos": 144, "end_pos": 170, "type": "TASK", "confidence": 0.6501666307449341}, {"text": "Switchboard corpus", "start_pos": 174, "end_pos": 192, "type": "DATASET", "confidence": 0.8715440034866333}, {"text": "Switchboard corpus", "start_pos": 275, "end_pos": 293, "type": "DATASET", "confidence": 0.6985842734575272}]}], "introductionContent": [{"text": "Conditional Maximum Entropy (CME) modeling has received a great amount of attention within natural language processing community for the past decade (e.g.,;.", "labels": [], "entities": [{"text": "Conditional Maximum Entropy (CME) modeling", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.6830052435398102}]}, {"text": "One of the main advantages of CME modeling is the ability to incorporate a variety of features in a uniform framework with a sound mathematical foundation.", "labels": [], "entities": [{"text": "CME modeling", "start_pos": 30, "end_pos": 42, "type": "TASK", "confidence": 0.916520357131958}]}, {"text": "Recent improvements on the original incremental feature selection (IFS) algorithm, such as and, greatly speedup the feature selection process.", "labels": [], "entities": [{"text": "feature selection (IFS)", "start_pos": 48, "end_pos": 71, "type": "TASK", "confidence": 0.6923389554023742}, {"text": "feature selection", "start_pos": 116, "end_pos": 133, "type": "TASK", "confidence": 0.7816656529903412}]}, {"text": "However, like many other statistical modeling algorithms, such as boosting () and support vector machine, the algorithm is limited by the size of the defined feature space.", "labels": [], "entities": []}, {"text": "Past results show that larger feature spaces tend to give better results.", "labels": [], "entities": []}, {"text": "However, finding away to include an unlimited amount of features is still an open research problem.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel progressive feature selection (PFS) algorithm that addresses the feature space size limitation.", "labels": [], "entities": [{"text": "progressive feature selection (PFS)", "start_pos": 34, "end_pos": 69, "type": "TASK", "confidence": 0.7749972442785898}]}, {"text": "The algorithm is implemented on top of the Selective Gain Computation (SGC) algorithm (, which offers fast training and high quality models.", "labels": [], "entities": []}, {"text": "Theoretically, the new algorithm is able to explore an unlimited amount of features.", "labels": [], "entities": []}, {"text": "Because of the improved capability of the CME algorithm, we are able to consider many new features and feature combinations during model construction.", "labels": [], "entities": []}, {"text": "To demonstrate the effectiveness of our new algorithm, we conducted a number of experiments on the task of identifying edit regions, a practical task in spoken language processing.", "labels": [], "entities": []}, {"text": "Based on the convention from and, a disfluent spoken utterance is divided into three parts: the reparandum, the part that is repaired; the inter-regnum, which can be filler words or empty; and the repair/repeat, the part that replaces or repeats the reparandum.", "labels": [], "entities": []}, {"text": "The first two parts combined are called an editor edit region.", "labels": [], "entities": []}, {"text": "An example is shown below: interregnum It is, you know, this is a tough problem.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we will demonstrate the benefits of the PFS algorithm for identifying edit regions.", "labels": [], "entities": []}, {"text": "The main reason that we use this task is that the edit region detection task uses features from several levels, including prosodic, lexical, and syntactic ones.", "labels": [], "entities": [{"text": "edit region detection task", "start_pos": 50, "end_pos": 76, "type": "TASK", "confidence": 0.7299741506576538}]}, {"text": "It presents a big challenge to find a set of good features from a huge feature space.", "labels": [], "entities": []}, {"text": "First we will present the additional features that the PFS algorithm allows us to include.", "labels": [], "entities": []}, {"text": "Then, we will briefly introduce the variant of the Switchboard corpus used in the experiments.", "labels": [], "entities": [{"text": "Switchboard corpus", "start_pos": 51, "end_pos": 69, "type": "DATASET", "confidence": 0.8763335347175598}]}, {"text": "Finally, we will compare results from two variants of the PFS algorithm.", "labels": [], "entities": []}, {"text": "The best result on the UW Switchboard for edit region identification uses a TAG-based approach ( ).", "labels": [], "entities": [{"text": "UW Switchboard", "start_pos": 23, "end_pos": 37, "type": "DATASET", "confidence": 0.9315734505653381}, {"text": "edit region identification", "start_pos": 42, "end_pos": 68, "type": "TASK", "confidence": 0.721367597579956}, {"text": "TAG-based", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.9388218522071838}]}, {"text": "On the original Switchboard corpus, reported nearly 20% better results using the boosting method with a much larger feature space 2 . To allow comparison with the best past results, we create anew CME baseline with the same set of features as that used in.", "labels": [], "entities": [{"text": "Switchboard corpus", "start_pos": 16, "end_pos": 34, "type": "DATASET", "confidence": 0.8581319749355316}]}, {"text": "We design a number of experiments to test the following hypotheses: 1.", "labels": [], "entities": []}, {"text": "PFS can include a huge number of new features, which leads to an overall performance improvement.", "labels": [], "entities": [{"text": "PFS", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7904658913612366}]}, {"text": "2. Richer context, represented by the combinations of different variables, has a positive impact on performance.", "labels": [], "entities": []}, {"text": "3. When the same feature space is used, PFS performs equally well as the original SGC algorithm.", "labels": [], "entities": []}, {"text": "The new models from the PFS algorithm are trained on the training data and tuned on the development data.", "labels": [], "entities": []}, {"text": "The results of our experiments on the test data are summarized in.", "labels": [], "entities": []}, {"text": "The first three lines show that the TAG-based approach is outperformed by the new CME baseline (line 3) using all the features in  CME is significantly smaller than the reported results using the boosting method.", "labels": [], "entities": [{"text": "TAG-based", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.581289529800415}]}, {"text": "In other words, using CME instead of boosting incurs a performance hit.", "labels": [], "entities": []}, {"text": "The next four lines in show that additional combinations of the feature variables used in give an absolute improvement of more than 1%.", "labels": [], "entities": []}, {"text": "This improvement is realized through increasing the search space to more than 20 million features, 8 times the maximum size that the original boosting and CME algorithms are able to handle.", "labels": [], "entities": []}, {"text": "shows that prosody labels alone make no difference in performance.", "labels": [], "entities": []}, {"text": "Instead, for each position in the sentence, we compute the entropy of the distribution of the labels' confidence scores.", "labels": [], "entities": []}, {"text": "We normalize the entropy to the range, according to the formula below: Including this feature does result in a good improvement.", "labels": [], "entities": []}, {"text": "In the table, cut2 means that we equally divide the feature scores into 10 buckets and any number below 0.2 is ignored.", "labels": [], "entities": []}, {"text": "The total contribution from the combined feature variables leads to a 1.9% absolute improvement.", "labels": [], "entities": []}, {"text": "This confirms the first two hypotheses.", "labels": [], "entities": []}, {"text": "When Gaussian smoothing), labeled as +Gau, and postprocessing (), labeled as +post, are added, we observe 17.66% relative improvement (or 3.85% absolute) over the previous best f-score of 78.2 from . To test hypothesis 3, we are constrained to the feature spaces that both PFS and SGC algorithms can process.", "labels": [], "entities": []}, {"text": "Therefore, we take all the variables from as the feature space for the experiments.", "labels": [], "entities": []}, {"text": "The results are listed in  The last set of experiments for edit identification is designed to find out what split strategies PFS algorithm should adopt in order to obtain good results.", "labels": [], "entities": [{"text": "edit identification", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.8374920785427094}]}, {"text": "Two different split strategies are tested here.", "labels": [], "entities": []}, {"text": "In all the experiments reported so far, we use 10 random splits, i.e., all the features are randomly assigned to 10 subsets of equal size.", "labels": [], "entities": []}, {"text": "We may also envision a split strategy that divides the features based on feature variables (or dimensions), such as word-based, tag-based, etc.", "labels": [], "entities": []}, {"text": "The four dimensions used in the experiments are listed as the top categories in, and the results are given in  In, the first two columns show criteria for splitting feature spaces and the number of features to be allocated for each group.", "labels": [], "entities": []}, {"text": "Random and Dimension mean random-split and dimension-based-split, respectively.", "labels": [], "entities": []}, {"text": "When the criterion is Random, the features are allocated to different groups randomly, and each group gets the same number of features.", "labels": [], "entities": []}, {"text": "In the case of dimensionbased split, we determine the number of features allocated for each dimension in two ways.", "labels": [], "entities": []}, {"text": "When the split is Uniform, the same number of features is allocated for each dimension.", "labels": [], "entities": []}, {"text": "When the split is Prior, the number of features to be allocated in each dimension is determined in proportion to the importance of each dimension.", "labels": [], "entities": []}, {"text": "To determine the importance, we use the distribution of the selected features from each dimension in the model \"+ HTag + HTagComb + WTComb + RCComb + PComb: cut2\", namely: Word-based 15%, Tag-based 70%, RoughCopy-based 7.5% and Prosody-based 7.5% . From the results, we can see no significant difference between the random-split and the dimension-based-split.", "labels": [], "entities": [{"text": "RoughCopy-based", "start_pos": 203, "end_pos": 218, "type": "DATASET", "confidence": 0.919122040271759}]}, {"text": "To see whether the improvements are translated into parsing results, we have conducted one more set of experiments on the UW Switchboard corpus.", "labels": [], "entities": [{"text": "parsing", "start_pos": 52, "end_pos": 59, "type": "TASK", "confidence": 0.9700983166694641}, {"text": "UW Switchboard corpus", "start_pos": 122, "end_pos": 143, "type": "DATASET", "confidence": 0.9607453544934591}]}, {"text": "We apply the latest version of Charniak's parser) and the same procedure as and  to the output from our best edit detector in this paper.", "labels": [], "entities": []}, {"text": "To make it more comparable with the results in , we repeat the same experiment with the gold edits, using the latest parser.", "labels": [], "entities": []}, {"text": "Both results are listed in.", "labels": [], "entities": []}, {"text": "The difference between our best detector and the gold edits in parsing (1.51%) is smaller than the difference between the TAG-based detector and the gold edits (1.9%).", "labels": [], "entities": []}, {"text": "In other words, if we use the gold edits as the upper bound, we see a relative error reduction of 20.5%.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 79, "end_pos": 94, "type": "METRIC", "confidence": 0.9647636413574219}]}], "tableCaptions": [{"text": " Table 4. Summary of experimental results with PFS.", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.6218873262405396}, {"text": "PFS", "start_pos": 47, "end_pos": 50, "type": "DATASET", "confidence": 0.7745482325553894}]}, {"text": " Table 5. Comparison between PFS and SGC with  all the variables from Zhang and Weng (2005).", "labels": [], "entities": []}, {"text": " Table 6. Comparison of split strategies using feature space", "labels": [], "entities": []}, {"text": " Table 7.  The difference between our best detector and the  gold edits in parsing (1.51%) is smaller than the  difference between the TAG-based detector and  the gold edits (1.9%). In other words, if we use  the gold edits as the upper bound, we see a rela- tive error reduction of 20.5%.", "labels": [], "entities": [{"text": "rela- tive error reduction", "start_pos": 253, "end_pos": 279, "type": "METRIC", "confidence": 0.9547916531562806}]}, {"text": " Table 7. Parsing F-score various different edit  region identification results.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.6945610046386719}, {"text": "F-score", "start_pos": 18, "end_pos": 25, "type": "METRIC", "confidence": 0.5807160139083862}]}]}