{"title": [{"text": "Exact Decoding for Jointly Labeling and Chunking Sequences", "labels": [], "entities": [{"text": "Exact Decoding", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8191448748111725}, {"text": "Jointly Labeling", "start_pos": 19, "end_pos": 35, "type": "TASK", "confidence": 0.6982997357845306}, {"text": "Chunking Sequences", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.7057870924472809}]}], "abstractContent": [{"text": "There are two decoding algorithms essential to the area of natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 59, "end_pos": 86, "type": "TASK", "confidence": 0.6446931064128876}]}, {"text": "One is the Viterbi algorithm for linear-chain models, such as HMMs or CRFs.", "labels": [], "entities": []}, {"text": "The other is the CKY algorithm for probabilistic context free grammars.", "labels": [], "entities": []}, {"text": "However, tasks such as noun phrase chunking and relation extraction seem to fall between the two, neither of them being the best fit.", "labels": [], "entities": [{"text": "noun phrase chunking", "start_pos": 23, "end_pos": 43, "type": "TASK", "confidence": 0.7514641284942627}, {"text": "relation extraction", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.9087496101856232}]}, {"text": "Ideally we would like to model entities and relations, with two layers of labels.", "labels": [], "entities": []}, {"text": "We present a tractable algorithm for exact inference over two layers of labels and chunks with time complexity O(n 2), and provide empirical results comparing our model with linear-chain models .", "labels": [], "entities": []}], "introductionContent": [{"text": "The Viterbi algorithm and the CKY algorithms are two decoding algorithms essential to the area of natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 98, "end_pos": 125, "type": "TASK", "confidence": 0.6521633366743723}]}, {"text": "The former models a linear chain of labels such as part of speech tags, and the latter models a parse tree.", "labels": [], "entities": []}, {"text": "Both are used to extract the best prediction from the model ().", "labels": [], "entities": []}, {"text": "However, some tasks seem to fall between the two, having more than one layer but flatter than the trees created by parsers.", "labels": [], "entities": []}, {"text": "For example, in relation extraction, we have entities in one layer and relations between entities as another layer.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.8221736252307892}]}, {"text": "Another task is shallow parsing.", "labels": [], "entities": [{"text": "shallow parsing", "start_pos": 16, "end_pos": 31, "type": "TASK", "confidence": 0.6942988932132721}]}, {"text": "We may want to model part-ofspeech tags and noun/verb chunks at the same time, since performing simultaneous labeling may result in increased joint accuracy by sharing information between the two layers of labels.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 148, "end_pos": 156, "type": "METRIC", "confidence": 0.9445907473564148}]}, {"text": "To apply the Viterbi decoder to such tasks, we need two models, one for each layer.", "labels": [], "entities": []}, {"text": "We must feed the output of one layer to the next layer.", "labels": [], "entities": []}, {"text": "In such an approach, errors in earlier processing nearly always accumulate and produce erroneous results at the end.", "labels": [], "entities": []}, {"text": "If we use CKY, we usually end up flattening the output tree to obtain the desired output.", "labels": [], "entities": []}, {"text": "This seems like a round-about way of modeling two layers.", "labels": [], "entities": []}, {"text": "There are previous attempts at modeling two layer labeling.", "labels": [], "entities": [{"text": "two layer labeling", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.5802379747231802}]}, {"text": "Dynamic Conditional Random Fields (DCRFs) by) is one such attempt, however, exact inference is in general intractable for these models and the authors were forced to settle for approximate inference.", "labels": [], "entities": []}, {"text": "Our contribution is a novel model for two layer labeling, for which exact decoding is tractable.", "labels": [], "entities": [{"text": "two layer labeling", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.5271339615186056}]}, {"text": "Our experiments show that our use of label-chunk structures results in significantly better performance over cascaded CRFs, and that the model is a promising alternative to DCRFs.", "labels": [], "entities": []}, {"text": "The paper is organaized a follows: In Section 2 and 3, we describe the model and present the decoding algorithm.", "labels": [], "entities": []}, {"text": "Section 4 describes the learning methods applicable to our model and the baseline models.", "labels": [], "entities": []}, {"text": "In Section 5 and 6, we describe the experiments and the results.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 5: Rules to create t q for each token x q", "labels": [], "entities": []}, {"text": " Table 6: Lexicalized Features for Joint Models", "labels": [], "entities": []}, {"text": " Table 8: Iterations needed for the result", "labels": [], "entities": []}]}