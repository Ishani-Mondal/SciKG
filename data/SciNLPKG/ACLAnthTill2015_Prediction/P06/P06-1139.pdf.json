{"title": [{"text": "Stochastic Language Generation Using WIDL-expressions and its Application in Machine Translation and Summarization", "labels": [], "entities": [{"text": "Stochastic Language Generation", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7654391328493754}, {"text": "Machine Translation and Summarization", "start_pos": 77, "end_pos": 114, "type": "TASK", "confidence": 0.7859914153814316}]}], "abstractContent": [{"text": "We propose WIDL-expressions as a flexible formalism that facilitates the integration of a generic sentence realization system within end-to-end language processing applications.", "labels": [], "entities": []}, {"text": "WIDL-expressions represent compactly probability distributions over finite sets of candidate realizations, and have optimal algorithms for realization via interpolation with language model probability distributions.", "labels": [], "entities": []}, {"text": "We show the effectiveness of a WIDL-based NLG system in two sentence realization tasks: automatic translation and headline generation.", "labels": [], "entities": [{"text": "sentence realization", "start_pos": 60, "end_pos": 80, "type": "TASK", "confidence": 0.7340988963842392}, {"text": "automatic translation", "start_pos": 88, "end_pos": 109, "type": "TASK", "confidence": 0.6491382122039795}, {"text": "headline generation", "start_pos": 114, "end_pos": 133, "type": "TASK", "confidence": 0.8294674456119537}]}], "introductionContent": [{"text": "The Natural Language Generation (NLG) community has produced over the years a considerable number of generic sentence realization systems: Penman, FUF,, Fergus), HALogen), Amalgam), etc.", "labels": [], "entities": [{"text": "Natural Language Generation (NLG)", "start_pos": 4, "end_pos": 37, "type": "TASK", "confidence": 0.7703476846218109}, {"text": "generic sentence realization", "start_pos": 101, "end_pos": 129, "type": "TASK", "confidence": 0.6989565292994181}, {"text": "Penman, FUF,, Fergus)", "start_pos": 139, "end_pos": 160, "type": "DATASET", "confidence": 0.7099728754588536}]}, {"text": "However, when it comes to end-to-end, text-totext applications -Machine Translation, Summarization, Question Answering -these generic systems either cannot be employed, or, in instances where they can be, the results are significantly below that of state-of-the-art, application-specific systems (.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.7265584617853165}, {"text": "Question Answering", "start_pos": 100, "end_pos": 118, "type": "TASK", "confidence": 0.8134109079837799}]}, {"text": "We believe two reasons explain this state of affairs.", "labels": [], "entities": []}, {"text": "First, these generic NLG systems use input representation languages with complex syntax and semantics.", "labels": [], "entities": []}, {"text": "These languages involve deep, semanticbased subject-verb or verb-object relations (such as ACTOR, AGENT, PATIENT, etc., for Penman and FUF), syntactic relations (such as subject, object, premod, etc., for HALogen), or lexical dependencies.", "labels": [], "entities": [{"text": "ACTOR", "start_pos": 91, "end_pos": 96, "type": "METRIC", "confidence": 0.9749777317047119}, {"text": "AGENT", "start_pos": 98, "end_pos": 103, "type": "METRIC", "confidence": 0.9491572380065918}, {"text": "PATIENT", "start_pos": 105, "end_pos": 112, "type": "METRIC", "confidence": 0.8625467419624329}]}, {"text": "Such inputs cannot be accurately produced by state-of-the-art analysis components from arbitrary textual input in the context of text-to-text applications.", "labels": [], "entities": []}, {"text": "Second, most of the recent systems (starting with Nitrogen) have adopted a hybrid approach to generation, which has increased their robustness.", "labels": [], "entities": []}, {"text": "These hybrid systems use, in a first phase, symbolic knowledge to (over)generate a large set of candidate realizations, and, in a second phase, statistical knowledge about the target language (such as stochastic language models) to rank the candidate realizations and find the best scoring one.", "labels": [], "entities": []}, {"text": "The disadvantage of the hybrid approach -from the perspective of integrating these systems within end-to-end applications -is that the two generation phases cannot be tightly coupled.", "labels": [], "entities": []}, {"text": "More precisely, input-driven preferences and target language-driven preferences cannot be integrated in a true probabilistic model that can be trained and tuned for maximum performance.", "labels": [], "entities": []}, {"text": "In this paper, we propose WIDL-expressions (WIDL stands for Weighted Interleave, Disjunction, and Lock, after the names of the main operators) as a representation formalism that facilitates the integration of a generic sentence realization system within end-to-end language applications.", "labels": [], "entities": [{"text": "Lock", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.9438887238502502}]}, {"text": "The WIDL formalism, an extension of the IDL-expressions formalism of, has several crucial properties that differentiate it from previously-proposed NLG representation formalisms.", "labels": [], "entities": []}, {"text": "First, it has a simple syntax (expressions are built using four operators) and a simple, formal semantics (probability distributions over finite sets of strings).", "labels": [], "entities": []}, {"text": "Second, it is a compact representation that grows linearly in the number of words available for generation (see Section 2).", "labels": [], "entities": []}, {"text": "(In contrast, representations such as word lattices or non-recursive CFGs) require exponential space in the number of words available for generation ().)", "labels": [], "entities": []}, {"text": "Third, it has good computational properties, such as optimal algorithms for intersection with -gram language models (Section 3).", "labels": [], "entities": [{"text": "intersection with -gram language", "start_pos": 76, "end_pos": 108, "type": "TASK", "confidence": 0.8161197066307068}]}, {"text": "Fourth, it is flexible with respect to the amount of linguistic processing required to produce WIDLexpressions directly from text (Sections 4 and 5).", "labels": [], "entities": []}, {"text": "Fifth, it allows fora tight integration of inputspecific preferences and target-language preferences via interpolation of probability distributions using log-linear models.", "labels": [], "entities": []}, {"text": "We show the effectiveness of our proposal by directly employing a generic WIDL-based generation system in two end-to-end tasks: machine translation and automatic headline generation.", "labels": [], "entities": [{"text": "WIDL-based generation", "start_pos": 74, "end_pos": 95, "type": "TASK", "confidence": 0.7076821327209473}, {"text": "machine translation", "start_pos": 128, "end_pos": 147, "type": "TASK", "confidence": 0.8071084320545197}, {"text": "headline generation", "start_pos": 162, "end_pos": 181, "type": "TASK", "confidence": 0.7970864176750183}]}], "datasetContent": [], "tableCaptions": []}