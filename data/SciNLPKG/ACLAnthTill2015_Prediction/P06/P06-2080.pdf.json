{"title": [{"text": "Semantic parsing with Structured SVM Ensemble Classification Models", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8176883161067963}, {"text": "Structured SVM Ensemble Classification", "start_pos": 22, "end_pos": 60, "type": "TASK", "confidence": 0.6173793897032738}]}], "abstractContent": [{"text": "We present a learning framework for struc-tured support vector models in which boosting and bagging methods are used to construct ensemble models.", "labels": [], "entities": []}, {"text": "We also propose a selection method which is based on a switching model among a set of outputs of individual classifiers when dealing with natural language parsing problems.", "labels": [], "entities": [{"text": "natural language parsing", "start_pos": 138, "end_pos": 162, "type": "TASK", "confidence": 0.6847078998883566}]}, {"text": "The switching model uses subtrees mined from the corpus and a boosting-based algorithm to select the most appropriate output.", "labels": [], "entities": []}, {"text": "The application of the proposed framework on the domain of semantic parsing shows advantages in comparison with the original large margin methods.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 59, "end_pos": 75, "type": "TASK", "confidence": 0.7564062178134918}]}], "introductionContent": [{"text": "Natural language semantic parsing is an interesting problem in NLP ( as it would very likely be part of any interesting NLP applications.", "labels": [], "entities": [{"text": "Natural language semantic parsing", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.734864667057991}]}, {"text": "For example, the necessary of semantic parsing for most of NLP application and the ability to map natural language to a formal query or command language is critical for developing more user-friendly interfaces.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 30, "end_pos": 46, "type": "TASK", "confidence": 0.7445784509181976}]}, {"text": "Recent approaches have focused on using structured prediction for dealing with syntactic parsing) and text chunking problems ().", "labels": [], "entities": [{"text": "structured prediction", "start_pos": 40, "end_pos": 61, "type": "TASK", "confidence": 0.7651145458221436}, {"text": "syntactic parsing", "start_pos": 79, "end_pos": 96, "type": "TASK", "confidence": 0.7269333004951477}, {"text": "text chunking", "start_pos": 102, "end_pos": 115, "type": "TASK", "confidence": 0.789756566286087}]}, {"text": "For semantic parsing, proposed a method for mapping a NL sentence to its logical form by structured classification using a log-linear model that represents a distribution over syntactic and semantic analyses conditioned on the input sentence.) present a discriminative approach to parsing inspired by the large-margin criterion underlying support vector machines in which the loss function is factorized analogous to the decoding process.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.7320078313350677}]}, {"text": "Tsochantaridis et al) propose a large-margin models based on SVMs for structured prediction (SSVM) in general and apply it for syntactic parsing problem so that the models can adapt to overlap features, kernels, and any loss functions.", "labels": [], "entities": [{"text": "structured prediction (SSVM)", "start_pos": 70, "end_pos": 98, "type": "TASK", "confidence": 0.8529340147972106}, {"text": "syntactic parsing", "start_pos": 127, "end_pos": 144, "type": "TASK", "confidence": 0.7019402086734772}]}, {"text": "Following the successes of the SSVM algorithm to structured prediction, in this paper we exploit the use of SSVM to the semantic parsing problem by modifying the loss function, feature representation, maximization algorithm in the original algorithm for structured outputs.", "labels": [], "entities": [{"text": "structured prediction", "start_pos": 49, "end_pos": 70, "type": "TASK", "confidence": 0.803504228591919}, {"text": "semantic parsing", "start_pos": 120, "end_pos": 136, "type": "TASK", "confidence": 0.7183378785848618}]}, {"text": "Beside that, forming committees or ensembles of learned systems is known to improve accuracy and bagging and boosting are two popular ensemble methods that typically achieve better accuracy than a single classifier.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9985226988792419}, {"text": "accuracy", "start_pos": 181, "end_pos": 189, "type": "METRIC", "confidence": 0.9882583022117615}]}, {"text": "This leads to employing ensemble learning models for SSVM is worth to investigate.", "labels": [], "entities": [{"text": "SSVM", "start_pos": 53, "end_pos": 57, "type": "TASK", "confidence": 0.8856727480888367}]}, {"text": "The first problem of forming an ensemble learning for semantic parsing is how to obtain individual parsers with respect to the fact that each individual parser performs well enough as well as they make different types of errors.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 54, "end_pos": 70, "type": "TASK", "confidence": 0.7226419746875763}]}, {"text": "The second one is that of combining outputs from individual semantic parsers.", "labels": [], "entities": []}, {"text": "The natural way is to use the majority voting strategy that the semantic tree with highest frequency among the outputs obtained by individual parsers is selected.", "labels": [], "entities": []}, {"text": "However, it is not sure that the majority voting technique is effective for combining complex outputs such as a logical form structure.", "labels": [], "entities": []}, {"text": "Thus, a better combination method for semantic tree output should be investigated.", "labels": [], "entities": []}, {"text": "To deal with these problems, we proposed an ensemble method which consists of learning and averaging phases in which the learning phases are either a boosting or a bagging model, and the averaging phase is based on a switching method on outputs obtained from all individual SSVMs.", "labels": [], "entities": []}, {"text": "For the averaging phase, the switching model is used subtrees mined from the corpus and a boostingbased algorithm to select the most appropriate output.", "labels": [], "entities": []}, {"text": "Applications of SSVM ensemble in the semantic parsing problem show that the proposed SSVM ensemble is better than the SSVM in term of the Fmeasure score and accuracy measurements.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 37, "end_pos": 53, "type": "TASK", "confidence": 0.794432133436203}, {"text": "Fmeasure score", "start_pos": 138, "end_pos": 152, "type": "METRIC", "confidence": 0.9727723896503448}, {"text": "accuracy", "start_pos": 157, "end_pos": 165, "type": "METRIC", "confidence": 0.9945354461669922}]}, {"text": "The rest of this paper are organized as follows: Section 2 gives some background about the structured support vector machine model for structured predictions and related works.", "labels": [], "entities": []}, {"text": "Section 3 proposes our ensemble method for structured SVMs on the semantic parsing problem.", "labels": [], "entities": [{"text": "semantic parsing problem", "start_pos": 66, "end_pos": 90, "type": "TASK", "confidence": 0.7906060914198557}]}, {"text": "Section 4 shows experimental results and Section 5 discusses the advantage of our methods and describes future works.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the purpose of testing our SSVM ensembles on semantic parsing, we used the CLANG corpus which is the RoboCup Coach Language (www.robocup.org).", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 49, "end_pos": 65, "type": "TASK", "confidence": 0.7248830199241638}, {"text": "CLANG corpus", "start_pos": 79, "end_pos": 91, "type": "DATASET", "confidence": 0.7538473308086395}]}, {"text": "In the Coach Competition, teams of agents compete on a simulated soccer field and receive advice from a team coach in a formal language.", "labels": [], "entities": []}, {"text": "The CLANG consists of 37 non-terminal and 133 productions; the corpus for CLANG includes 300 sentences and their structured representation in SAPT (), then the logical form representations were built from the trees.", "labels": [], "entities": []}, {"text": "shows the statistic on the CLANG corpus.", "labels": [], "entities": [{"text": "CLANG corpus", "start_pos": 27, "end_pos": 39, "type": "DATASET", "confidence": 0.9783852100372314}]}, {"text": "To create an ensemble learning with SSVM, we used the following parameters with the linear kernel, the polynomial kernel, and RBF kernel, respectively.", "labels": [], "entities": [{"text": "RBF", "start_pos": 126, "end_pos": 129, "type": "METRIC", "confidence": 0.9761665463447571}]}, {"text": "shows that they obtained different accuracies on the training corpus, and their accuracies are good enough to form a SSVM ensemble.", "labels": [], "entities": []}, {"text": "The parameters in is used to form our proposed SSVM model.", "labels": [], "entities": []}, {"text": "The following is the performance of the SSVM 1 , the boosting model, the bagging model, and the models with different parameters on the CLANG corpus 2 . Note that the numbers of individual SSVMs in our ensemble models are set to 10 for boosting and bagging, and each individual SSVM can be used the zero-one and F1 loss function.", "labels": [], "entities": [{"text": "CLANG corpus 2", "start_pos": 136, "end_pos": 150, "type": "DATASET", "confidence": 0.9563966790835062}, {"text": "F1 loss", "start_pos": 312, "end_pos": 319, "type": "METRIC", "confidence": 0.9856665730476379}]}, {"text": "In addition, we also compare the performance of the proposed ensemble SSVM models and the conventional ensemble models to assert that our models are more effective in forming SSVM ensemble learning.", "labels": [], "entities": [{"text": "SSVM ensemble learning", "start_pos": 175, "end_pos": 197, "type": "TASK", "confidence": 0.8427404363950094}]}, {"text": "We used the standard 10-fold cross validation test for evaluating the methods.", "labels": [], "entities": []}, {"text": "To train a BT model for the switching phase in each fold test, we separated the training data into 10-folds.", "labels": [], "entities": []}, {"text": "We keep 9/10 for forming a SSVM ensemble, and 1/10 for producing training data for the switching model.", "labels": [], "entities": []}, {"text": "In addition, we mined a subset of subtrees in which a frequency of each subtree is greater than 2, and used them as weak functions for the boosting tree model.", "labels": [], "entities": []}, {"text": "Note that in testing the whole training data in each fold is formed a SSVM ensemble model to use the BT model estimated above for selecting outputs obtained by the SSVM ensemble.", "labels": [], "entities": []}, {"text": "To evaluate the proposed methods in parsing NL sentences to logical form, we measured the number of test sentences that produced complete logical forms, and the number of those logical forms that were correct.", "labels": [], "entities": [{"text": "parsing NL sentences", "start_pos": 36, "end_pos": 56, "type": "TASK", "confidence": 0.8821594516436259}]}, {"text": "For CLANG, a logical form is correct if it exactly matches the correct representation, up to reordering of the arguments of commutative operators.", "labels": [], "entities": []}, {"text": "We used the evaluation method presented in) as the formula below.", "labels": [], "entities": []}, {"text": "shows the results of SSVM, the SCSIS-SOR system, and the SILT system () on the CLANG corpus, respectively.", "labels": [], "entities": [{"text": "CLANG corpus", "start_pos": 79, "end_pos": 91, "type": "DATASET", "confidence": 0.8834313452243805}]}, {"text": "It shows that SCSISSOR obtained approximately 89% precision and 72.3% recall while on the same corpus our best single SSVM method 3 achieved a recall (74.3%) and lower precision (84.2%).", "labels": [], "entities": [{"text": "precision", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.9988459348678589}, {"text": "recall", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.997853696346283}, {"text": "recall", "start_pos": 143, "end_pos": 149, "type": "METRIC", "confidence": 0.9990309476852417}, {"text": "precision", "start_pos": 168, "end_pos": 177, "type": "METRIC", "confidence": 0.997286319732666}]}, {"text": "The SILT system achieved approximately 83.9% precision and 51.3% recall 4 which is lower than the best single SSVM.", "labels": [], "entities": [{"text": "precision", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9995167255401611}, {"text": "recall 4", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.962292492389679}]}, {"text": "also shows the performance of Bagging, Boosting, and the proposed SSVM ensemble models with bagging and boosting models.", "labels": [], "entities": [{"text": "Bagging", "start_pos": 30, "end_pos": 37, "type": "TASK", "confidence": 0.7360336780548096}]}, {"text": "It is important to note that the switching model using a boosting tree method (BT) to learn the outputs of individual SSVMs within the SSVM ensemble model.", "labels": [], "entities": []}, {"text": "It clearly indicates that our proposed ensemble method can enhance the performance of the SSVM model and the proposed methods are more effective than the conventional ensemble method for SSVM.", "labels": [], "entities": []}, {"text": "This was because the output of each SSVM is complex (i.e a logical form) so it is not sure that the voting method can select a corrected output.", "labels": [], "entities": []}, {"text": "In other words, the boosting tree algorithms can utilize subtrees mined from the corpus to estimate the good weight values for subtrees, and then combines them to determine whether or not a tree is selected.", "labels": [], "entities": []}, {"text": "In our opinion, with the boosting tree algorithm we can have a chance to obtain more accurate outputs.", "labels": [], "entities": []}, {"text": "These results in effectively support for this evidence.", "labels": [], "entities": []}, {"text": "Moreover, depicts that the proposed ensemble method using different parameters for either bagging and boosting models can effectively improve the performance of bagging and boosting in term of precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 193, "end_pos": 202, "type": "METRIC", "confidence": 0.9992853999137878}, {"text": "recall", "start_pos": 207, "end_pos": 213, "type": "METRIC", "confidence": 0.9978577494621277}]}, {"text": "This was because the accuracy of each individual parser in the model with different parameters is better than each one in either the boosting or the bagging model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9994698166847229}]}, {"text": "In addition, when performing SSVM on the test set, we might obtain some 'NULL' outputs since the grammar generated by SSVM could not derive this sentence.", "labels": [], "entities": []}, {"text": "Forming a number of individual SSVMs to an ensemble model is the way to handle this case, but it could make the numbers of completed outputs and corrected outputs increase.", "labels": [], "entities": []}, {"text": "Ta-ble 4 indicates that the proposed SSVM ensemble model obtained 88.4% precision and 79.3% recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.9980018734931946}, {"text": "recall", "start_pos": 92, "end_pos": 98, "type": "METRIC", "confidence": 0.9992740750312805}]}, {"text": "Therefore it shows substantially a better F1 score in comparison with previous work on the CLANG corpus.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9877027273178101}, {"text": "CLANG corpus", "start_pos": 91, "end_pos": 103, "type": "DATASET", "confidence": 0.9187162220478058}]}, {"text": "Summarily, our method achieved the best recall result and a high precision on CLANG corpus.", "labels": [], "entities": [{"text": "recall", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.9993060827255249}, {"text": "precision", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9991105198860168}, {"text": "CLANG corpus", "start_pos": 78, "end_pos": 90, "type": "DATASET", "confidence": 0.9297615885734558}]}, {"text": "The proposed ensemble models outperformed the original SSVM on CLANG corpus and its performances also is better than that of the best published result.", "labels": [], "entities": [{"text": "CLANG corpus", "start_pos": 63, "end_pos": 75, "type": "DATASET", "confidence": 0.8617114722728729}]}], "tableCaptions": [{"text": " Table 1: Subtrees mined from the corpus", "labels": [], "entities": []}, {"text": " Table 2: Statistics on CLANG corpus. The average length", "labels": [], "entities": [{"text": "CLANG corpus", "start_pos": 24, "end_pos": 36, "type": "DATASET", "confidence": 0.8565596342086792}, {"text": "average length", "start_pos": 42, "end_pos": 56, "type": "METRIC", "confidence": 0.5697464346885681}]}, {"text": " Table 3: Training accuracy on CLANG corpus", "labels": [], "entities": [{"text": "Training", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9052264094352722}, {"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9744362831115723}]}, {"text": " Table 4: Experiment results with CLANG corpus. Each", "labels": [], "entities": [{"text": "CLANG corpus", "start_pos": 34, "end_pos": 46, "type": "DATASET", "confidence": 0.8253738582134247}]}]}