{"title": [{"text": "Annotation Schemes and their Influence on Parsing Results", "labels": [], "entities": []}], "abstractContent": [{"text": "Most of the work on treebank-based statistical parsing exclusively uses the Wall-Street-Journal part of the Penn treebank for evaluation purposes.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.6302566826343536}, {"text": "Wall-Street-Journal part of the Penn treebank", "start_pos": 76, "end_pos": 121, "type": "DATASET", "confidence": 0.9447399377822876}]}, {"text": "Due to the presence of this quasi-standard, the question of to which degree parsing results depend on the properties of treebanks was often ignored.", "labels": [], "entities": []}, {"text": "In this paper, we use two similar German treebanks, T\u00fcBa-D/Z and NeGra, and investigate the role that different annotation decisions play for parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 142, "end_pos": 149, "type": "TASK", "confidence": 0.9672903418540955}]}, {"text": "For these purposes, we approximate the two treebanks by gradually taking out or inserting the corresponding annotation components and test the performance of a standard PCFG parser on all treebank versions.", "labels": [], "entities": []}, {"text": "Our results give an indication of which structures are favorable for parsing and which ones are not.", "labels": [], "entities": [{"text": "parsing", "start_pos": 69, "end_pos": 76, "type": "TASK", "confidence": 0.9795594215393066}]}], "introductionContent": [{"text": "The Wall-Street-Journal part (WSJ) of the Penn Treebank () plays a central role in research on statistical treebank-based parsing.", "labels": [], "entities": [{"text": "Wall-Street-Journal part (WSJ) of the Penn Treebank", "start_pos": 4, "end_pos": 55, "type": "DATASET", "confidence": 0.8866533835728964}, {"text": "statistical treebank-based parsing", "start_pos": 95, "end_pos": 129, "type": "TASK", "confidence": 0.5352608462174734}]}, {"text": "It has not only become a standard for parser evaluation, but also the foundation for the development of new parsing models.", "labels": [], "entities": [{"text": "parser evaluation", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.9597322344779968}]}, {"text": "For the English WSJ, high accuracy parsing models have been created, some of them using extensions to classical PCFG parsing such as lexicalization and markovization).", "labels": [], "entities": [{"text": "English WSJ", "start_pos": 8, "end_pos": 19, "type": "DATASET", "confidence": 0.6512162089347839}, {"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9612838625907898}, {"text": "PCFG parsing", "start_pos": 112, "end_pos": 124, "type": "TASK", "confidence": 0.6728886365890503}]}, {"text": "However, since most research has been limited to a single language (English) and to a single treebank (WSJ), the question of how portable the parsers and their extensions are across languages and across treebanks often remained open.", "labels": [], "entities": []}, {"text": "Only recently, there have been attempts to evaluate parsing results with respect to the properties and the language of the treebank that is used.", "labels": [], "entities": []}, {"text": "investigates the effects that certain treebank characteristics have on parsing results, such as the distribution of verb subcategorization frames.", "labels": [], "entities": []}, {"text": "He conducts experiments on the WSJ and the Brown Corpus, parsing one of the treebanks while having trained on the other one.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 31, "end_pos": 34, "type": "DATASET", "confidence": 0.7121982574462891}, {"text": "Brown Corpus", "start_pos": 43, "end_pos": 55, "type": "DATASET", "confidence": 0.8748182058334351}, {"text": "parsing", "start_pos": 57, "end_pos": 64, "type": "TASK", "confidence": 0.9696952700614929}]}, {"text": "He draws the conclusion that a small amount of matched training data is better than a large amount of unmatched training data.", "labels": [], "entities": []}, {"text": "analyze the difficulties that German imposes on parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 48, "end_pos": 55, "type": "TASK", "confidence": 0.966402530670166}]}, {"text": "They use the NeGra treebank for their experiments and show that lexicalization, while highly effective for English, has no benefit for German.", "labels": [], "entities": [{"text": "NeGra treebank", "start_pos": 13, "end_pos": 27, "type": "DATASET", "confidence": 0.9647532105445862}]}, {"text": "This result motivates them to create a parsing model for German based on sisterhead-dependencies.", "labels": [], "entities": []}, {"text": "conduct experiments with model 2 of) and the Stanford parser () on two Italian treebanks.", "labels": [], "entities": []}, {"text": "They report disappointing results which they trace back to the different difficulties of different parsing tasks in Italian and English and to differences in annotation styles across treebanks.", "labels": [], "entities": []}, {"text": "In the present paper, our goal is to determine the effects of different annotation decisions on the results of plain PCFG parsing without extensions.", "labels": [], "entities": [{"text": "PCFG parsing", "start_pos": 117, "end_pos": 129, "type": "TASK", "confidence": 0.6262918710708618}]}, {"text": "Our motivation is two-fold: first, we want to present research on a language different from English, second, we want to investigate the influences of annotation schemes via a realistic comparison, i.e. use two different annotation schemes.", "labels": [], "entities": []}, {"text": "Therefore, we take advantage of the availability of two similar treebanks of German, T\u00fcBa-D/Z () and NeGra ().", "labels": [], "entities": [{"text": "NeGra", "start_pos": 101, "end_pos": 106, "type": "DATASET", "confidence": 0.9046713709831238}]}, {"text": "The strategy we adopt extends.", "labels": [], "entities": []}, {"text": "Treebanks and their annotation schemes respectively are compared using a stepwise approximation.", "labels": [], "entities": []}, {"text": "Annotation components corresponding to certain annotation decisions are taken out or inserted, submitting each time the resulting modified treebank to the parser.", "labels": [], "entities": []}, {"text": "This method allows us to investigate the role of single annotation decisions in two different environments.", "labels": [], "entities": []}, {"text": "In section 2, we describe the annotation of both treebanks in detail.", "labels": [], "entities": []}, {"text": "Section 3 introduces the methodology used.", "labels": [], "entities": []}, {"text": "In section 4, we describe our experimental setup and discuss the results.", "labels": [], "entities": []}, {"text": "Section 5 presents a conclusion and plans for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "For our experiments, we use lopar), a standard PCFG parser.", "labels": [], "entities": []}, {"text": "We read the grammar and the lexicon directly off the trees together with their frequencies.", "labels": [], "entities": []}, {"text": "The parser is given the gold POS tagging to avoid parsing errors that are caused by wrong POS tags.", "labels": [], "entities": []}, {"text": "Only sentences up to a length of 40 words are considered due to memory limitations.", "labels": [], "entities": []}, {"text": "Traditionally, most of the work on WSJ uses the same section of the treebank for testing.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 35, "end_pos": 38, "type": "DATASET", "confidence": 0.8642922639846802}]}, {"text": "However, for our aims, this method has a shortcoming: since both treebanks consist of text created by different authors, linguistic phenomena are not evenly distributed over the treebank.", "labels": [], "entities": []}, {"text": "When using a whole section as test set, some phenomena may only occur there and thus not occur in the grammar.", "labels": [], "entities": []}, {"text": "To reduce data sparseness, we use another test/training-set split for the treebanks and their variations.", "labels": [], "entities": []}, {"text": "Each 10th sentence is put into the test set, all other sentences go into the training set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Properties of the treebank modifications 1", "labels": [], "entities": []}, {"text": " Table 2: Parsing NeGra: Results", "labels": [], "entities": [{"text": "Parsing NeGra", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.7088495343923569}]}, {"text": " Table 3: Parsing T\u00fcBa-D/Z: Results", "labels": [], "entities": [{"text": "Parsing T\u00fcBa-D/Z", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.7338779866695404}]}]}