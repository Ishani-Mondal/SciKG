{"title": [{"text": "Modeling Commonality among Related Classes in Relation Extraction", "labels": [], "entities": [{"text": "Relation Extraction", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.9045416116714478}]}], "abstractContent": [{"text": "This paper proposes a novel hierarchical learning strategy to deal with the data sparseness problem in relation extraction by modeling the commonality among related classes.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 103, "end_pos": 122, "type": "TASK", "confidence": 0.8943331241607666}]}, {"text": "For each class in the hierarchy either manually prede-fined or automatically clustered, a linear dis-criminative function is determined in a top-down way using a perceptron algorithm with the lower-level weight vector derived from the upper-level weight vector.", "labels": [], "entities": []}, {"text": "As the upper-level class normally has much more positive training examples than the lower-level class, the corresponding linear discriminative function can be determined more reliably.", "labels": [], "entities": []}, {"text": "The upper-level discriminative function then can effectively guide the discriminative function learning in the lower-level, which otherwise might suffer from limited training data.", "labels": [], "entities": []}, {"text": "Evaluation on the ACE RDC 2003 corpus shows that the hierarchical strategy much improves the performance by 5.6 and 5.1 in F-measure on least-and medium-frequent relations respectively.", "labels": [], "entities": [{"text": "ACE RDC 2003 corpus", "start_pos": 18, "end_pos": 37, "type": "DATASET", "confidence": 0.9725981950759888}, {"text": "F-measure", "start_pos": 123, "end_pos": 132, "type": "METRIC", "confidence": 0.9852965474128723}]}, {"text": "It also shows that our system outper-forms the previous best-reported system by 2.7 in F-measure on the 24 subtypes using the same feature set.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 87, "end_pos": 96, "type": "METRIC", "confidence": 0.9985719919204712}]}], "introductionContent": [{"text": "With the dramatic increase in the amount of textual information available in digital archives and the WWW, there has been growing interest in techniques for automatically extracting information from text.", "labels": [], "entities": [{"text": "WWW", "start_pos": 102, "end_pos": 105, "type": "DATASET", "confidence": 0.9112971425056458}, {"text": "automatically extracting information from text", "start_pos": 157, "end_pos": 203, "type": "TASK", "confidence": 0.7776825189590454}]}, {"text": "Information Extraction (IE) is such a technology that IE systems are expected to identify relevant information (usually of predefined types) from text documents in a certain domain and put them in a structured format.", "labels": [], "entities": [{"text": "Information Extraction (IE)", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.848470950126648}]}, {"text": "According to the scope of the ACE program (ACE, current research in IE has three main objectives: Entity Detection and Tracking (EDT), Relation Detection and Characterization (RDC), and Event Detection and Characterization (EDC).", "labels": [], "entities": [{"text": "IE", "start_pos": 68, "end_pos": 70, "type": "TASK", "confidence": 0.9592107534408569}, {"text": "Entity Detection and Tracking (EDT)", "start_pos": 98, "end_pos": 133, "type": "TASK", "confidence": 0.7836945312363761}, {"text": "Relation Detection and Characterization (RDC)", "start_pos": 135, "end_pos": 180, "type": "TASK", "confidence": 0.776742696762085}, {"text": "Event Detection and Characterization (EDC)", "start_pos": 186, "end_pos": 228, "type": "TASK", "confidence": 0.8191246560641697}]}, {"text": "This paper will focus on the ACE RDC task, which detects and classifies various semantic relations between two entities.", "labels": [], "entities": [{"text": "ACE RDC task", "start_pos": 29, "end_pos": 41, "type": "TASK", "confidence": 0.801722526550293}]}, {"text": "For example, we want to determine whether a person is at a location, based on the evidence in the context.", "labels": [], "entities": []}, {"text": "Extraction of semantic relationships between entities can be very useful for applications such as question answering, e.g. to answer the query \"Who is the president of the United States?\".", "labels": [], "entities": [{"text": "Extraction of semantic relationships between entities", "start_pos": 0, "end_pos": 53, "type": "TASK", "confidence": 0.8667237261931101}, {"text": "question answering", "start_pos": 98, "end_pos": 116, "type": "TASK", "confidence": 0.8801880776882172}, {"text": "answer the query \"Who is the president of the United States?\"", "start_pos": 126, "end_pos": 187, "type": "TASK", "confidence": 0.6377213574372805}]}, {"text": "One major challenge in relation extraction is due to the data sparseness problem (.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.9565863013267517}]}, {"text": "As the largest annotated corpus in relation extraction, the ACE RDC 2003 corpus shows that different subtypes/types of relations are much unevenly distributed and a few relation subtypes, such as the subtype \"Founder\" under the type \"ROLE\", suffers from a small amount of annotated data.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.8717713057994843}, {"text": "ACE RDC 2003 corpus", "start_pos": 60, "end_pos": 79, "type": "DATASET", "confidence": 0.9644663035869598}]}, {"text": "Further experimentation in this paper (please see) shows that most relation subtypes suffer from the lack of the training data and fail to achieve steady performance given the current corpus size.", "labels": [], "entities": []}, {"text": "Given the relative large size of this corpus, it will be time-consuming and very expensive to further expand the corpus with a reasonable gain in performance.", "labels": [], "entities": []}, {"text": "Even if we can somehow expend the corpus and achieve steady performance on major relation subtypes, it will be still far beyond practice for those minor subtypes given the much unevenly distribution among different relation subtypes.", "labels": [], "entities": []}, {"text": "While various machine learning approaches, such as generative modeling, maximum entropy) and support vector machines (, have been applied in the relation extraction task, no explicit learning strategy is proposed to deal with the inherent data sparseness problem caused by the much uneven distribution among different relations.", "labels": [], "entities": [{"text": "generative modeling", "start_pos": 51, "end_pos": 70, "type": "TASK", "confidence": 0.9760281145572662}, {"text": "relation extraction task", "start_pos": 145, "end_pos": 169, "type": "TASK", "confidence": 0.8481724262237549}]}, {"text": "This paper proposes a novel hierarchical learning strategy to deal with the data sparseness problem by modeling the commonality among related classes.", "labels": [], "entities": []}, {"text": "Through organizing various classes hierarchically, a linear discriminative function is determined for each class in a topdown way using a perceptron algorithm with the lower-level weight vector derived from the upper-level weight vector.", "labels": [], "entities": []}, {"text": "Evaluation on the ACE RDC 2003 corpus shows that the hierarchical strategy achieves much better performance than the flat strategy on least-and medium-frequent relations.", "labels": [], "entities": [{"text": "ACE RDC 2003 corpus", "start_pos": 18, "end_pos": 37, "type": "DATASET", "confidence": 0.9737547039985657}]}, {"text": "It also shows that our system based on the hierarchical strategy outperforms the previous best-reported system.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents related work.", "labels": [], "entities": []}, {"text": "Section 3 describes the hierarchical learning strategy using the perceptron algorithm.", "labels": [], "entities": []}, {"text": "Finally, we present experimentation in Section 4 and conclude this paper in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "This paper uses the ACE RDC 2003 corpus provided by LDC to train and evaluate the hierarchical learning strategy.", "labels": [], "entities": [{"text": "ACE RDC 2003 corpus", "start_pos": 20, "end_pos": 39, "type": "DATASET", "confidence": 0.9562385380268097}, {"text": "LDC", "start_pos": 52, "end_pos": 55, "type": "DATASET", "confidence": 0.5632608532905579}]}, {"text": "Same as, we only model explicit relations and explicitly model the argument order of the two mentions involved.", "labels": [], "entities": []}, {"text": "The training data consists of 674 documents (~300k words) with 9683 relation examples while the held-out testing data consists of 97 documents (~50k words) with 1386 relation examples.", "labels": [], "entities": []}, {"text": "All the experiments are done five times on the 24 relation subtypes in the ACE corpus, except otherwise specified, with the final performance averaged using the same re-sampling with replacement strategy as the one in the bagging technique.", "labels": [], "entities": [{"text": "ACE corpus", "start_pos": 75, "end_pos": 85, "type": "DATASET", "confidence": 0.9636285603046417}]}, {"text": "lists various types and subtypes of relations for the ACE RDC 2003 corpus, along with their occurrence frequency in the training data.", "labels": [], "entities": [{"text": "ACE RDC 2003 corpus", "start_pos": 54, "end_pos": 73, "type": "DATASET", "confidence": 0.9463820159435272}]}, {"text": "It shows that this corpus suffers from a small amount of annotated data fora few subtypes such as the subtype \"Founder\" under the type \"ROLE\".", "labels": [], "entities": [{"text": "Founder", "start_pos": 111, "end_pos": 118, "type": "DATASET", "confidence": 0.9260640740394592}, {"text": "ROLE", "start_pos": 136, "end_pos": 140, "type": "METRIC", "confidence": 0.9135227203369141}]}, {"text": "For comparison, we also adopt the same feature set as: word, entity type, mention level, overlap, base phrase chunking, dependency tree, parse tree and semantic information.", "labels": [], "entities": [{"text": "base phrase chunking", "start_pos": 98, "end_pos": 118, "type": "TASK", "confidence": 0.6707241535186768}]}, {"text": "shows the performance of the hierarchical learning strategy using the existing class hierarchy in the given ACE corpus and its comparison with the flat learning strategy, using the perceptron algorithm.", "labels": [], "entities": [{"text": "ACE corpus", "start_pos": 108, "end_pos": 118, "type": "DATASET", "confidence": 0.9091005027294159}]}, {"text": "It shows that the pure hierarchical strategy outperforms the pure flat strategy by 1.5 (56.9 vs. 55.4) in F-measure.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 106, "end_pos": 115, "type": "METRIC", "confidence": 0.8289760947227478}]}, {"text": "It also shows that further smoothing and bagging improve the performance of the hierarchical and flat strategies by 0.6 and 0.9 in F-measure respectively.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 131, "end_pos": 140, "type": "METRIC", "confidence": 0.9861323833465576}]}, {"text": "As a result, the final hierarchical strategy achieves F-measure of 57.8 and outperforms the final flat strategy by: Performance of the hierarchical learning strategy using different class hierarchies compares the performance of the hierarchical learning strategy using different class hierarchies.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9992048144340515}]}, {"text": "It shows that, the lowest-level hybrid approach, which only automatically updates the existing class hierarchy at the lowest level, improves the performance by 0.3 in F-measure while further updating the class hierarchy at upper levels in the all-level hybrid approach only has very slight effect.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 167, "end_pos": 176, "type": "METRIC", "confidence": 0.9965544939041138}]}, {"text": "This is largely due to the fact that the major data sparseness problem occurs at the lowest level, i.e. the relation subtype level in the ACE corpus.", "labels": [], "entities": [{"text": "ACE corpus", "start_pos": 138, "end_pos": 148, "type": "DATASET", "confidence": 0.9452669322490692}]}, {"text": "As a result, the final hierarchical learning strategy using the class hierarchy built with the all-level hybrid approach achieves F-measure of 58.2 in F-measure, which outperforms the final flat strategy by 2.2 in Fmeasure.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 130, "end_pos": 139, "type": "METRIC", "confidence": 0.9977896213531494}, {"text": "F-measure", "start_pos": 151, "end_pos": 160, "type": "METRIC", "confidence": 0.8846073746681213}]}, {"text": "In order to justify the usefulness of our hierarchical learning strategy when a rough class hierarchy is not available and difficult to determine manually, we also experiment using entirely automatically built class hierarchy (using the traditional binary hierarchical clustering algorithm and the cosine similarity measurement) without considering the existing class hierarchy.", "labels": [], "entities": []}, {"text": "shows that using automatically built class hierarchy performs comparably with using only the existing one.", "labels": [], "entities": []}, {"text": "With the major goal of resolving the data sparseness problem for the classes with a small amount of training examples, compares the best-performed hierarchical and flat learning strategies on the relation subtypes of different training data sizes.", "labels": [], "entities": []}, {"text": "Here, we divide various relation subtypes into three bins: large/middle/small, according to their available training data sizes.", "labels": [], "entities": []}, {"text": "For the ACE RDC 2003 corpus, we use 400 as the lower threshold for the large bin 6 and 200 as the upper threshold for the small bin . As a result, the large/medium/small bin includes 5/8/11 relation subtypes, respectively.", "labels": [], "entities": [{"text": "ACE RDC 2003 corpus", "start_pos": 8, "end_pos": 27, "type": "DATASET", "confidence": 0.9605747610330582}]}, {"text": "shows that the hierarchical strategy outperforms the flat strategy by 1.0/5.1/5.6 in F-measure on the large/middle/small bin respectively.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9942505359649658}]}, {"text": "This indicates that the hierarchical strategy performs much better than the flat strategy for those classes with a small or medium amount of annotated examples although the hierarchical strategy only performs slightly better by 1.0 and 2.2 in Fmeasure than the flat strategy on those classes with a large size of annotated corpus and on all classes as a whole respectively.", "labels": [], "entities": [{"text": "Fmeasure", "start_pos": 243, "end_pos": 251, "type": "METRIC", "confidence": 0.5247581005096436}]}, {"text": "This suggests that the proposed hierarchical strategy can well deal with the data sparseness problem in the ACE RDC 2003 corpus.", "labels": [], "entities": [{"text": "ACE RDC 2003 corpus", "start_pos": 108, "end_pos": 127, "type": "DATASET", "confidence": 0.9641302973031998}]}, {"text": "An interesting question is about the similarity between the linear discriminative functions learned using the hierarchical and flat learning strategies.", "labels": [], "entities": []}, {"text": "compares the cosine similarities between the weight vectors of the linear discriminative functions using the two strategies for different bins, weighted by the training data sizes of different relation subtypes.", "labels": [], "entities": []}, {"text": "It shows that the linear discriminative functions learned using the two strategies are very similar (with the cosine similarity 0.98) for the relation subtypes belonging to the large bin while the linear discriminative functions learned using the two strategies are not for the relation subtypes belonging to the medium/small bin with the cosine similarity 0.92/0.81 respectively.", "labels": [], "entities": []}, {"text": "This means that the use of the hierarchical strategy over the flat strategy only has very slight change on the linear discriminative functions for those classes with a large amount of annotated examples while its effect on those with a small amount of annotated examples is obvious.", "labels": [], "entities": []}, {"text": "This contributes to and explains (the degree of) the performance difference between the two strategies on the different training data sizes as shown in.", "labels": [], "entities": []}, {"text": "Due to the difficulty of building a large annotated corpus, another interesting question is about the learning curve of the hierarchical learning strategy and its comparison with the flat learning strategy.", "labels": [], "entities": []}, {"text": "shows the effect of different training data sizes for some major relation subtypes while keeping all the training examples of remaining relation subtypes.", "labels": [], "entities": []}, {"text": "It shows that the hierarchical strategy performs much better than the flat strategy when only a small amount of training examples is available.", "labels": [], "entities": []}, {"text": "It also shows that the hierarchical strategy can achieve stable performance much faster than the flat strategy.", "labels": [], "entities": []}, {"text": "Finally, it shows that the ACE RDC 2003 task suffers from the lack of training examples.", "labels": [], "entities": [{"text": "ACE RDC 2003 task", "start_pos": 27, "end_pos": 44, "type": "DATASET", "confidence": 0.7706646770238876}]}, {"text": "Among the three major relation subtypes, only the subtype \"Located\" achieves steady performance.", "labels": [], "entities": []}, {"text": "Finally, we also compare our system with the previous best-reported systems, such as Kambhatla (2004) and.", "labels": [], "entities": []}, {"text": "shows that our system outperforms the previous best-reported system by 2.7 (58.2 vs. 55.5) in Fmeasure, largely due to the gain in recall.", "labels": [], "entities": [{"text": "Fmeasure", "start_pos": 94, "end_pos": 102, "type": "DATASET", "confidence": 0.7645701169967651}, {"text": "recall", "start_pos": 131, "end_pos": 137, "type": "METRIC", "confidence": 0.9981850981712341}]}, {"text": "It indicates that, although support vector machines and maximum entropy models always perform better than the simple perceptron algorithm inmost (if not all) applications, the hierarchical learning strategy using the perceptron algorithm can easily overcome the difference and outperforms the flat learning strategy using the overwhelming support vector machines and maximum entropy models in relation extraction, at least on the ACE: Comparison of the hierarchical and flat learning strategies on the relation subtypes of different training data sizes.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 393, "end_pos": 412, "type": "TASK", "confidence": 0.808415949344635}, {"text": "ACE", "start_pos": 430, "end_pos": 433, "type": "DATASET", "confidence": 0.9713847637176514}]}, {"text": "Notes: the figures in the parentheses indicate the cosine similarities between the weight vectors of the linear discriminative functions learned using the two strategies.: Comparison of our system with other best-reported systems", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Performance of the hierarchical learning  strategy using the existing class hierarchy and its  comparison with the flat learning strategy", "labels": [], "entities": []}, {"text": " Table 3: Performance of the hierarchical learning  strategy using different class hierarchies", "labels": [], "entities": []}, {"text": " Table 4: Comparison of the hierarchical and flat learning strategies on the relation subtypes of differ- ent training data sizes. Notes: the figures in the parentheses indicate the cosine similarities between  the weight vectors of the linear discriminative functions learned using the two strategies.", "labels": [], "entities": []}, {"text": " Table 5: Comparison of our system with other best-reported systems", "labels": [], "entities": []}]}