{"title": [{"text": "Spontaneous Speech Understanding for Robust Multi-Modal Human-Robot Communication", "labels": [], "entities": [{"text": "Spontaneous Speech Understanding", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.876734713713328}, {"text": "Robust Multi-Modal Human-Robot Communication", "start_pos": 37, "end_pos": 81, "type": "TASK", "confidence": 0.5780952498316765}]}], "abstractContent": [{"text": "This paper presents a speech understanding component for enabling robust situated human-robot communication.", "labels": [], "entities": [{"text": "speech understanding", "start_pos": 22, "end_pos": 42, "type": "TASK", "confidence": 0.7615397572517395}]}, {"text": "The aim is to gain semantic interpretations of utterances that serve as a basis for multi-modal dialog management also in cases where the recognized word-stream is not grammatically correct.", "labels": [], "entities": [{"text": "multi-modal dialog management", "start_pos": 84, "end_pos": 113, "type": "TASK", "confidence": 0.6277710298697153}]}, {"text": "For the understanding process, we designed semantic pro-cessable units, which are adapted to the domain of situated communication.", "labels": [], "entities": []}, {"text": "Our framework supports the specific characteristics of spontaneous speech used in combination with gestures in areal world scenario.", "labels": [], "entities": []}, {"text": "It also provides information about the dialog acts.", "labels": [], "entities": []}, {"text": "Finally, we present a processing mechanism using these concept structures to generate the most likely semantic interpretation of the utterances and to evaluate the interpretation with respect to semantic coherence.", "labels": [], "entities": []}], "introductionContent": [{"text": "Over the past years interest in mobile robot applications has increased.", "labels": [], "entities": []}, {"text": "One aim is to allow for intuitive interaction with a personal robot which is based on the idea that people want to communicate in a natural way ()).", "labels": [], "entities": []}, {"text": "Although often people use speech as the main modality, they tend to revert to additional modalities such as gestures and mimics in face-to-face situations.", "labels": [], "entities": []}, {"text": "Also, they refer to objects in the physical environment.", "labels": [], "entities": []}, {"text": "Furthermore, speech, gestures and information of the environment are used in combination in instructions for the robot.", "labels": [], "entities": []}, {"text": "When participants perceive a shared environment and act in it we call this communication \"situated\" ().", "labels": [], "entities": []}, {"text": "In addition to these features that are characteristic for situated communication, situated dialog systems have to deal with several problems caused by spontaneous speech phenomena like ellipses, indirect speech acts or incomplete sentences.", "labels": [], "entities": []}, {"text": "Large pauses or breaks occur inside an utterance and people tend to correct themselves.", "labels": [], "entities": []}, {"text": "Utterances often do not follow a standard grammar as written text.", "labels": [], "entities": []}, {"text": "Service robots have not only to be able to cope with this special kind of communication but they also have to cope with noise that is produced by their own actuators or the environment.", "labels": [], "entities": []}, {"text": "Speech recognition in such scenarios is a complex and difficult task, leading to severe degradations of the recognition performance.", "labels": [], "entities": [{"text": "Speech recognition", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7951012253761292}]}, {"text": "The goal of this paper is to present a framework for human-robot interaction (HRI) that enables robust interpretation of utterances under the specific conditions in HRI.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the evaluation of the entire robot system BIRON we recruited 14 naive user between 12 and 37 years with the goal to test the intuitiveness and the robustness of all system modules as well as its performance.", "labels": [], "entities": [{"text": "BIRON", "start_pos": 46, "end_pos": 51, "type": "METRIC", "confidence": 0.617825984954834}]}, {"text": "Therefore, in the first of two runs the users were asked to familiarize themselves with the robot without any further information of the system.", "labels": [], "entities": []}, {"text": "In the second run the users were given more information about technical details of BIRON (such as its limited vocabulary).", "labels": [], "entities": [{"text": "BIRON", "start_pos": 83, "end_pos": 88, "type": "METRIC", "confidence": 0.5929909348487854}]}, {"text": "We observed similar effects as described in section 2.", "labels": [], "entities": []}, {"text": "In average, one utterance contained 3.23 words indicating that the users are more likely to utter short phrases.", "labels": [], "entities": []}, {"text": "They also tend to pause in the middle of an utterance and they often uttered so called meta-comments such as \"that\"s fine\".", "labels": [], "entities": []}, {"text": "In some excerptions of the dialogs during the experiment settings are presented.", "labels": [], "entities": []}, {"text": "Thus, not surprisingly the speech recognition error rate in the first run was 60% which decreased in the second run to 42%, with an average of 52%.", "labels": [], "entities": [{"text": "speech recognition error rate", "start_pos": 27, "end_pos": 56, "type": "TASK", "confidence": 0.7850362360477448}]}, {"text": "High error rate seems to be a general problem in settings with spontaneous speech as other systems also observed this problem (see also).", "labels": [], "entities": [{"text": "error rate", "start_pos": 5, "end_pos": 15, "type": "METRIC", "confidence": 0.9660583436489105}]}, {"text": "But even in such a restricted experiment setting, speech understanding will have to deal with speech recognition error which can never be avoided.", "labels": [], "entities": [{"text": "speech understanding", "start_pos": 50, "end_pos": 70, "type": "TASK", "confidence": 0.8916270434856415}, {"text": "speech recognition", "start_pos": 94, "end_pos": 112, "type": "TASK", "confidence": 0.6729529350996017}]}, {"text": "In order to address the two questions of (1) how well our approach of automatic speech understanding (ASU) can deal with automatic speech recognition (ASR) errors and (2) how its performance compares to syntactic analysis, we performed two analyses.", "labels": [], "entities": [{"text": "automatic speech understanding (ASU)", "start_pos": 70, "end_pos": 106, "type": "TASK", "confidence": 0.8006520370642344}, {"text": "automatic speech recognition (ASR)", "start_pos": 121, "end_pos": 155, "type": "TASK", "confidence": 0.8008900384108225}]}, {"text": "In order to answer question (1) we compared the results from the semantic analysis based on the real speech recognition re-sults with an accuracy of 52% with those based on the really uttered words as transcribed manually, thus simulating a recognition rate of 100%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.9986639022827148}]}, {"text": "In total, the semantic speech processing received 1642 utterances from the speech recognition system.", "labels": [], "entities": []}, {"text": "From these utterances 418 utterances were randomly chosen for manual transcription and syntactic analysis.", "labels": [], "entities": [{"text": "manual transcription", "start_pos": 62, "end_pos": 82, "type": "TASK", "confidence": 0.6965214908123016}, {"text": "syntactic analysis", "start_pos": 87, "end_pos": 105, "type": "TASK", "confidence": 0.7490610778331757}]}, {"text": "All 1642 utterances were processed and performed on a standard PC with an average processing time of 20ms, which fully fulfills the requirements of real-time applications.", "labels": [], "entities": []}, {"text": "As shown in 39% of the results were rated as complete or partial misunderstandings and 61% as correct utterances with full semantic meaning.", "labels": [], "entities": []}, {"text": "Only 4% of the utterances which were correctly recognized were misinterpreted or refused by the speech understanding system.", "labels": [], "entities": [{"text": "speech understanding", "start_pos": 96, "end_pos": 116, "type": "TASK", "confidence": 0.687615156173706}]}, {"text": "Most errors occurred due to missing words in the lexicon.", "labels": [], "entities": []}, {"text": "Thus, the performance of the speech understanding system (ASU) decreases to the same degree as that of the speech recognition system (ASR): with a 50% ASR recognition rate the number of non-interpretable utterances is doubled indicating a linear relationship between ASR and ASU.", "labels": [], "entities": [{"text": "speech understanding system (ASU)", "start_pos": 29, "end_pos": 62, "type": "TASK", "confidence": 0.7108753323554993}, {"text": "speech recognition system (ASR)", "start_pos": 107, "end_pos": 138, "type": "TASK", "confidence": 0.7016814897457758}, {"text": "ASR recognition", "start_pos": 151, "end_pos": 166, "type": "TASK", "confidence": 0.5388317853212357}]}, {"text": "For the second question we performed a manual classification of the utterances into syntactically correct (and thus parseable by a standard parsing algorithm) and not-correct.", "labels": [], "entities": []}, {"text": "Utterances following the English standard grammar (e.g. imperative, descriptive, interrogative) or containing a single word or an NP, as to be expected in answers, were classified as correct.", "labels": [], "entities": []}, {"text": "Incomplete utterances or utterances with a non-standard structure (as occurred often in the baby-talk style utterances) were rated as not-correct.", "labels": [], "entities": []}, {"text": "In detail, 58 utterances were either truncated at the end or beginning due to errors of the attention system, resulting in utterances such as \"where is\", \"can you find\", or \"is a cube\".", "labels": [], "entities": []}, {"text": "These utterances also include instances where users interrupted themselves.", "labels": [], "entities": []}, {"text": "In 51 utterances we found words missing in our lexicon database.", "labels": [], "entities": []}, {"text": "314 utterances where syntactically correct, whereas in 28 of these utterances a lexicon entry is missing in the system and therefore would ASR=100% ASR=52% ASU not or part. interpret.", "labels": [], "entities": [{"text": "ASR", "start_pos": 139, "end_pos": 142, "type": "METRIC", "confidence": 0.9936848282814026}, {"text": "ASR", "start_pos": 148, "end_pos": 151, "type": "METRIC", "confidence": 0.9275583028793335}, {"text": "ASU", "start_pos": 156, "end_pos": 159, "type": "METRIC", "confidence": 0.890559732913971}]}, {"text": "15% 39% ASU fully interpretable 84% 61% lead to a failure of the parsing mechanism.", "labels": [], "entities": [{"text": "ASU", "start_pos": 8, "end_pos": 11, "type": "DATASET", "confidence": 0.6399146318435669}, {"text": "parsing", "start_pos": 65, "end_pos": 72, "type": "TASK", "confidence": 0.9831188321113586}]}, {"text": "104 utterances have been classified as syntactically notcorrect.", "labels": [], "entities": []}, {"text": "In contrast, the result from our mechanism performed significantly better.", "labels": [], "entities": []}, {"text": "Our system was able to interprete 352 utterances and generate a full semantic interpretation, whereas 66 utterances could only be partially interpreted or were marked as not interpretable.", "labels": [], "entities": []}, {"text": "21 interpretations of the utterances were semantically incorrect (labeled from the system wrongly as correct) or were not assigned to the correct speech act, e.g., \"okay\" was assigned to no speech act (fragment) instead to confirmation.", "labels": [], "entities": []}, {"text": "Missing lexicon entries often lead to partial interpretations (20 times) or sometimes to complete misinterpretations (8 times).", "labels": [], "entities": []}, {"text": "But still in many cases the system was able to interprete the utterance correctly (23 times).", "labels": [], "entities": [{"text": "interprete the utterance", "start_pos": 47, "end_pos": 71, "type": "TASK", "confidence": 0.9005916913350424}]}, {"text": "For example \"can you go fora walk with me\" was interpreted as \"can you go with me\" only ignoring the unknown \"for a walk\".The utterance \"can you come closer\" was interpreted as a partial understanding \"can you come\" (ignoring the unknown word \"closer\").", "labels": [], "entities": []}, {"text": "The results are summarized in.", "labels": [], "entities": []}, {"text": "As can be seen the semantic error rate with 15% non-interpretable utterances is just half of the syntactic correctness with 31%.", "labels": [], "entities": [{"text": "semantic error rate", "start_pos": 19, "end_pos": 38, "type": "METRIC", "confidence": 0.656185507774353}]}, {"text": "This indicates that the semantic analysis can recover about half of the information that would not be recoverable from syntactic analysis.", "labels": [], "entities": []}], "tableCaptions": []}