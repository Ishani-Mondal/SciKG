{"title": [{"text": "Exploiting Non-local Features for Spoken Language Understanding", "labels": [], "entities": [{"text": "Spoken Language Understanding", "start_pos": 34, "end_pos": 63, "type": "TASK", "confidence": 0.8647707104682922}]}], "abstractContent": [{"text": "In this paper, we exploit non-local features as an estimate of long-distance dependencies to improve performance on the statistical spoken language understanding (SLU) problem.", "labels": [], "entities": [{"text": "statistical spoken language understanding (SLU)", "start_pos": 120, "end_pos": 167, "type": "TASK", "confidence": 0.75310965520995}]}, {"text": "The statistical natural language parsers trained on text perform unreliably to encode non-local information on spoken language.", "labels": [], "entities": [{"text": "statistical natural language parsers", "start_pos": 4, "end_pos": 40, "type": "TASK", "confidence": 0.6246707141399384}]}, {"text": "An alternative method we propose is to use trigger pairs that are automatically extracted by a feature induction algorithm.", "labels": [], "entities": []}, {"text": "We describe alight version of the inducer in which a simple modification is efficient and successful.", "labels": [], "entities": []}, {"text": "We evaluate our method on an SLU task and show an error reduction of up to 27% over the base local model.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 50, "end_pos": 65, "type": "METRIC", "confidence": 0.9780548810958862}]}], "introductionContent": [{"text": "For most sequential labeling problems in natural language processing (NLP), a decision is made based on local information.", "labels": [], "entities": [{"text": "sequential labeling problems in natural language processing (NLP)", "start_pos": 9, "end_pos": 74, "type": "TASK", "confidence": 0.7311644434928894}]}, {"text": "However, processing that relies on the Markovian assumption cannot represent higher-order dependencies.", "labels": [], "entities": []}, {"text": "This longdistance dependency problem has been considered at length in computational linguistics.", "labels": [], "entities": []}, {"text": "It is the key limitation in bettering sequential models in various natural language tasks.", "labels": [], "entities": []}, {"text": "Thus, we need new methods to import non-local information into sequential models.", "labels": [], "entities": []}, {"text": "There are two types of method for using nonlocal information.", "labels": [], "entities": []}, {"text": "One is to add edges to structure to allow higher-order dependencies and another is to add features (or observable variables) to encode the non-locality.", "labels": [], "entities": []}, {"text": "An additional consistent edge of a linear-chain conditional random field (CRF) explicitly models the dependencies between distant occurrences of similar words).", "labels": [], "entities": []}, {"text": "However, this approach requires additional time complexity in inference/learning time and it is only suitable for representing constraints by enforcing label consistency.", "labels": [], "entities": []}, {"text": "We wish to identify ambiguous labels with more general dependency without additional time cost in inference/learning time.", "labels": [], "entities": []}, {"text": "Another approach to modeling non-locality is to use observational features which can capture non-local information.", "labels": [], "entities": []}, {"text": "Traditionally, many systems prefer to use a syntactic parser.", "labels": [], "entities": []}, {"text": "Ina language understanding task, the headword dependencies or parse tree path are successfully applied to learn and predict semantic roles, especially those with ambiguous labels ().", "labels": [], "entities": [{"text": "language understanding", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.716152548789978}]}, {"text": "Although the power of syntactic structure is impressive, using the parser-based feature fails to encode correct global information because of the low accuracy of a modern parser.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 150, "end_pos": 158, "type": "METRIC", "confidence": 0.9982703924179077}]}, {"text": "Furthermore the inaccurate result of parsing is more serious in a spoken language understanding (SLU) task.", "labels": [], "entities": [{"text": "parsing", "start_pos": 37, "end_pos": 44, "type": "TASK", "confidence": 0.9769405126571655}, {"text": "spoken language understanding (SLU) task", "start_pos": 66, "end_pos": 106, "type": "TASK", "confidence": 0.8313746707780021}]}, {"text": "In contrast to written language, spoken language loses much information including grammar, structure or morphology and contains some errors in automatically recognized speech.", "labels": [], "entities": []}, {"text": "To solve the above problems, we present one method to exploit non-local information -the trigger feature.", "labels": [], "entities": []}, {"text": "In this paper, we incorporate trigger pairs into a sequential model, a linear-chain CRF.", "labels": [], "entities": []}, {"text": "Then we describe an efficient algorithm to extract the trigger feature from the training data itself.", "labels": [], "entities": []}, {"text": "The framework for inducing trigger features is based on the Kullback-Leibler divergence criterion which measures the improvement of loglikelihood on the current parameters by adding anew feature ().", "labels": [], "entities": []}, {"text": "To reduce the cost of feature selection, we suggest a modified version of an inducing algorithm which is quite efficient.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.7496103346347809}]}, {"text": "We evaluate our method on an SLU task, and demonstrate the improvements on both transcripts and recognition outputs.", "labels": [], "entities": [{"text": "SLU task", "start_pos": 29, "end_pos": 37, "type": "TASK", "confidence": 0.7893071472644806}]}, {"text": "On a real-world problem, our modified version of a feature selection algorithm is very efficient for both performance and time complexity.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 51, "end_pos": 68, "type": "TASK", "confidence": 0.7181240767240524}]}], "datasetContent": [{"text": "We evaluate our method on the CU-Communicator corpus.", "labels": [], "entities": [{"text": "CU-Communicator corpus", "start_pos": 30, "end_pos": 52, "type": "DATASET", "confidence": 0.9495812952518463}]}, {"text": "It consists of 13,983 utterances.", "labels": [], "entities": []}, {"text": "The semantic categories correspond to city names, timerelated information, airlines and other miscellaneous entities.", "labels": [], "entities": []}, {"text": "The semantic labels are automatically generated by a Phoenix parser and manually corrected.", "labels": [], "entities": []}, {"text": "In the data set, the semantic category has a two-level hierarchy: 31 first level classes and 7 second level classes, fora total of 62 class combinations.", "labels": [], "entities": []}, {"text": "The data set is 630k words with 29k entities.", "labels": [], "entities": []}, {"text": "Roughly half of the entities are timerelated information, a quarter of the entities are city names, a tenth are state and country names, and a fifth are airline and airport names.", "labels": [], "entities": []}, {"text": "For the second level hierarchy, approximately three quarters of the entities are \"NONE\", a tenth are \"TOLOC\", a tenth are \"FROMLOC\", and the remaining are \"RETURN\", \"DEPERT\", \"ARRIVE\", and \"STOPLOC.\"", "labels": [], "entities": [{"text": "NONE", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.8063856959342957}, {"text": "TOLOC", "start_pos": 102, "end_pos": 107, "type": "METRIC", "confidence": 0.9310731887817383}, {"text": "FROMLOC", "start_pos": 123, "end_pos": 130, "type": "METRIC", "confidence": 0.9951905012130737}, {"text": "RETURN", "start_pos": 156, "end_pos": 162, "type": "METRIC", "confidence": 0.9864943027496338}, {"text": "DEPERT", "start_pos": 166, "end_pos": 172, "type": "METRIC", "confidence": 0.8021190762519836}, {"text": "ARRIVE", "start_pos": 176, "end_pos": 182, "type": "METRIC", "confidence": 0.9937788844108582}, {"text": "STOPLOC", "start_pos": 190, "end_pos": 197, "type": "METRIC", "confidence": 0.844367265701294}]}, {"text": "For spoken inputs, we used the open source speech recognizer Sphinx2.", "labels": [], "entities": []}, {"text": "We trained the recognizer with only the domain-specific speech corpus.", "labels": [], "entities": []}, {"text": "The reported accuracy for Sphinx2 speech recognition is about 85%, but the accuracy of our speech recognizer is 76.27%; we used only a subset of the data without tuning and the sentences of this subset are longer and more complex than those of the removed ones, most of which are single-word responses.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9993385672569275}, {"text": "Sphinx2 speech recognition", "start_pos": 26, "end_pos": 52, "type": "TASK", "confidence": 0.7446098327636719}, {"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9994673132896423}, {"text": "speech recognizer", "start_pos": 91, "end_pos": 108, "type": "TASK", "confidence": 0.7225237488746643}]}, {"text": "All of our results have averaged over 5-fold cross validation with an 80/20 split of the data.", "labels": [], "entities": []}, {"text": "As it is standard, we compute precision and recall, which are evaluated on a per-entity basis and combined into a micro-averaged F1 score (F1 = 2PR/(P+R)).", "labels": [], "entities": [{"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9992417097091675}, {"text": "recall", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.9993206262588501}, {"text": "F1 score", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.9724521636962891}, {"text": "F1", "start_pos": 139, "end_pos": 141, "type": "METRIC", "confidence": 0.9653962254524231}]}, {"text": "A final model (a first-order linear chain CRF) is trained for 100 iterations with a Gaussian prior variance of 20, and 200 or fewer trigger features (down to again threshold of 1.0) for each round of inducing iteration (100 iterations of L-BFGS for the ME inducer and 10\u223c20 iterations of L-BFGS for the CRF inducer).", "labels": [], "entities": []}, {"text": "All experiments are implemented in C++ and executed on Linux with XEON 2.8 GHz dual processors and 2.0 Gbyte of main memory.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The result of local features, parser-based  features and trigger features", "labels": [], "entities": []}, {"text": " Table 2: Result of the trigger selection methods", "labels": [], "entities": []}, {"text": " Table 3: Comparison of variations in the induction algorithm (performed on one of the 5-fold validation  sets); columns are induction and total training time (h:m:s), number of trigger and total features, and  f-score on test data.  Inducer type  Approx.  Induction/total time # triggers/features F1 (Text) F1 (ASR)  CRF (empty) No approx.  3:55:01 / 5:27:13  682 / 2,693  90.23  67.60  CRF (local)  Approx. 1  1:25:28 / 2:56:49  750 / 5,241  94.87  71.65  ME (empty)  Approx. 2  20:57 / 1:54:22  618 / 2,080  94.85  71.46  ME (local)  Approx. 1+2  6:30 / 1:36:14  608 / 5,099  95.17  71.81", "labels": [], "entities": [{"text": "F1", "start_pos": 308, "end_pos": 310, "type": "METRIC", "confidence": 0.8450658917427063}, {"text": "Approx.", "start_pos": 401, "end_pos": 408, "type": "METRIC", "confidence": 0.8539553284645081}, {"text": "ME", "start_pos": 458, "end_pos": 460, "type": "METRIC", "confidence": 0.9866739511489868}, {"text": "Approx.", "start_pos": 470, "end_pos": 477, "type": "METRIC", "confidence": 0.5285621285438538}, {"text": "Approx.", "start_pos": 537, "end_pos": 544, "type": "METRIC", "confidence": 0.807776689529419}]}]}