{"title": [{"text": "Reinforcing English Countability Prediction with One Countability per Discourse Property", "labels": [], "entities": [{"text": "Reinforcing English Countability Prediction", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.8404443562030792}]}], "abstractContent": [{"text": "Countability of English nouns is important in various natural language processing tasks.", "labels": [], "entities": [{"text": "Countability of English nouns", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8955303728580475}, {"text": "natural language processing", "start_pos": 54, "end_pos": 81, "type": "TASK", "confidence": 0.7002800504366556}]}, {"text": "It especially plays an important role in machine translation since it determines the range of possible determiners.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.8318441808223724}]}, {"text": "This paper proposes a method for reinforcing countability prediction by introducing a novel concept called one countability per discourse.", "labels": [], "entities": [{"text": "reinforcing countability prediction", "start_pos": 33, "end_pos": 68, "type": "TASK", "confidence": 0.6769671142101288}]}, {"text": "It claims that when a noun appears more than once in a discourse, they will all share the same countability in the discourse.", "labels": [], "entities": []}, {"text": "The basic idea of the proposed method is that mispredictions can be correctly overridden using efficiently the one countability per discourse property.", "labels": [], "entities": []}, {"text": "Experiments show that the proposed method successfully reinforces countabil-ity prediction and outperforms other methods used for comparison.", "labels": [], "entities": [{"text": "countabil-ity prediction", "start_pos": 66, "end_pos": 90, "type": "TASK", "confidence": 0.6171067804098129}]}], "introductionContent": [{"text": "Countability of English nouns is important in various natural language processing tasks.", "labels": [], "entities": [{"text": "Countability of English nouns", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8955303728580475}, {"text": "natural language processing", "start_pos": 54, "end_pos": 81, "type": "TASK", "confidence": 0.7002800504366556}]}, {"text": "It is particularly important in machine translation from a source language that does not have an article system similar to that of English, such as Chinese and Japanese, into English since it determines the range of possible determiners including articles.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.708952397108078}]}, {"text": "It also plays an important role in determining whether a noun can take singular and plural forms.", "labels": [], "entities": []}, {"text": "Another useful application is to detect errors in article usage and singular/plural usage in the writing of second language learners.", "labels": [], "entities": []}, {"text": "Given countability, these errors can be detected in many cases.", "labels": [], "entities": []}, {"text": "For example, an error can be detected from \"We have a furniture.\" given that the noun furniture is uncountable since uncountable nouns do not tolerate the indefinite article.", "labels": [], "entities": []}, {"text": "Because of the wide range of applications, researchers have done a lot of work related to countability.", "labels": [], "entities": []}, {"text": "have proposed a method for automatically learning countability from corpus data. and have proposed web-based models for learning countability.", "labels": [], "entities": []}, {"text": "Others including Bond and Vatikiotis- and O' use ontology to determine countability.", "labels": [], "entities": [{"text": "O", "start_pos": 42, "end_pos": 43, "type": "DATASET", "confidence": 0.5723320841789246}]}, {"text": "In the application to error detection, researchers have explored alternative approaches since sources of evidence for determining countability are limited compared to other applications.", "labels": [], "entities": [{"text": "error detection", "start_pos": 22, "end_pos": 37, "type": "TASK", "confidence": 0.7066697627305984}]}, {"text": "Articles and the singular/plural distinction, which are informative for countability, cannot be used in countability prediction aiming at detecting errors in article usage and singular/plural usage.", "labels": [], "entities": [{"text": "countability prediction", "start_pos": 104, "end_pos": 127, "type": "TASK", "confidence": 0.7700185477733612}]}, {"text": "Returning to the previous example, the countability of the noun furniture cannot be determined as uncountable by the indefinite article; first, its countability has to be predicted without the indefinite article, and only then whether or not it tolerates the indefinite article is examined using the predicted countability.", "labels": [], "entities": []}, {"text": "Also, unlike in machine translation, the source language is not given in the writing of second language learners such as essays, which means that information available is limited.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.7439084649085999}]}, {"text": "To overcome these limitations, have proposed a method for predicting countability that relies solely on words (except articles and other determiners) surrounding the target noun.", "labels": [], "entities": [{"text": "predicting countability", "start_pos": 58, "end_pos": 81, "type": "TASK", "confidence": 0.8911956250667572}]}, {"text": "have shown that the method is effective to detecting errors in article usage and singular/plural usage in the writing of Japanese learners of English.", "labels": [], "entities": []}, {"text": "They also have shown that it is likely that performance of the error detection will improve as accuracy of the countability prediction increases since most of false positives are due to mispredictions.", "labels": [], "entities": [{"text": "error detection", "start_pos": 63, "end_pos": 78, "type": "TASK", "confidence": 0.7475557029247284}, {"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9992045760154724}]}, {"text": "In this paper, we propose a method for reinforcing countability prediction by introducing a novel concept called one countability per discourse that is an extension of one sense per discourse proposed by.", "labels": [], "entities": [{"text": "reinforcing countability prediction", "start_pos": 39, "end_pos": 74, "type": "TASK", "confidence": 0.6102018455664316}]}, {"text": "It claims that when a noun appears more than once in a discourse, they will all share the same countability in the discourse.", "labels": [], "entities": []}, {"text": "The basic idea of the proposed method is that initially mispredicted countability can be corrected using efficiently the one countability per discourse property.", "labels": [], "entities": []}, {"text": "The next section introduces the one countability per discourse concept and shows that it can be a good source of evidence for predicting countability.", "labels": [], "entities": [{"text": "predicting countability", "start_pos": 126, "end_pos": 149, "type": "TASK", "confidence": 0.8524573743343353}]}, {"text": "Section 3 discusses how it can be efficiently exploited to predict countability.", "labels": [], "entities": []}, {"text": "Section 4 describes the proposed method.", "labels": [], "entities": []}, {"text": "Section 5 describes experiments conducted to evaluate the proposed method and discusses the results.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the experiments, we chose Nagata et al.", "labels": [], "entities": []}, {"text": "(2005a)'s method as the one to be reinforced by the proposed method.", "labels": [], "entities": []}, {"text": "In this method, the decision list (DL) learning algorithm is used.", "labels": [], "entities": [{"text": "decision list (DL) learning", "start_pos": 20, "end_pos": 47, "type": "TASK", "confidence": 0.6365174303452173}]}, {"text": "However, we used the ME algorithm because we found that the method with the ME algorithm instead of the DL learning algorithm performed better when trained on the same training data.", "labels": [], "entities": []}, {"text": "As the target noun, we selected 23 nouns that were also used in's experiments.", "labels": [], "entities": []}, {"text": "They are exemplified as nouns that are used as both countable and uncountable by.", "labels": [], "entities": []}, {"text": "Training data were generated from the written part of the British National Corpus.", "labels": [], "entities": [{"text": "British National Corpus", "start_pos": 58, "end_pos": 81, "type": "DATASET", "confidence": 0.9415023326873779}]}, {"text": "A text tagged with the text tags was used as a discourse unit.", "labels": [], "entities": []}, {"text": "From the corpus, 314 texts, which amounted to about 10% of all texts, were randomly taken to obtain test data.", "labels": [], "entities": []}, {"text": "The rest of texts were used to generate training data.", "labels": [], "entities": []}, {"text": "We evaluated performance of prediction by accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9992877840995789}]}, {"text": "We defined accuracy by the ratio of the number of correct predictions to that of instances of the target noun in the test data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9994887113571167}]}, {"text": "First, we generated training data for each target noun from the texts using the tagging rules explained in Subsection 4.1.", "labels": [], "entities": []}, {"text": "We used the OAK system 5 to extract noun phrases and their heads.", "labels": [], "entities": []}, {"text": "Of the extracted instances, we excluded those that had no contextual cues from the training data (and also the test data).", "labels": [], "entities": []}, {"text": "We also generated another set of training data by removing the majority countability features from them.", "labels": [], "entities": []}, {"text": "This set of training data was used for comparison.", "labels": [], "entities": []}, {"text": "Second, we obtained test data by applying the tagging rules described in Subsection 4.1 to each instance of the target noun in the 314 texts.", "labels": [], "entities": []}, {"text": "showed that the tagging rules 5 http://www.cs.nyu.edu/ sekine/PROJECT/OAK/ achieved an accuracy of 0.997 in the texts that contained no errors.", "labels": [], "entities": [{"text": "OAK", "start_pos": 70, "end_pos": 73, "type": "METRIC", "confidence": 0.8514944314956665}, {"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9992319345474243}]}, {"text": "Considering these results, we used the tagging rules to obtain test data.", "labels": [], "entities": []}, {"text": "Instances tagged with \"?\" were excluded in the experiments.", "labels": [], "entities": []}, {"text": "Third, we applied the ME algorithm 6 to the training data without the majority countability feature.", "labels": [], "entities": [{"text": "ME algorithm 6", "start_pos": 22, "end_pos": 36, "type": "METRIC", "confidence": 0.9338422815004984}]}, {"text": "Using the resulting model, countability of the target nouns in the test data was predicted.", "labels": [], "entities": []}, {"text": "Then, the predictions were reinforced by the proposed method.", "labels": [], "entities": []}, {"text": "The threshold to filter out spurious predictions was set to ! # \" . For comparison, the predictions obtained by the ME model were simply replaced with the estimated majority countability for each discourse.", "labels": [], "entities": []}, {"text": "In this method, the original predictions were used when the estimated majority countability was unknown.", "labels": [], "entities": []}, {"text": "Also,'s method that was based on the DL learning algorithm was implemented for comparison.", "labels": [], "entities": []}, {"text": "Finally, we calculated accuracy of each method.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9997615218162537}]}, {"text": "In addition to the results, we evaluated the baseline on the same test data where all predictions were done by the majority countability for the whole corpus (training data).", "labels": [], "entities": []}, {"text": "shows the accuracies 7 . \"ME\" and \"Proposed\" in refer to accuracies of the ME model and the ME model reinforced by the proposed method, respectively.", "labels": [], "entities": [{"text": "ME", "start_pos": 26, "end_pos": 28, "type": "METRIC", "confidence": 0.9219045042991638}, {"text": "Proposed", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.971221923828125}]}, {"text": "\"ME+MCD\" refers to accuracy obtained by replacing predictions of the ME model with the estimated majority countability for each discourse.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9992971420288086}]}, {"text": "Also, \"DL\" refers to accuracy of the DL-based method.", "labels": [], "entities": [{"text": "DL", "start_pos": 7, "end_pos": 9, "type": "METRIC", "confidence": 0.9599639773368835}, {"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9995495676994324}]}, {"text": "shows that the three ME-based methods (\"Proposed\", \"ME\", and \"ME+MCD\") perform better than \"DL\" and the baseline.", "labels": [], "entities": []}, {"text": "Especially, \"Proposed\" outperforms the other methods inmost of the target nouns.", "labels": [], "entities": []}, {"text": "summarizes the comparison between the three ME-based methods.", "labels": [], "entities": []}, {"text": "Each plot in represents each target noun.", "labels": [], "entities": []}, {"text": "The horizontal and vertical axises correspond to accuracy of \"ME\" and that of \"Proposed\" (or \"ME+MCD\"), respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9996423721313477}, {"text": "ME", "start_pos": 62, "end_pos": 64, "type": "METRIC", "confidence": 0.9641550779342651}]}, {"text": "The diagonal line corresponds to the line $ % ' & . So if \"Proposed\" (or \"ME+MCD\") achieved no improvement at allover \"ME\", all the 6 All ME models were generated using the opennlp.maxent package (http://maxent.sourceforge.net/).", "labels": [], "entities": []}, {"text": "The baseline in is different from that in because discourses where the target noun appears only once are not taken into account in.: Comparison between \"ME\" and \"Proposed/ME+MCD\" in each target noun plots would be on the line.", "labels": [], "entities": []}, {"text": "Plots above the line mean improvement over \"ME\" and the distance from the line expresses the amount of improvement.", "labels": [], "entities": [{"text": "ME", "start_pos": 44, "end_pos": 46, "type": "METRIC", "confidence": 0.996405839920044}]}, {"text": "Plots below the line mean the opposite.", "labels": [], "entities": []}, {"text": "clearly shows that most of the plots (( ) corresponding to the comparison between \"ME\" and \"Proposed\" are above the line.", "labels": [], "entities": [{"text": "ME", "start_pos": 83, "end_pos": 85, "type": "METRIC", "confidence": 0.9386726021766663}]}, {"text": "This means that the proposed method successfully reinforced \"ME\" inmost of the target nouns.", "labels": [], "entities": [{"text": "ME", "start_pos": 61, "end_pos": 63, "type": "METRIC", "confidence": 0.9813169240951538}]}, {"text": "Indeed, the average accuracy of \"Proposed\" is significantly superior to that of \"ME\" at the 99% confidence level (paired t-test).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9995325803756714}, {"text": "ME", "start_pos": 81, "end_pos": 83, "type": "METRIC", "confidence": 0.9846634864807129}]}, {"text": "This improvement is close to that of one sense per discourse (Yarowsky, 1995) (improvement ranging from 1.3% to 1.7%), which seems to be a sensible upper bound of the proposed method.", "labels": [], "entities": []}, {"text": "By contrast, about half of the plots () ) corresponding to the comparison between \"ME\" and \"ME+MCD\" are below the line.", "labels": [], "entities": [{"text": "ME", "start_pos": 83, "end_pos": 85, "type": "METRIC", "confidence": 0.8506479859352112}]}, {"text": "From these results, it follows that the one countability per discourse property is a good source of evidence for predicting countability, but it is crucial to devise away of exploiting the property as we did in this paper.", "labels": [], "entities": []}, {"text": "Namely, simply replacing original predictions with the majority countability for the discourse causes side effects, which has been already suggested in.", "labels": [], "entities": []}, {"text": "This is also exemplified as follows.", "labels": [], "entities": []}, {"text": "Suppose that several instances of the target noun advantage appear in a discourse and that its majority countably is countable.", "labels": [], "entities": []}, {"text": "Further suppose that an idiomatic phrase \"take advantage of\" of which countability is uncountable happens to appear in it.", "labels": [], "entities": []}, {"text": "On one hand, simply replacing all the predictions with its majority countability (countable) would lead to a misprediction for the idiomatic phrase even if the original prediction is correct.", "labels": [], "entities": []}, {"text": "On the other hand, the proposed method would correctly predict the countability because the contextual cues strongly indicate that it is uncountable.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracy obtained by Majority Count- ability for Discourse", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9989432692527771}, {"text": "Count- ability", "start_pos": 40, "end_pos": 54, "type": "METRIC", "confidence": 0.5639183024565378}]}]}