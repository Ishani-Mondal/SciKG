{"title": [{"text": "Using Machine-Learning to Assign Function Labels to Parser Output for Spanish", "labels": [], "entities": []}], "abstractContent": [{"text": "Data-driven grammatical function tag assignment has been studied for English using the Penn-II Treebank data.", "labels": [], "entities": [{"text": "Data-driven grammatical function tag assignment", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.6569839656352997}, {"text": "Penn-II Treebank data", "start_pos": 87, "end_pos": 108, "type": "DATASET", "confidence": 0.9944554964701334}]}, {"text": "In this paper we address the question of whether such methods can be applied successfully to other languages and treebank resources.", "labels": [], "entities": []}, {"text": "In addition to tag assignment accuracy and f-scores we also present results of a task-based evaluation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9684716463088989}]}, {"text": "We use three machine-learning methods to assign Cast3LB function tags to sentences parsed with Bikel's parser trained on the Cast3LB treebank.", "labels": [], "entities": [{"text": "Cast3LB treebank", "start_pos": 125, "end_pos": 141, "type": "DATASET", "confidence": 0.8816215991973877}]}, {"text": "The best performing method, SVM, achieves an f-score of 86.87% on gold-standard trees and 66.67% on parser output-a statistically significant improvement of 6.74% over the baseline.", "labels": [], "entities": [{"text": "f-score", "start_pos": 45, "end_pos": 52, "type": "METRIC", "confidence": 0.9872838258743286}]}, {"text": "Ina task-based evaluation we generate LFG functional-structures from the function-tag-enriched trees.", "labels": [], "entities": []}, {"text": "On this task we achive an f-score of 75.67%, a statistically significant 3.4% improvement over the baseline.", "labels": [], "entities": [{"text": "f-score", "start_pos": 26, "end_pos": 33, "type": "METRIC", "confidence": 0.9916127920150757}]}], "introductionContent": [{"text": "The research presented in this paper forms part of an ongoing effort to develop methods to induce wide-coverage multilingual LexicalFunctional Grammar (LFG)) resources from treebanks by means of automatically associating LFG f-structure information with constituency trees produced by probabilistic parsers ).", "labels": [], "entities": []}, {"text": "Inducing deep syntactic analyses from treebank data avoids the cost and time involved in manually creating wide-coverage resources.", "labels": [], "entities": []}, {"text": "Lexical Functional Grammar f-structures provide a level of syntactic representation based on the notion of grammatical functions (e.g. Subject, Object, Oblique, Adjunct etc.).", "labels": [], "entities": []}, {"text": "This level is more abstract and cross-linguistically more uniform than constituency trees.", "labels": [], "entities": []}, {"text": "F-structures also include explicit encodings of phenomena such as control and raising, pro-drop or long distance dependencies.", "labels": [], "entities": []}, {"text": "Those characteristics make this level a suitable representation for many NLP applications such as transfer-based Machine Translation or Question Answering.", "labels": [], "entities": [{"text": "transfer-based Machine Translation", "start_pos": 98, "end_pos": 132, "type": "TASK", "confidence": 0.623453269402186}, {"text": "Question Answering", "start_pos": 136, "end_pos": 154, "type": "TASK", "confidence": 0.8254435360431671}]}, {"text": "The f-structure annotation algorithm used for inducing LFG resources from the Penn-II treebank for English ( ) uses configurational, categorial, function tag and trace information.", "labels": [], "entities": [{"text": "Penn-II treebank", "start_pos": 78, "end_pos": 94, "type": "DATASET", "confidence": 0.9781261682510376}]}, {"text": "In contrast to English, in many other languages configurational information is not a good predictor for LFG grammatical function assignment.", "labels": [], "entities": [{"text": "LFG grammatical function assignment", "start_pos": 104, "end_pos": 139, "type": "TASK", "confidence": 0.7670229524374008}]}, {"text": "For such languages the function tags included in many treebanks area much more important source of information for the LFG annotation algorithm than Penn-II tags are for English.", "labels": [], "entities": []}, {"text": "Cast3LB), the Spanish treebank used in the current research, contains comprehensive grammatical function annotation.", "labels": [], "entities": [{"text": "Spanish treebank", "start_pos": 14, "end_pos": 30, "type": "DATASET", "confidence": 0.769390732049942}]}, {"text": "In the present paper we use a machine-learning approach in order to add Cast3LB function tags to nodes of basic constituent trees output by a probabilistic parser trained on Cast3LB.", "labels": [], "entities": []}, {"text": "To our knowledge, this paper is the first to describe applying a data-driven approach to function-tag assignment to a language other than English.", "labels": [], "entities": [{"text": "function-tag assignment", "start_pos": 89, "end_pos": 112, "type": "TASK", "confidence": 0.7267907559871674}]}, {"text": "Our method statistically significantly outperforms the previously used approach which relied exclusively on the parser to produce trees with Cast3LB tags (O').", "labels": [], "entities": []}, {"text": "Additionally, we perform a task-driven evaluation of our Cast3LB tag assignment method by using the tag-enriched trees as input to the Spanish LFG fstructure annotation algorithm and evaluating the quality of the resulting f-structures.", "labels": [], "entities": [{"text": "Cast3LB tag assignment", "start_pos": 57, "end_pos": 79, "type": "TASK", "confidence": 0.6031211614608765}]}, {"text": "Section 2 describes the Spanish Cast3LB treebank.", "labels": [], "entities": [{"text": "Spanish Cast3LB treebank", "start_pos": 24, "end_pos": 48, "type": "DATASET", "confidence": 0.8629683454831442}]}, {"text": "In Section 3 we describe previous research in LFG induction for English and Spanish as well as research on data-driven function tag assignment to parsed text in English.", "labels": [], "entities": [{"text": "LFG induction", "start_pos": 46, "end_pos": 59, "type": "TASK", "confidence": 0.8869084417819977}, {"text": "function tag assignment", "start_pos": 119, "end_pos": 142, "type": "TASK", "confidence": 0.6360174020131429}]}, {"text": "Section 4 provides the details of our approach to the Cast3LB function tag assignment task.", "labels": [], "entities": [{"text": "Cast3LB function tag assignment task", "start_pos": 54, "end_pos": 90, "type": "TASK", "confidence": 0.6717949748039246}]}, {"text": "In Sections 5 and 6 we present evaluation results for our method.", "labels": [], "entities": []}, {"text": "In Section 7 we present the error analysis of the results.", "labels": [], "entities": [{"text": "error analysis", "start_pos": 28, "end_pos": 42, "type": "METRIC", "confidence": 0.951165646314621}]}, {"text": "Finally, in Section 8 we conclude and discuss ideas for further research.", "labels": [], "entities": []}], "datasetContent": [{"text": "We present evaluation results on the original goldstandard trees of the test set as well as on the test-set sentences parsed by Bikel's parser.", "labels": [], "entities": []}, {"text": "For the evaluation of Cast3LB function tagging performance on gold trees the most straightforward metric is the accuracy, or the proportion of all candidate nodes that were assigned the correct label.", "labels": [], "entities": [{"text": "Cast3LB function tagging", "start_pos": 22, "end_pos": 46, "type": "TASK", "confidence": 0.5724246799945831}, {"text": "accuracy", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.9994620680809021}]}, {"text": "However we cannot use this metric for evaluating results on the parser output.", "labels": [], "entities": []}, {"text": "The trees output by the parser are not identical to gold standard trees due to parsing errors, and the set of candidate nodes extracted from parsed trees will not be the same as for gold trees.", "labels": [], "entities": []}, {"text": "For this reason we use an alternative metric which is independent of tree configuration and uses only the Cast3LB function labels and positional indices of tokens in a sentence.", "labels": [], "entities": []}, {"text": "For each function-tagged tree we first remove the punctuation tokens.", "labels": [], "entities": []}, {"text": "Then we extract a set of tuples of the form GF, i, j, where GF is the Cast3LB function tag and i \u2212 j is the range of tokens spanned by the node annotated with this function.", "labels": [], "entities": []}, {"text": "We use the standard measures of precision, recall and f-score to evaluate the results.", "labels": [], "entities": [{"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9997482895851135}, {"text": "recall", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.9995906949043274}, {"text": "f-score", "start_pos": 54, "end_pos": 61, "type": "METRIC", "confidence": 0.9910536408424377}]}, {"text": "Results for the three algorithms are shown in.", "labels": [], "entities": []}, {"text": "MBL and MaxEnt show a very similar performance, while SVM outperforms both,, are also informative, with SVM outperforming the other two methods for all training set sizes.", "labels": [], "entities": [{"text": "MBL", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8999184966087341}]}, {"text": "In particular, the last section of the plot shows SVM performing almost as well as MBL with half as much learning material.", "labels": [], "entities": [{"text": "SVM", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.5476279854774475}, {"text": "MBL", "start_pos": 83, "end_pos": 86, "type": "DATASET", "confidence": 0.6502811312675476}]}, {"text": "Neither of the three curves shows signs of having reached a maximum, which indicates that in- shows the performance of the three methods on parser output.", "labels": [], "entities": []}, {"text": "The baseline contains the results achieved by treating compound category-function labels as atomic during parser training so that they are included in parser output.", "labels": [], "entities": []}, {"text": "For this task we present two sets of results: (i) for all constituents, and (ii) for correctly parsed constituents only.", "labels": [], "entities": []}, {"text": "Again the best algorithm turns out to be SVM.", "labels": [], "entities": [{"text": "SVM", "start_pos": 41, "end_pos": 44, "type": "DATASET", "confidence": 0.802219033241272}]}, {"text": "It outperforms the baseline by a large margin (6.74% for all constituents).", "labels": [], "entities": []}, {"text": "The difference in performance for gold standard trees, and the correctly parsed constituents in parser output is rather larger than what Blaheta and Charniak report.", "labels": [], "entities": []}, {"text": "Further analysis is needed to identify the source of this difference but we suspect that one contributing factor is the use of greater number of context features combined with a higher parse error rate in comparison to their experiments on the Penn II Treebank.", "labels": [], "entities": [{"text": "parse error rate", "start_pos": 185, "end_pos": 201, "type": "METRIC", "confidence": 0.9123418132464091}, {"text": "Penn II Treebank", "start_pos": 244, "end_pos": 260, "type": "DATASET", "confidence": 0.9819947083791097}]}, {"text": "Since any misanalysis of constituency structure in the vicinity of target node can have negative impact, greater reliance on context means greater susceptibility to parse errors.", "labels": [], "entities": []}, {"text": "Another factor to consider is the fact that we trained and adjusted parameters on goldstandard trees, and the model learned may rely on features of those trees that the parser is unable to reproduce.", "labels": [], "entities": []}, {"text": "For the experiments on parser output (all constituents) we performed a series of sign tests in order to determine to what extent the differences in performance between the different methods are statistically significant.", "labels": [], "entities": []}, {"text": "For each pair of methods we calculate the f-score for each sentence in the test set.", "labels": [], "entities": []}, {"text": "For those sentences on which the scores differ (i.e. the number of trials) we calculate in how many cases the second method is better than the first (i.e. the number of successes).", "labels": [], "entities": []}, {"text": "We then perform the test with the null hypothesis that the probability of success is chance (= 0.5) and the alternative hypothesis that the probability of success is greater than chance (> 0.5).", "labels": [], "entities": []}, {"text": "The results are summarized in.", "labels": [], "entities": []}, {"text": "Given that we perform 4 pairwise comparisons, we apply the Bonferroni correction and adjust our target \u03b1 \u03b2 = \u03b1 4 . For the confidence level 95% (\u03b1 \u03b2 = 0.0125) all pairs give statistically significant results, except for MBL vs MaxEnt.", "labels": [], "entities": [{"text": "MBL", "start_pos": 220, "end_pos": 223, "type": "DATASET", "confidence": 0.7305823564529419}, {"text": "MaxEnt", "start_pos": 227, "end_pos": 233, "type": "DATASET", "confidence": 0.8436046242713928}]}, {"text": "Finally, we also evaluated the actual f-structures obtained by running the LFG-annotation algorithm on trees produced by the parser and enriched with Cast3LB function tags assigned using SVM.", "labels": [], "entities": []}, {"text": "For this task-based evaluation we produced a gold standard consisting of f-structures corresponding to all sentences in the test set.", "labels": [], "entities": []}, {"text": "The LFG-annotation algorithm was run on the test set trees (which contained original Cast3LB treebank function tags), and the resulting f-structures were manually corrected.", "labels": [], "entities": [{"text": "Cast3LB treebank function tags", "start_pos": 85, "end_pos": 115, "type": "DATASET", "confidence": 0.9162221401929855}]}, {"text": "Following, we convert the f-structures to triples of the form GF, P i , P j , where P i is the value of the PRED attribute of the f-structure, GF is an LFG grammatical function attribute, and P j is the value of the PRED attribute of the f-structure which is the value of the GF attribute.", "labels": [], "entities": []}, {"text": "This is done recursively for each level of embedding in the f-structure.", "labels": [], "entities": []}, {"text": "Attributes with atomic values are ignored for the purposes of this evaluation.", "labels": [], "entities": []}, {"text": "The results obtained are shown in Table 7.", "labels": [], "entities": []}, {"text": "We also performed a statistical significance test for these results, using the same method as for the Cast3LB tag assigment task.", "labels": [], "entities": [{"text": "Cast3LB tag assigment task", "start_pos": 102, "end_pos": 128, "type": "TASK", "confidence": 0.62649255245924}]}, {"text": "The p-value given by the sign test was 2.118\u00d710 \u22125 , comfortably below \u03b1 = 1%.", "labels": [], "entities": []}, {"text": "The higher scores achieved in the LFG fstructure evaluation in comparison with the preceding Cast3LB tag assignment evaluation (  CC does not affect the f-structure score.", "labels": [], "entities": []}, {"text": "On the other hand the Cast3LB CD tag can be mapped to OBJ, COMP, or XCOMP, and it can be easily decided which one is appropriate depending on the category label of the target node.", "labels": [], "entities": [{"text": "Cast3LB CD tag", "start_pos": 22, "end_pos": 36, "type": "DATASET", "confidence": 0.89690633614858}, {"text": "OBJ", "start_pos": 54, "end_pos": 57, "type": "DATASET", "confidence": 0.5541847348213196}]}, {"text": "Additionally many nodes which receive no function tag in Cast3LB, such as noun modifiers, are straightforwardly mapped to LFG ADJUNCT.", "labels": [], "entities": [{"text": "LFG ADJUNCT", "start_pos": 122, "end_pos": 133, "type": "DATASET", "confidence": 0.8298399448394775}]}, {"text": "Similarly, objects of prepositions receive the LFG OBJ function.", "labels": [], "entities": [{"text": "LFG OBJ function", "start_pos": 47, "end_pos": 63, "type": "METRIC", "confidence": 0.7422869404157003}]}, {"text": "Secondly, the f-structure evaluation metric is less sensitive to small constituency misconfigurations: it is not necessary to correctly identify the token range spanned by a target node as long as the head (which provides the PRED attribute) is correct.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Features included in POS tags. Type  refers to subcategories of parts of speech such as  e.g. common and proper for nouns, or main, aux- iliary and semiauxiliary for verbs. For details see  (Civit, 2000).", "labels": [], "entities": []}, {"text": " Table 4: Cast3LB function tagging performance  for gold-standard trees", "labels": [], "entities": [{"text": "Cast3LB function tagging", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.6618329385916392}]}, {"text": " Table 5: Cast3LB function tagging performance  for parser output, for all constituents, and for cor- rectly parsed constituents only", "labels": [], "entities": [{"text": "Cast3LB function tagging", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.5748973190784454}]}, {"text": " Table 6: Statistical significance testing results on  for the Cast3LB tag assignment on parser output.", "labels": [], "entities": []}, {"text": " Table 7: LFG F-structure evaluation results for  parser output", "labels": [], "entities": []}, {"text": " Table 8: Simplified confusion matrix for SVM  on test-set gold-standard trees. The gold-standard  Cast3LB function tags are shown in the first row,  the predicted tags in the first column. So e.g. SUJ  was mistagged as CD in 26 cases. Low frequency  function tags as well as those rarely mispredicted  have been omitted for clarity.", "labels": [], "entities": [{"text": "SVM", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.9638208746910095}]}]}