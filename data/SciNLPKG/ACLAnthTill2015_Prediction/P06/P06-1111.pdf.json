{"title": [], "abstractContent": [{"text": "We investigate prototype-driven learning for primarily unsupervised grammar induction.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 68, "end_pos": 85, "type": "TASK", "confidence": 0.7485639452934265}]}, {"text": "Prior knowledge is specified declaratively, by providing a few canonical examples of each target phrase type.", "labels": [], "entities": []}, {"text": "This sparse prototype information is then propagated across a corpus using distributional similarity features, which augment an otherwise standard PCFG model.", "labels": [], "entities": []}, {"text": "We show that distributional features are effective at distinguishing bracket labels, but not determining bracket locations.", "labels": [], "entities": []}, {"text": "To improve the quality of the induced trees, we combine our PCFG induction with the CCM model of Klein and Manning (2002), which has complementary stengths: it identifies brackets but does not label them.", "labels": [], "entities": []}, {"text": "Using only a handful of prototypes, we show substantial improvements over naive PCFG induction for English and Chinese grammar induction.", "labels": [], "entities": [{"text": "English and Chinese grammar induction", "start_pos": 99, "end_pos": 136, "type": "TASK", "confidence": 0.6195955216884613}]}], "introductionContent": [{"text": "There has been a great deal of work on unsupervised grammar induction, with motivations ranging from scientific interest in language acquisition to engineering interest in parser construction.", "labels": [], "entities": [{"text": "unsupervised grammar induction", "start_pos": 39, "end_pos": 69, "type": "TASK", "confidence": 0.6819286048412323}, {"text": "language acquisition", "start_pos": 124, "end_pos": 144, "type": "TASK", "confidence": 0.7229385673999786}, {"text": "parser construction", "start_pos": 172, "end_pos": 191, "type": "TASK", "confidence": 0.9025195837020874}]}, {"text": "Recent work has successfully induced unlabeled grammatical structure, but has not successfully learned labeled tree structure () . In this paper, our goal is to build a system capable of producing labeled parses in a target grammar with as little total effort as possible.", "labels": [], "entities": []}, {"text": "We investigate a prototype-driven approach to grammar induction, in which one supplies canonical examples of each target concept.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.798879861831665}]}, {"text": "For example, we might specify that we are interested in trees which use the symbol NP and then list several examples of prototypical NPs (determiner noun, pronouns, etc., see fora sample prototype list).", "labels": [], "entities": []}, {"text": "This prototype information is similar to specifying an annotation scheme, which even human annotators must be provided before they can begin the construction of a treebank.", "labels": [], "entities": []}, {"text": "In principle, prototypedriven learning is just a kind of semi-supervised learning.", "labels": [], "entities": []}, {"text": "However, in practice, the information we provide is on the order of dozens of total seed instances, instead of a handful of fully parsed trees, and is of a different nature.", "labels": [], "entities": []}, {"text": "The prototype-driven approach has three strengths.", "labels": [], "entities": []}, {"text": "First, since we provide a set of target symbols, we can evaluate induced trees using standard labeled parsing metrics, rather than the far more forgiving unlabeled metrics described in, for example,.", "labels": [], "entities": []}, {"text": "Second, knowledge is declaratively specified in an interpretable way (see).", "labels": [], "entities": []}, {"text": "If a user of the system is unhappy with its systematic behavior, they can alter it by altering the prototype information (see section 7.1 for examples).", "labels": [], "entities": []}, {"text": "Third, and related to the first two, one does not confuse the ability of the system to learn a consistent grammar with its ability to learn the grammar a user has in mind.", "labels": [], "entities": []}, {"text": "In this paper, we present a series of experiments in the induction of labeled context-free trees using a combination of unlabeled data and sparse prototypes.", "labels": [], "entities": []}, {"text": "We first affirm the well-known result that simple, unconstrained PCFG induction produces grammars of poor quality as measured against treebank structures.", "labels": [], "entities": []}, {"text": "We then augment a PCFG with prototype features, and show that these features, when propagated to non-prototype sequences using distributional similarity, are effective at learning bracket labels on fixed unlabeled trees, but are still not enough to learn good tree structures without bracketing information.", "labels": [], "entities": []}, {"text": "Finally, we intersect the feature-augmented PCFG with the CCM model of, a highquality bracketing model.", "labels": [], "entities": []}, {"text": "The intersected model is able to learn trees with higher unlabeled F 1 than those in.", "labels": [], "entities": [{"text": "F 1", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.9473317563533783}]}, {"text": "More impor-tantly, its trees are labeled and can be evaluated according to labeled metrics.", "labels": [], "entities": []}, {"text": "Against the English Penn Treebank, our final trees achieve a labeled F 1 of 65.1 on short sentences, a 51.7% error reduction over naive PCFG induction.", "labels": [], "entities": [{"text": "English Penn Treebank", "start_pos": 12, "end_pos": 33, "type": "DATASET", "confidence": 0.9056019385655721}, {"text": "F 1", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.9168045818805695}, {"text": "error", "start_pos": 109, "end_pos": 114, "type": "METRIC", "confidence": 0.9691861867904663}]}], "datasetContent": [{"text": "The majority of our experiments induced tree structures from the WSJ section of the English Penn treebank, though see section 7.4 for an experiment on Chinese.", "labels": [], "entities": [{"text": "WSJ section of the English Penn treebank", "start_pos": 65, "end_pos": 105, "type": "DATASET", "confidence": 0.9254743882587978}]}, {"text": "To facilitate comparison with previous work, we extracted WSJ-10, the 7,422 sentences which contain 10 or fewer words after the removal of punctuation and null elements according to the scheme detailed in.", "labels": [], "entities": [{"text": "WSJ-10", "start_pos": 58, "end_pos": 64, "type": "DATASET", "confidence": 0.8835223913192749}]}, {"text": "We learned models on all or part of this data and compared their predictions to the manually annotated treebank trees for the sentences on which the model was trained.", "labels": [], "entities": []}, {"text": "As in previous work, we begin with the part-of-speech (POS) tag sequences for each sentence rather than lexical sequences.", "labels": [], "entities": []}, {"text": "Following, we report unlabeled bracket precision, recall, and F 1 . Note that according to their metric, brackets of size 1 are omitted from the evaluation.", "labels": [], "entities": [{"text": "precision", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.8604970574378967}, {"text": "recall", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.9996236562728882}, {"text": "F 1", "start_pos": 62, "end_pos": 65, "type": "METRIC", "confidence": 0.995392233133316}]}, {"text": "Unlike that work, all of our induction methods produce trees labeled with symbols which are identified with treebank categories.", "labels": [], "entities": []}, {"text": "Therefore, we also report labeled precision, recall, and F 1 , still ignoring brackets of size 1.", "labels": [], "entities": [{"text": "precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9480201005935669}, {"text": "recall", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9995301961898804}, {"text": "F 1", "start_pos": 57, "end_pos": 60, "type": "METRIC", "confidence": 0.9921444356441498}]}, {"text": "As an initial experiment, we used the insideoutside algorithm to induce a PCFG in the straightforward way).", "labels": [], "entities": []}, {"text": "For all the experiments in this paper, we considered binary PCFGs over the nonterminals and terminals occuring in WSJ-10.", "labels": [], "entities": [{"text": "WSJ-10", "start_pos": 114, "end_pos": 120, "type": "DATASET", "confidence": 0.9639347195625305}]}, {"text": "The PCFG rules were of the following forms: \u2022 X \u2192 Y Z, for nonterminal types X, Y, and Z, with Y = X or Z = X \u2022 X \u2192 t Y , X \u2192 Y t, for each terminal t \u2022 X \u2192 t t , for terminals t and t For a given sentence S, our CFG generates labeled trees T over S.", "labels": [], "entities": []}, {"text": "Each tree consists of binary productions X(i, j) \u2192 \u03b1 over constituent spans, where \u03b1 is a pair of non-terminal and/or terminal symbols in the grammar.", "labels": [], "entities": []}, {"text": "The generative probability of a tree T for S is: In the inside-outside algorithm, we iteratively compute posterior expectations overproduction occurences at each training span, then use those expectations to re-estimate production probabilities.", "labels": [], "entities": []}, {"text": "This process is guaranteed to converge to a local extremum of the data likelihood, but initial production probability estimates greatly influence the final grammar).", "labels": [], "entities": []}, {"text": "In particular, uniform initial estimates are an (unstable) fixed point.", "labels": [], "entities": []}, {"text": "The classic approach is to add a small amount of random noise to the initial probabilities in order to break the symmetry between grammar symbols.", "labels": [], "entities": []}, {"text": "We randomly initialized 5 grammars using treebank non-terminals and trained each to convergence on the first 2000 sentences of WSJ-10.", "labels": [], "entities": [{"text": "WSJ-10", "start_pos": 127, "end_pos": 133, "type": "DATASET", "confidence": 0.9745995402336121}]}, {"text": "Viterbi parses were extracted for each of these 2000 sentences according to each grammar.", "labels": [], "entities": []}, {"text": "Of course, the parses' symbols have nothing to anchor them to our intended treebank symbols.", "labels": [], "entities": []}, {"text": "That is, an NP in one of these grammars may correspond to the target symbol VP, or may not correspond well to any target symbol.", "labels": [], "entities": []}, {"text": "To evaluate these learned grammars, we must map the models' phrase types to target phrase types.", "labels": [], "entities": []}, {"text": "For each grammar, we followed the common approach of greedily mapping model symbols to target symbols in the way which maximizes the labeled F 1 . Note that this can, and does, result in mapping multiple model symbols to the most frequent target symbols.", "labels": [], "entities": []}, {"text": "This experiment, labeled PCFG \u00d7 NONE in, resulted in an average labeled F 1 of 26.3 and an unlabeled F 1 of 45.7.", "labels": [], "entities": [{"text": "F 1", "start_pos": 72, "end_pos": 75, "type": "METRIC", "confidence": 0.9839633107185364}, {"text": "F 1", "start_pos": 101, "end_pos": 104, "type": "METRIC", "confidence": 0.9702070951461792}]}, {"text": "The unlabeled F 1 is better than randomly choosing a tree (34.7), but not better than always choosing aright branching structure (61.7).", "labels": [], "entities": [{"text": "F 1", "start_pos": 14, "end_pos": 17, "type": "METRIC", "confidence": 0.9610272943973541}]}, {"text": "suggest that the task of labeling constituents is significantly easier than identifying them.", "labels": [], "entities": []}, {"text": "Perhaps it is too much to ask a PCFG induction algorithm to perform both of these tasks simultaneously.", "labels": [], "entities": []}, {"text": "Along the lines of, we reran the insideoutside algorithm, but this time placed zero mass on all trees which did not respect the bracketing of the gold trees.", "labels": [], "entities": []}, {"text": "This constraint does not fully: English phrase type prototype list manually specified (The entire supervision for our system).", "labels": [], "entities": []}, {"text": "The second part of the table is additional prototypes discussed in section 7.1.", "labels": [], "entities": []}, {"text": "eliminate the structural uncertainty since we are inducing binary trees and the gold trees are flatter than binary in many cases.", "labels": [], "entities": []}, {"text": "This approach of course achieved the upper bound on unlabeled F 1 , because of the gold bracket constraints.", "labels": [], "entities": [{"text": "F 1", "start_pos": 62, "end_pos": 65, "type": "METRIC", "confidence": 0.9522531628608704}]}, {"text": "However, it only resulted in an average labeled F 1 of 52.6 (experiment PCFG \u00d7 GOLD in).", "labels": [], "entities": [{"text": "F 1", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.9483706653118134}]}, {"text": "While this labeled score is an improvement over the PCFG \u00d7 NONE experiment, it is still relatively disappointing.", "labels": [], "entities": [{"text": "PCFG \u00d7 NONE experiment", "start_pos": 52, "end_pos": 74, "type": "DATASET", "confidence": 0.8107618987560272}]}], "tableCaptions": []}