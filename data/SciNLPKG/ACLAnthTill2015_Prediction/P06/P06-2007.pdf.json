{"title": [{"text": "N Semantic Classes are Harder than Two", "labels": [], "entities": []}], "abstractContent": [{"text": "We show that we can automatically classify semantically related phrases into 10 classes.", "labels": [], "entities": []}, {"text": "Classification robustness is improved by training with multiple sources of evidence, including within-document cooccurrence, HTML markup, syntactic relationships in sentences, substitutability in query logs, and string similarity.", "labels": [], "entities": [{"text": "Classification robustness", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8793205916881561}]}, {"text": "Our work provides a benchmark for automatic n-way classification into WordNet's semantic classes, both on a TREC news corpus and on a corpus of substitutable search query phrases.", "labels": [], "entities": [{"text": "TREC news corpus", "start_pos": 108, "end_pos": 124, "type": "DATASET", "confidence": 0.8478021820386251}]}], "introductionContent": [{"text": "Identifying semantically related phrases has been demonstrated to be useful in information retrieval) and sponsored search).", "labels": [], "entities": [{"text": "Identifying semantically related phrases", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.8785023093223572}, {"text": "information retrieval", "start_pos": 79, "end_pos": 100, "type": "TASK", "confidence": 0.7552958130836487}]}, {"text": "Work on semantic entailment often includes lexical entailment as a subtask).", "labels": [], "entities": [{"text": "semantic entailment", "start_pos": 8, "end_pos": 27, "type": "TASK", "confidence": 0.7233604192733765}]}, {"text": "We draw a distinction between the task of identifying terms which are topically related and identifying the specific semantic class.", "labels": [], "entities": []}, {"text": "For example, the terms \"dog\", \"puppy\", \"canine\", \"schnauzer\", \"cat\" and \"pet\" are highly related terms, which can be identified using techniques that include distributional similarity) and withindocument cooccurrence measures such as pointwise mutual information (.", "labels": [], "entities": []}, {"text": "These techniques, however, do not allow us to distinguish the more specific relationships: \u2022 hypernym(dog,puppy) * This work was carried out while these authors were at Yahoo!", "labels": [], "entities": []}, {"text": "\u2022 hyponym(dog,canine) \u2022 coordinate(dog,cat) Lexical resources such as WordNet are extremely useful, but are limited by being manually constructed.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 70, "end_pos": 77, "type": "DATASET", "confidence": 0.9554535746574402}]}, {"text": "They do not contain semantic class relationships for the many new terms we encounter in text such as web documents, for example \"mp3 player\" or \"ipod\".", "labels": [], "entities": []}, {"text": "We can use WordNet as training data for such classification to the extent that the training on pairs found in WordNet and testing on pairs found outside WordNet provides accurate generalization.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 11, "end_pos": 18, "type": "DATASET", "confidence": 0.954058051109314}, {"text": "WordNet", "start_pos": 110, "end_pos": 117, "type": "DATASET", "confidence": 0.9698140025138855}, {"text": "WordNet", "start_pos": 153, "end_pos": 160, "type": "DATASET", "confidence": 0.9617294073104858}]}, {"text": "We describe a set of features used to train nway supervised machine-learned classification of semantic classes for arbitrary pairs of phrases.", "labels": [], "entities": []}, {"text": "Redundancy in the sources of our feature information means that we are able to provide coverage over an extremely large vocabulary of phrases.", "labels": [], "entities": []}, {"text": "We contrast this with techniques that require parsing of natural language sentences () which, while providing reasonable performance, can only be applied to a restricted vocabulary of phrases cooccuring in sentences.", "labels": [], "entities": [{"text": "parsing of natural language sentences", "start_pos": 46, "end_pos": 83, "type": "TASK", "confidence": 0.8549384474754333}]}, {"text": "Our contributions are: \u2022 Demonstration that binary classification removes the difficult cases of classification into closely related semantic classes \u2022 Demonstration that dependency parser paths are inadequate for semantic classification into 7 WordNet classes on TREC news corpora \u2022 A benchmark of 10-class semantic classification over highly substitutable query phrases \u2022 Demonstration that training a classifier using WordNet for labeling does not generalize well to query pairs \u2022 Demonstration that much of the performance in classification can be attained using only syntactic features \u2022 A learning curve for classification of query phrase pairs that suggests the primary bottleneck is manually labeled training instances: we expect our benchmark to be surpassed.", "labels": [], "entities": [{"text": "TREC news corpora", "start_pos": 264, "end_pos": 281, "type": "DATASET", "confidence": 0.8515021800994873}]}, {"text": "demonstrated binary classification of hypernyms and non-hypernyms using WordNet as a source of training labels.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 72, "end_pos": 79, "type": "DATASET", "confidence": 0.9751197695732117}]}, {"text": "Using dependency parse tree paths as features, they were able to generalize from WordNet labelings to human labelings.", "labels": [], "entities": []}, {"text": "combined features to answer multiple-choice synonym questions from the TOEFL test and verbal analogy questions from the SAT college entrance exam.", "labels": [], "entities": [{"text": "TOEFL test", "start_pos": 71, "end_pos": 81, "type": "DATASET", "confidence": 0.7379429042339325}]}, {"text": "The multiplechoice questions typically do not consist of multiple closely related terms.", "labels": [], "entities": []}, {"text": "A typical example is given by Turney:", "labels": [], "entities": [{"text": "Turney", "start_pos": 30, "end_pos": 36, "type": "DATASET", "confidence": 0.9214352369308472}]}], "datasetContent": [{"text": "Binary classifiers are evaluated by ranking instances by classification score and finding the Max F1 (the harmonic mean of precision and recall; ranges from 0 to 1) and area under the ROC curve (AUC; ranges from 0.5 to 1 with at least 0.8 being \"good\").", "labels": [], "entities": [{"text": "Max F1", "start_pos": 94, "end_pos": 100, "type": "METRIC", "confidence": 0.8950378894805908}, {"text": "precision", "start_pos": 123, "end_pos": 132, "type": "METRIC", "confidence": 0.9746280312538147}, {"text": "recall", "start_pos": 137, "end_pos": 143, "type": "METRIC", "confidence": 0.9877685904502869}, {"text": "ROC curve", "start_pos": 184, "end_pos": 193, "type": "METRIC", "confidence": 0.9673450291156769}, {"text": "AUC", "start_pos": 195, "end_pos": 198, "type": "METRIC", "confidence": 0.55987548828125}]}, {"text": "The meta-classifier is evaluated by precision and recall of each class and classification accuracy of all instances.", "labels": [], "entities": [{"text": "precision", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9993308782577515}, {"text": "recall", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.9984702467918396}, {"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.8085306882858276}]}, {"text": "ponym of the first sense of the second and both have no more than one tagged sense in the Brown corpus) and \"Known Non-Hypernyms\" (no sense of the first word is a hyponym of any sense of the second).", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 90, "end_pos": 102, "type": "DATASET", "confidence": 0.9262636303901672}]}, {"text": "We wished to test whether making the classes less cleanly separable would affect the results, and also whether we could use these features for n-way classification.", "labels": [], "entities": []}, {"text": "From the same TREC corpus we extracted known synonym, known hyponym, known coordinate, known meronym, and known holonym pairs.", "labels": [], "entities": [{"text": "TREC corpus", "start_pos": 14, "end_pos": 25, "type": "DATASET", "confidence": 0.8355603516101837}]}, {"text": "Each of these classes is defined analogously to the known hypernym class; we selected these six relationships because they are the six most common.", "labels": [], "entities": []}, {"text": "A pair is labeled known no-relationship if no sense of the first word has any relationship to any sense of the second word.", "labels": [], "entities": []}, {"text": "The class distribution was selected to match as closely as possible that observed in query logs.", "labels": [], "entities": []}, {"text": "We labeled 50,000 pairs total.", "labels": [], "entities": []}, {"text": "Results are shown in(a).", "labels": [], "entities": []}, {"text": "Although AUC is fairly high for all classes, MaxF is low for all but two.", "labels": [], "entities": [{"text": "AUC", "start_pos": 9, "end_pos": 12, "type": "METRIC", "confidence": 0.9971317052841187}, {"text": "MaxF", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9936684966087341}]}, {"text": "MaxF has degraded quite a bit for hypernyms from.", "labels": [], "entities": [{"text": "MaxF", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8629708290100098}]}, {"text": "Removing all instances except hypernym and no relationship brings MaxF up to 0.45, suggesting that the additional classes make it harder to separate hypernyms.", "labels": [], "entities": []}, {"text": "Metaclassifier accuracy is very good, but this is due to high recall of no relationship and coordinate pairs: more than 80% of instances with some relationship are predicted to be coordinates, and most of the rest are predicted no relationship.", "labels": [], "entities": [{"text": "Metaclassifier", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.8656007051467896}, {"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.924022912979126}, {"text": "recall", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9992831349372864}]}, {"text": "It seems that we are only distinguishing between no vs. some relationship.", "labels": [], "entities": []}, {"text": "The size of the no relationship class maybe biasing the results.", "labels": [], "entities": []}, {"text": "We removed those instances, but performance of the n-class classifier did not improve(b)).", "labels": [], "entities": []}, {"text": "MaxF of binary classifiers did improve, even though AUC is much worse.", "labels": [], "entities": [{"text": "MaxF", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.6993582844734192}, {"text": "AUC", "start_pos": 52, "end_pos": 55, "type": "METRIC", "confidence": 0.7289720177650452}]}], "tableCaptions": [{"text": " Table 4: Performance of 7 binary classifier and metaclassifiers on phrase-pairs cooccuring in TREC data labeled with WordNet  classes, using minipar dependency features. These features do not seem to be adequate for distinguishing classes other than  coordinate and no-relationship.", "labels": [], "entities": [{"text": "TREC data labeled with WordNet  classes", "start_pos": 95, "end_pos": 134, "type": "DATASET", "confidence": 0.7609979858001074}]}, {"text": " Table 5: Binary and metaclassifier performance on the 32% of hand-labeled instances with dependency path features. Adding  all our features significantly improves performance over just using dependency paths.", "labels": [], "entities": []}, {"text": " Table 6: Binary and metaclassifier performance on all classes and all hand-labeled instances. Table (a) provides a benchmark  for 10-class classification over highly substitutable query phrases. Table (b) shows that a lot of our performance can be achieved  without computationally-expensive parsing.", "labels": [], "entities": []}, {"text": " Table 7: Binary and metaclassifier performance on WordNet- labeled instances with all features.", "labels": [], "entities": []}, {"text": " Table 8: Training on WordNet-labeled pairs and testing on  hand-labeled pairs. Classifiers trained on WordNet do not  generalize well.", "labels": [], "entities": [{"text": "WordNet-labeled", "start_pos": 22, "end_pos": 37, "type": "DATASET", "confidence": 0.9283611178398132}, {"text": "WordNet", "start_pos": 103, "end_pos": 110, "type": "DATASET", "confidence": 0.9721860885620117}]}]}