{"title": [{"text": "Evaluating the Accuracy of an Unlexicalized Statistical Parser on the PARC DepBank", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9949201941490173}, {"text": "PARC DepBank", "start_pos": 70, "end_pos": 82, "type": "DATASET", "confidence": 0.9277468621730804}]}], "abstractContent": [{"text": "We evaluate the accuracy of an unlexi-calized statistical parser, trained on 4K treebanked sentences from balanced data and tested on the PARC DepBank.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9995076656341553}, {"text": "PARC DepBank", "start_pos": 138, "end_pos": 150, "type": "DATASET", "confidence": 0.9205805361270905}]}, {"text": "We demonstrate that a parser which is competitive inaccuracy (without sacrificing processing speed) can be quickly tuned without reliance on large in-domain manually-constructed treebanks.", "labels": [], "entities": []}, {"text": "This makes it more practical to use statistical parsers in applications that need access to aspects of predicate-argument structure.", "labels": [], "entities": []}, {"text": "The comparison of systems using DepBank is not straightforward, so we extend and validate DepBank and highlight a number of representation and scoring issues for relational evaluation schemes.", "labels": [], "entities": []}], "introductionContent": [{"text": "Considerable progress has been made in accurate statistical parsing of realistic texts, yielding rooted, hierarchical and/or relational representations of full sentences.", "labels": [], "entities": [{"text": "statistical parsing of realistic texts", "start_pos": 48, "end_pos": 86, "type": "TASK", "confidence": 0.7849319458007813}]}, {"text": "However, much of this progress has been made with systems based on large lexicalized probabilistic contextfree like (PCFG-like) models trained on the Wall Street Journal (WSJ) subset of the Penn TreeBank (PTB).", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ) subset of the Penn TreeBank (PTB)", "start_pos": 150, "end_pos": 209, "type": "DATASET", "confidence": 0.8759489187172481}]}, {"text": "Evaluation of these systems has been mostly in terms of the PARSEVAL scheme using tree similarity measures of (labelled) precision and recall and crossing bracket rate applied to section 23 of the WSJ PTB.", "labels": [], "entities": [{"text": "PARSEVAL", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.7168097496032715}, {"text": "precision", "start_pos": 121, "end_pos": 130, "type": "METRIC", "confidence": 0.9865478873252869}, {"text": "recall", "start_pos": 135, "end_pos": 141, "type": "METRIC", "confidence": 0.9995394945144653}, {"text": "crossing bracket rate", "start_pos": 146, "end_pos": 167, "type": "METRIC", "confidence": 0.8757086793581644}, {"text": "WSJ PTB", "start_pos": 197, "end_pos": 204, "type": "DATASET", "confidence": 0.955278754234314}]}, {"text": "(See e.g. for detailed exposition of one such very fruitful line of research.)", "labels": [], "entities": []}, {"text": "We evaluate the comparative accuracy of an unlexicalized statistical parser trained on a smaller treebank and tested on a subset of section 23 of the WSJ using a relational evaluation scheme.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9905669093132019}, {"text": "WSJ", "start_pos": 150, "end_pos": 153, "type": "DATASET", "confidence": 0.633426308631897}]}, {"text": "We demonstrate that a parser which is competitive inaccuracy (without sacrificing processing speed) can be quickly developed without reliance on large in-domain manually-constructed treebanks.", "labels": [], "entities": []}, {"text": "This makes it more practical to use statistical parsers in diverse applications needing access to aspects of predicate-argument structure.", "labels": [], "entities": []}, {"text": "We define a lexicalized statistical parser as one which utilizes probabilistic parameters concerning lexical subcategorization and/or bilexical relations over tree configurations.", "labels": [], "entities": []}, {"text": "Current lexicalized statistical parsers developed, trained and tested on PTB achieve a labelled F 1 -score -the harmonic mean of labelled precision and recall -of around 90%.", "labels": [], "entities": [{"text": "PTB", "start_pos": 73, "end_pos": 76, "type": "DATASET", "confidence": 0.8646760582923889}, {"text": "labelled F 1 -score", "start_pos": 87, "end_pos": 106, "type": "METRIC", "confidence": 0.8547659873962402}, {"text": "precision", "start_pos": 138, "end_pos": 147, "type": "METRIC", "confidence": 0.7849922180175781}, {"text": "recall", "start_pos": 152, "end_pos": 158, "type": "METRIC", "confidence": 0.9970302581787109}]}, {"text": "argue that such results represent about 4% absolute improvement over a carefully constructed unlexicalized PCFGlike model trained and tested in the same manner.", "labels": [], "entities": []}, {"text": "1 shows that WSJ-derived bilexical parameters in Collins' (1999) Model 1 parser contribute less than 1% to parse selection accuracy when test data is in the same domain, and yield no improvement for test data selected from the Brown Corpus.", "labels": [], "entities": [{"text": "parse selection", "start_pos": 107, "end_pos": 122, "type": "TASK", "confidence": 0.9077305197715759}, {"text": "accuracy", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.8322568535804749}, {"text": "Brown Corpus", "start_pos": 227, "end_pos": 239, "type": "DATASET", "confidence": 0.9619021117687225}]}, {"text": "shows that, in Collins' (1999) Model 2, bilexical parameters contribute less than 0.5% to accuracy on in-domain data while lexical subcategorization-like parameters contribute just over 1%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9988180994987488}]}, {"text": "Several alternative relational evaluation schemes have been developed (e.g..", "labels": [], "entities": []}, {"text": "However, until recently, no WSJ data has been carefully annotated to support relational evaluation.", "labels": [], "entities": [{"text": "WSJ data", "start_pos": 28, "end_pos": 36, "type": "DATASET", "confidence": 0.8457981646060944}, {"text": "relational evaluation", "start_pos": 77, "end_pos": 98, "type": "TASK", "confidence": 0.8638046681880951}]}, {"text": "describe the PARC 700 Dependency Bank (hereinafter DepBank), which consists of 700 WSJ sentences randomly drawn from section 23.", "labels": [], "entities": [{"text": "PARC 700 Dependency Bank (hereinafter DepBank)", "start_pos": 13, "end_pos": 59, "type": "DATASET", "confidence": 0.879687987267971}]}, {"text": "These sentences have been annotated with syntactic features and with bilexical head-dependent relations derived from the F-structure representation of Lexical Functional Grammar (LFG).", "labels": [], "entities": [{"text": "Lexical Functional Grammar (LFG)", "start_pos": 151, "end_pos": 183, "type": "TASK", "confidence": 0.6839312861363093}]}, {"text": "DepBank facilitates comparison of PCFG-like statistical parsers developed from the PTB with other parsers whose output is not designed to yield PTB-style trees, using an evaluation which is closer to the protypical parsing task of recovering predicate-argument structure.", "labels": [], "entities": []}, {"text": "compare the accuracy and speed of the PARC XLE Parser to Collins' Model 3 parser.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9997507929801941}, {"text": "PARC XLE Parser", "start_pos": 38, "end_pos": 53, "type": "DATASET", "confidence": 0.8339889248212179}, {"text": "Collins' Model 3 parser", "start_pos": 57, "end_pos": 80, "type": "DATASET", "confidence": 0.9356081932783127}]}, {"text": "They develop transformation rules for both, designed to map native output to a subset of the features and relations in DepBank.", "labels": [], "entities": [{"text": "DepBank", "start_pos": 119, "end_pos": 126, "type": "DATASET", "confidence": 0.9538246989250183}]}, {"text": "They compare performance of a grammatically cut-down and complete version of the XLE parser to the publically available version of Collins' parser.", "labels": [], "entities": [{"text": "Collins'", "start_pos": 131, "end_pos": 139, "type": "DATASET", "confidence": 0.9487076997756958}]}, {"text": "One fifth of DepBank is held out to optimize the speed and accuracy of the three systems.", "labels": [], "entities": [{"text": "DepBank", "start_pos": 13, "end_pos": 20, "type": "DATASET", "confidence": 0.9453803896903992}, {"text": "speed", "start_pos": 49, "end_pos": 54, "type": "METRIC", "confidence": 0.9765259623527527}, {"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9980186223983765}]}, {"text": "They conclude from the results of these experiments that the cut-down XLE parser is two-thirds the speed of Collins' Model 3 but 12% more accurate, while the complete XLE system is 20% more accurate but five times slower.", "labels": [], "entities": [{"text": "XLE parser", "start_pos": 70, "end_pos": 80, "type": "TASK", "confidence": 0.7038728892803192}, {"text": "Collins' Model 3", "start_pos": 108, "end_pos": 124, "type": "DATASET", "confidence": 0.9386650919914246}]}, {"text": "F 1 -score percentages range from the mid-to high-70s, suggesting that the relational evaluation is harder than PARSEVAL.", "labels": [], "entities": [{"text": "F 1 -score percentages", "start_pos": 0, "end_pos": 22, "type": "METRIC", "confidence": 0.9745587229728698}]}, {"text": "Both Collins' Model 3 and the XLE Parser use lexicalized models for parse selection trained on the rest of the WSJ PTB.", "labels": [], "entities": [{"text": "Collins' Model 3", "start_pos": 5, "end_pos": 21, "type": "DATASET", "confidence": 0.9436897039413452}, {"text": "parse selection", "start_pos": 68, "end_pos": 83, "type": "TASK", "confidence": 0.9721241891384125}, {"text": "WSJ PTB", "start_pos": 111, "end_pos": 118, "type": "DATASET", "confidence": 0.9611497521400452}]}, {"text": "Therefore, although Kaplan et al. demonstrate an improvement inaccuracy at some cost to speed, there remain questions concerning viability for applications, at some remove from the financial news domain, for which substantial treebanks are not available.", "labels": [], "entities": []}, {"text": "The parser we deploy, like the XLE one, is based on a manually-defined feature-based unification grammar.", "labels": [], "entities": []}, {"text": "However, the approach is somewhat different, making maximal use of more generic structural rather than lexical information, both within the grammar and the probabilistic parse selection model.", "labels": [], "entities": []}, {"text": "Here we compare the accuracy of our parser with Kaplan et al.'s results, by repeating their experiment with our parser.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9995054006576538}]}, {"text": "This comparison is not straightforward, given both the systemspecific nature of some of the annotation in DepBank and the scoring reported.", "labels": [], "entities": [{"text": "DepBank", "start_pos": 106, "end_pos": 113, "type": "DATASET", "confidence": 0.9537369012832642}]}, {"text": "We, therefore, extend DepBank with a set of grammatical relations derived from our own system output and highlight how issues of representation and scoring can affect results and their interpretation.", "labels": [], "entities": [{"text": "DepBank", "start_pos": 22, "end_pos": 29, "type": "DATASET", "confidence": 0.8812394142150879}]}, {"text": "In \u00a72, we describe our development methodology and the resulting system in greater detail.", "labels": [], "entities": []}, {"text": "\u00a73 describes the extended Depbank that we have developed and motivates our additions.", "labels": [], "entities": [{"text": "Depbank", "start_pos": 26, "end_pos": 33, "type": "DATASET", "confidence": 0.8567947149276733}]}, {"text": "\u00a72.4 discusses how we trained and tuned our current system and describes our limited use of information derived from WSJ text.", "labels": [], "entities": [{"text": "WSJ text", "start_pos": 117, "end_pos": 125, "type": "DATASET", "confidence": 0.9378377497196198}]}, {"text": "\u00a74 details the various experiments undertaken with the extended DepBank and gives detailed results.", "labels": [], "entities": [{"text": "DepBank", "start_pos": 64, "end_pos": 71, "type": "DATASET", "confidence": 0.8107516169548035}]}, {"text": "\u00a75 discusses these results and proposes further lines of research.", "labels": [], "entities": []}], "datasetContent": [{"text": "We selected the same 560 sentences as test data as, and all modifications that we made to our system (see \u00a72.4) were made on the basis of (very limited) information from other sections of WSJ text.", "labels": [], "entities": [{"text": "WSJ text", "start_pos": 188, "end_pos": 196, "type": "DATASET", "confidence": 0.9315819144248962}]}, {"text": "We have made no use of the further 140 held out sentences in DepBank.", "labels": [], "entities": [{"text": "DepBank", "start_pos": 61, "end_pos": 68, "type": "DATASET", "confidence": 0.9865046143531799}]}, {"text": "The results we report below are derived by choosing the most probable tag for each word returned by the PoS tagger and by choosing the unweighted GR set returned for the most probable parse with no lexical information guiding parse ranking.", "labels": [], "entities": []}, {"text": "The DepBank num feature on nouns is evaluated by Kaplan et al. on the grounds that it is semantically-relevant for applications.", "labels": [], "entities": []}, {"text": "There are over 5K num features in DepBank so the overall microaveraged scores fora system will be significantly affected by accuracy on num.", "labels": [], "entities": [{"text": "DepBank", "start_pos": 34, "end_pos": 41, "type": "DATASET", "confidence": 0.9599979519844055}, {"text": "accuracy", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9986051917076111}]}, {"text": "We expected our system, which incorporates a tagger with good empirical (97.1%) accuracy on the test data, to recover this feature with 95% accuracy or better, as it will correlate with tags NNx1 and NNx2 (where 'x' represents zero or more capitals in the CLAWS Our system DepBank/GR 81.5 78.1 79.7: Microaveraged overall scores from Kaplan et al. and for our system. tagset).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9915231466293335}, {"text": "accuracy", "start_pos": 140, "end_pos": 148, "type": "METRIC", "confidence": 0.9966668486595154}, {"text": "DepBank/GR 81.5 78.1 79.7", "start_pos": 273, "end_pos": 298, "type": "DATASET", "confidence": 0.8544186651706696}]}, {"text": "However, DepBank treats the majority of prenominal modifiers as adjectives rather than nouns and, therefore, associates them with an adegree rather than a num feature.", "labels": [], "entities": [{"text": "DepBank", "start_pos": 9, "end_pos": 16, "type": "DATASET", "confidence": 0.9289209842681885}]}, {"text": "The PoS tag selected depends primarily on the relative lexical probabilities of each tag fora given lexical item recorded in the tagger lexicon.", "labels": [], "entities": []}, {"text": "But, regardless of this lexical decision, the correct GR is recovered, and neither adegree(positive) or num(sg) add anything semantically-relevant when the lexical item is a nominal premodifier.", "labels": [], "entities": []}, {"text": "A strategy which only provided a num feature for nominal heads would be both more semantically-relevant and would also yield higher precision (95.2%).", "labels": [], "entities": [{"text": "precision", "start_pos": 132, "end_pos": 141, "type": "METRIC", "confidence": 0.9989179372787476}]}, {"text": "However, recall (48.4%) then suffers against DepBank as noun premodifiers have a num feature.", "labels": [], "entities": [{"text": "recall", "start_pos": 9, "end_pos": 15, "type": "METRIC", "confidence": 0.9992792010307312}, {"text": "DepBank", "start_pos": 45, "end_pos": 52, "type": "DATASET", "confidence": 0.9433072805404663}]}, {"text": "Therefore, in the results presented in we have not counted cases where either DepBank or our system assign a premodifier adegree(positive) or num(sg).", "labels": [], "entities": [{"text": "DepBank", "start_pos": 78, "end_pos": 85, "type": "DATASET", "confidence": 0.9634501338005066}]}, {"text": "There are similar issues with other DepBank features and relations.", "labels": [], "entities": []}, {"text": "For instance, the form of a subordinator with clausal complements is annotated as a relation between verb and subordinator, while there is a separate comp relation between verb and complement head.", "labels": [], "entities": []}, {"text": "The GR representation adds the subordinator as a subtype of ccomp recording essentially identical information in a single relation.", "labels": [], "entities": []}, {"text": "So evaluation scores based on aggregated counts of correct decisions will be doubled fora system which structures this information as in DepBank.", "labels": [], "entities": [{"text": "DepBank", "start_pos": 137, "end_pos": 144, "type": "DATASET", "confidence": 0.9315207600593567}]}, {"text": "However, reproducing the exact DepBank subord form relation from the GR ccomp one is non-trivial because DepBank treats modal auxiliaries as syntactic heads while the GRscheme treats the main verb as head in all ccomp relations.", "labels": [], "entities": []}, {"text": "We have not attempted to compensate for any further such discrepancies other than the one discussed in the previous paragraph.", "labels": [], "entities": []}, {"text": "However, we do believe that they collectively damage scores for our system.", "labels": [], "entities": []}, {"text": "As King et al. note, it is difficult to identify such informational redundancies to avoid doublecounting and to eradicate all system specific biases.", "labels": [], "entities": []}, {"text": "However, reporting precision, recall and F 1 -scores for each relation and feature separately and microaveraging these scores on the basis of a hierarchy, as in our GR scheme, ameliorates many of these problems and gives a better indication of the strengths and weaknesses of a particular parser, which may also be useful in a decision about its usefulness fora specific application.", "labels": [], "entities": [{"text": "precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.989379346370697}, {"text": "recall", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9992063641548157}, {"text": "F 1 -scores", "start_pos": 41, "end_pos": 52, "type": "METRIC", "confidence": 0.9840518236160278}]}, {"text": "Unfortunately, Kaplan et al. do not report their results broken down by relation or feature so it is not possible, for example, on the basis of the arguments made above, to choose to compare the performance of our system on ccomp to theirs for comp, ignoring subord form.", "labels": [], "entities": []}, {"text": "King et al. do report individual results for selected features and relations from an evaluation of the complete XLE parser on all 700 DepBank sentences with an almost identical overall microaveraged F 1 score of 79.5%, suggesting that these results provide a reasonably accurate idea of the XLE parser's relative performance on different features and relations.", "labels": [], "entities": [{"text": "DepBank", "start_pos": 134, "end_pos": 141, "type": "DATASET", "confidence": 0.9151813387870789}, {"text": "microaveraged F 1 score", "start_pos": 185, "end_pos": 208, "type": "METRIC", "confidence": 0.8163711875677109}]}, {"text": "Where we believe that the information captured by a DepBank feature or relation is roughly comparable to that expressed by a GR in our extended DepBank, we have included King et al.'s scores in the rightmost column in for comparison purposes.", "labels": [], "entities": []}, {"text": "Even if these features and relations were drawn from the same experiment, however, they would still not be exactly comparable.", "labels": [], "entities": []}, {"text": "For instance, as discussed in \u00a73 nearly half (just over 1K) the DepBank subj relations include pro as one element, mostly double counting a corresponding xcomp relation.", "labels": [], "entities": [{"text": "DepBank", "start_pos": 64, "end_pos": 71, "type": "DATASET", "confidence": 0.9313850402832031}]}, {"text": "On the other hand, our ta relation syntactically underspecifies many DepBank adjunct relations.", "labels": [], "entities": []}, {"text": "Nevertheless, it is possible to see, for instance, that while both parsers perform badly on second objects ours is worse, presumably because of lack of lexical subcategorization information.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracy of our parser, and where  roughly comparable, the XLE as reported by", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9990059733390808}, {"text": "XLE", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.9913108348846436}]}, {"text": " Table 2: Microaveraged overall scores from Kaplan et al. and for our system.", "labels": [], "entities": []}]}