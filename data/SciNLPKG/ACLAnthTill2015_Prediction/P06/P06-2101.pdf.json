{"title": [{"text": "Minimum Risk Annealing for Training Log-Linear Models *", "labels": [], "entities": [{"text": "Minimum Risk Annealing", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6659522652626038}]}], "abstractContent": [{"text": "When training the parameters fora natural language system, one would prefer to minimize 1-best loss (error) on an evaluation set.", "labels": [], "entities": [{"text": "1-best loss (error)", "start_pos": 88, "end_pos": 107, "type": "METRIC", "confidence": 0.9352360367774963}]}, {"text": "Since the error surface for many natural language problems is piecewise constant and riddled with local minima , many systems instead optimize log-likelihood, which is conveniently differentiable and convex.", "labels": [], "entities": []}, {"text": "We propose training instead to minimize the expected loss, or risk.", "labels": [], "entities": []}, {"text": "We define this expectation using a probability distribution over hypotheses that we gradually sharpen (anneal) to focus on the 1-best hypothesis.", "labels": [], "entities": []}, {"text": "Besides the linear loss functions used in previous work, we also describe techniques for optimizing nonlinear functions such as precision or the BLEU metric.", "labels": [], "entities": [{"text": "precision", "start_pos": 128, "end_pos": 137, "type": "METRIC", "confidence": 0.9991599321365356}, {"text": "BLEU", "start_pos": 145, "end_pos": 149, "type": "METRIC", "confidence": 0.9921963214874268}]}, {"text": "We present experiments training log-linear combinations of models for dependency parsing and for machine translation.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 70, "end_pos": 88, "type": "TASK", "confidence": 0.8123182058334351}, {"text": "machine translation", "start_pos": 97, "end_pos": 116, "type": "TASK", "confidence": 0.8181580007076263}]}, {"text": "In machine translation, annealed minimum risk training achieves significant improvements in BLEU over standard minimum error training.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.8164428472518921}, {"text": "BLEU", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.9992208480834961}]}, {"text": "We also show improvements in labeled dependency parsing.", "labels": [], "entities": [{"text": "labeled dependency parsing", "start_pos": 29, "end_pos": 55, "type": "TASK", "confidence": 0.6240981916586558}]}], "introductionContent": [], "datasetContent": [{"text": "We tested the above training methods on two different tasks: dependency parsing and phrasebased machine translation.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 61, "end_pos": 79, "type": "TASK", "confidence": 0.8776988089084625}, {"text": "phrasebased machine translation", "start_pos": 84, "end_pos": 115, "type": "TASK", "confidence": 0.7321574290593466}]}, {"text": "Since the basic setup was the same for both, we outline it here before describing the tasks in detail.", "labels": [], "entities": []}, {"text": "In both cases, we start with 8 to 10 models (the \"experts\") already trained on separate training data.", "labels": [], "entities": []}, {"text": "To find the optimal coefficients \u03b8 fora loglinear combination of these experts, we use separate development data, using the following procedure due to Och (2003): 1.", "labels": [], "entities": []}, {"text": "Initialization: Initialize \u03b8 to the 0 vector.", "labels": [], "entities": []}, {"text": "For each development sentence xi , set its K i -best list to \u2205 (thus K i = 0).", "labels": [], "entities": []}, {"text": "7 BLEU is careful when measuring ci on a particular decoding y i,k . It only counts the first two copies of the (e.g.) as correct if the occurs at most twice in any reference translation of xi.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 2, "end_pos": 6, "type": "METRIC", "confidence": 0.999000608921051}]}, {"text": "This \"clipping\" does not affect the rest of our method.", "labels": [], "entities": []}, {"text": "8 Reasonable fora large corpus, by Lyapunov's central limit theorem (allows non-identically distributed summands).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: BLEU 4n1 percentage on translating 2000-", "labels": [], "entities": [{"text": "BLEU 4n1 percentage", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.9112780292828878}, {"text": "translating 2000-", "start_pos": 33, "end_pos": 50, "type": "DATASET", "confidence": 0.9878463943799337}]}, {"text": " Table 2: Labeled dependency accuracy on parsing 200-", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.7353459596633911}, {"text": "parsing", "start_pos": 41, "end_pos": 48, "type": "TASK", "confidence": 0.9651196599006653}]}]}