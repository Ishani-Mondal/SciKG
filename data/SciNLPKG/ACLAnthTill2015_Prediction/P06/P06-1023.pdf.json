{"title": [{"text": "Trace Prediction and Recovery With Unlexicalized PCFGs and Slash Features", "labels": [], "entities": [{"text": "Trace Prediction", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.9134761393070221}]}], "abstractContent": [{"text": "This paper describes a parser which generates parse trees with empty elements in which traces and fillers are co-indexed.", "labels": [], "entities": []}, {"text": "The parser is an unlexicalized PCFG parser which is guaranteed to return the most probable parse.", "labels": [], "entities": []}, {"text": "The grammar is extracted from aversion of the PENN treebank which was automatically annotated with features in the style of Klein and Manning (2003).", "labels": [], "entities": [{"text": "PENN treebank", "start_pos": 46, "end_pos": 59, "type": "DATASET", "confidence": 0.9167990982532501}]}, {"text": "The annotation includes GPSG-style slash features which link traces and fillers, and other features which improve the general parsing accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 134, "end_pos": 142, "type": "METRIC", "confidence": 0.9210407733917236}]}, {"text": "In an evaluation on the PENN tree-bank (Marcus et al., 1993), the parser outperformed other unlexicalized PCFG parsers in terms of labeled bracketing f-score.", "labels": [], "entities": [{"text": "PENN tree-bank", "start_pos": 24, "end_pos": 38, "type": "DATASET", "confidence": 0.7983048260211945}]}, {"text": "Its results for the empty category prediction task and the trace-filler co-indexation task exceed all previously reported results with 84.1% and 77.4% f-score, respectively.", "labels": [], "entities": [{"text": "empty category prediction task", "start_pos": 20, "end_pos": 50, "type": "TASK", "confidence": 0.6901112943887711}, {"text": "f-score", "start_pos": 151, "end_pos": 158, "type": "METRIC", "confidence": 0.9940345883369446}]}], "introductionContent": [{"text": "Empty categories (also called null elements) are used in the annotation of the PENN treebank) in order to represent syntactic phenomena like constituent movement (e.g. whextraction), discontinuous constituents, and missing elements (PRO elements, empty complementizers and relative pronouns).", "labels": [], "entities": [{"text": "PENN treebank", "start_pos": 79, "end_pos": 92, "type": "DATASET", "confidence": 0.9204089343547821}]}, {"text": "Moved constituents are co-indexed with a trace which is located at the position where the moved constituent is to be interpreted.", "labels": [], "entities": []}, {"text": "shows an example of constituent movement in a relative clause.", "labels": [], "entities": []}, {"text": "Empty categories provide important information for the semantic interpretation, in particular for determining the predicate-argument structure of a sentence.", "labels": [], "entities": [{"text": "semantic interpretation", "start_pos": 55, "end_pos": 78, "type": "TASK", "confidence": 0.7391643822193146}]}, {"text": "However, most broad-coverage statistical parsers, and others) which are trained on the PENN treebank generate parse trees without empty categories.", "labels": [], "entities": [{"text": "PENN treebank", "start_pos": 87, "end_pos": 100, "type": "DATASET", "confidence": 0.8441196084022522}]}, {"text": "In order to augment such parsers with empty category prediction, three rather different strategies have been proposed: (i) pre-processing of the input sentence with a tagger which inserts empty categories into the input string of the parser).", "labels": [], "entities": [{"text": "empty category prediction", "start_pos": 38, "end_pos": 63, "type": "TASK", "confidence": 0.711199959119161}]}, {"text": "The parser treats the empty elements like normal input tokens.", "labels": [], "entities": []}, {"text": "(ii) post-processing of the parse trees with a pattern matcher which adds empty categories after parsing) (iii) in-processing of the empty categories with a slash percolation mechanism ().", "labels": [], "entities": []}, {"text": "The empty elements are here generated by the grammar.", "labels": [], "entities": []}, {"text": "Good results have been obtained with all three approaches, but) reported that in their experiments, the in-processing of the empty categories only worked with lexicalized parsing.", "labels": [], "entities": []}, {"text": "They explain that their unlex-icalized PCFG parser produced poor results because the beam search strategy applied there eliminated many correct constituents with empty elements.", "labels": [], "entities": []}, {"text": "The scores of these constituents were too low compared with the scores of constituents without empty elements.", "labels": [], "entities": []}, {"text": "They speculated that \"doing an exhaustive search might help\" here.", "labels": [], "entities": []}, {"text": "In this paper, we confirm this hypothesis and show that it is possible to accurately predict empty categories with unlexicalized PCFG parsing and slash features if the true Viterbi parse is computed.", "labels": [], "entities": [{"text": "PCFG parsing", "start_pos": 129, "end_pos": 141, "type": "TASK", "confidence": 0.783119410276413}]}, {"text": "In our experiments, we used the BitPar parser) and a PCFG which was extracted from aversion of the PENN treebank that was automatically annotated with features in the style of).", "labels": [], "entities": [{"text": "PCFG", "start_pos": 53, "end_pos": 57, "type": "DATASET", "confidence": 0.8578563332557678}, {"text": "PENN treebank", "start_pos": 99, "end_pos": 112, "type": "DATASET", "confidence": 0.8041979968547821}]}], "datasetContent": [{"text": "In shows the labeled bracketing accuracy of the parser on the whole section 23 and compares it to the results reported in for sentences with up to 100 words.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.8484773635864258}]}, {"text": "reports the accuracy of the parser in the empty category (EC) prediction task for ECs occurring more than 6 times.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9995800852775574}, {"text": "empty category (EC) prediction task", "start_pos": 42, "end_pos": 77, "type": "TASK", "confidence": 0.7004329477037702}]}, {"text": "Following, an empty category was considered correct if the treebank parse contained an empty node of the same category at the same string position.", "labels": [], "entities": []}, {"text": "Empty SBAR nodes which dominate an empty S node are treated as a single empty element and listed as SBAR-S in table 2.", "labels": [], "entities": []}, {"text": "We ran a series of evaluations on held-out data in order to determine the impact of the different features which we described in section 2 on the parsing accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 146, "end_pos": 153, "type": "TASK", "confidence": 0.9724641442298889}, {"text": "accuracy", "start_pos": 154, "end_pos": 162, "type": "METRIC", "confidence": 0.9482364654541016}]}, {"text": "In each run, we deleted one of the features and measured how the accuracy changed compared to the baseline system with all features.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.999472439289093}]}, {"text": "The results are shown in: Differences between the baseline f-scores for labeled bracketing, EC prediction, and coindexation (CI) and the f-scores without the specified feature.", "labels": [], "entities": [{"text": "labeled bracketing", "start_pos": 72, "end_pos": 90, "type": "TASK", "confidence": 0.6054653823375702}, {"text": "EC prediction", "start_pos": 92, "end_pos": 105, "type": "TASK", "confidence": 0.7997842729091644}, {"text": "coindexation (CI)", "start_pos": 111, "end_pos": 128, "type": "METRIC", "confidence": 0.9294090569019318}]}, {"text": "The good performance of our parser on the empty element recognition task is remarkable considering the fact that its performance on the labeled bracketing task is 3% lower than that of the Charniak (2000) parser used by: Co-indexation accuracy on section 23 compares our co-indexation results with those reported in Johnson (2001),,.", "labels": [], "entities": [{"text": "empty element recognition task", "start_pos": 42, "end_pos": 72, "type": "TASK", "confidence": 0.7237209156155586}, {"text": "accuracy", "start_pos": 235, "end_pos": 243, "type": "METRIC", "confidence": 0.8593051433563232}]}, {"text": "Our parser achieves the highest precision and f-score.", "labels": [], "entities": [{"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9997445940971375}, {"text": "f-score", "start_pos": 46, "end_pos": 53, "type": "METRIC", "confidence": 0.9883560538291931}]}, {"text": "Campbell (2004) reports a higher recall, but lower precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.9998021721839905}, {"text": "precision", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9992254972457886}]}, {"text": "shows the trace prediction accuracies of our parser, Johnson's (2001) parser with parser input and perfect input, and parser with perfect input.", "labels": [], "entities": []}, {"text": "The accuracy of Johnson's parser is consistently lower than that of the other parsers and it has particular difficulties with ADVP traces, SBAR traces, and empty relative pronouns (WHNP 0).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9995898604393005}, {"text": "WHNP 0)", "start_pos": 181, "end_pos": 188, "type": "METRIC", "confidence": 0.9284072717030843}]}, {"text": "Campbell's parser and our parser cannot be directly compared, but when we take the respective performance difference to Johnson's parser as evidence, we might conclude that Campbell's parser works particularly well on NP *, *U*, and WHNP 0, whereas our system paper J1 J2 C: Comparison of the empty category prediction accuracies for different categories in this paper (paper), in) with parser input (J1), in) with perfect input (J2), and in) with perfect input.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Labeled bracketing accuracy on sec- tion 23", "labels": [], "entities": [{"text": "Labeled bracketing", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.5281106680631638}, {"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9671043753623962}]}, {"text": " Table 2: Accuracy of empty category prediction  on section 23. The first column shows the type of  the empty element and -except for empty comple- mentizers and empty units -also the category. The  last column shows the frequency in the test data.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9757153987884521}, {"text": "empty category prediction", "start_pos": 22, "end_pos": 47, "type": "TASK", "confidence": 0.6684154470761617}]}, {"text": " Table 3: Co-indexation accuracy on section 23.  The first column shows the category and type of  the trace. If the filler category of the filler is dif- ferent from the category of the trace, it is added in  front. The filler category is abbreviated to \"WH\"  if the rest is identical to the trace category. The  last column shows the frequency in the test data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9837144017219543}]}, {"text": " Table 4: Differences between the baseline f-scores  for labeled bracketing, EC prediction, and co- indexation (CI) and the f-scores without the spec- ified feature.", "labels": [], "entities": [{"text": "labeled bracketing", "start_pos": 57, "end_pos": 75, "type": "TASK", "confidence": 0.6029312759637833}, {"text": "EC prediction", "start_pos": 77, "end_pos": 90, "type": "TASK", "confidence": 0.7949882745742798}, {"text": "co- indexation (CI)", "start_pos": 96, "end_pos": 115, "type": "METRIC", "confidence": 0.8307170768578848}]}, {"text": " Table 5: Accuracy of empty category prediction  on section 23", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9938145875930786}, {"text": "empty category prediction", "start_pos": 22, "end_pos": 47, "type": "TASK", "confidence": 0.6137800812721252}]}, {"text": " Table 6: Co-indexation accuracy on section 23", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.8711563944816589}]}, {"text": " Table 7: Comparison of the empty category pre- diction accuracies for different categories in this  paper (paper), in", "labels": [], "entities": []}]}