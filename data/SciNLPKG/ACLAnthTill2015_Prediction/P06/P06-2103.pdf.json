{"title": [{"text": "Discourse Generation Using Utility-Trained Coherence Models", "labels": [], "entities": [{"text": "Discourse Generation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7627884745597839}]}], "abstractContent": [{"text": "We describe a generic framework for integrating various stochastic models of discourse coherence in a manner that takes advantage of their individual strengths.", "labels": [], "entities": []}, {"text": "An integral part of this framework are algorithms for searching and training these stochastic coherence models.", "labels": [], "entities": []}, {"text": "We evaluate the performance of our models and algorithms and show empirically that utility-trained log-linear coherence models out-perform each of the individual coherence models considered.", "labels": [], "entities": []}], "introductionContent": [{"text": "Various theories of discourse coherence) have been applied successfully in discourse analysis) and discourse generation (Scott and de).", "labels": [], "entities": [{"text": "discourse analysis", "start_pos": 75, "end_pos": 93, "type": "TASK", "confidence": 0.7781446278095245}, {"text": "discourse generation", "start_pos": 99, "end_pos": 119, "type": "TASK", "confidence": 0.7678577303886414}]}, {"text": "Most of these efforts, however, have limited applicability.", "labels": [], "entities": []}, {"text": "Those that use manually written rules model only the most visible discourse constraints (e.g., the discourse connective \"although\" marks a CONCESSION relation), while being oblivious to fine-grained lexical indicators.", "labels": [], "entities": []}, {"text": "And the methods that utilize manually annotated corpora) and supervised learning algorithms have high costs associated with the annotation procedure, and cannot be easily adapted to different domains and genres.", "labels": [], "entities": []}, {"text": "In contrast, more recent research has focused on stochastic approaches that model discourse coherence at the local lexical and global levels (), while preserving regularities recognized by classic discourse theories ().", "labels": [], "entities": []}, {"text": "These stochastic coherence models use simple, non-hierarchical representations of discourse, and can be trained with minimal human intervention, using large collections of existing human-authored documents.", "labels": [], "entities": []}, {"text": "These models are attractive due to their increased scalability and portability.", "labels": [], "entities": []}, {"text": "As each of these stochastic models captures different aspects of coherence, an important question is whether we can combine them in a model capable of exploiting all coherence indicators.", "labels": [], "entities": []}, {"text": "A frequently used testbed for coherence models is the discourse ordering problem, which occurs often in text generation, complex question answering, and multi-document summarization: given discourse units, what is the most coherent ordering of them)?", "labels": [], "entities": [{"text": "text generation", "start_pos": 104, "end_pos": 119, "type": "TASK", "confidence": 0.7193141579627991}, {"text": "question answering", "start_pos": 129, "end_pos": 147, "type": "TASK", "confidence": 0.6839046776294708}, {"text": "multi-document summarization", "start_pos": 153, "end_pos": 181, "type": "TASK", "confidence": 0.5896505117416382}]}, {"text": "Because the problem is NP-complete (), it is critical how coherence model evaluation is intertwined with search: if the search for the best ordering is greedy and has many errors, one is notable to properly evaluate whether a model is better than another.", "labels": [], "entities": []}, {"text": "If the search is exhaustive, the ordering procedure may take too long to be useful.", "labels": [], "entities": []}, {"text": "In this paper, we propose an A\u00a1 search algorithm for the discourse ordering problem that comes with strong theoretical guarantees.", "labels": [], "entities": [{"text": "A", "start_pos": 29, "end_pos": 30, "type": "METRIC", "confidence": 0.8659737706184387}, {"text": "discourse ordering problem", "start_pos": 57, "end_pos": 83, "type": "TASK", "confidence": 0.7969354490439097}]}, {"text": "For a wide range of practical problems (discourse ordering of up to 15 units), the algorithm finds an optimal solution in reasonable time (on the order of seconds).", "labels": [], "entities": []}, {"text": "A beam search version of the algorithm enables one to find good, approximate solutions for very large reordering tasks.", "labels": [], "entities": []}, {"text": "These algorithms enable us not only to compare head-to-head, for the first time, a set of coherence models, but also to combine these models so as to benefit from their complementary strengths.", "labels": [], "entities": []}, {"text": "The model com-bination is accomplished using statistically wellfounded utility training procedures which automatically optimize the contributions of the individual models on a development corpus.", "labels": [], "entities": []}, {"text": "We empirically show that utility-based models of discourse coherence outperform each of the individual coherence models considered.", "labels": [], "entities": []}, {"text": "In the following section, we describe previously-proposed and new coherence models.", "labels": [], "entities": []}, {"text": "Then, we present our search algorithms and the input representation they use.", "labels": [], "entities": []}, {"text": "Finally, we show evaluation results and discuss their implications.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate empirically two different aspects of our work.", "labels": [], "entities": []}, {"text": "First, we measure the performance of our search algorithms across different models.", "labels": [], "entities": []}, {"text": "Second, we compare the performance of each individual coherence model, and also the performance of the discriminatively trained log-linear models.", "labels": [], "entities": []}, {"text": "We also compare the overall performance (model & decoding strategy) obtained in our framework with previously reported results.", "labels": [], "entities": []}, {"text": "The task on which we conduct our evaluation is information ordering.", "labels": [], "entities": [{"text": "information ordering", "start_pos": 47, "end_pos": 67, "type": "TASK", "confidence": 0.8313017785549164}]}, {"text": "In this task, a pre-selected set of information-bearing document units (in our case, sentences) needs to be arranged in a sequence which maximizes some specific information quality (in our case, document coherence).", "labels": [], "entities": []}, {"text": "We use the information-ordering task as a means to measure the performance of our algorithms and models in a well-controlled setting.", "labels": [], "entities": []}, {"text": "As described in Section 3, our framework can be used in applications such as multi-document summarization.", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 77, "end_pos": 105, "type": "TASK", "confidence": 0.7180571854114532}]}, {"text": "In fact, formulate the multi-document summarization problem as an information ordering problem, and show that naive ordering algorithms such as majority ordering (select most frequent orders across input documents) and chronological ordering (order facts according to publication date) do not always yield coherent summaries.", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 23, "end_pos": 51, "type": "TASK", "confidence": 0.5716075301170349}, {"text": "information ordering", "start_pos": 66, "end_pos": 86, "type": "TASK", "confidence": 0.7307239472866058}]}, {"text": "For training and testing, we use documents from two different genres: newspaper articles and accident reports written by government officials (  Board's database.", "labels": [], "entities": []}, {"text": "For both collections, we used 100 documents for training and 100 documents for testing.", "labels": [], "entities": []}, {"text": "A fraction of 40% of the training documents was temporarily removed and used as a development set, on which we performed the discriminative training procedure.", "labels": [], "entities": []}, {"text": "We evaluated the performance of several search algorithms across four stochastic models of document coherence: the IBM and IBM \u00a3 coherence models, the content model of, and the entity-based model of Barzilay and Lapata (2005) (EB) (Section 2).", "labels": [], "entities": []}, {"text": "We measure search performance using an Estimated Search Error (ESE) figure, which reports the percentage of times when the search algorithm proposes a sentence order which scores lower than Overall performance  the original sentence order (OSO).", "labels": [], "entities": [{"text": "Estimated Search Error (ESE)", "start_pos": 39, "end_pos": 67, "type": "METRIC", "confidence": 0.8516922394434611}]}, {"text": "We also measure the quality of the proposed documents using TAU and BLEU, using as reference the OSO.", "labels": [], "entities": [{"text": "TAU", "start_pos": 60, "end_pos": 63, "type": "METRIC", "confidence": 0.9836077094078064}, {"text": "BLEU", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.9981923699378967}, {"text": "OSO", "start_pos": 97, "end_pos": 100, "type": "DATASET", "confidence": 0.9088606834411621}]}, {"text": "In, we report the performance of four search algorithms.", "labels": [], "entities": []}, {"text": "The first three, IDL-CH-A\u00a1 , IDL-CH-HB \u00a3 V r V , and IDL-CH-HB \u00a3 are the IDLbased search algorithms of Section 3, implementing A\u00a1 search, histogram beam search with abeam of 100, and histogram beam search with abeam of 1, respectively.", "labels": [], "entities": []}, {"text": "We compare our algorithms against the greedy algorithm used by.", "labels": [], "entities": []}, {"text": "We note here that the comparison is rendered meaningful by the observation that this algorithm performs search identically with algorithm IDL-CH-HB \u00a3 (histogram beam 1), when setting the heuristic function for future costs to constant 0.", "labels": [], "entities": []}, {"text": "The results in clearly show the superiority of the IDL-CH-A\u00a1 and IDL-CH-HB \u00a3 V r V algo-rithms.", "labels": [], "entities": []}, {"text": "Across all models considered, they consistently propose documents with scores at least as good as OSO (0% Estimated Search Error).", "labels": [], "entities": [{"text": "OSO (0% Estimated Search Error)", "start_pos": 98, "end_pos": 129, "type": "METRIC", "confidence": 0.7574130855500698}]}, {"text": "As the original documents were coherent, it follows that the proposed document realizations also exhibit coherence.", "labels": [], "entities": []}, {"text": "In contrast, the greedy algorithm of Lapata (2003) makes grave search errors.", "labels": [], "entities": []}, {"text": "As the comparison between IDL-CH-HB \u00a3 V r V and IDL-CH-HB \u00a3 shows, the superiority of the IDL-CH algorithms depends more on the admissible heuristic function 3 than in the ability to maintain multiple hypotheses while searching.", "labels": [], "entities": []}, {"text": "For this round of experiments, we held constant the search procedure (IDL-CH-HB \u00a3 V r V ), and varied the 0 # parameters of Equation 1.", "labels": [], "entities": [{"text": "IDL-CH-HB \u00a3 V r V )", "start_pos": 70, "end_pos": 89, "type": "METRIC", "confidence": 0.8616419831911722}, {"text": "Equation", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9887832999229431}]}, {"text": "The utility-trained log-linear models are compared here against a baseline log-linear model loglinear\u00a1 , for which all 0 C # parameters are set to 1, and also against the individual models.", "labels": [], "entities": []}, {"text": "The results are presented in.", "labels": [], "entities": []}, {"text": "If not properly weighted, the log-linear combination may yield poorer results than those of individual models (average TAU of .34 for loglinear\u00a1 parameters (.16 for EARTHQUAKES, .24 for the ACCIDENTS).", "labels": [], "entities": [{"text": "TAU", "start_pos": 119, "end_pos": 122, "type": "METRIC", "confidence": 0.9978275895118713}, {"text": "EARTHQUAKES", "start_pos": 165, "end_pos": 176, "type": "METRIC", "confidence": 0.8713535666465759}]}, {"text": "For both genres, the differences between the highest accuracy figures (in bold) and the accuracy of the individual models are statistically significant at 95% confidence (using bootstrap resampling).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9969897270202637}, {"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9985666871070862}]}, {"text": "The last comparison we provide is between the performance provided by our framework and previously-reported performance results.", "labels": [], "entities": []}, {"text": "We are able to provide this comparison based on the TAU figures reported in ().", "labels": [], "entities": [{"text": "TAU", "start_pos": 52, "end_pos": 55, "type": "DATASET", "confidence": 0.5651381015777588}]}, {"text": "The training and test data for both genres is the same, and therefore the figures can be directly compared.", "labels": [], "entities": []}, {"text": "These figures account for combined model and search performance.", "labels": [], "entities": []}, {"text": "We first note that, unfortunately, we failed to accurately reproduce the model of.", "labels": [], "entities": []}, {"text": "Our reproduction has an average TAU figure of only .39 versus the original figure of .81 for EARTHQUAKES, and .36 versus .44 for ACCIDENTS.", "labels": [], "entities": [{"text": "TAU figure", "start_pos": 32, "end_pos": 42, "type": "METRIC", "confidence": 0.9794653058052063}, {"text": "EARTHQUAKES", "start_pos": 93, "end_pos": 104, "type": "METRIC", "confidence": 0.9059690237045288}, {"text": "ACCIDENTS", "start_pos": 129, "end_pos": 138, "type": "DATASET", "confidence": 0.8569945693016052}]}, {"text": "On the other hand, we reproduced successfully the model of, and the average TAU figure is .19 for EARTHQUAKES, and .12 for ACCIDENTS 3 . The large difference on the EARTHQUAKES corpus between the performance of and our reproduction of their model is responsible for the overall lower performance (0.47) of our log-linear \u00a9 model and IDL-CH-HB \u00a3 V r V search algorithm, which is nevertheless higher than that of its component model CM (0.39).", "labels": [], "entities": [{"text": "TAU", "start_pos": 76, "end_pos": 79, "type": "METRIC", "confidence": 0.9979453682899475}, {"text": "EARTHQUAKES", "start_pos": 98, "end_pos": 109, "type": "DATASET", "confidence": 0.5176406502723694}, {"text": "EARTHQUAKES corpus", "start_pos": 165, "end_pos": 183, "type": "DATASET", "confidence": 0.8839347064495087}]}, {"text": "On the other hand, we achieve the highest accuracy figure (0.50) on the ACCIDENTS corpus, outperforming the previous-highest figure (0.44) of.", "labels": [], "entities": [{"text": "accuracy figure (0.50)", "start_pos": 42, "end_pos": 64, "type": "METRIC", "confidence": 0.9368274331092834}, {"text": "ACCIDENTS corpus", "start_pos": 72, "end_pos": 88, "type": "DATASET", "confidence": 0.9484642446041107}]}, {"text": "These result empirically show that utility-trained log-linear models of discourse coherence outperform each of the individual coherence models considered.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Evaluation of search algorithms for document coherence, for both EARTHQUAKES and  ACCIDENTS genres, across the IBM", "labels": [], "entities": [{"text": "EARTHQUAKES", "start_pos": 75, "end_pos": 86, "type": "METRIC", "confidence": 0.7673102617263794}, {"text": "IBM", "start_pos": 121, "end_pos": 124, "type": "DATASET", "confidence": 0.6493427753448486}]}, {"text": " Table 2: Evaluation of stochastic models for doc- ument coherence, for both EARTHQUAKES and  ACCIDENTS genre, using IDL-CH-HB  \u00a3  V  r V  .", "labels": [], "entities": [{"text": "EARTHQUAKES", "start_pos": 77, "end_pos": 88, "type": "METRIC", "confidence": 0.8297913670539856}]}, {"text": " Table 3: Comparison of overall performance (af- fected by both model & search procedure) of our  framework with previous results.", "labels": [], "entities": []}]}