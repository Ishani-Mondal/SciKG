{"title": [], "abstractContent": [{"text": "Combination methods are an effective way of improving system performance.", "labels": [], "entities": []}, {"text": "This paper examines the benefits of system combination for unsupervised WSD.", "labels": [], "entities": [{"text": "WSD", "start_pos": 72, "end_pos": 75, "type": "TASK", "confidence": 0.9468676447868347}]}, {"text": "We investigate several voting-and arbiter-based combination strategies over a diverse pool of unsupervised WSD systems.", "labels": [], "entities": [{"text": "WSD", "start_pos": 107, "end_pos": 110, "type": "TASK", "confidence": 0.9535624384880066}]}, {"text": "Our combination methods rely on predominant senses which are derived automatically from raw text.", "labels": [], "entities": []}, {"text": "Experiments using the SemCor and Senseval-3 data sets demonstrate that our ensembles yield significantly better results when compared with state-of-the-art.", "labels": [], "entities": [{"text": "Senseval-3 data sets", "start_pos": 33, "end_pos": 53, "type": "DATASET", "confidence": 0.8770470023155212}]}], "introductionContent": [{"text": "Word sense disambiguation (WSD), the task of identifying the intended meanings (senses) of words in context, holds promise for many NLP applications requiring broad-coverage language understanding.", "labels": [], "entities": [{"text": "Word sense disambiguation (WSD)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7948750654856364}, {"text": "identifying the intended meanings (senses) of words in context", "start_pos": 45, "end_pos": 107, "type": "TASK", "confidence": 0.5267948752099817}, {"text": "broad-coverage language understanding", "start_pos": 159, "end_pos": 196, "type": "TASK", "confidence": 0.6200340688228607}]}, {"text": "Examples include summarization, question answering, and text simplification.", "labels": [], "entities": [{"text": "summarization", "start_pos": 17, "end_pos": 30, "type": "TASK", "confidence": 0.9894368648529053}, {"text": "question answering", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.887933611869812}, {"text": "text simplification", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.7728120386600494}]}, {"text": "Recent studies have also shown that WSD can benefit machine translation () and information retrieval.", "labels": [], "entities": [{"text": "WSD", "start_pos": 36, "end_pos": 39, "type": "TASK", "confidence": 0.9791019558906555}, {"text": "machine translation", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.7735650539398193}, {"text": "information retrieval", "start_pos": 79, "end_pos": 100, "type": "TASK", "confidence": 0.8357585370540619}]}, {"text": "Given the potential of WSD for many NLP tasks, much work has focused on the computational treatment of sense ambiguity, primarily using data-driven methods.", "labels": [], "entities": [{"text": "WSD", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.9284475445747375}, {"text": "computational treatment of sense ambiguity", "start_pos": 76, "end_pos": 118, "type": "TASK", "confidence": 0.7493487358093261}]}, {"text": "Most accurate WSD systems to date are supervised and rely on the availability of training data, i.e., corpus occurrences of ambiguous words marked up with labels indicating the appropriate sense given the context (see and the references therein).", "labels": [], "entities": [{"text": "WSD", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.9748870730400085}]}, {"text": "A classifier automatically learns disambiguation cues from these hand-labeled examples.", "labels": [], "entities": []}, {"text": "Although supervised methods typically achieve better performance than unsupervised alternatives, their applicability is limited to those words for which sense labeled data exists, and their accuracy is strongly correlated with the amount of labeled data available ).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 190, "end_pos": 198, "type": "METRIC", "confidence": 0.9970802664756775}]}, {"text": "Furthermore, obtaining manually labeled corpora with word senses is costly and the task must be repeated for new domains, languages, or sense inventories.", "labels": [], "entities": []}, {"text": "estimates that a high accuracy domain independent system for WSD would probably need a corpus of about 3.2 million sense tagged words.", "labels": [], "entities": [{"text": "WSD", "start_pos": 61, "end_pos": 64, "type": "TASK", "confidence": 0.9726864099502563}]}, {"text": "At a throughput of one word per minute, this would require about 27 person-years of human annotation effort.", "labels": [], "entities": []}, {"text": "This paper focuses on unsupervised methods which we argue are useful for broad coverage sense disambiguation.", "labels": [], "entities": [{"text": "broad coverage sense disambiguation", "start_pos": 73, "end_pos": 108, "type": "TASK", "confidence": 0.8224807381629944}]}, {"text": "Unsupervised WSD algorithms fall into two general classes: those that perform token-based WSD by exploiting the similarity or relatedness between an ambiguous word and its context (e.g.,; and those that perform type-based WSD, simply by assigning all instances of an ambiguous word its most frequent (i.e., predominant) sense (e.g.,.", "labels": [], "entities": []}, {"text": "The predominant senses are automatically acquired from raw text without recourse to manually annotated data.", "labels": [], "entities": []}, {"text": "The motivation for assigning all instances of a word to its most prevalent sense stems from the observation that current supervised approaches rarely outperform the simple heuristic of choosing the most commonsense in the training data, despite taking local context into account).", "labels": [], "entities": []}, {"text": "Furthermore, the approach allows sense inventories to be tailored to specific domains.", "labels": [], "entities": []}, {"text": "The work presented here evaluates and compares the performance of well-established unsupervised WSD algorithms.", "labels": [], "entities": [{"text": "WSD", "start_pos": 96, "end_pos": 99, "type": "TASK", "confidence": 0.9497628211975098}]}, {"text": "We show that these algorithms yield sufficiently diverse outputs, thus motivating the use of combination methods for improving WSD performance.", "labels": [], "entities": [{"text": "WSD", "start_pos": 127, "end_pos": 130, "type": "TASK", "confidence": 0.9879617094993591}]}, {"text": "While combination approaches have been studied previously for supervised WSD (), their use in an unsupervised setting is, to our knowledge, novel.", "labels": [], "entities": [{"text": "WSD", "start_pos": 73, "end_pos": 76, "type": "TASK", "confidence": 0.94334876537323}]}, {"text": "We examine several existing and novel combination methods and demonstrate that our combined systems consistently outperform the state-of-the-art (e.g.,).", "labels": [], "entities": []}, {"text": "Importantly, our WSD algorithms and combination methods do not make use of training material in anyway, nor do they use the first sense information available in WordNet.", "labels": [], "entities": [{"text": "WSD", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.9436707496643066}, {"text": "WordNet", "start_pos": 161, "end_pos": 168, "type": "DATASET", "confidence": 0.9784855842590332}]}, {"text": "In the following section, we briefly describe the unsupervised WSD algorithms considered in this paper.", "labels": [], "entities": [{"text": "WSD", "start_pos": 63, "end_pos": 66, "type": "TASK", "confidence": 0.9496597051620483}]}, {"text": "Then, we present a detailed comparison of their performance on SemCor (.", "labels": [], "entities": []}, {"text": "Next, we introduce our system combination methods and report on our evaluation experiments.", "labels": [], "entities": []}, {"text": "We conclude the paper by discussing our results.", "labels": [], "entities": []}], "datasetContent": [{"text": "Unsupervised Algorithms for WSD  Unsupervised WSD", "labels": [], "entities": [{"text": "WSD  Unsupervised WSD", "start_pos": 28, "end_pos": 49, "type": "TASK", "confidence": 0.5279496212800344}]}], "tableCaptions": [{"text": " Table 2: Results of individual disambiguation al- gorithms on SemCor nouns 2 (  *  : sig. diff. from  Baseline,  \u2020 : sig. diff. from Similarity, $ : sig diff.  from SSI, # : sig. diff. from Overlap, p < 0.01)", "labels": [], "entities": []}, {"text": " Table 3: Algorithms' pairwise agreement in de- tecting the predominant sense (as % of all words)", "labels": [], "entities": []}, {"text": " Table 4: Ensemble Combination Results (  \u2020 : sig.  diff. from Similarity, $: sig. diff. from SSI,  \u2021: sig.  diff. from Voting, p < 0.01)", "labels": [], "entities": []}, {"text": " Table 5: Decrease in accuracy as a result of re- moval of each method from the rank-based ensem- ble.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9995618462562561}]}, {"text": " Table 6: Results of individual disambiguation al- gorithms and rank-based ensemble on Senseval-3  nouns", "labels": [], "entities": []}]}