{"title": [{"text": "Inducing Word Alignments with Bilexical Synchronous Trees", "labels": [], "entities": [{"text": "Inducing Word Alignments", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.824135402838389}]}], "abstractContent": [{"text": "This paper compares different bilexical tree-based models for bilingual alignment.", "labels": [], "entities": [{"text": "bilingual alignment", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.6864111125469208}]}, {"text": "EM training for the new model benefits from the dynamic programming \"hook trick\".", "labels": [], "entities": []}, {"text": "The model produces improved dependency structure for both languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "A major difficulty in statistical machine translation is the trade-off between representational power and computational complexity.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 22, "end_pos": 53, "type": "TASK", "confidence": 0.7478379408518473}]}, {"text": "Real-world corpora for language pairs such as Chinese-English have complex reordering relationships that are not captured by current phrase-based MT systems, despite their state-of-the-art performance measured in competitive evaluations.", "labels": [], "entities": [{"text": "MT", "start_pos": 146, "end_pos": 148, "type": "TASK", "confidence": 0.8585955500602722}]}, {"text": "Synchronous grammar formalisms that are capable of modeling such complex relationships while maintaining the context-free property in each language have been proposed for many years, (), but have not been scaled to large corpora and long sentences until recently.", "labels": [], "entities": []}, {"text": "In Synchronous Context Free Grammars, there are two sources of complexity, grammar branching factor and lexicalization.", "labels": [], "entities": [{"text": "Synchronous Context Free Grammars", "start_pos": 3, "end_pos": 36, "type": "TASK", "confidence": 0.7436894476413727}]}, {"text": "In this paper we focus on the second issue, constraining the grammar to the binary-branching Inversion Transduction Grammar of.", "labels": [], "entities": []}, {"text": "Lexicalization seems likely to help models predict alignment patterns between languages, and has been proposed by and implemented by and.", "labels": [], "entities": []}, {"text": "However, each piece of lexical information considered by a model multiplies the number of states of dynamic programming algorithms for inference, meaning that we must choose how to lexicalize very carefully to control complexity.", "labels": [], "entities": []}, {"text": "In this paper we compare two approaches to lexicalization, both of which incorporate bilexical probabilities.", "labels": [], "entities": []}, {"text": "One model uses bilexical probabilities across languages, while the other uses bilexical probabilities within one language.", "labels": [], "entities": []}, {"text": "We compare results on word-level alignment, and investigate the implications of the choice of lexicalization on the specifics of our alignment algorithms.", "labels": [], "entities": [{"text": "word-level alignment", "start_pos": 22, "end_pos": 42, "type": "TASK", "confidence": 0.7732289135456085}]}, {"text": "The new model, which bilexicalizes within languages, allows us to use the \"hook trick\") and therefore reduces complexity.", "labels": [], "entities": []}, {"text": "We describe the application of the hook trick to estimation with Expectation Maximization (EM).", "labels": [], "entities": [{"text": "estimation", "start_pos": 49, "end_pos": 59, "type": "TASK", "confidence": 0.958092451095581}, {"text": "Expectation Maximization (EM)", "start_pos": 65, "end_pos": 94, "type": "METRIC", "confidence": 0.8043209254741669}]}, {"text": "Despite the theoretical benefits of the hook trick, it is not widely used in statistical monolingual parsers, because the savings do not exceed those obtained with simple pruning.", "labels": [], "entities": []}, {"text": "We speculate that the advantages maybe greater in an EM setting, where parameters to guide pruning are not (initially) available.", "labels": [], "entities": []}, {"text": "In order to better understand the model, we analyze its performance in terms of both agreement with human-annotated alignments, and agreement with the dependencies produced by monolingual parsers.", "labels": [], "entities": []}, {"text": "We find that within-language bilexicalization does not improve alignment over crosslanguage bilexicalization, but does improve recovery of dependencies.", "labels": [], "entities": []}, {"text": "We find that the hook trick significantly speeds training, even in the presence of pruning.", "labels": [], "entities": [{"text": "hook", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.8666421175003052}]}, {"text": "Section 2 describes the generative model.", "labels": [], "entities": [{"text": "generative", "start_pos": 24, "end_pos": 34, "type": "TASK", "confidence": 0.9818328619003296}]}, {"text": "The hook trick for EM is explained in Section 3.", "labels": [], "entities": [{"text": "hook", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9790041446685791}, {"text": "EM", "start_pos": 19, "end_pos": 21, "type": "TASK", "confidence": 0.8977617621421814}]}, {"text": "In Section 4, we evaluate the model in terms of alignment error rate and dependency error rate.", "labels": [], "entities": [{"text": "alignment error rate", "start_pos": 48, "end_pos": 68, "type": "METRIC", "confidence": 0.8388501604398092}, {"text": "dependency error rate", "start_pos": 73, "end_pos": 94, "type": "METRIC", "confidence": 0.8528110980987549}]}, {"text": "We conclude with discussions in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "First of all, we are interested in finding out how much speedup can be achieved by doing the hook trick for EM.", "labels": [], "entities": [{"text": "speedup", "start_pos": 56, "end_pos": 63, "type": "METRIC", "confidence": 0.9673075079917908}, {"text": "hook", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.9499114155769348}, {"text": "EM", "start_pos": 108, "end_pos": 110, "type": "TASK", "confidence": 0.6031889319419861}]}, {"text": "We implemented both versions in C++ and turned off pruning for both.", "labels": [], "entities": []}, {"text": "We ran the two inside-outside parsing algorithms on a small test set of 46 sentence pairs that are no longer than 25 words in both languages.", "labels": [], "entities": []}, {"text": "Then we put the results into buckets of (1 \u2212 4), (5 \u2212 9), (10 \u2212 14),, and (20\u221224) according to the maximum length of two sentences in each pair and took averages of these timing results.", "labels": [], "entities": []}, {"text": "shows clearly that as the sentences get longer the hook trick is helping more and more.", "labels": [], "entities": []}, {"text": "We also tried to turn on pruning for both, which is the normal condition for the parsers.", "labels": [], "entities": []}, {"text": "Both are much faster due to the effectiveness of pruning.", "labels": [], "entities": []}, {"text": "The speedup ratio is lower because the hooks will less often be used again since many cells are pruned away.", "labels": [], "entities": [{"text": "speedup", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9907734394073486}]}, {"text": "(b) shows the speedup curve in this situation.", "labels": [], "entities": [{"text": "speedup", "start_pos": 14, "end_pos": 21, "type": "METRIC", "confidence": 0.9680891633033752}]}, {"text": "We trained both the unlexicalized and the lexicalized ITGs on a parallel corpus of ChineseEnglish newswire text.", "labels": [], "entities": [{"text": "ChineseEnglish newswire text", "start_pos": 83, "end_pos": 111, "type": "DATASET", "confidence": 0.9016336798667908}]}, {"text": "The Chinese data were automatically segmented into tokens, and English capitalization was retained.", "labels": [], "entities": []}, {"text": "We replaced words occurring only once with an unknown word token, resulting in a Chinese vocabulary of 23,783 words and an English vocabulary of 27,075 words.", "labels": [], "entities": []}, {"text": "We did two types of comparisons.", "labels": [], "entities": []}, {"text": "In the first comparison, we measured the performance of five word aligners, including IBM models, ITG, the lexical ITG (LITG) of, and our bilexical ITG (BLITG), on a hand-aligned bilingual corpus.", "labels": [], "entities": [{"text": "BLITG", "start_pos": 153, "end_pos": 158, "type": "METRIC", "confidence": 0.9108504056930542}]}, {"text": "All the models were trained using the same amount of data.", "labels": [], "entities": []}, {"text": "We ran the experiments on sentences up to 25 words long in both languages.", "labels": [], "entities": []}, {"text": "The resulting training corpus had 18,773 sentence pairs with a total of 276,113 Chinese words and 315,415 English words.", "labels": [], "entities": []}, {"text": "For scoring the Viterbi alignments of each system against gold-standard annotated alignments, we use the alignment error rate (AER) of, which measures agreement at the level of pairs of words: where A is the set of word pairs aligned by the automatic system, G S is the set marked in the gold standard as \"sure\", and GP is the set marked as \"possible\" (including the \"sure\" pairs).", "labels": [], "entities": [{"text": "alignment error rate (AER)", "start_pos": 105, "end_pos": 131, "type": "METRIC", "confidence": 0.9144453505674998}, {"text": "GP", "start_pos": 317, "end_pos": 319, "type": "METRIC", "confidence": 0.9691147804260254}]}, {"text": "In our Chinese-English data, only one type of alignment was marked, meaning that GP = G S . In our hand-aligned data, 47 sentence pairs are no longer than 25 words in either language and were used to evaluate the aligners.", "labels": [], "entities": [{"text": "GP", "start_pos": 81, "end_pos": 83, "type": "METRIC", "confidence": 0.9619187712669373}]}, {"text": "A separate development set of hand-aligned sentence pairs was used to control overfitting.", "labels": [], "entities": []}, {"text": "The subset of up to 25 words in both languages was used.", "labels": [], "entities": []}, {"text": "We chose the number of iterations for EM  training as the turning point of AER on the development data set.", "labels": [], "entities": [{"text": "EM  training", "start_pos": 38, "end_pos": 50, "type": "TASK", "confidence": 0.9354268014431}, {"text": "AER", "start_pos": 75, "end_pos": 78, "type": "METRIC", "confidence": 0.9482564926147461}, {"text": "development data set", "start_pos": 86, "end_pos": 106, "type": "DATASET", "confidence": 0.777664581934611}]}, {"text": "The unlexicalized ITG was trained for 3 iterations.", "labels": [], "entities": [{"text": "ITG", "start_pos": 18, "end_pos": 21, "type": "DATASET", "confidence": 0.8266063332557678}]}, {"text": "LITG was trained for only 1 iteration, partly because it was initialized with fully trained ITG parameters.", "labels": [], "entities": [{"text": "LITG", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7782451510429382}]}, {"text": "BLITG was trained for 3 iterations.", "labels": [], "entities": [{"text": "BLITG", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.5637254118919373}]}, {"text": "For comparison, we also included the results from IBM Model 1 and Model 4.", "labels": [], "entities": [{"text": "IBM Model 1", "start_pos": 50, "end_pos": 61, "type": "DATASET", "confidence": 0.8919176260630289}]}, {"text": "The numbers of iterations for the training of the IBM models were also chosen to be the turning points of AER changing on the development data set.", "labels": [], "entities": [{"text": "AER", "start_pos": 106, "end_pos": 109, "type": "METRIC", "confidence": 0.7698533535003662}, {"text": "development data set", "start_pos": 126, "end_pos": 146, "type": "DATASET", "confidence": 0.79182368516922}]}, {"text": "We also want to know whether or not BLITG can model dependencies better than LITG.", "labels": [], "entities": [{"text": "BLITG", "start_pos": 36, "end_pos": 41, "type": "METRIC", "confidence": 0.7160272002220154}]}, {"text": "For this purpose, we also used the AER measurement, since the goal is still getting higher precision/recall fora set of recovered word links, although the dependency word links are within one language.", "labels": [], "entities": [{"text": "AER measurement", "start_pos": 35, "end_pos": 50, "type": "METRIC", "confidence": 0.9780146181583405}, {"text": "precision", "start_pos": 91, "end_pos": 100, "type": "METRIC", "confidence": 0.9974735379219055}, {"text": "recall", "start_pos": 101, "end_pos": 107, "type": "METRIC", "confidence": 0.9335639476776123}]}, {"text": "For this reason, we rename AER to Dependency Error Rate.", "labels": [], "entities": [{"text": "AER", "start_pos": 27, "end_pos": 30, "type": "METRIC", "confidence": 0.9978631138801575}, {"text": "Dependency Error Rate", "start_pos": 34, "end_pos": 55, "type": "METRIC", "confidence": 0.5134716033935547}]}, {"text": "Table 1(right) is the dependency results on English side of the test data set.", "labels": [], "entities": [{"text": "English side of the test data set", "start_pos": 44, "end_pos": 77, "type": "DATASET", "confidence": 0.7678345484392983}]}, {"text": "The dependency results on Chinese are similar.", "labels": [], "entities": []}, {"text": "The gold standard dependencies were extracted from Collins' parser output on the sentences.", "labels": [], "entities": [{"text": "Collins' parser output", "start_pos": 51, "end_pos": 73, "type": "DATASET", "confidence": 0.8700227538744608}]}, {"text": "The LITG and BLITG dependencies were extracted from the Viterbi synchronous trees by following the head words.", "labels": [], "entities": [{"text": "LITG", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9522163271903992}, {"text": "BLITG", "start_pos": 13, "end_pos": 18, "type": "METRIC", "confidence": 0.984613299369812}]}, {"text": "For comparison, we also included two base-line results.", "labels": [], "entities": []}, {"text": "ITG-lh is unlexicalized ITG with left-head assumption, meaning the head words always come from the left branches.", "labels": [], "entities": [{"text": "ITG-lh", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8987342119216919}]}, {"text": "ITG-rh is ITG with righthead assumption.", "labels": [], "entities": [{"text": "ITG-rh", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8967268466949463}]}, {"text": "To make more confident conclusions, we also did tests on a larger hand-aligned data set used in.", "labels": [], "entities": []}, {"text": "We used 165 sentence pairs that are up to 25 words in length on both sides.", "labels": [], "entities": []}, {"text": "To get the reasons, we need further and deeper analysis.", "labels": [], "entities": []}, {"text": "One might guess that the dependencies are modeled but are not yet strong and good enough given the amount of training data.", "labels": [], "entities": []}, {"text": "Since the training algorithm EM has the problem of local maxima, we might also need to adjust the training algorithm to obtain good parameters for the alignment task.", "labels": [], "entities": []}, {"text": "Initializing the model with good dependency parameters is a possible adjustment.", "labels": [], "entities": []}, {"text": "We would also like to point out that alignment task is simpler than decoding where a stronger component of reordering is required to produce a fluent English sentence.", "labels": [], "entities": [{"text": "alignment", "start_pos": 37, "end_pos": 46, "type": "TASK", "confidence": 0.9394695162773132}]}, {"text": "Investigating the impact of bilexical dependencies on decoding is our future work.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Bilingual alignment and English dependency results on Chinese-English corpus (\u2264 25 words on  both sides). LITG stands for the cross-language Lexicalized ITG. BLITG is the within-English Bilexical  ITG. ITG-lh is ITG with left-head assumption on English. ITG-rh is with right-head assumption.", "labels": [], "entities": [{"text": "Bilingual alignment", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.8771446943283081}, {"text": "LITG", "start_pos": 116, "end_pos": 120, "type": "METRIC", "confidence": 0.9521106481552124}, {"text": "BLITG", "start_pos": 168, "end_pos": 173, "type": "METRIC", "confidence": 0.9814165830612183}]}, {"text": " Table 2: Alignment and dependency results on a larger Chinese-English corpus.", "labels": [], "entities": [{"text": "Alignment", "start_pos": 10, "end_pos": 19, "type": "TASK", "confidence": 0.9498461484909058}]}]}