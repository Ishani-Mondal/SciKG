{"title": [{"text": "Graph Branch Algorithm: An Optimum Tree Search Method for Scored Dependency Graph with Arc Co-occurrence Constraints", "labels": [], "entities": []}], "abstractContent": [{"text": "Various kinds of scored dependency graphs are proposed as packed shared data structures in combination with optimum dependency tree search algorithms.", "labels": [], "entities": []}, {"text": "This paper classifies the scored dependency graphs and discusses the specific features of the \"Dependency Forest\" (DF) which is the packed shared data structure adopted in the \"Preference Dependency Grammar\" (PDG), and proposes the \"Graph Branch Algorithm\" for computing the optimum dependency tree from a DF.", "labels": [], "entities": [{"text": "Preference Dependency Grammar\" (PDG)", "start_pos": 177, "end_pos": 213, "type": "TASK", "confidence": 0.5980575510433742}]}, {"text": "This paper also reports the experiment showing the computational amount and behavior of the graph branch algorithm.", "labels": [], "entities": []}], "introductionContent": [{"text": "The dependency graph (DG) is a packed shared data structure which consists of the nodes corresponding to the words in a sentence and the arcs showing dependency relations between the nodes.", "labels": [], "entities": []}, {"text": "The scored DG has preference scores attached to the arcs and is widely used as a basis of the optimum tree search method.", "labels": [], "entities": []}, {"text": "For example, the scored DG is used in Japanese Kakari-uke analysis to represent all possible kakari-uke(dependency) trees,.) proposed a dependency analysis method using a scored DG and some maximum spanning tree search algorithms.", "labels": [], "entities": [{"text": "Kakari-uke analysis", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.6502169817686081}]}, {"text": "In this method, scores on arcs are computed from a set of features obtained from the dependency trees based on the optimum parameters for scoring dependency arcs obtained by the discriminative learning method.", "labels": [], "entities": []}, {"text": "There are various kinds of dependency analysis methods based on the scored DGs.", "labels": [], "entities": [{"text": "dependency analysis", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.8210708200931549}]}, {"text": "This paper classifies these methods based on the types of the DGs and the basic well-formed constraints and explains the features of the DF adopted in PDG).", "labels": [], "entities": []}, {"text": "This paper proposes the graph branch algorithm which searches the optimum dependency tree from a DF based on the branch and bound (B&B) method and reports the experiment showing the computational amount and behavior of the graph branch algorithm.", "labels": [], "entities": []}, {"text": "As shown below, the combination of the DF and the graph branch algorithm enables the treatment of non-projective dependency analysis and optimum solution search satisfying the single valence occupation constraint, which are out of the scope of most of the DP(dynamic programming)-based parsing methods.", "labels": [], "entities": [{"text": "DP(dynamic programming)-based parsing", "start_pos": 256, "end_pos": 293, "type": "TASK", "confidence": 0.5280335247516632}]}, {"text": "shows the basic framework of the optimum dependency tree search in a scored DG.", "labels": [], "entities": []}, {"text": "In general, nodes in a DG correspond to words in the sentence and the arcs show some kind of Figure 1: The optimum tree search in a scored DG a preference score representing plausibility of the relation.", "labels": [], "entities": []}, {"text": "The well-formed dependency tree constraint is a set of well-formed constraints which should be satisfied by all dependency trees representing sentence interpretations.", "labels": [], "entities": []}, {"text": "A DG and a wellformed dependency tree constraint prescribe a set of well-formed dependency trees.", "labels": [], "entities": []}, {"text": "The score of a dependency tree is the sum total of arc scores.", "labels": [], "entities": []}, {"text": "The optimum tree is a dependency tree with the highest score in the set of dependency trees.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section describes some experimental results showing the computational amount of the graph branch algorithm.", "labels": [], "entities": []}, {"text": "An evaluation experiment for the open data is performed using a prototype PDG system implemented in Prolog.", "labels": [], "entities": [{"text": "Prolog", "start_pos": 100, "end_pos": 106, "type": "DATASET", "confidence": 0.9708789587020874}]}, {"text": "The sentences containing more than 22 words are neglected due to the limitation of Prolog system resources in the parsing process.", "labels": [], "entities": [{"text": "parsing", "start_pos": 114, "end_pos": 121, "type": "TASK", "confidence": 0.9737768173217773}]}, {"text": "4334 sentences out of the remaining 6882 test sentences are parsable.", "labels": [], "entities": []}, {"text": "According to, the experiment on the B&B-based algorithm for the SDG shows the overall averages of AVE:EPN-T, AVE:EPN-F are 2.91, 1.33 and the average CPU time is 305.8ms (on EWS).", "labels": [], "entities": [{"text": "AVE", "start_pos": 98, "end_pos": 101, "type": "METRIC", "confidence": 0.9966488480567932}, {"text": "AVE", "start_pos": 109, "end_pos": 112, "type": "METRIC", "confidence": 0.986132025718689}, {"text": "EWS", "start_pos": 174, "end_pos": 177, "type": "DATASET", "confidence": 0.9128831028938293}]}, {"text": "These values are close to those in the experiment based on the graph branch algorithm.", "labels": [], "entities": []}, {"text": "Two experiments show a tendency for the optimum solution to be obtained in the early stage of the search process.", "labels": [], "entities": []}, {"text": "The graph branch algorithm is expected to obtain the comparable performance with the SDG search algorithm.", "labels": [], "entities": []}, {"text": "() introduced the improved upper bound function g'(P) into the B&B-based algorithm for the SDG and found Ave:EPN-T is reduced from 2.91 to 1.82.", "labels": [], "entities": [{"text": "Ave", "start_pos": 105, "end_pos": 108, "type": "METRIC", "confidence": 0.9797232747077942}]}, {"text": "The same technique is introduced to the graph branch algorithm and has obtained the reduction of the Ave:EPN-T from 3.00 to 2.68.", "labels": [], "entities": []}, {"text": "The tendency for the optimum solution to be obtained in the early stage of the search process suggests that limiting the number of problems to expand is an effective pruning strategy.", "labels": [], "entities": []}, {"text": "shows the ratios of the sentences obtaining the whole problem expansion, the first optimum solu-: ARs for EPS-F, EPS-A, EPS-T tion and the last optimum solution to whole sentences with respect to the EPNs.", "labels": [], "entities": [{"text": "ARs", "start_pos": 98, "end_pos": 101, "type": "METRIC", "confidence": 0.8533167243003845}]}, {"text": "This kind of ratio is called an achievement ratio (AR) in this paper.", "labels": [], "entities": [{"text": "achievement ratio (AR)", "start_pos": 32, "end_pos": 54, "type": "METRIC", "confidence": 0.9355744242668151}]}, {"text": "From, the ARs for EPN-T, EPN-L, EPN-F (plotted in solid lines) are 97.1%,99.6%,99.8% respectively at the EPN 10.", "labels": [], "entities": [{"text": "ARs", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9987137317657471}, {"text": "EPN 10", "start_pos": 105, "end_pos": 111, "type": "DATASET", "confidence": 0.9536857008934021}]}, {"text": "The dotted line shows the AR for EPN-T of the improved algorithm using g'(P).", "labels": [], "entities": [{"text": "AR", "start_pos": 26, "end_pos": 28, "type": "METRIC", "confidence": 0.9976617097854614}]}, {"text": "The use of g'(P) increases the AR for EPN-T from 97.1% to 99.1% at the EPN 10.", "labels": [], "entities": [{"text": "AR", "start_pos": 31, "end_pos": 33, "type": "METRIC", "confidence": 0.9979110360145569}, {"text": "EPN 10", "start_pos": 71, "end_pos": 77, "type": "DATASET", "confidence": 0.95226189494133}]}, {"text": "However, the effect of g'(P) is quite small for EPN-F and EPN-L.", "labels": [], "entities": [{"text": "EPN-L", "start_pos": 58, "end_pos": 63, "type": "DATASET", "confidence": 0.904448390007019}]}, {"text": "This result shows that the pruning strategy based on the EPN is effective and g'(P) works for the reduction of the problems generated in the posterior part of the search processes.", "labels": [], "entities": []}], "tableCaptions": []}