{"title": [{"text": "A Bootstrapping Approach to Unsupervised Detection of Cue Phrase Variants", "labels": [], "entities": [{"text": "Unsupervised Detection of Cue Phrase Variants", "start_pos": 28, "end_pos": 73, "type": "TASK", "confidence": 0.6870231280724207}]}], "abstractContent": [{"text": "We investigate the unsupervised detection of semi-fixed cue phrases such as \"This paper proposes a novel approach..", "labels": [], "entities": []}, {"text": "1 \" from unseen text, on the basis of only a handful of seed cue phrases with the desired semantics.", "labels": [], "entities": []}, {"text": "The problem, in contrast to bootstrapping approaches for Question Answering and Information Extraction, is that it is hard to find a constraining context for occurrences of semi-fixed cue phrases.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 57, "end_pos": 75, "type": "TASK", "confidence": 0.8205828666687012}, {"text": "Information Extraction", "start_pos": 80, "end_pos": 102, "type": "TASK", "confidence": 0.7061749547719955}]}, {"text": "Our method uses components of the cue phrase itself, rather than external context , to bootstrap.", "labels": [], "entities": []}, {"text": "It successfully excludes phrases which are different from the target semantics, but which look superficially similar.", "labels": [], "entities": []}, {"text": "The method achieves 88% accuracy , outperforming standard bootstrap-ping approaches.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9994080066680908}]}], "introductionContent": [{"text": "Cue phrases such as \"This paper proposes a novel approach to.", "labels": [], "entities": []}, {"text": "\", \"no method for . .", "labels": [], "entities": []}, {"text": "exists\" or even \"you will hear from my lawyer\" are semi-fixed in that they constitute a formulaic pattern with a clear semantics, but with syntactic and lexical variations which are hard to predict and thus hard to detect in unseen text (e.g. \"a new algorithm for . .", "labels": [], "entities": []}, {"text": ". is suggested in the current paper\" or \"I envisage legal action\").", "labels": [], "entities": []}, {"text": "In scientific discourse, such metadiscourse) abounds and plays an important role in marking the discourse structure of the texts.", "labels": [], "entities": []}, {"text": "Finding these variants can be useful for many text understanding tasks because semi-fixed cue phrases act as linguistic markers indicating the importance and/or the rhetorical role of some adjacent text.", "labels": [], "entities": [{"text": "text understanding", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.8297228515148163}]}, {"text": "For the summarisation of scientific 1 In contrast to standard work in discourse linguistics, which mostly considers sentence connectives and adverbials as cue phrases, our definition includes longer phrases, sometimes even entire sentences.", "labels": [], "entities": []}, {"text": "papers, cue phrases such as \"Our paper deals with.", "labels": [], "entities": []}, {"text": "\" are commonly used as indicators of extraction-worthiness of sentences (.", "labels": [], "entities": []}, {"text": "Re-generative (rather than extractive) summarisation methods may want to go further than that and directly use the knowledge that a certain sentence contains the particular research aim of a paper, or a claimed gap in the literature.", "labels": [], "entities": [{"text": "extractive) summarisation", "start_pos": 27, "end_pos": 52, "type": "TASK", "confidence": 0.5568389495213827}]}, {"text": "Similarly, in the task of automatic routing of customer emails and automatic answering of some of these, the detection of threats of legal action could be useful.", "labels": [], "entities": [{"text": "automatic routing of customer emails", "start_pos": 26, "end_pos": 62, "type": "TASK", "confidence": 0.7890729367733001}, {"text": "automatic answering of", "start_pos": 67, "end_pos": 89, "type": "TASK", "confidence": 0.6914192338784536}]}, {"text": "However, systems that use cue phrases usually rely on manually compiled lists, the acquisition of which is time-consuming and error-prone and results in cue phrases which are genre-specific.", "labels": [], "entities": []}, {"text": "Methods for finding cue phrases automatically include (using the ratio of word frequency counts in summaries and their corresponding texts), Teufel (1998) (using the most frequent n-grams), and (using a pattern matching grammar and a lexicon of manually collected equivalence classes).", "labels": [], "entities": []}, {"text": "The main issue with string-based pattern matching techniques is that they cannot capture syntactic generalisations such as active/passive constructions, different tenses and modification by adverbial, adjectival or prepositional phrases, appositions and other parenthetical material.", "labels": [], "entities": [{"text": "string-based pattern matching", "start_pos": 20, "end_pos": 49, "type": "TASK", "confidence": 0.706853965918223}]}, {"text": "For instance, we maybe looking for sentences expressing the goal or main contribution of a paper; shows candidates of such sentences.", "labels": [], "entities": []}, {"text": "Cases a)-e), which do indeed describe the authors' goal, display a wide range of syntactic variation.", "labels": [], "entities": []}, {"text": "a) In this paper, we introduce a method for similaritybased estimation of . .", "labels": [], "entities": []}, {"text": "b) We introduce and justify a method.", "labels": [], "entities": []}, {"text": "c) A method (described in section 1) is introduced d) The method introduced here is a variation.", "labels": [], "entities": []}, {"text": "e) We wanted to introduce a method.", "labels": [], "entities": []}, {"text": "f) We do not introduce a method.", "labels": [], "entities": []}, {"text": "g) We introduce and adopt the method given in.", "labels": [], "entities": []}, {"text": "h) Previously we introduced a similar method.", "labels": [], "entities": []}, {"text": "i) They introduce a similar method.", "labels": [], "entities": []}, {"text": "Cases f)-i) in contrast are false matches: they do not express the authors' goals, although they are superficially similar to the correct contexts.", "labels": [], "entities": []}, {"text": "While string-based approaches are too restrictive to cover the wide variation within the correct contexts, bag-of-words approaches such as Agichtein and are too permissive and would miss many of the distinctions between correct and incorrect contexts.", "labels": [], "entities": []}, {"text": "address the task of identifying \"paradigm shift\" sentences in the biomedical literature, i.e. statements of thwarted expectation.", "labels": [], "entities": []}, {"text": "This task is somewhat similar to ours in its definition by rhetorical context.", "labels": [], "entities": []}, {"text": "Their method goes beyond string-based matching: In order fora sentence to qualify, the right set of concepts must be present in a sentence, with any syntactic relationship holding between them.", "labels": [], "entities": []}, {"text": "Each concept set is encoded as a fixed, manually compiled lists of strings.", "labels": [], "entities": []}, {"text": "Their method covers only one particular context (the paradigm shift one), whereas we are looking fora method where many types of cue phrases can be acquired.", "labels": [], "entities": []}, {"text": "Whereas it relies on manually assembled lists, we advocate data-driven acquisition of new contexts.", "labels": [], "entities": []}, {"text": "This is generally preferrable to manual definition, as language use is changing, inventive and hard to predict and as many of the relevant concepts in a domain maybe infrequent (cf. the formulation \"be cursed\", which was used in our corpus as away of describing a method's problems).", "labels": [], "entities": []}, {"text": "It also allows the acquisition of cue phrases in new domains, where the exact prevalent meta-discourse might not be known.", "labels": [], "entities": []}, {"text": "method for learning information extraction (IE) patterns uses a syntactic parse and correspondences between the text and filled MUCstyle templates to learn context in terms of lexicosemantic patterns.", "labels": [], "entities": [{"text": "learning information extraction (IE) patterns", "start_pos": 11, "end_pos": 56, "type": "TASK", "confidence": 0.8214953030858722}]}, {"text": "However, it too requires substantial hand-crafted knowledge: 1500 filled templates as training material, and a lexicon of semantic features for roughly 3000 nouns for constraint checking.", "labels": [], "entities": [{"text": "constraint checking", "start_pos": 167, "end_pos": 186, "type": "TASK", "confidence": 0.7219704389572144}]}, {"text": "Unsupervised methods for similar tasks include Agichtein and work, which shows that clusters of vector-spacebased patterns can be successfully employed to detect specific IE relationships (companies and their headquarters), and Ravichandran and Hovy's (2002) algorithm for finding patterns fora Question Answering (QA) task.", "labels": [], "entities": [{"text": "Question Answering (QA) task", "start_pos": 295, "end_pos": 323, "type": "TASK", "confidence": 0.854317327340444}]}, {"text": "Based on training material in the shape of pairs of question and answer terms -e.g., (e.g. {Mozart, 1756}), they learn the a) In this paper, we introduce a method for similaritybased estimation of . .", "labels": [], "entities": []}, {"text": "b) Here, we present a similarity-based approach for estimation of.", "labels": [], "entities": []}, {"text": "c) In this paper, we propose an algorithm which is . .", "labels": [], "entities": []}, {"text": "d) We will here define a technique for similarity-based.", "labels": [], "entities": []}, {"text": "semantics holding between these terms (\"birth year\") via frequent string patterns occurring in the context, such as \"A was born in B\", by considering n-grams of all repeated substrings.", "labels": [], "entities": []}, {"text": "What is common to these three works is that bootstrapping relies on constraints between the context external to the extracted material and the extracted material itself, and that the target extraction material is defined by real-world relations.", "labels": [], "entities": []}, {"text": "Our task differs in that the cue phrases we extract are based on general rhetorical relations holding in all scientific discourse.", "labels": [], "entities": []}, {"text": "Our approach for finding semantically similar variants in an unsupervised fashion relies on bootstrapping of seeds from within the cue phrase.", "labels": [], "entities": []}, {"text": "The assumption is that every semi-fixed cue phrase contains at least two main concepts whose syntax and semantics mutually constrain each other (e.g. verb and direct object in phrases such as \"(we) present an approach for\").", "labels": [], "entities": []}, {"text": "The expanded cue phrases are recognised in various syntactic contexts using a parser 2 . General semantic constraints valid for groups of semantically similar cue phrases are then applied to model, e.g., the fact that it must be the authors who present the method, not somebody else.", "labels": [], "entities": []}, {"text": "We demonstrate that such an approach is more appropriate for our task than IE/QA bootstrapping mechanisms based on cue phrase-external context.", "labels": [], "entities": []}, {"text": "Part of the reason for why normal bootstrapping does notwork for our phrases is the difficulty of finding negatives contexts, essential in bootstrapping to evaluate the quality of the patterns automatically.", "labels": [], "entities": []}, {"text": "IE and QA approaches, due to uniqueness assumptions of the real-world relations that these methods search for, have an automatic definition of negative contexts by hard constraints (i.e., all contexts involving Mozart and any other year are by definition of the wrong semantics; so are all contexts involving Microsoft and a city other than Redmond).", "labels": [], "entities": []}, {"text": "As our task is not grounded in real-world relations but in rhetorical ones, constraints found in the context tend to be 2 Thus, our task shows some parallels to work in paraphrasing () and syntactic variant generation (), but the methods are very different.", "labels": [], "entities": [{"text": "syntactic variant generation", "start_pos": 189, "end_pos": 217, "type": "TASK", "confidence": 0.6362860997517904}]}, {"text": "soft rather than hard (cf.: while it it possible that strings such as \"we\" and \"in this paper\" occur more often in the context of a given cue phrase, they also occur in many other places in the paper where the cue phrase is not present.", "labels": [], "entities": []}, {"text": "Thus, it is hard to define clear negative contexts for our task.", "labels": [], "entities": []}, {"text": "The novelty of our work is thus the new pattern extraction task (finding variants of semi-fixed cue phrases), a task for which it is hard to directly use the context the patterns appear in, and an iterative unsupervised bootstrapping algorithm for lexical variants, using phrase-internal seeds and ranking similar candidates based on relation strength between the seeds.", "labels": [], "entities": [{"text": "pattern extraction task", "start_pos": 40, "end_pos": 63, "type": "TASK", "confidence": 0.7906347016493479}]}, {"text": "While our method is applicable to general cue phrases, we demonstrate it herewith transitive verb-direct object pairs, namely a) cue phrases introducing anew methodology (and thus the main research goal of the scientific article; e.g. \"In this paper, we propose a novel algorithm.", "labels": [], "entities": []}, {"text": "\") -we call those goal-type cue phrases; and b) cue phrases indicating continuation of previous other research (e.g. \"Therefore, we adopt the approach presented in.", "labels": [], "entities": []}, {"text": "\") -continuation-type cue phrases.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated the quality of the extracted phrases in two ways: by comparing our system output to gold standard annotation, and by human judgement of the quality of the returned sentences.", "labels": [], "entities": []}, {"text": "In both cases bootstrapping was done using the seed tuples &.", "labels": [], "entities": []}, {"text": "For the gold standard-evaluation, we ran our system on a test set of 121 scientific articles drawn from the CmpLg corpus (Teufel, 1999) -entirely different texts from the ones the system was trained on.", "labels": [], "entities": [{"text": "CmpLg corpus (Teufel, 1999)", "start_pos": 108, "end_pos": 135, "type": "DATASET", "confidence": 0.940852301461356}]}, {"text": "Documents were manually annotated by the second author for (possibly more than one) goal-type sentence; annotation of that type has been previously shown to be reliable at K=.71.", "labels": [], "entities": []}, {"text": "Our evaluation recorded how often the system's highest-ranked candidate was indeed a goal-type sentence; as this is a precision-critical task, we do not measure recall here.", "labels": [], "entities": [{"text": "precision-critical", "start_pos": 118, "end_pos": 136, "type": "METRIC", "confidence": 0.9950849413871765}, {"text": "recall", "start_pos": 161, "end_pos": 167, "type": "METRIC", "confidence": 0.9984390139579773}]}, {"text": "We compared our system against our reimplementation of paraphrase learning.", "labels": [], "entities": []}, {"text": "The seed words were of the form {goal-verb, goal-noun}, and we submitted each of the 4 combinations of the seed pair to Google Scholar.", "labels": [], "entities": []}, {"text": "From the top 1000 documents for each query, we harvested 3965 sentences containing both the goal-verb and the goal-noun.", "labels": [], "entities": []}, {"text": "By considering all possible substrings, an extensive list of candidate patterns was assembled.", "labels": [], "entities": []}, {"text": "Patterns with single occurrences were discarded, leaving a list of 5580 patterns (examples in.", "labels": [], "entities": []}, {"text": "In order to rank the patterns by precision, the goal-verbs were submitted as queries and the top 1000 documents were downloaded for each.", "labels": [], "entities": [{"text": "precision", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9985051155090332}]}, {"text": "From these, we <verb> a <noun> for of anew <noun> to <verb> the In this section , we <verb> the <noun> of the <noun> <verb> in this paper is to <verb> the <noun> after 58 (48%) Our system, no bootstrapping, WordNet 50 (41%) Our system, no bootstrapping, seeds only 37 (30%): Gold standard evaluation: results the precision of each pattern was calculated by dividing the number of strings matching the pattern instantiated with both the goal-verb and all WordNet synonyms of the goal-noun, by the number of strings matching the patterns instantiated with the goal-verb only.", "labels": [], "entities": [{"text": "precision", "start_pos": 313, "end_pos": 322, "type": "METRIC", "confidence": 0.9991241097450256}]}, {"text": "An important point here is that while the tight semantic coupling between the question and answer terms in the original method accurately identifies all the positive and negative examples, we can only approximate this by using a sensible synonym set for the seed goal-nouns.", "labels": [], "entities": []}, {"text": "For each document in the test set, the sentence containing the pattern with the highest precision (if any) was extracted as the goal sentence.", "labels": [], "entities": [{"text": "precision", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.9936182498931885}]}, {"text": "We also compared our system to two baselines.", "labels": [], "entities": []}, {"text": "We replaced the lists obtained from the lexical bootstrapping module with a) just the seed pair and b) the seed pair and all the WordNet synonyms of the components of the seed pair 9 . The results of these experiments are given in.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 129, "end_pos": 136, "type": "DATASET", "confidence": 0.9246867299079895}]}, {"text": "All differences are statistically significant with the \u03c7 2 test at p=.01 (except those between Ravichandran/Hovy and our nonbootstrapping/WordNet system).", "labels": [], "entities": []}, {"text": "Our bootstrapping system outperforms the Ravichandran and Hovy algorithm by 34%.", "labels": [], "entities": []}, {"text": "This is not surprising, because this algorithm was not designed to perform well in tasks where there is no clear negative context.", "labels": [], "entities": []}, {"text": "The results also show that bootstrapping outperforms a general thesaurus such as WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 81, "end_pos": 88, "type": "DATASET", "confidence": 0.9645639061927795}]}, {"text": "Out of the 33 articles where our system's favourite was not an annotated goal-type sentence, only 15 are due to bootstrapping errors (i.e., to an incorrect ranking of the lexical variants), corre- sponding to a 88% accuracy of the bootstrapping module.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 215, "end_pos": 223, "type": "METRIC", "confidence": 0.9982683658599854}]}, {"text": "Examples from those 15 error cases are given in.", "labels": [], "entities": []}, {"text": "The other errors were due to the cue phrase not being a transitive verb-direct object pattern (e.g. we show that, our goal is and we focus on), so the system could not have found anything (11 cases, or an 80% accuracy), ungrammatical English or syntactic construction too complex, resulting in alack of RASP detection of the crucial grammatical relation (2) and failure of the semantic filter to catch non-goal contexts (5).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 209, "end_pos": 217, "type": "METRIC", "confidence": 0.9915412068367004}, {"text": "RASP detection", "start_pos": 303, "end_pos": 317, "type": "TASK", "confidence": 0.9442736208438873}]}, {"text": "We next perform two human experiments to indirectly evaluate the quality of the automatically generated cue phrase variants.", "labels": [], "entities": []}, {"text": "Given an abstract of an article and a sentence extracted from the article, judges are asked to assign a score ranging from 1 (low) to 5 (high) depending on how well the sentence expresses the goal of that article (Exp. A), or the continuation of previous work (Exp. B).", "labels": [], "entities": []}, {"text": "Each experiment involves 24 articles drawn randomly from a subset of 80 articles in the CmpLg corpus that contain manual annotation for goaltype and continuation-type sentences.", "labels": [], "entities": [{"text": "CmpLg corpus", "start_pos": 88, "end_pos": 100, "type": "DATASET", "confidence": 0.9290780127048492}]}, {"text": "The experiments use three external judges (graduate students in computational linguistics), and a Latin Square experimental design with three conditions: Baseline (see below), System-generated and Ceiling (extracted from the gold standard annotation used in).", "labels": [], "entities": []}, {"text": "Judges were not told how the sentences were generated, and no judge saw an item in more than one condition.", "labels": [], "entities": []}, {"text": "The baseline for Experiment A was a random selection of sentences with the highest T F *IDF scores, because goal-type sentences typically contain many content-words.", "labels": [], "entities": [{"text": "T F *IDF scores", "start_pos": 83, "end_pos": 98, "type": "METRIC", "confidence": 0.7960180282592774}]}, {"text": "The baseline for experiment B (continuation-type) were randomly selected sentences containing citations, because they often co-occur with statements of continuation.", "labels": [], "entities": []}, {"text": "In both cases, the length of the baseline sentence was controlled for by the average lengths of the gold standard and the system-extracted sentences in the document.", "labels": [], "entities": []}, {"text": "shows that judges gave an average score of 3.08 to system-extracted sentences in Exp.", "labels": [], "entities": [{"text": "Exp.", "start_pos": 81, "end_pos": 85, "type": "DATASET", "confidence": 0.9186951518058777}]}, {"text": "A, compared with a baseline of 1.58 and a ceiling of 3.91 10 ; in Exp.", "labels": [], "entities": [{"text": "A", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.9722264409065247}, {"text": "baseline", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9936277866363525}, {"text": "Exp.", "start_pos": 66, "end_pos": 70, "type": "DATASET", "confidence": 0.938141256570816}]}, {"text": "B, the system scored 3.67, with a higher baseline of 2.50 and a ceiling of 4.33.", "labels": [], "entities": [{"text": "baseline", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9886041879653931}]}, {"text": "According to the Wilcoxon signed-ranks test at \u03b1 = .01, the system is indistinguishable from the gold standard, but significantly different from the baseline, in both experiments.", "labels": [], "entities": []}, {"text": "Although this study is on a small scale, it indicates that humans judged sentences obtained with our method as almost equally characteristic of their rhetorical function as human-chosen sentences, and much better than non-trivial baselines.", "labels": [], "entities": []}], "tableCaptions": []}