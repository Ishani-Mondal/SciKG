{"title": [{"text": "Semantic Discourse Segmentation and Labeling for Route Instructions", "labels": [], "entities": [{"text": "Semantic Discourse Segmentation", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.780517578125}]}], "abstractContent": [{"text": "In order to build a simulated robot that accepts instructions in unconstrained natural language, a corpus of 427 route instructions was collected from human subjects in the office navigation domain.", "labels": [], "entities": []}, {"text": "The instructions were segmented by the steps in the actual route and labeled with the action taken in each step.", "labels": [], "entities": []}, {"text": "This flat formulation reduced the problem to an IE/Segmentation task, to which we applied Conditional Random Fields.", "labels": [], "entities": []}, {"text": "We compared the performance of CRFs with a set of handwritten rules.", "labels": [], "entities": []}, {"text": "The result showed that CRFs perform better with a 73.7% success rate.", "labels": [], "entities": [{"text": "CRFs", "start_pos": 23, "end_pos": 27, "type": "TASK", "confidence": 0.9055270552635193}]}], "introductionContent": [{"text": "To have seamless interactions with computers, advances in task-oriented deep semantic understanding are of utmost importance.", "labels": [], "entities": [{"text": "task-oriented deep semantic understanding", "start_pos": 58, "end_pos": 99, "type": "TASK", "confidence": 0.7240157574415207}]}, {"text": "The examples include tutoring, dialogue systems and the one described in this paper, a natural language interface to mobile robots.", "labels": [], "entities": []}, {"text": "Compared to more typical text processing tasks on newspapers for which we attempt shallow understandings and broad coverage, for these domains vocabulary is limited and very strong domain knowledge is available.", "labels": [], "entities": []}, {"text": "Despite this, deeper understanding of unrestricted natural language instructions poses areal challenge, due to the incredibly rich structures and creative expressions that people use.", "labels": [], "entities": []}, {"text": "For example, \"Just head straight through the hallway ignoring the rooms to the left and right of you, but while going straight your going to eventually see a room facing you, which is north, enter it.\"", "labels": [], "entities": []}, {"text": "continue straight past the first three doors until you hit a corner.", "labels": [], "entities": []}, {"text": "On that corner there are two doors, one straight ahead of you and one on the right.", "labels": [], "entities": []}, {"text": "Turn right and enter the room to the right and stop within.\"", "labels": [], "entities": []}, {"text": "These utterances are taken from an office navigation corpus collected from undergrad volunteers at SUNY/Albany.", "labels": [], "entities": []}, {"text": "There is a good deal of variety.", "labels": [], "entities": []}, {"text": "Previous efforts in this domain include the classic SHRDLU program by, using a simulated robot, and the more ambitious IBL (Instruction-based Learning for Mobile Robots) project () which tried to integrate vision, voice recognition, natural language understanding and robotics.", "labels": [], "entities": [{"text": "IBL", "start_pos": 119, "end_pos": 122, "type": "METRIC", "confidence": 0.8562446236610413}, {"text": "natural language understanding", "start_pos": 233, "end_pos": 263, "type": "TASK", "confidence": 0.7424633105595907}]}, {"text": "This group has yet to publish performance statistics.", "labels": [], "entities": []}, {"text": "In this paper we will focus on the application of machine learning to the understanding of written route instructions, and on testing by following the instructions in a simulated office environment.", "labels": [], "entities": [{"text": "understanding of written route instructions", "start_pos": 74, "end_pos": 117, "type": "TASK", "confidence": 0.7649284720420837}]}], "datasetContent": [{"text": "As noted, we have 427 route instructions, and the average number of steps was 1.86 steps per instruction.", "labels": [], "entities": []}, {"text": "We had 189 cases in which a sentence boundary was found in the middle of a step.", "labels": [], "entities": []}, {"text": "shows how often action steps occurred in the corpus and average length of the segments.", "labels": [], "entities": []}, {"text": "One thing we noticed is that somehow people do not use a short phrase to say the equivalent of \"enter the door straight ahead of you\", as seen by the average length of EDSZ.", "labels": [], "entities": [{"text": "length", "start_pos": 158, "end_pos": 164, "type": "METRIC", "confidence": 0.9627667665481567}, {"text": "EDSZ", "start_pos": 168, "end_pos": 172, "type": "DATASET", "confidence": 0.5081952810287476}]}, {"text": "Also, it is more common to say the equivalent of \"take aright at the end of the hallway\" than that of \"go to the second hallway on the right\", as seen by the count of GHR2 and GHRZ.", "labels": [], "entities": [{"text": "GHR2", "start_pos": 167, "end_pos": 171, "type": "METRIC", "confidence": 0.8020334839820862}, {"text": "GHRZ", "start_pos": 176, "end_pos": 180, "type": "METRIC", "confidence": 0.7448834180831909}]}, {"text": "The distribution is highly skewed; there area lot more GHL1 than GHL2.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Steps found in the dataset", "labels": [], "entities": []}, {"text": " Table 4: Recall, Precision, F-1 and Success Rate", "labels": [], "entities": [{"text": "Recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9966437816619873}, {"text": "Precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9992122650146484}, {"text": "F-1", "start_pos": 29, "end_pos": 32, "type": "METRIC", "confidence": 0.9969183206558228}, {"text": "Success Rate", "start_pos": 37, "end_pos": 49, "type": "METRIC", "confidence": 0.9505810141563416}]}]}