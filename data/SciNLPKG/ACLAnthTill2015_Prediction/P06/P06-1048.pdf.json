{"title": [{"text": "Models for Sentence Compression: A Comparison across Domains, Training Requirements and Evaluation Measures", "labels": [], "entities": [{"text": "Sentence Compression", "start_pos": 11, "end_pos": 31, "type": "TASK", "confidence": 0.9316224157810211}]}], "abstractContent": [{"text": "Sentence compression is the task of producing a summary at the sentence level.", "labels": [], "entities": [{"text": "Sentence compression", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9226698577404022}]}, {"text": "This paper focuses on three aspects of this task which have not received detailed treatment in the literature: training requirements, scalability, and automatic evaluation.", "labels": [], "entities": []}, {"text": "We provide a novel comparison between a supervised constituent-based and an weakly supervised word-based compression algorithm and examine how these models port to different domains (written vs. spoken text).", "labels": [], "entities": []}, {"text": "To achieve this, a human-authored compression corpus has been created and our study highlights potential problems with the automatically gathered compression corpora currently used.", "labels": [], "entities": []}, {"text": "Finally, we assess whether automatic evaluation measures can be used to determine compression quality.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic sentence compression has recently attracted much attention, in part because of its affinity with summarisation.", "labels": [], "entities": [{"text": "Automatic sentence compression", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.6189006467660269}, {"text": "summarisation", "start_pos": 107, "end_pos": 120, "type": "TASK", "confidence": 0.9692730903625488}]}, {"text": "The task can be viewed as producing a summary of a single sentence that retains the most important information while remaining grammatically correct.", "labels": [], "entities": []}, {"text": "An ideal compression algorithm will involve complex text rewriting operations such as word reordering, paraphrasing, substitution, deletion, and insertion.", "labels": [], "entities": []}, {"text": "In default of a more sophisticated compression algorithm, current approaches have simplified the problem to a single rewriting operation, namely word deletion.", "labels": [], "entities": [{"text": "word deletion", "start_pos": 145, "end_pos": 158, "type": "TASK", "confidence": 0.7181892544031143}]}, {"text": "More formally, given an input sentence of words W = w 1 , w 2 , . .", "labels": [], "entities": []}, {"text": ", w n , a compression is formed by dropping any subset of these words.", "labels": [], "entities": []}, {"text": "Viewing the task as word removal reduces the number of possible compressions to 2 n ; naturally, many of these compressions will not be reasonable or grammatical (.", "labels": [], "entities": [{"text": "word removal", "start_pos": 20, "end_pos": 32, "type": "TASK", "confidence": 0.7696021199226379}]}, {"text": "Sentence compression could be usefully employed in wide range of applications.", "labels": [], "entities": [{"text": "Sentence compression", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9317786693572998}]}, {"text": "For example, to automatically generate subtitles for television programs; the transcripts cannot usually be used verbatim due to the rate of speech being too high).", "labels": [], "entities": []}, {"text": "Other applications include compressing text to be displayed on small screens (Corston-Oliver 2001) such as mobile phones or PDAs, and producing audio scanning devices for the blind).", "labels": [], "entities": []}, {"text": "Algorithms for sentence compression fall into two broad classes depending on their training requirements.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 15, "end_pos": 35, "type": "TASK", "confidence": 0.7763057053089142}]}, {"text": "Many algorithms exploit parallel corpora) to learn the correspondences between long and short sentences in a supervised manner, typically using a rich feature space induced from parse trees.", "labels": [], "entities": []}, {"text": "The learnt rules effectively describe which constituents should be deleted in a given context.", "labels": [], "entities": []}, {"text": "Approaches that do not employ parallel corpora require minimal or no supervision.", "labels": [], "entities": []}, {"text": "They operationalise compression in terms of word deletion without learning specific rules and can therefore rely on little linguistic knowledge such as part-of-speech tags or merely the lexical items alone.", "labels": [], "entities": [{"text": "operationalise compression", "start_pos": 5, "end_pos": 31, "type": "TASK", "confidence": 0.8049123883247375}, {"text": "word deletion", "start_pos": 44, "end_pos": 57, "type": "TASK", "confidence": 0.6917637288570404}]}, {"text": "Alternatively, the rules of compression are approximated from a non-parallel corpus (e.g., the Penn Treebank) by considering context-free grammar derivations with matching expansions.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 95, "end_pos": 108, "type": "DATASET", "confidence": 0.9897494316101074}]}, {"text": "Previous approaches have been developed and tested almost exclusively on written text, a notable exception being who focus on spoken language.", "labels": [], "entities": []}, {"text": "While parallel corpora of original-compressed sentences are not naturally available in the way multilingual corpora are, researchers have obtained such corpora automatically by exploiting documents accompanied by abstracts.", "labels": [], "entities": []}, {"text": "Automatic corpus creation affords the opportunity to study compression mechanisms cheaply, yet these mechanisms may not be representative of human performance.", "labels": [], "entities": [{"text": "corpus creation", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.7030192762613297}]}, {"text": "It is unlikely that authors routinely carryout sentence compression while creating abstracts for their articles.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 47, "end_pos": 67, "type": "TASK", "confidence": 0.7370410859584808}]}, {"text": "Collecting human judgements is the method of choice for evaluating sentence compression models.", "labels": [], "entities": [{"text": "Collecting human judgements", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8437626957893372}, {"text": "sentence compression", "start_pos": 67, "end_pos": 87, "type": "TASK", "confidence": 0.6963571310043335}]}, {"text": "However, human evaluations tend to be expensive and cannot be repeated frequently; furthermore, comparisons across different studies can be difficult, particularly if subjects employ different scales, or are given different instructions.", "labels": [], "entities": []}, {"text": "In this paper we examine some aspects of the sentence compression task that have received little attention in the literature.", "labels": [], "entities": [{"text": "sentence compression task", "start_pos": 45, "end_pos": 70, "type": "TASK", "confidence": 0.8588987191518148}]}, {"text": "First, we provide a novel comparison of supervised and weakly supervised approaches.", "labels": [], "entities": []}, {"text": "Specifically, we study how constituent-based and word-based methods port to different domains and show that the latter tend to be more robust.", "labels": [], "entities": []}, {"text": "Second, we create a corpus of human-authored compressions, and discuss some potential problems with currently used compression corpora.", "labels": [], "entities": []}, {"text": "Finally, we present automatic evaluation measures for sentence compression and examine whether they correlate reliably with behavioural data.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 54, "end_pos": 74, "type": "TASK", "confidence": 0.7578588128089905}]}], "datasetContent": [{"text": "Previous studies relied almost exclusively on human judgements for assessing the wellformedness of automatically derived compressions.", "labels": [], "entities": []}, {"text": "Although human evaluations of compression systems are not as large-scale as in other fields (e.g., machine translation), they are typically performed once, at the end of the development cycle.", "labels": [], "entities": [{"text": "machine translation)", "start_pos": 99, "end_pos": 119, "type": "TASK", "confidence": 0.832231859366099}]}, {"text": "Automatic evaluation measures would allow more extensive parameter tuning and crucially experimentation with larger data sets.", "labels": [], "entities": [{"text": "parameter tuning", "start_pos": 57, "end_pos": 73, "type": "TASK", "confidence": 0.7086695432662964}]}, {"text": "Most human studies to date are conducted on a small compression sample, the test portion of the Ziff-Davis corpus (32 sentences).", "labels": [], "entities": []}, {"text": "Larger sample sizes would expectedly render human evaluations time consuming and generally more difficult to conduct frequently.", "labels": [], "entities": []}, {"text": "Here, we review two automatic evaluation measures that hold promise for the compression task.", "labels": [], "entities": []}, {"text": "Simple String Accuracy (SSA, Bangalore et al. 2000) has been proposed as a baseline evaluation metric for natural language generation.", "labels": [], "entities": [{"text": "Simple String Accuracy (SSA, Bangalore et al. 2000", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.6321665316820144}, {"text": "natural language generation", "start_pos": 106, "end_pos": 133, "type": "TASK", "confidence": 0.6520027915636698}]}, {"text": "It is based on the string edit distance between the generated output and a gold standard.", "labels": [], "entities": []}, {"text": "It is a measure of the number of insertion (I), deletion (D) and substitution (S) errors between two strings.", "labels": [], "entities": [{"text": "number of insertion (I), deletion (D) and substitution (S) errors", "start_pos": 23, "end_pos": 88, "type": "METRIC", "confidence": 0.8422857733333812}]}, {"text": "It is defined in (4) where R is the length of the gold standard string.", "labels": [], "entities": [{"text": "gold standard string", "start_pos": 50, "end_pos": 70, "type": "DATASET", "confidence": 0.8511361877123514}]}, {"text": "Simple String Accuracy The SSA score will assess whether appropriate words have been included in the compression.", "labels": [], "entities": []}, {"text": "Another stricter automatic evaluation method is to compare the grammatical relations found in the system compressions against those found in a gold standard.", "labels": [], "entities": []}, {"text": "This allows us \"to measure the semantic aspects of summarisation quality in terms of grammatical-functional information\" ().", "labels": [], "entities": [{"text": "summarisation", "start_pos": 51, "end_pos": 64, "type": "TASK", "confidence": 0.975368857383728}]}, {"text": "The standard metrics of precision, recall and F-score can then be used to measure the quality of a system against a gold standard.", "labels": [], "entities": [{"text": "precision", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9996627569198608}, {"text": "recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9996222257614136}, {"text": "F-score", "start_pos": 46, "end_pos": 53, "type": "METRIC", "confidence": 0.9987483024597168}]}, {"text": "Our implementation of the F-score measure used the grammatical relations annotations provided by RASP (.", "labels": [], "entities": [{"text": "F-score", "start_pos": 26, "end_pos": 33, "type": "METRIC", "confidence": 0.9732416272163391}, {"text": "RASP", "start_pos": 97, "end_pos": 101, "type": "DATASET", "confidence": 0.8807238936424255}]}, {"text": "This parser is particularly appropriate for the compression task since it provides parses for both full sentences and sentence fragments and is generally robust enough to analyse semi-grammatical compressions.", "labels": [], "entities": [{"text": "analyse semi-grammatical compressions", "start_pos": 171, "end_pos": 208, "type": "TASK", "confidence": 0.6186199684937795}]}, {"text": "We calculated F-score overall the relations provided by RASP (e.g., subject, direct/indirect object, modifier; 15 in total).", "labels": [], "entities": [{"text": "F-score", "start_pos": 14, "end_pos": 21, "type": "METRIC", "confidence": 0.9962748289108276}, {"text": "RASP", "start_pos": 56, "end_pos": 60, "type": "TASK", "confidence": 0.5967596769332886}]}, {"text": "Correlation with human judgements is an important prerequisite for the wider use of automatic evaluation measures.", "labels": [], "entities": []}, {"text": "In the following section we describe an evaluation study examining whether the measures just presented indeed correlate with human ratings of compression quality.", "labels": [], "entities": []}, {"text": "In this section we present our experimental setup for assessing the performance of the two algorithms discussed above.", "labels": [], "entities": []}, {"text": "We explain how different model parameters were estimated.", "labels": [], "entities": []}, {"text": "We also describe a judgement elicitation study on automatic and human-authored compressions.", "labels": [], "entities": []}, {"text": "We randomly selected 40 sentences for evaluation purposes, 20 from the testing portion of the Ziff-Davis corpus (32 sentences) and 20 sentences from the Broadcast News corpus (133 sentences were set aside for testing).", "labels": [], "entities": [{"text": "Broadcast News corpus", "start_pos": 153, "end_pos": 174, "type": "DATASET", "confidence": 0.93435138463974}]}, {"text": "This is comparable to previous studies which have used the 32 test sentences from the Ziff-Davis corpus.", "labels": [], "entities": []}, {"text": "None of the 20 Broadcast News sentences were used for optimisation.", "labels": [], "entities": [{"text": "Broadcast News sentences", "start_pos": 15, "end_pos": 39, "type": "DATASET", "confidence": 0.9073201219240824}]}, {"text": "We ran the decision-tree system and the word-based system on these 40 sentences.", "labels": [], "entities": []}, {"text": "One annotator was randomly selected to act as the gold standard for the Broadcast News corpus; the gold standard for the Ziff-Davis corpus was the sentence that occurred in the abstract.", "labels": [], "entities": [{"text": "Broadcast News corpus", "start_pos": 72, "end_pos": 93, "type": "DATASET", "confidence": 0.9483315547307333}]}, {"text": "For each original sentence we had three compressions; two generated automatically by our systems and a human authored gold standard.", "labels": [], "entities": []}, {"text": "Thus, the total number of compressions was 120 (3x40).", "labels": [], "entities": []}, {"text": "The 120 compressions were rated by human subjects.", "labels": [], "entities": [{"text": "compressions", "start_pos": 8, "end_pos": 20, "type": "METRIC", "confidence": 0.9704838395118713}]}, {"text": "Their judgements were also used to examine whether the automatic evaluation measures discussed in Section 4 correlate reliably with behavioural data.", "labels": [], "entities": []}, {"text": "Sixty unpaid volunteers participated in our elicitation study, all were self reported native English speakers.", "labels": [], "entities": []}, {"text": "The study was conducted remotely over the Internet.", "labels": [], "entities": []}, {"text": "Participants were presented with a set of instructions that explained the task and defined sentence compression with the aid of examples.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 91, "end_pos": 111, "type": "TASK", "confidence": 0.7324094176292419}]}, {"text": "They first read the original sentence with the compression hidden.", "labels": [], "entities": []}, {"text": "Then the compression was revealed by pressing a button.", "labels": [], "entities": []}, {"text": "Each participant saw 40 compressions.", "labels": [], "entities": []}, {"text": "A Latin square design prevented subjects from seeing two different compressions of the same sentence.", "labels": [], "entities": []}, {"text": "The order of the sentences was randomised.", "labels": [], "entities": []}, {"text": "Participants were asked to rate each compression they saw on a five point scale taking into account the information retained by the compression and its grammaticality.", "labels": [], "entities": []}, {"text": "They were told all  compressions were automatically generated.", "labels": [], "entities": []}, {"text": "Examples of the compressions our participants saw are given in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Compression Rates (Comp% measures  the percentage of sentences compressed; CompR  is the mean compression rate of all sentences)", "labels": [], "entities": [{"text": "CompR", "start_pos": 85, "end_pos": 90, "type": "METRIC", "confidence": 0.9617453217506409}]}, {"text": " Table 4: Results using automatic evaluation mea- sures", "labels": [], "entities": [{"text": "automatic evaluation mea- sures", "start_pos": 24, "end_pos": 55, "type": "METRIC", "confidence": 0.5426169157028198}]}, {"text": " Table 5: Mean ratings from human evaluation", "labels": [], "entities": [{"text": "Mean", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9926367998123169}]}]}