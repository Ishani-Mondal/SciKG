{"title": [], "abstractContent": [{"text": "We present BAYESUM (for \"Bayesian summarization\"), a model for sentence extraction in query-focused summarization.", "labels": [], "entities": [{"text": "BAYESUM", "start_pos": 11, "end_pos": 18, "type": "METRIC", "confidence": 0.9774718880653381}, {"text": "sentence extraction", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.7471522986888885}]}, {"text": "BAYESUM leverages the common casein which multiple documents are relevant to a single query.", "labels": [], "entities": [{"text": "BAYESUM", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8179755210876465}]}, {"text": "Using these documents as reinforcement for query terms, BAYESUM is not afflicted by the paucity of information in short queries.", "labels": [], "entities": []}, {"text": "We show that approximate inference in BAYESUM is possible on large data sets and results in a state-of-the-art summarization system.", "labels": [], "entities": [{"text": "BAYESUM", "start_pos": 38, "end_pos": 45, "type": "DATASET", "confidence": 0.6346868872642517}]}, {"text": "Furthermore , we show how BAYESUM can be understood as a justified query expansion technique in the language modeling for IR framework.", "labels": [], "entities": [{"text": "BAYESUM", "start_pos": 26, "end_pos": 33, "type": "DATASET", "confidence": 0.5018793344497681}]}], "introductionContent": [{"text": "We describe BAYESUM, an algorithm for performing query-focused summarization in the common case that there are many relevant documents fora given query.", "labels": [], "entities": [{"text": "BAYESUM", "start_pos": 12, "end_pos": 19, "type": "METRIC", "confidence": 0.8571863174438477}, {"text": "query-focused summarization", "start_pos": 49, "end_pos": 76, "type": "TASK", "confidence": 0.5694697499275208}]}, {"text": "Given a query and a collection of relevant documents, our algorithm functions by asking itself the following question: what is it about these relevant documents that differentiates them from the non-relevant documents?", "labels": [], "entities": []}, {"text": "BAYESUM can be seen as providing a statistical formulation of this exact question.", "labels": [], "entities": [{"text": "BAYESUM", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9149002432823181}]}, {"text": "The key requirement of BAYESUM is that multiple relevant documents are known for the query in question.", "labels": [], "entities": [{"text": "BAYESUM", "start_pos": 23, "end_pos": 30, "type": "METRIC", "confidence": 0.5660285949707031}]}, {"text": "This is not a severe limitation.", "labels": [], "entities": []}, {"text": "In two well-studied problems, it is the de-facto standard.", "labels": [], "entities": []}, {"text": "In standard multidocument summarization (with or without a query), we have access to known relevant documents for some user need.", "labels": [], "entities": [{"text": "multidocument summarization", "start_pos": 12, "end_pos": 39, "type": "TASK", "confidence": 0.6408858597278595}]}, {"text": "Similarly, in the case of a web-search application, an underlying IR engine will retrieve multiple (presumably) relevant documents fora given query.", "labels": [], "entities": []}, {"text": "For both of these tasks, BAYESUM performs well, even when the underlying retrieval model is noisy.", "labels": [], "entities": [{"text": "BAYESUM", "start_pos": 25, "end_pos": 32, "type": "METRIC", "confidence": 0.788942813873291}]}, {"text": "The idea of leveraging known relevant documents is known as query expansion in the information retrieval community, where it has been shown to be successful in ad hoc retrieval tasks.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 60, "end_pos": 75, "type": "TASK", "confidence": 0.844567060470581}, {"text": "information retrieval community", "start_pos": 83, "end_pos": 114, "type": "TASK", "confidence": 0.7862737973531088}]}, {"text": "Viewed from the perspective of IR, our work can be interpreted in two ways.", "labels": [], "entities": [{"text": "IR", "start_pos": 31, "end_pos": 33, "type": "TASK", "confidence": 0.9011279344558716}]}, {"text": "First, it can be seen as an application of query expansion to the summarization task (or, in IR terminology, passage retrieval); see ().", "labels": [], "entities": [{"text": "query expansion", "start_pos": 43, "end_pos": 58, "type": "TASK", "confidence": 0.7284368127584457}, {"text": "summarization task", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.9116671085357666}, {"text": "passage retrieval", "start_pos": 109, "end_pos": 126, "type": "TASK", "confidence": 0.7015713900327682}]}, {"text": "Second, and more importantly, it can be seen as a method for query expansion in a non-ad-hoc manner.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 61, "end_pos": 76, "type": "TASK", "confidence": 0.8563953936100006}]}, {"text": "That is, BAYESUM is a statistically justified query expansion method in the language modeling for IR framework).", "labels": [], "entities": [{"text": "BAYESUM", "start_pos": 9, "end_pos": 16, "type": "METRIC", "confidence": 0.924534261226654}]}], "datasetContent": [{"text": "The first experiments we run are for query-focused single document summarization, where relevant documents are returned from a search engine, and a short summary is desired of each document.", "labels": [], "entities": [{"text": "single document summarization", "start_pos": 51, "end_pos": 80, "type": "TASK", "confidence": 0.5815096199512482}]}, {"text": "Since there are differing numbers of sentences selected per document by the human judges, one cannot compute precision and recall; instead, we opt for other standard IR performance measures.", "labels": [], "entities": [{"text": "precision", "start_pos": 109, "end_pos": 118, "type": "METRIC", "confidence": 0.9993353486061096}, {"text": "recall", "start_pos": 123, "end_pos": 129, "type": "METRIC", "confidence": 0.9993089437484741}, {"text": "IR", "start_pos": 166, "end_pos": 168, "type": "TASK", "confidence": 0.9485952258110046}]}, {"text": "We consider three related criteria: mean average precision (MAP), mean reciprocal rank (MRR) and precision at 2 (P@2).", "labels": [], "entities": [{"text": "mean average precision (MAP)", "start_pos": 36, "end_pos": 64, "type": "METRIC", "confidence": 0.9108087221781412}, {"text": "mean reciprocal rank (MRR)", "start_pos": 66, "end_pos": 92, "type": "METRIC", "confidence": 0.9303918480873108}, {"text": "precision", "start_pos": 97, "end_pos": 106, "type": "METRIC", "confidence": 0.9990359544754028}]}, {"text": "MAP is computed by calculating precision at every sentence as ordered by the system up until all relevant sentences are selected and averaged.", "labels": [], "entities": [{"text": "MAP", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.4830020070075989}, {"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9991946816444397}]}, {"text": "MRR is the reciprocal of the rank of the first relevant sentence.", "labels": [], "entities": [{"text": "MRR", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.8867391347885132}]}, {"text": "P@2 is the precision computed at the first point that two relevant sentences have been selected (in the rare case that humans selected only one sentence, we use P@1).", "labels": [], "entities": [{"text": "P@2", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9527172048886617}, {"text": "precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9980080723762512}]}, {"text": "We present two results using BAYESUM in the multidocument summarization settings, based on the official results from the Multilingual Summarization Evaluation (MSE) and Document Understanding Conference (DUC) competitions in 2005.", "labels": [], "entities": [{"text": "BAYESUM", "start_pos": 29, "end_pos": 36, "type": "DATASET", "confidence": 0.5349443554878235}, {"text": "Multilingual Summarization Evaluation (MSE) and Document Understanding Conference (DUC)", "start_pos": 121, "end_pos": 208, "type": "TASK", "confidence": 0.8118983186208285}]}], "tableCaptions": [{"text": " Table 1: Empirical results for the baseline models  as well as BAYESUM, when all query fields are  used.", "labels": [], "entities": [{"text": "Empirical", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9843050837516785}, {"text": "BAYESUM", "start_pos": 64, "end_pos": 71, "type": "METRIC", "confidence": 0.8513092398643494}]}, {"text": " Table 2: Empirical results for the position-based  model, the KL-based models and BAYESUM, with  different inputs.", "labels": [], "entities": [{"text": "Empirical", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9608486890792847}, {"text": "BAYESUM", "start_pos": 83, "end_pos": 90, "type": "METRIC", "confidence": 0.9552741646766663}]}]}