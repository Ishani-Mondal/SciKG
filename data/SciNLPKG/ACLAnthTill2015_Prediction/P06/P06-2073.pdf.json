{"title": [{"text": "Segmented and unsegmented dialogue-act annotation with statistical dialogue models *", "labels": [], "entities": []}], "abstractContent": [{"text": "Dialogue systems are one of the most challenging applications of Natural Language Processing.", "labels": [], "entities": []}, {"text": "In recent years, some statistical dialogue models have been proposed to cope with the dialogue problem.", "labels": [], "entities": []}, {"text": "The evaluation of these models is usually performed by using them as annotation models.", "labels": [], "entities": []}, {"text": "Many of the works on annotation use information such as the complete sequence of dialogue turns or the correct segmentation of the dialogue.", "labels": [], "entities": []}, {"text": "This information is not usually available for dialogue systems.", "labels": [], "entities": []}, {"text": "In this work, we propose a statistical model that uses only the information that is usually available and performs the segmentation and annotation at the same time.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 119, "end_pos": 131, "type": "TASK", "confidence": 0.9620155096054077}]}, {"text": "The results of this model reveal the great influence that the availability of a correct segmentation has in obtaining an accurate annotation of the dialogues .", "labels": [], "entities": []}], "introductionContent": [{"text": "In the Natural Language Processing (NLP) field, one of the most challenging applications is dialogue systems.", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 7, "end_pos": 40, "type": "TASK", "confidence": 0.726783295472463}]}, {"text": "A dialogue system is usually defined as a computer system that can interact with a human being through dialogue in order to complete a specific task (e.g., ticket reservation, timetable consultation, bank operations,.", "labels": [], "entities": [{"text": "ticket reservation, timetable consultation", "start_pos": 156, "end_pos": 198, "type": "TASK", "confidence": 0.6590107321739197}]}, {"text": "Most dialogue system have a characteristic behaviour with respect to dialogue * Work partially supported by the Spanish project TIC2003-08681-C02-02 and by Spanish Ministry of Culture under FPI grants.", "labels": [], "entities": []}, {"text": "management, which is known as dialogue strategy.", "labels": [], "entities": [{"text": "dialogue strategy", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.8390310704708099}]}, {"text": "It defines what the dialogue system must do at each point of the dialogue.", "labels": [], "entities": []}, {"text": "Most of these strategies are rule-based, i.e., the dialogue strategy is defined by rules that are usually defined by a human expert (.", "labels": [], "entities": []}, {"text": "This approach is usually difficult to adapt or extend to new domains where the dialogue structure could be completely different, and it requires the definition of new rules.", "labels": [], "entities": []}, {"text": "Similar to other NLP problems (like speech recognition and understanding, or statistical machine translation), an alternative data-based approach has been developed in the last decade).", "labels": [], "entities": [{"text": "speech recognition and understanding", "start_pos": 36, "end_pos": 72, "type": "TASK", "confidence": 0.8795893341302872}, {"text": "statistical machine translation", "start_pos": 77, "end_pos": 108, "type": "TASK", "confidence": 0.6523925364017487}]}, {"text": "This approach relies on statistical models that can be automatically estimated from annotated data, which in this case, are dialogues from the task.", "labels": [], "entities": []}, {"text": "Statistical modelling learns the appropriate parameters of the models from the annotated dialogues.", "labels": [], "entities": [{"text": "Statistical modelling", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8446575999259949}]}, {"text": "As a simplification, it could be considered that each label is associated to a situation in the dialogue, and the models learn how to identify and react to the different situations by estimating the associations between the labels and the dialogue events (words, the speaker, previous turns, etc.).", "labels": [], "entities": []}, {"text": "An appropriate annotation scheme should be defined to capture the elements that are really important for the dialogue, eliminating the information that is irrelevant to the dialogue process.", "labels": [], "entities": []}, {"text": "Several annotation schemes have been proposed in the last few years).", "labels": [], "entities": []}, {"text": "One of the most popular annotation schemes at the dialogue level is based on Dialogue Acts (DA).", "labels": [], "entities": []}, {"text": "A DA is a label that defines the function of the annotated utterance with respect to the dialogue process.", "labels": [], "entities": []}, {"text": "In other words, every turn in the dialogue is supposed to be composed of one or more utterances.", "labels": [], "entities": []}, {"text": "In this context, from the dialogue management viewpoint an utterance is a relevant subsequence . Several DA annotation schemes have been proposed in recent years (DAMSL), VerbMobil (),).", "labels": [], "entities": [{"text": "VerbMobil", "start_pos": 171, "end_pos": 180, "type": "DATASET", "confidence": 0.8508645296096802}]}, {"text": "In all these studies, it is necessary to annotate a large amount of dialogues to estimate the parameters of the statistical models.", "labels": [], "entities": []}, {"text": "Manual annotation is the usual solution, although is very timeconsuming and there is a tendency for error (the annotation instructions are not usually easy to interpret and apply, and human annotators can commit errors) (.", "labels": [], "entities": []}, {"text": "Therefore, the possibility of applying statistical models to the annotation problem is really interesting.", "labels": [], "entities": []}, {"text": "Moreover, it gives the possibility of evaluating the statistical models.", "labels": [], "entities": []}, {"text": "The evaluation of the performance of dialogue strategies models is a difficult task.", "labels": [], "entities": []}, {"text": "Although many proposals have been made (), there is no real agreement in the NLP community about the evaluation technique to apply.", "labels": [], "entities": []}, {"text": "Our main aim is the evaluation of strategy models, which provide the reaction of the system given a user input and a dialogue history.", "labels": [], "entities": []}, {"text": "Using these models as annotation models gives us a possible evaluation: the correct recognition of the labels implies the correct recognition of the dialogue situation; consequently this information can help the system to react appropriately.", "labels": [], "entities": []}, {"text": "Many recent works have attempted this approach ().", "labels": [], "entities": []}, {"text": "However, many of these works are based on the hypothesis of the availability of the segmentation into utterances of the turns of the dialogue.", "labels": [], "entities": []}, {"text": "This is an important drawback in order to evaluate these models as strategy models, where segmentation is usually not available.", "labels": [], "entities": []}, {"text": "Other works rely on a decoupled scheme of segmentation and DA classification (.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 42, "end_pos": 54, "type": "TASK", "confidence": 0.972118079662323}, {"text": "DA classification", "start_pos": 59, "end_pos": 76, "type": "TASK", "confidence": 0.8206413388252258}]}, {"text": "In this paper, we present anew statistical model that computes the segmentation and the annotation of the turns at the same time, using a statistical framework that is simpler than the models that have been proposed to solve both problems at the same time ().", "labels": [], "entities": []}, {"text": "The results demonstrate that segmentation accuracy is really important in obtaining an accurate annotation of the dialogue, and consequently in obtaining quality strategy models.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 29, "end_pos": 41, "type": "TASK", "confidence": 0.9729034304618835}, {"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.8969999551773071}]}, {"text": "Therefore, more accurate segmentation models are needed to perform this process efficiently.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 25, "end_pos": 37, "type": "TASK", "confidence": 0.9683024287223816}]}, {"text": "This paper is organised as follows: Section 2, presents the annotation models (for both the unsegmented and segmented versions); Section 3, describes the dialogue corpora used in the experiments; Section 4 establishes the experimental framework and presents a summary of the results; Section 5, presents our conclusions and future research directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "Two corpora with very different features were used in the experiment with the models proposed in Section 2.", "labels": [], "entities": []}, {"text": "The SwitchBoard corpus is composed of human-human, non task-oriented dialogues with a large vocabulary.", "labels": [], "entities": []}, {"text": "The Dihana corpus is composed of human-computer, task-oriented dialogues with a small vocabulary.", "labels": [], "entities": [{"text": "Dihana corpus", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.7286086529493332}]}, {"text": "Although two corpora are not enough to let us draw general conclusions, they give us more reliable results than using only one corpus.", "labels": [], "entities": []}, {"text": "Moreover, the very different nature of both corpora makes our conclusions more independent from the corpus type, the annotation scheme, the vocabulary size, etc.", "labels": [], "entities": []}, {"text": "The SwitchBoard database was processed to remove certain particularities.", "labels": [], "entities": [{"text": "SwitchBoard database", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.8140569627285004}]}, {"text": "The main adaptations performed were: \u2022 The interrupted utterances (which were labelled with '+') were joined to the correct previous utterance, thereby avoiding interruptions (i.e., all the words of the interrupted utterance were annotated with the same DA).", "labels": [], "entities": [{"text": "DA", "start_pos": 254, "end_pos": 256, "type": "METRIC", "confidence": 0.9027827978134155}]}, {"text": "\u2022 All the words were transcribed in lowercase.", "labels": [], "entities": []}, {"text": "\u2022 Puntuaction marks were separated from words.", "labels": [], "entities": []}, {"text": "The experiments were performed using a crossvalidation approach to avoid the statistical bias that can be introduced by the election of fixed training and test partitions.", "labels": [], "entities": []}, {"text": "This cross-validation approach has also been adopted in other recent works on this corpus ().", "labels": [], "entities": []}, {"text": "In our case, we performed 10 different experiments.", "labels": [], "entities": []}, {"text": "In each experiment, the training partition was composed of 1,136 dialogues, and the test partition was composed of 19 dialogues.", "labels": [], "entities": []}, {"text": "This proportion was adopted so that our results could be compared with the results in (), where similar training and test sizes were used.", "labels": [], "entities": []}, {"text": "The mean figures for the training and test partitions are shown in.", "labels": [], "entities": []}, {"text": "With respect to the Dihana database, the preprocessing included the following points: \u2022 A categorisation process was performed for categories such as town names, the time, dates, train types, etc.", "labels": [], "entities": [{"text": "Dihana database", "start_pos": 20, "end_pos": 35, "type": "DATASET", "confidence": 0.9337075054645538}, {"text": "preprocessing", "start_pos": 41, "end_pos": 54, "type": "METRIC", "confidence": 0.957665205001831}]}, {"text": "\u2022 All the words were transcribed in lowercase.", "labels": [], "entities": []}, {"text": "\u2022 Puntuaction marks were separated from words.", "labels": [], "entities": []}, {"text": "\u2022 All the words were preceded by the speaker identification (U for user, M for system).", "labels": [], "entities": []}, {"text": "A cross-validation approach was adopted in Dihana as well.", "labels": [], "entities": []}, {"text": "In this case, only 5 different partitions were used.", "labels": [], "entities": []}, {"text": "Each of them had 720 dialogues for training and 180 for testing.", "labels": [], "entities": []}, {"text": "The statistics on the Dihana corpus are presented in For both corpora, different N-gram models, with N = 2, 3, 4, and HMM of one state were trained from the training database.", "labels": [], "entities": [{"text": "Dihana corpus", "start_pos": 22, "end_pos": 35, "type": "DATASET", "confidence": 0.7913876473903656}, {"text": "HMM", "start_pos": 118, "end_pos": 121, "type": "METRIC", "confidence": 0.9404748678207397}]}, {"text": "In the case of the SwitchBoard database, all the turns in the test set were used to compute the labelling accuracy.", "labels": [], "entities": [{"text": "SwitchBoard database", "start_pos": 19, "end_pos": 39, "type": "DATASET", "confidence": 0.8389047384262085}, {"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.955292284488678}]}, {"text": "However, for the Dihana database, only the user turns were taken into account (because system turns follow a regular, template-based scheme, which presents artificially high labelling accuracies).", "labels": [], "entities": [{"text": "Dihana database", "start_pos": 17, "end_pos": 32, "type": "DATASET", "confidence": 0.8900276720523834}]}, {"text": "Furthermore, in order to use a really significant set of labels in the Dihana corpus, we performed the experiments using only two-level labels instead of the complete three-level labels.", "labels": [], "entities": [{"text": "Dihana corpus", "start_pos": 71, "end_pos": 84, "type": "DATASET", "confidence": 0.8776876926422119}]}, {"text": "This restriction allowed us to be more independent from the understanding issues, which are strongly related to the third level.", "labels": [], "entities": []}, {"text": "It also allowed us to concentrate on the dialogue issues, which relate more to the first and second levels.", "labels": [], "entities": []}, {"text": "The results in the case of the segmented approach described in Section 2 for SwitchBoard are presented in.", "labels": [], "entities": []}, {"text": "Two different definitions of accuracy were used to assess the results: \u2022 Utterance accuracy: computes the proportion of well-labelled utterances.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9991734623908997}, {"text": "Utterance accuracy", "start_pos": 73, "end_pos": 91, "type": "METRIC", "confidence": 0.7470748722553253}]}, {"text": "\u2022 Turn accuracy: computes the proportion of totally well-labelled turns (i.e.: if the labelling has the same labels in the same order as in the reference, it is taken as a welllabelled turn).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 7, "end_pos": 15, "type": "METRIC", "confidence": 0.771586000919342}]}, {"text": "As expected, the utterance accuracy results area bit worse than those presented in ().", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9928656816482544}]}, {"text": "This maybe due to the use of only the past history and possibly to the cross-validation approach used in the experiments.", "labels": [], "entities": []}, {"text": "The turn accuracy was calculated to compare the segmented and the unsegmented models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9493290185928345}]}, {"text": "This was necessary because the utterance accuracy does not make sense for the unsegmented model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9759682416915894}]}, {"text": "The results for the unsegmented approach for SwitchBoard are presented in.", "labels": [], "entities": []}, {"text": "In this case, three different definitions of accuracy were used to assess the results: \u2022 Accuracy at DA level: the edit distance between the reference and the labelling of the turn was computed; then, the number of correct substitutions (c), wrong substitutions (s), deletions (d) and insertions (i) was com- puted, and the accuracy was calculated as 100 \u00b7 c (c+s+i+d) . \u2022 Accuracy at turn level: this provides the proportion of well-labelled turns, without taking into account the segmentation (i.e., if the labelling has the same labels in the same order as in the reference, it is taken as a welllabelled turn).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.998993456363678}, {"text": "Accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9983296990394592}, {"text": "accuracy", "start_pos": 324, "end_pos": 332, "type": "METRIC", "confidence": 0.9993299245834351}, {"text": "Accuracy", "start_pos": 373, "end_pos": 381, "type": "METRIC", "confidence": 0.9977044463157654}]}, {"text": "\u2022 Accuracy at segmentation level: this provides the proportion of well-labelled and segmented turns (i.e., the labels are the same as in the reference and they affect the same utterances).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 2, "end_pos": 10, "type": "METRIC", "confidence": 0.9972212314605713}]}, {"text": "The WIP parameter used in was 50, which is the one that offered the best results.", "labels": [], "entities": [{"text": "WIP", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.6004778742790222}]}, {"text": "The segmentation accuracy in must be compared with the turn accuracy in shows, the accuracy of the labelling decreased dramatically.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 4, "end_pos": 16, "type": "TASK", "confidence": 0.9318552017211914}, {"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.5997318625450134}, {"text": "turn accuracy", "start_pos": 55, "end_pos": 68, "type": "METRIC", "confidence": 0.7936171293258667}, {"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9990376234054565}]}, {"text": "This reveals the strong influence of the availability of the real segmentation of the turns.", "labels": [], "entities": []}, {"text": "To confirm this hypothesis, similar experiments were performed with the Dihana database.", "labels": [], "entities": [{"text": "Dihana database", "start_pos": 72, "end_pos": 87, "type": "DATASET", "confidence": 0.9366922676563263}]}, {"text": "Table 5 presents the results with the segmented corpus, and presents the results with the unsegmented corpus (with WIP=50, which gave the best results).", "labels": [], "entities": [{"text": "WIP", "start_pos": 115, "end_pos": 118, "type": "METRIC", "confidence": 0.9968583583831787}]}, {"text": "In this case, only user turns were taken into account to compute the accuracy, although the model was applied to all the turns (both user and system turns).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9995866417884827}]}, {"text": "For the Dihana corpus, the degradation of the results of the unsegmented approach with respect to the segmented approach was not as high as in the SwitchBoard corpus, due to the smaller vocabulary and complexity of the dialogues.", "labels": [], "entities": [{"text": "Dihana corpus", "start_pos": 8, "end_pos": 21, "type": "DATASET", "confidence": 0.826745331287384}]}, {"text": "These results led us to the same conclusion, even for such a different corpus (much more labels, task-oriented, etc.).", "labels": [], "entities": []}, {"text": "In any case, these accuracy figures must betaken as a lower bound on the model performance because sometimes an incorrect recognition of segment boundaries or dialogue acts does not cause an inappropriate reaction of the dialogue strategy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9988242983818054}]}, {"text": "An illustrative example of annotation errors in the SwitchBoard database, is presented in for the same turns as in.", "labels": [], "entities": [{"text": "SwitchBoard database", "start_pos": 52, "end_pos": 72, "type": "DATASET", "confidence": 0.9272088706493378}]}, {"text": "An error analysis of the segmented model was performed.", "labels": [], "entities": []}, {"text": "The results reveals that, in the case of most of the errors were produced by the confusion of the 'sv' and 'sd' classes (about 50% of the times 'sv' was badly labelled, the wrong label was 'sd') The second turn in is an example of this type of error.", "labels": [], "entities": []}, {"text": "The confusions between the 'aa' and 'b' classes were also significant (about 27% of the times 'aa' was badly labelled, the wrong label was 'b').", "labels": [], "entities": []}, {"text": "This was reasonable due to the similar definitions of these classes (which makes the annotation difficult, even for human experts).", "labels": [], "entities": []}, {"text": "These errors were similar for all the N-grams used.", "labels": [], "entities": []}, {"text": "In the case of the unsegmented model, most of the errors were produced by deletions of the 'sd' and 'sv' classes, as in the first turn in (about 50% of the errors).", "labels": [], "entities": []}, {"text": "This can be explained by the presence of very short and very long utterances in both classes (i.e., utterances for 'sd' and 'sv' did not present a regular length).", "labels": [], "entities": []}, {"text": "Some examples of errors in the Dihana corpus are shown in (in this case, for the same turns as those presented in).", "labels": [], "entities": [{"text": "Dihana corpus", "start_pos": 31, "end_pos": 44, "type": "DATASET", "confidence": 0.8732489049434662}]}, {"text": "In the segmented model, most of the errors were substitutions between labels with the same first level (especially questions and answers) where the second level was difficult to recognise.", "labels": [], "entities": []}, {"text": "The first and third turn in are examples of this type of error.", "labels": [], "entities": []}, {"text": "This was because sometimes the expressions only differed with each other by one word, or Utt Label 1 % Yeah, to get references and that, so, but, uh, I don't 2 sd feel comfortable about leaving my kids in a big daycare center, simply because there's so many kids and so many <sniffing> <throat clearing> Utt Label 1 sv I think she has problems with that, too.", "labels": [], "entities": []}, {"text": "Figure 3: An example of errors produced by the model in the SwitchBoard corpus the previous segment influence (i.e., the language model weight) was not enough to get the appropriate label.", "labels": [], "entities": [{"text": "SwitchBoard corpus", "start_pos": 60, "end_pos": 78, "type": "DATASET", "confidence": 0.8444956541061401}]}, {"text": "This was true for all the N-grams tested.", "labels": [], "entities": []}, {"text": "In the case of the unsegmented model, most of the errors were caused by similar misrecognitions in the second level (which are more frequent due to the absence of utterance boundaries); however, deletion and insertion errors were also significant.", "labels": [], "entities": [{"text": "insertion", "start_pos": 208, "end_pos": 217, "type": "METRIC", "confidence": 0.9557831883430481}]}, {"text": "The deletion errors corresponded to acceptance utterances, which were too short (most of them were \"Yes\").", "labels": [], "entities": []}, {"text": "The insertion errors corresponded to \"Yes\" words that were placed after a new-consult system utterance, which is the case of the second turn presented in.", "labels": [], "entities": []}, {"text": "These words should not have been labelled as a separate utterance.", "labels": [], "entities": []}, {"text": "In both cases, these errors were very dependant on the WIP factor, and we had to get an adequate WIP value which did not increase the insertions and did not cause too many deletions.", "labels": [], "entities": [{"text": "WIP", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.5374520421028137}, {"text": "insertions", "start_pos": 134, "end_pos": 144, "type": "METRIC", "confidence": 0.9734736084938049}]}], "tableCaptions": [{"text": " Table 1: SwitchBoard database statistics (mean for  the ten cross-validation partitions)", "labels": [], "entities": []}, {"text": " Table 2: Dihana database statistics (mean for the  five cross-validation partitions)", "labels": [], "entities": [{"text": "Dihana database", "start_pos": 10, "end_pos": 25, "type": "DATASET", "confidence": 0.7417656481266022}]}, {"text": " Table 3: SwitchBoard results for the segmented  model  N-gram Utt. accuracy Turn accuracy  2-gram  68.19%  59.33%  3-gram  68.50%  59.75%  4-gram  67.90%  59.14%", "labels": [], "entities": [{"text": "Utt. accuracy Turn accuracy", "start_pos": 63, "end_pos": 90, "type": "METRIC", "confidence": 0.7467949628829956}]}, {"text": " Table 4: SwitchBoard results for the unsegmented  model (WIP=50)", "labels": [], "entities": [{"text": "WIP", "start_pos": 58, "end_pos": 61, "type": "METRIC", "confidence": 0.9201517105102539}]}, {"text": " Table 5: Dihana results for the segmented model  (only two-level labelling for user turns)", "labels": [], "entities": [{"text": "Dihana", "start_pos": 10, "end_pos": 16, "type": "TASK", "confidence": 0.9317315220832825}]}, {"text": " Table 6: Dihana results for the unsegmented  model (WIP=50, only two-level labelling for user  turns)", "labels": [], "entities": [{"text": "Dihana", "start_pos": 10, "end_pos": 16, "type": "TASK", "confidence": 0.8379536867141724}, {"text": "WIP", "start_pos": 53, "end_pos": 56, "type": "METRIC", "confidence": 0.9584221243858337}]}]}