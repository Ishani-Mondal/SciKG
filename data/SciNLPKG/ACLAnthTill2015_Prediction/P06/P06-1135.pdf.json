{"title": [{"text": "Improving QA Accuracy by Question Inversion", "labels": [], "entities": [{"text": "Improving QA", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.8329825699329376}, {"text": "Accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.577399492263794}, {"text": "Question Inversion", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.6331049799919128}]}], "abstractContent": [{"text": "This paper demonstrates a conceptually simple but effective method of increasing the accuracy of QA systems on factoid-style questions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9986507296562195}]}, {"text": "We define the notion of an inverted question, and show that by requiring that the answers to the original and inverted questions be mutually consistent , incorrect answers get demoted in confidence and correct ones promoted.", "labels": [], "entities": []}, {"text": "Additionally, we show that lack of validation can be used to assert no-answer (nil) conditions.", "labels": [], "entities": []}, {"text": "We demonstrate increases of performance on TREC and other question-sets, and discuss the kinds of future activities that can be particularly beneficial to approaches such as ours.", "labels": [], "entities": []}], "introductionContent": [{"text": "Most QA systems nowadays consist of the following standard modules: QUESTION PROCESSING, to determine the bag of words fora query and the desired answer type (the type of the entity that will be offered as a candidate answer); SEARCH, which will use the query to extract a set of documents or passages from a corpus; and ANSWER SELECTION, which will analyze the returned documents or passages for instances of the answer type in the most favorable contexts.", "labels": [], "entities": [{"text": "SEARCH", "start_pos": 227, "end_pos": 233, "type": "METRIC", "confidence": 0.9937480688095093}, {"text": "ANSWER SELECTION", "start_pos": 321, "end_pos": 337, "type": "METRIC", "confidence": 0.8450028300285339}]}, {"text": "Each of these components implements a set of heuristics or hypotheses, as devised by their authors (cf.).", "labels": [], "entities": []}, {"text": "When we perform failure analysis on questions incorrectly answered by our system, we find that there are broadly speaking two kinds of failure.", "labels": [], "entities": []}, {"text": "There are errors (we might call them bugs) on the implementation of the said heuristics: errors in tagging, parsing, named-entity recognition; omissions in synonym lists; missing patterns, and just plain programming errors.", "labels": [], "entities": [{"text": "named-entity recognition", "start_pos": 117, "end_pos": 141, "type": "TASK", "confidence": 0.7147293835878372}]}, {"text": "This class can be characterized by being fixable by identifying incorrect code and fixing it, or adding more items, either explicitly or through training.", "labels": [], "entities": []}, {"text": "The other class of errors (what we might call unlucky) are at the boundaries of the heuristics; situations were the system did not do anything \"wrong,\" in the sense of bug, but circumstances conspired against finding the correct answer.", "labels": [], "entities": []}, {"text": "Usually when unlucky errors occur, the system generates a reasonable query and an appropriate answer type, and at least one passage containing the right answer is returned.", "labels": [], "entities": []}, {"text": "However, there maybe returned passages that have a larger number of query terms and an incorrect answer of the right type, or the query terms might just be physically closer to the incorrect answer than to the correct one.", "labels": [], "entities": []}, {"text": "ANSWER SELECTION modules typically work either by trying to prove the answer is corrector by giving them a weight produced by summing a collection of heuristic features (); in the latter case candidates having a larger number of matching query terms, even if they do not exactly match the context in the question, might generate a larger score than a correct passage with fewer matching terms.", "labels": [], "entities": [{"text": "ANSWER SELECTION", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.6692890822887421}]}, {"text": "To be sure, unlucky errors are usually bugs when considered from the standpoint of a system with a more sophisticated heuristic, but any system at any point in time will have limits on what it tries to do; therefore the distinction is not absolute but is relative to a heuristic and system.", "labels": [], "entities": []}, {"text": "It has been argued) that the success of a QA system is proportional to the impedance match between the question and the knowledge sources available.", "labels": [], "entities": []}, {"text": "Moreover, we believe that this is true not only in terms of the correct answer, but the distracters, 1 or incorrect answers too.", "labels": [], "entities": []}, {"text": "In QA, an unlucky incorrect answer is not usually predictable in advance; it occurs because of a coincidence of terms and syntactic contexts that cause it to be preferred over the correct answer.", "labels": [], "entities": []}, {"text": "It has no connection with the correct answer and is only returned because its enclosing passage so happens to exist in the same corpus as the correct answer context.", "labels": [], "entities": []}, {"text": "This would lead us to believe that if a different corpus containing the correct answer were to be processed, while there would be no guarantee that the correct answer would be found, it would be unlikely (i.e. very unlucky) if the same incorrect answer as before were returned.", "labels": [], "entities": []}, {"text": "We have demonstrated elsewhere) how using multiple corpora can improve QA performance, but in this paper we achieve similar goals without using additional corpora.", "labels": [], "entities": []}, {"text": "We note that factoid questions are usually about relations between entities, e.g. \"What is the capital of France?\", where one of the arguments of the relationship is sought and the others given.", "labels": [], "entities": []}, {"text": "We can invert the question by substituting the candidate answer back into the question, while making one of the given entities the socalled wh-word, thus \"Of what country is Paris the capital?\"", "labels": [], "entities": []}, {"text": "We hypothesize that asking this question (and those formed from other candidate answers) will locate a largely different set of passages in the corpus than the first time around.", "labels": [], "entities": []}, {"text": "As will be explained in Section 3, this can be used to decrease the confidence in the incorrect answers, and also increase it for the correct answer, so that the latter becomes the answer the system ultimately proposes.", "labels": [], "entities": []}, {"text": "This work is part of a continuing program of demonstrating how meta-heuristics, using what might be called \"collateral\" information, can be used to constrain or adjust the results of the primary QA system.", "labels": [], "entities": []}, {"text": "In the next Section we review related work.", "labels": [], "entities": []}, {"text": "In Section 3 we describe our algorithm in detail, and in Section 4 present evaluation results.", "labels": [], "entities": []}, {"text": "In Section 5 we discuss our conclusions and future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "Due to the complexity of the learned algorithm, we decided to evaluate in stages.", "labels": [], "entities": []}, {"text": "We first performed an evaluation with a fixed question type, to verify that the purely arithmetic components of the algorithm were performing reasonably.", "labels": [], "entities": []}, {"text": "We then evaluated on the entire TREC12 factoid question set.", "labels": [], "entities": [{"text": "TREC12 factoid question set", "start_pos": 32, "end_pos": 59, "type": "DATASET", "confidence": 0.8599650859832764}]}, {"text": "We created a fixed question set of 50 questions of the form \"What is the capital of X?\", for each state in the U.S. The inverted question \"What state is Z the capital of?\" was correctly generated in each case.", "labels": [], "entities": []}, {"text": "We evaluated against two corpora: the AQUAINT corpus, of a little over a million newswire documents, and the CNS corpus, with about 37,000 documents from the Center for Nonproliferation Studies in Monterey, CA.", "labels": [], "entities": [{"text": "AQUAINT corpus", "start_pos": 38, "end_pos": 52, "type": "DATASET", "confidence": 0.8147514164447784}, {"text": "CNS corpus", "start_pos": 109, "end_pos": 119, "type": "DATASET", "confidence": 0.8518766760826111}]}, {"text": "We expected thereto be answers to most questions in the former corpus, so we hoped there our method would be useful in converting 2 nd place answers to first place.", "labels": [], "entities": []}, {"text": "The latter corpus is about WMDs, so we expected thereto be holes in the state capital coverage 2 , for which nil identification would be useful.", "labels": [], "entities": [{"text": "nil identification", "start_pos": 109, "end_pos": 127, "type": "TASK", "confidence": 0.689384713768959}]}, {"text": "The baseline is our regular search-based QA-System without the Constraint process.", "labels": [], "entities": [{"text": "Constraint", "start_pos": 63, "end_pos": 73, "type": "METRIC", "confidence": 0.7624204754829407}]}, {"text": "In this baseline system there was no special processing for nil questions, other than if the search (which always contained some required terms) returned no documents.", "labels": [], "entities": []}, {"text": "Our results are shown in.", "labels": [], "entities": []}, {"text": "On the AQUAINT corpus, four out of seven 2 nd place finishers went to first place.", "labels": [], "entities": [{"text": "AQUAINT corpus", "start_pos": 7, "end_pos": 21, "type": "DATASET", "confidence": 0.9484839141368866}]}, {"text": "On the CNS corpus 16 out of a possible 26 correct no-answer cases were discovered, at a cost of losing three previously correct answers.", "labels": [], "entities": [{"text": "CNS corpus", "start_pos": 7, "end_pos": 17, "type": "DATASET", "confidence": 0.8410497903823853}]}, {"text": "The percentage correct score increased by a relative 10.3% for AQUAINT and 186% for CNS.", "labels": [], "entities": [{"text": "percentage correct score", "start_pos": 4, "end_pos": 28, "type": "METRIC", "confidence": 0.7517778277397156}, {"text": "AQUAINT", "start_pos": 63, "end_pos": 70, "type": "METRIC", "confidence": 0.6182462573051453}, {"text": "CNS", "start_pos": 84, "end_pos": 87, "type": "DATASET", "confidence": 0.8413349986076355}]}, {"text": "In both cases, the error rate was reduced by about a third.", "labels": [], "entities": [{"text": "error rate", "start_pos": 19, "end_pos": 29, "type": "METRIC", "confidence": 0.9910295009613037}]}, {"text": "For the second evaluation, we processed the 414 factoid questions from TREC12.", "labels": [], "entities": [{"text": "TREC12", "start_pos": 71, "end_pos": 77, "type": "DATASET", "confidence": 0.6984370946884155}]}, {"text": "Of special interest here are the questions initially in first and second places, and in addition any questions for which nils were found.", "labels": [], "entities": []}, {"text": "As seen in, there were 32 questions which originally evaluated in rank 2.", "labels": [], "entities": []}, {"text": "Of these, four questions were not invertible because they had no terms that were annotated with any of our named-entity types, e.g. #2285 \"How much does it cost for gastric bypass surgery?\"", "labels": [], "entities": [{"text": "gastric bypass surgery", "start_pos": 165, "end_pos": 187, "type": "TASK", "confidence": 0.8048656185468038}]}, {"text": "Of the remaining 28 questions, 12 were promoted to first place.", "labels": [], "entities": []}, {"text": "In addition, two new nils were found.", "labels": [], "entities": []}, {"text": "On the downside, four out of 108 previous first place answers were lost.", "labels": [], "entities": []}, {"text": "There was of course movement in the ranks two and beyond whenever nils were introduced in first place, but these do not affect the current TREC-QA factoid correctness measure, which is whether the top answer is corrector not.", "labels": [], "entities": [{"text": "TREC-QA factoid correctness measure", "start_pos": 139, "end_pos": 174, "type": "METRIC", "confidence": 0.6732568144798279}]}, {"text": "These results are summarized in.", "labels": [], "entities": []}, {"text": "While the overall percentage improvement was small, note that only second-place answers were candidates for re-ranking, and 43% of these were promoted to first place and hence judged correct.", "labels": [], "entities": []}, {"text": "Only 3.7% of originally correct questions were casualties.", "labels": [], "entities": []}, {"text": "To the extent that these percentages are stable across other collections, as long as the size of the set of second-place answers is at least about 1/10 of the set of first-place answers, this form of the Constraint process can be applied effectively.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Baseline statistics for TREC11-12.", "labels": [], "entities": [{"text": "TREC11-12", "start_pos": 34, "end_pos": 43, "type": "TASK", "confidence": 0.529860258102417}]}, {"text": " Table 2. Evaluation on AQUAINT and CNS  corpora.", "labels": [], "entities": []}]}