{"title": [{"text": "Scaling Distributional Similarity to Large Corpora", "labels": [], "entities": [{"text": "Scaling Distributional Similarity", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8433875838915507}]}], "abstractContent": [{"text": "Accurately representing synonymy using distributional similarity requires large volumes of data to reliably represent infrequent words.", "labels": [], "entities": []}, {"text": "However, the na\u00a8\u0131vena\u00a8\u0131ve nearest-neighbour approach to comparing context vectors extracted from large corpora scales poorly (O(n 2) in the vocabulary size).", "labels": [], "entities": [{"text": "O", "start_pos": 126, "end_pos": 127, "type": "METRIC", "confidence": 0.9920765161514282}]}, {"text": "In this paper, we compare several existing approaches to approximating the nearest-neighbour search for distributional similarity.", "labels": [], "entities": []}, {"text": "We investigate the trade-off between efficiency and accuracy, and find that SASH (Houle and Sakuma, 2005) provides the best balance.", "labels": [], "entities": [{"text": "efficiency", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.9832151532173157}, {"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9971181154251099}, {"text": "SASH", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.8857600092887878}]}], "introductionContent": [{"text": "It is a general property of Machine Learning that increasing the volume of training data increases the accuracy of results.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9990196228027344}]}, {"text": "This is no more evident than in Natural Language Processing (NLP), where massive quantities of text are required to model rare language events.", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 32, "end_pos": 65, "type": "TASK", "confidence": 0.7022989988327026}]}, {"text": "Despite the rapid increase in computational power available for NLP systems, the volume of raw data available still outweighs our ability to process it.", "labels": [], "entities": []}, {"text": "Unsupervised learning, which does not require the expensive and timeconsuming human annotation of data, offers an opportunity to use this wealth of data.", "labels": [], "entities": []}, {"text": "show that synonymy extraction for lexical semantic resources using distributional similarity produces continuing gains inaccuracy as the volume of input data increases.", "labels": [], "entities": [{"text": "synonymy extraction", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.7706920802593231}]}, {"text": "Extracting synonymy relations using distributional similarity is based on the distributional hypothesis that similar words appear in similar contexts.", "labels": [], "entities": [{"text": "Extracting synonymy", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8855621814727783}]}, {"text": "Terms are described by collating information about their occurrence in a corpus into vectors.", "labels": [], "entities": []}, {"text": "These context vectors are then compared for similarity.", "labels": [], "entities": [{"text": "similarity", "start_pos": 44, "end_pos": 54, "type": "METRIC", "confidence": 0.9749318361282349}]}, {"text": "Existing approaches differ primarily in their definition of \"context\", e.g. the surrounding words or the entire document, and their choice of distance metric for calculating similarity between the context vectors representing each term.", "labels": [], "entities": []}, {"text": "Manual creation of lexical semantic resources is open to the problems of bias, inconsistency and limited coverage.", "labels": [], "entities": []}, {"text": "It is difficult to account for the needs of the many domains in which NLP techniques are now being applied and for the rapid change in language use.", "labels": [], "entities": []}, {"text": "The assisted or automatic creation and maintenance of these resources would be of great advantage.", "labels": [], "entities": []}, {"text": "Finding synonyms using distributional similarity requires a nearest-neighbour search over the context vectors of each term.", "labels": [], "entities": []}, {"text": "This is computationally intensive, scaling to O(n 2 m) for the number of terms n and the size of their context vectors m.", "labels": [], "entities": [{"text": "O", "start_pos": 46, "end_pos": 47, "type": "METRIC", "confidence": 0.9981854557991028}]}, {"text": "Increasing the volume of input data will increase the size of both n and m, decreasing the efficiency of a na\u00a8\u0131vena\u00a8\u0131ve nearest-neighbour approach.", "labels": [], "entities": []}, {"text": "Many approaches to reduce this complexity have been suggested.", "labels": [], "entities": []}, {"text": "In this paper we evaluate state-of-the-art techniques proposed to solve this problem.", "labels": [], "entities": []}, {"text": "We find that the Spatial Approximation Sample Hierarchy) provides the best accuracy/efficiency trade-off.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9988610744476318}]}], "datasetContent": [{"text": "The simplest method for evaluation is the direct comparison of extracted synonyms with a manually created gold standard.", "labels": [], "entities": []}, {"text": "To reduce the problem of limited coverage, our evaluation combines three electronic thesauri: the Macquarie, Roget's and Moby thesauri.", "labels": [], "entities": [{"text": "Macquarie, Roget's and Moby thesauri", "start_pos": 98, "end_pos": 134, "type": "DATASET", "confidence": 0.8400807806423732}]}, {"text": "We follow and use two performance measures: direct matches (DIRECT) and inverse rank (INVR).", "labels": [], "entities": [{"text": "inverse rank (INVR)", "start_pos": 72, "end_pos": 91, "type": "METRIC", "confidence": 0.9172580361366272}]}, {"text": "DIRECT is the percentage of returned synonyms found in the gold standard.", "labels": [], "entities": [{"text": "DIRECT", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9328731298446655}]}, {"text": "INVR is the sum of the inverse rank of each matching synonym, e.g. matches at ranks 3, 5 and 28 CORPUS 35,618 1,400 give an inverse rank score of 1 3 + 1 5 + 1 28 . With at most 100 matching synonyms, the maximum INVR is 5.187.", "labels": [], "entities": [{"text": "INVR", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9342777729034424}, {"text": "CORPUS 35,618 1,400", "start_pos": 96, "end_pos": 115, "type": "DATASET", "confidence": 0.868206799030304}, {"text": "INVR", "start_pos": 213, "end_pos": 217, "type": "METRIC", "confidence": 0.9971336126327515}]}, {"text": "This more fine grained as it incorporates the both the number of matches and their ranking.", "labels": [], "entities": []}, {"text": "The same 300 single word nouns were used for evaluation as used by for his large scale evaluation.", "labels": [], "entities": []}, {"text": "These were chosen randomly from WordNet such that they covered a range over the following properties: frequency, number of senses, specificity and concreteness.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 32, "end_pos": 39, "type": "DATASET", "confidence": 0.9555870890617371}]}, {"text": "For each of these terms, the closest 100 terms and their similarity scores were extracted.", "labels": [], "entities": [{"text": "similarity scores", "start_pos": 57, "end_pos": 74, "type": "METRIC", "confidence": 0.9563612937927246}]}, {"text": "We use two corpora in our experiments: the smaller is the non-speech portion of the British National Corpus (BNC), 90 million words covering a wide range of domains and formats; the larger consists of the BNC, the Reuters Corpus Volume 1 and most of the English news holdings of the LDC in 2003, representing over 2 billion words of text).", "labels": [], "entities": [{"text": "British National Corpus (BNC)", "start_pos": 84, "end_pos": 113, "type": "DATASET", "confidence": 0.963310569524765}, {"text": "BNC", "start_pos": 205, "end_pos": 208, "type": "DATASET", "confidence": 0.9866430759429932}, {"text": "Reuters Corpus Volume 1", "start_pos": 214, "end_pos": 237, "type": "DATASET", "confidence": 0.9491147100925446}, {"text": "English news holdings of the LDC in 2003", "start_pos": 254, "end_pos": 294, "type": "DATASET", "confidence": 0.8361255526542664}]}, {"text": "The semantic similarity system implemented by Curran (2004) provides our baseline.", "labels": [], "entities": []}, {"text": "This performs a brute-force k-NN search (NAIVE).", "labels": [], "entities": []}, {"text": "We present results for the canonical attribute heuristic (HEURISTIC), RI, LSH, PLEB, VPT and SASH.", "labels": [], "entities": [{"text": "HEURISTIC", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9764031767845154}, {"text": "RI", "start_pos": 70, "end_pos": 72, "type": "METRIC", "confidence": 0.9064517617225647}, {"text": "LSH", "start_pos": 74, "end_pos": 77, "type": "METRIC", "confidence": 0.8890296220779419}, {"text": "PLEB", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.9845215678215027}, {"text": "SASH", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.8127065300941467}]}, {"text": "We take the optimal canonical attribute vector length of 30 for HEURISTIC from Curran (2004).", "labels": [], "entities": [{"text": "HEURISTIC", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.7509490251541138}]}, {"text": "For SASH we take optimal values of p = 4 and c = 16 and use the folded ordering taking M = 1000 from Gorman and Curran (2005b).", "labels": [], "entities": [{"text": "SASH", "start_pos": 4, "end_pos": 8, "type": "TASK", "confidence": 0.9438857436180115}]}, {"text": "For RI, LSH and PLEB we found optimal values experimentally using the BNC.", "labels": [], "entities": [{"text": "PLEB", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9899320602416992}, {"text": "BNC", "start_pos": 70, "end_pos": 73, "type": "DATASET", "confidence": 0.8804343938827515}]}, {"text": "For LSH we chose d = 3, 000 (LSH 3,000 ) and 10, 000 (LSH 10,000 ), showing the effect of changing the dimensionality.", "labels": [], "entities": []}, {"text": "The frequency statistics were weighted using mutual information, as in: PLEB used the values q = 500 and B = 100.", "labels": [], "entities": [{"text": "PLEB", "start_pos": 72, "end_pos": 76, "type": "DATASET", "confidence": 0.7068869471549988}]}, {"text": "The initial experiments on RI produced quite poor results.", "labels": [], "entities": [{"text": "RI", "start_pos": 27, "end_pos": 29, "type": "TASK", "confidence": 0.8649759888648987}]}, {"text": "The intuition was that this was caused by the lack of smoothing in the algorithm.", "labels": [], "entities": []}, {"text": "Experiments were performed using the weights given in.", "labels": [], "entities": []}, {"text": "Of these, mutual information, evaluated with an extra log 2 (f (w, r, w \ud97b\udf59 ) + 1) factor and limited to positive values, produced the best results (RI MI ).", "labels": [], "entities": [{"text": "RI MI )", "start_pos": 147, "end_pos": 154, "type": "METRIC", "confidence": 0.9615035454432169}]}, {"text": "The values d = 1000 and \ud97b\udf59 = 5 were found to produce the best results.", "labels": [], "entities": []}, {"text": "All experiments were performed on 3.2GHz Xeon P4 machines with 4GB of RAM.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: INVR vs frequency cut-off", "labels": [], "entities": [{"text": "INVR", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.6422755718231201}]}, {"text": " Table 3: Full thesaurus extraction", "labels": [], "entities": []}]}