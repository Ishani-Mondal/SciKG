{"title": [{"text": "QuestionBank: Creating a Corpus of Parse-Annotated Questions", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes the development of QuestionBank, a corpus of 4000 parse-annotated questions for (i) use in training parsers employed in QA, and (ii) evaluation of question parsing.", "labels": [], "entities": [{"text": "evaluation of question parsing", "start_pos": 154, "end_pos": 184, "type": "TASK", "confidence": 0.6424680575728416}]}, {"text": "We present a series of experiments to investigate the effectiveness of QuestionBank as both an exclusive and supplementary training resource fora state-of-the-art parser in parsing both question and non-question test sets.", "labels": [], "entities": []}, {"text": "We introduce anew method for recovering empty nodes and their antecedents (capturing long distance dependencies) from parser output in CFG trees using LFG f-structure reentrancies.", "labels": [], "entities": []}, {"text": "Our main findings are (i) using QuestionBank training data improves parser performance to 89.75% labelled bracketing f-score, an increase of almost 11% over the base-line; (ii) back-testing experiments on non-question data (Penn-II WSJ Section 23) shows that the retrained parser does not suffer a performance drop on non-question material; (iii) ablation experiments show that the size of training material provided by QuestionBank is sufficient to achieve optimal results; (iv) our method for recovering empty nodes captures long distance dependencies in questions from the ATIS corpus with high precision (96.82%) and low recall (39.38%).", "labels": [], "entities": [{"text": "QuestionBank training data", "start_pos": 32, "end_pos": 58, "type": "DATASET", "confidence": 0.7860136230786642}, {"text": "Penn-II WSJ Section 23)", "start_pos": 224, "end_pos": 247, "type": "DATASET", "confidence": 0.9587434768676758}, {"text": "ATIS corpus", "start_pos": 576, "end_pos": 587, "type": "DATASET", "confidence": 0.9614670872688293}, {"text": "precision", "start_pos": 598, "end_pos": 607, "type": "METRIC", "confidence": 0.9968223571777344}, {"text": "recall", "start_pos": 625, "end_pos": 631, "type": "METRIC", "confidence": 0.9984902143478394}]}, {"text": "In summary, Ques-tionBank provides a useful new resource in parser-based QA research.", "labels": [], "entities": []}], "introductionContent": [{"text": "Parse-annotated corpora (treebanks) are crucial for developing machine learning and statistics-based parsing resources fora given language or task.", "labels": [], "entities": []}, {"text": "Large treebanks are available for major languages, however these are often based on a specific text type or genre, e.g. financial newspaper text (the Penn-II Treebank ().", "labels": [], "entities": [{"text": "Penn-II Treebank", "start_pos": 150, "end_pos": 166, "type": "DATASET", "confidence": 0.9903310239315033}]}, {"text": "This can limit the applicability of grammatical resources induced from treebanks in that such resources underperform when used on a different type of text or fora specific task.", "labels": [], "entities": []}, {"text": "In this paper we present work on creating QuestionBank, a treebank of parse-annotated questions, which can be used as a supplementary training resource to allow parsers to accurately parse questions (as well as other text).", "labels": [], "entities": []}, {"text": "Alternatively, the resource can be used as a stand-alone training corpus to train a parser specifically for questions.", "labels": [], "entities": []}, {"text": "Either scenario will be useful in training parsers for use in question answering (QA) tasks, and it also provides a suitable resource to evaluate the accuracy of these parsers on questions.", "labels": [], "entities": [{"text": "question answering (QA) tasks", "start_pos": 62, "end_pos": 91, "type": "TASK", "confidence": 0.8577718784411749}, {"text": "accuracy", "start_pos": 150, "end_pos": 158, "type": "METRIC", "confidence": 0.9965344667434692}]}, {"text": "We use a semi-automatic \"bootstrapping\" method to create the question treebank from raw text.", "labels": [], "entities": []}, {"text": "We show that a parser trained on the question treebank alone can accurately parse questions.", "labels": [], "entities": []}, {"text": "Training on a combined corpus consisting of the question treebank and an established training set (Sections 02-21 of the Penn-II Treebank), the parser gives state-of-the-art performance on both questions and a non-question test set (Section 23 of the Penn-II Treebank).", "labels": [], "entities": [{"text": "Penn-II Treebank)", "start_pos": 121, "end_pos": 138, "type": "DATASET", "confidence": 0.976080060005188}, {"text": "Penn-II Treebank", "start_pos": 251, "end_pos": 267, "type": "DATASET", "confidence": 0.9934569597244263}]}, {"text": "Section 2 describes background work and motivation for the research presented in this paper.", "labels": [], "entities": []}, {"text": "Section 3 describes the data we used to create the corpus.", "labels": [], "entities": []}, {"text": "In Section 4 we describe the semiautomatic method to \"bootstrap\" the question corpus, discuss some interesting and problematic phenomena, and show how the manual vs. automatic workload distribution changed as work progressed.", "labels": [], "entities": []}, {"text": "Two sets of experiments using our new question corpus are presented in Section 5.", "labels": [], "entities": []}, {"text": "In Section 6 we introduce anew method for recovering empty nodes and their antecedents using Lexical Functional Grammar (LFG) f-structure reen-trancies.", "labels": [], "entities": []}, {"text": "Section 7 concludes and outlines future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to test the effect training on the question corpus has on parser performance, we carried out a number of experiments.", "labels": [], "entities": []}, {"text": "In cross-validation experiments with 90%/10% splits we use all 4000 trees in the completed QuestionBank as the test set.", "labels": [], "entities": [{"text": "QuestionBank", "start_pos": 91, "end_pos": 103, "type": "DATASET", "confidence": 0.9404091835021973}]}, {"text": "We performed ablation experiments to investigate the effect of varying the amount of question and non-question training data on the parser's performance.", "labels": [], "entities": []}, {"text": "For these experiments we divided the 4000 questions into two sets.", "labels": [], "entities": []}, {"text": "We randomly selected 400 trees to beheld out as a gold standard test set against which to evaluate, the remaining 3600 trees were then used as a training corpus.", "labels": [], "entities": []}, {"text": "We carried out two cross-validation experiments.", "labels": [], "entities": []}, {"text": "In the first experiment we perform a 10-fold crossvalidation experiment using our 4000 question treebank.", "labels": [], "entities": []}, {"text": "In each case a randomly selected set of 10% of the questions in QuestionBank was held out during training and used as a test set.", "labels": [], "entities": [{"text": "QuestionBank", "start_pos": 64, "end_pos": 76, "type": "DATASET", "confidence": 0.9306487441062927}]}, {"text": "In this way parses from unseen data were generated for all 4000 questions and evaluated against the QuestionBank trees.", "labels": [], "entities": [{"text": "QuestionBank trees", "start_pos": 100, "end_pos": 118, "type": "DATASET", "confidence": 0.9326328635215759}]}, {"text": "The second cross-validation experiment was similar to the first, but in each of the 10 folds we train on 90% of the 4000 questions in QuestionBank and on all of Sections 02-21 of the Penn-II Treebank.", "labels": [], "entities": [{"text": "QuestionBank", "start_pos": 134, "end_pos": 146, "type": "DATASET", "confidence": 0.9577355980873108}, {"text": "Penn-II Treebank", "start_pos": 183, "end_pos": 199, "type": "DATASET", "confidence": 0.9707281291484833}]}, {"text": "In both experiments we also backtest each of the ten grammars on Section 23 of the Penn-II Treebank and report the average scores.", "labels": [], "entities": [{"text": "Section 23 of the Penn-II Treebank", "start_pos": 65, "end_pos": 99, "type": "DATASET", "confidence": 0.9218836923440298}]}, {"text": "shows the results for the second crossvalidation experiment using Sections 02-21 of the Penn-II Treebank and the 4000 questions in QuestionBank.", "labels": [], "entities": [{"text": "Penn-II Treebank", "start_pos": 88, "end_pos": 104, "type": "DATASET", "confidence": 0.9868702590465546}, {"text": "QuestionBank", "start_pos": 131, "end_pos": 143, "type": "DATASET", "confidence": 0.968475341796875}]}, {"text": "The results show an even greater increase on the baseline f-score than the experiments using only the question training set (.", "labels": [], "entities": []}, {"text": "The non-question results are also better and are comparable to the baseline.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Baseline parsing results", "labels": [], "entities": [{"text": "Baseline parsing", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.7498742341995239}]}, {"text": " Table 2: Cross-validation experiment using the  4000 question treebank", "labels": [], "entities": []}, {"text": " Table 3: Cross-validation experiment using Penn- II Treebank Sections 02-21 and 4000 questions", "labels": [], "entities": [{"text": "Penn- II Treebank Sections 02-21", "start_pos": 44, "end_pos": 76, "type": "DATASET", "confidence": 0.9511017302672068}]}, {"text": " Table 4: Scores for LDD recovery (empty nodes  and antecedents)", "labels": [], "entities": [{"text": "LDD recovery", "start_pos": 21, "end_pos": 33, "type": "TASK", "confidence": 0.9069958329200745}]}]}