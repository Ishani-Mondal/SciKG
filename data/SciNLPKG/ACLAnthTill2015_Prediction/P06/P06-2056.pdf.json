{"title": [{"text": "Unsupervised Segmentation of Chinese Text by Use of Branching Entropy", "labels": [], "entities": [{"text": "Unsupervised Segmentation of Chinese Text", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.7761929869651795}]}], "abstractContent": [{"text": "We propose an unsupervised segmen-tation method based on an assumption about language data: that the increasing point of entropy of successive characters is the location of a word boundary.", "labels": [], "entities": []}, {"text": "A large-scale experiment was conducted by using 200 MB of unseg-mented training data and 1 MB of test data, and precision of 90% was attained with recall being around 80%.", "labels": [], "entities": [{"text": "precision", "start_pos": 112, "end_pos": 121, "type": "METRIC", "confidence": 0.9996974468231201}, {"text": "recall", "start_pos": 147, "end_pos": 153, "type": "METRIC", "confidence": 0.9996424913406372}]}, {"text": "Moreover , we found that the precision was stable at around 90% independently of the learning data size.", "labels": [], "entities": [{"text": "precision", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9996535778045654}]}], "introductionContent": [{"text": "The theme of this paper is the following assumption: The uncertainty of tokens coming after a sequence helps determine whether a given position is at a boundary.", "labels": [], "entities": []}, {"text": "(A) Intuitively, as illustrated in, the variety of successive tokens at each character inside a word monotonically decreases according to the ooset length, because the longer the preceding character n-gram, the longer the preceding context and the more it restricts the appearance of possible next tokens.", "labels": [], "entities": []}, {"text": "For example, it is easier to guess which character comes after \\natura\" than after \\na\".", "labels": [], "entities": []}, {"text": "On the other hand, the uncertainty at the position of a word border becomes greater, and the complexity increases, as the position is out of context.", "labels": [], "entities": [{"text": "uncertainty", "start_pos": 23, "end_pos": 34, "type": "METRIC", "confidence": 0.9571483731269836}, {"text": "complexity", "start_pos": 93, "end_pos": 103, "type": "METRIC", "confidence": 0.9927502870559692}]}, {"text": "With the same example, it is diicult to guess which character comes after \\natural \".", "labels": [], "entities": []}, {"text": "This suggests that a word border can be detected by focusing on the diierentials of the uncertainty of branching.", "labels": [], "entities": []}, {"text": "In this paper, we report our study on applying this assumption to Chinese word seg-: Intuitive illustration of a variety of successive tokens and a word boundary mentation by formalizing the uncertainty of successive tokens via the branching entropy (which we mathematically deene in the next section).", "labels": [], "entities": []}, {"text": "Our intention in this paper is above all to study the fundamental and scientiic statistical property underlying language data, so that it can be applied to language engineering.", "labels": [], "entities": [{"text": "language engineering", "start_pos": 156, "end_pos": 176, "type": "TASK", "confidence": 0.7676112353801727}]}, {"text": "The above assumption (A) dates back to the fundamental work done by Harris, where he says that when the number of diierent tokens coming after every preex of a word marks the maximum value, then the location corresponds to the morpheme boundary.", "labels": [], "entities": [{"text": "A", "start_pos": 22, "end_pos": 23, "type": "METRIC", "confidence": 0.961044192314148}]}, {"text": "Recently, with the increasing availability of corpora, this property underlying language has been tested through segmentation into words and morphemes.", "labels": [], "entities": []}, {"text": "Kempe) reports a preliminary experiment to detect word borders in German and English texts by monitoring the entropy of successive characters for 4-grams.", "labels": [], "entities": [{"text": "detect word borders in German and English texts", "start_pos": 43, "end_pos": 90, "type": "TASK", "confidence": 0.7258991971611977}]}, {"text": "Also, the second author of this paper) have shown how Japanese and Chinese can be segmented into words by formalizing the uncertainty with the branching entropy.", "labels": [], "entities": []}, {"text": "Even though the test data was limited to a small amount in this work, the report suggested how assumption (A) holds better when each of the sequence elements forms a semantic unit.", "labels": [], "entities": [{"text": "assumption (A)", "start_pos": 95, "end_pos": 109, "type": "METRIC", "confidence": 0.9321315139532089}]}, {"text": "This motivated our work to conduct a further, larger-scale test in the Chinese language, which is the only human language consisting entirely of ideograms (i.e., semantic units).", "labels": [], "entities": []}, {"text": "In this sense, the choice of Chinese as the language in our work is essential.", "labels": [], "entities": []}, {"text": "If the assumption holds well, the most important and direct application is unsupervised text segmentation into words.", "labels": [], "entities": [{"text": "text segmentation", "start_pos": 88, "end_pos": 105, "type": "TASK", "confidence": 0.7468129098415375}]}, {"text": "Many works in unsupervised segmentation so far could be interpreted as formulating assumption (A) in a similar sense where branching stays low inside words but increases at a word or morpheme border.", "labels": [], "entities": [{"text": "assumption (A", "start_pos": 83, "end_pos": 96, "type": "METRIC", "confidence": 0.8602775533994039}]}, {"text": "None of these works, however, is directly based on (A), and they introduce other factors within their overall methodologies.", "labels": [], "entities": []}, {"text": "Some works are based on in-word branching frequencies formulated in an original evaluation function, as in) (boundary precision=84.5%,recall=78.0%, tested on 12500 Japanese ideogram words).) uses mutual information (boundary p=91.8%, no report for recall, 1588 Chinese characters), and) incorporates branching counts in the evaluation function to be optimized for obtaining boundaries (word precision=76%, recall=78%, 2000 sentences).", "labels": [], "entities": [{"text": "boundary precision", "start_pos": 109, "end_pos": 127, "type": "METRIC", "confidence": 0.7009789049625397}, {"text": "recall", "start_pos": 134, "end_pos": 140, "type": "METRIC", "confidence": 0.9909694790840149}, {"text": "recall", "start_pos": 248, "end_pos": 254, "type": "METRIC", "confidence": 0.9242371320724487}, {"text": "recall", "start_pos": 406, "end_pos": 412, "type": "METRIC", "confidence": 0.9890952706336975}]}, {"text": "From the performance results listed here, we can see that unsupervised segmentation is more diicult, by far, than supervised segmentation; therefore, the algorithms are complex, and previous studies have tended to be limited in terms of both the test corpus size and the target.", "labels": [], "entities": []}, {"text": "In contrast, as assumption (A) is simple, we keep this simplicity in our formalization and directly test the assumption on a large-scale test corpus consisting of 1001 KB manually segmented data with the training corpus consisting of 200 MB of Chinese text.", "labels": [], "entities": []}, {"text": "Chinese is such an important language that supervised segmentation methods are already very mature.", "labels": [], "entities": []}, {"text": "The current state-of-the-art segmentation software developed by, which ranks as the best in the SIGHAN bakeoo, attains word precision and recall of 96.9% and 96.8%, respectively, on the PKU track.", "labels": [], "entities": [{"text": "SIGHAN bakeoo", "start_pos": 96, "end_pos": 109, "type": "DATASET", "confidence": 0.7671331465244293}, {"text": "precision", "start_pos": 124, "end_pos": 133, "type": "METRIC", "confidence": 0.9711256623268127}, {"text": "recall", "start_pos": 138, "end_pos": 144, "type": "METRIC", "confidence": 0.9989364743232727}, {"text": "PKU track", "start_pos": 186, "end_pos": 195, "type": "DATASET", "confidence": 0.9018953442573547}]}, {"text": "There is also free Figure 2: Decrease in H(XjX n ) for Chinese characters when n is increased software such as () whose performance is also high.", "labels": [], "entities": []}, {"text": "Even then, as most supervised methods learn on manually segmented newspaper data, when the input text is not from newspapers, the performance can be insuucient.", "labels": [], "entities": []}, {"text": "Given that the construction of learning data is costly, we believe the performance can be raised by combining the supervised and unsupervised methods.", "labels": [], "entities": []}, {"text": "Consequently, this paper veriies assumption (A) in a fundamental manner for Chinese text and addresses the questions of why and to what extent (A) holds, when applying it to the Chinese word segmentation problem.", "labels": [], "entities": [{"text": "Chinese word segmentation problem", "start_pos": 178, "end_pos": 211, "type": "TASK", "confidence": 0.6813868433237076}]}, {"text": "We rst formalize assumption (A) in a general manner.", "labels": [], "entities": []}], "datasetContent": [{"text": "Usually, when precision and recall are addressed in the Chinese word segmentation domain, they are calculated based on the number of words.", "labels": [], "entities": [{"text": "precision", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.9989689588546753}, {"text": "recall", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.9963507652282715}, {"text": "Chinese word segmentation domain", "start_pos": 56, "end_pos": 88, "type": "TASK", "confidence": 0.6943649351596832}]}, {"text": "For example, consider a correctly segmented sequence \\aaajbbbjcccjddd\", with a,b,c,d being characters and \\j\" indicating a word boundary.", "labels": [], "entities": []}, {"text": "Suppose that the machine's result is \\aaabbbjcccjddd\"; then the correct words are only \\ccc\" and \\ddd\", giving a value of 2.", "labels": [], "entities": []}, {"text": "Therefore, the precision is 2 divided by the number of words in the results (i.e., 3 for the words \\aaabbb\", \\ccc\", \\ddd\"), giving 67%, and the recall is 2 divided by the total number of words in the golden standard (i.e., 4 for the words \\aaa\",\\bbb\", \\ccc\", \\ddd\") giving 50%.", "labels": [], "entities": [{"text": "precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9994877576828003}, {"text": "recall", "start_pos": 144, "end_pos": 150, "type": "METRIC", "confidence": 0.9996188879013062}]}, {"text": "We call these values the word precision and recall, respectively, throughout this paper.", "labels": [], "entities": [{"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.8712112903594971}, {"text": "recall", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.999091386795044}]}, {"text": "In our case, we use slightly diierent measures for the boundary precision and recall, which are based on the correct number of boundaries.", "labels": [], "entities": [{"text": "precision", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.8135554194450378}, {"text": "recall", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.9995601773262024}]}, {"text": "These scores are also utilized especially in previous works on unsupervised segmentation N true is the number of boundaries in the golden standard.", "labels": [], "entities": []}, {"text": "For example, in the case of the machine result being \\aaabbbjcccjddd\", the precision is 100% and the recall is 75%.", "labels": [], "entities": [{"text": "precision", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.9997093081474304}, {"text": "recall", "start_pos": 101, "end_pos": 107, "type": "METRIC", "confidence": 0.9995989203453064}]}, {"text": "Thus, we consider thereto be no imprecise result as a boundary in the output of \\aaabbbjcccjddd\".", "labels": [], "entities": []}, {"text": "The crucial reason for using the boundary precision and recall is that boundary detection and word extraction are not exactly the same task.", "labels": [], "entities": [{"text": "precision", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.503764808177948}, {"text": "recall", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.9968147873878479}, {"text": "boundary detection", "start_pos": 71, "end_pos": 89, "type": "TASK", "confidence": 0.7097432613372803}, {"text": "word extraction", "start_pos": 94, "end_pos": 109, "type": "TASK", "confidence": 0.7939311265945435}]}, {"text": "In this sense, assumption (A) or (B) is a general assumption about a boundary (of a sentence, phrase, word, morpheme).", "labels": [], "entities": [{"text": "assumption (A) or (B)", "start_pos": 15, "end_pos": 36, "type": "METRIC", "confidence": 0.7442792505025864}]}, {"text": "Therefore, the boundary precision and recall Note that all precision and recall scores from now on in this paper are boundary precision and recall.", "labels": [], "entities": [{"text": "boundary precision", "start_pos": 15, "end_pos": 33, "type": "METRIC", "confidence": 0.6719230711460114}, {"text": "recall", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.9986357092857361}, {"text": "precision", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9981766939163208}, {"text": "recall", "start_pos": 73, "end_pos": 79, "type": "METRIC", "confidence": 0.9907620549201965}, {"text": "boundary precision", "start_pos": 117, "end_pos": 135, "type": "METRIC", "confidence": 0.6566499769687653}, {"text": "recall", "start_pos": 140, "end_pos": 146, "type": "METRIC", "confidence": 0.995886504650116}]}, {"text": "Even in comparing the supervised methods with our unsupervised method later, the precision and recall values are all recalculated as boundary precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 81, "end_pos": 90, "type": "METRIC", "confidence": 0.9992862343788147}, {"text": "recall", "start_pos": 95, "end_pos": 101, "type": "METRIC", "confidence": 0.9864010214805603}, {"text": "boundary precision", "start_pos": 133, "end_pos": 151, "type": "METRIC", "confidence": 0.7333215177059174}, {"text": "recall", "start_pos": 156, "end_pos": 162, "type": "METRIC", "confidence": 0.9899881482124329}]}], "tableCaptions": []}