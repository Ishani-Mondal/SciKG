{"title": [], "abstractContent": [{"text": "Semantic parsing is the task of mapping natural language sentences to complete formal meaning representations.", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8221675455570221}]}, {"text": "The performance of semantic parsing can be potentially improved by using discrimina-tive reranking, which explores arbitrary global features.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 19, "end_pos": 35, "type": "TASK", "confidence": 0.7483895123004913}]}, {"text": "In this paper, we investigate discriminative reranking upon a base-line semantic parser, SCISSOR, where the composition of meaning representations is guided by syntax.", "labels": [], "entities": []}, {"text": "We examine if features used for syntactic parsing can be adapted for semantic parsing by creating similar semantic features based on the mapping between syntax and semantics.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.7215857803821564}, {"text": "semantic parsing", "start_pos": 69, "end_pos": 85, "type": "TASK", "confidence": 0.7428730130195618}]}, {"text": "We report experimental results on two real applications , an interpreter for coaching instructions in robotic soccer and a natural-language database interface.", "labels": [], "entities": []}, {"text": "The results show that reranking can improve the performance on the coaching interpreter, but not on the database interface.", "labels": [], "entities": []}], "introductionContent": [{"text": "A long-standing challenge within natural language processing has been to understand the meaning of natural language sentences.", "labels": [], "entities": [{"text": "understand the meaning of natural language sentences", "start_pos": 73, "end_pos": 125, "type": "TASK", "confidence": 0.7681822436196464}]}, {"text": "In comparison with shallow semantic analysis tasks, such as wordsense disambiguation and semantic role labeling (), which only partially tackle this problem by identifying the meanings of target words or finding semantic roles of predicates, semantic parsing () pursues a more ambitious goal -mapping natural language sentences to complete formal meaning representations (MRs), where the meaning of each part of a sentence is analyzed, including noun phrases, verb phrases, negation, quantifiers and soon.", "labels": [], "entities": [{"text": "wordsense disambiguation", "start_pos": 60, "end_pos": 84, "type": "TASK", "confidence": 0.7401242256164551}, {"text": "semantic role labeling", "start_pos": 89, "end_pos": 111, "type": "TASK", "confidence": 0.6667205691337585}, {"text": "semantic parsing", "start_pos": 242, "end_pos": 258, "type": "TASK", "confidence": 0.7305663973093033}]}, {"text": "Semantic parsing enables logic reasoning and is critical in many practical tasks, such as speech understanding), question answering () and advice taking ().", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7900868058204651}, {"text": "logic reasoning", "start_pos": 25, "end_pos": 40, "type": "TASK", "confidence": 0.8360822200775146}, {"text": "speech understanding", "start_pos": 90, "end_pos": 110, "type": "TASK", "confidence": 0.7701571881771088}, {"text": "question answering", "start_pos": 113, "end_pos": 131, "type": "TASK", "confidence": 0.9162878394126892}, {"text": "advice taking", "start_pos": 139, "end_pos": 152, "type": "TASK", "confidence": 0.8148170411586761}]}, {"text": "Ge and  introduced an approach, SCISSOR, where the composition of meaning representations is guided by syntax.", "labels": [], "entities": []}, {"text": "First, a statistical parser is used to generate a semanticallyaugmented parse tree (SAPT), where each internal node includes both a syntactic and semantic label.", "labels": [], "entities": []}, {"text": "Once a SAPT is generated, an additional meaningcomposition process guided by the tree structure is used to translate it into a final formal meaning representation.", "labels": [], "entities": []}, {"text": "The performance of semantic parsing can be potentially improved by using discriminative reranking, which explores arbitrary global features.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 19, "end_pos": 35, "type": "TASK", "confidence": 0.7528429627418518}]}, {"text": "While reranking has benefited many tagging and parsing tasks) including semantic role labeling (), it has not yet been applied to semantic parsing.", "labels": [], "entities": [{"text": "tagging and parsing", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.8421374956766764}, {"text": "semantic role labeling", "start_pos": 72, "end_pos": 94, "type": "TASK", "confidence": 0.7287472287813822}, {"text": "semantic parsing", "start_pos": 130, "end_pos": 146, "type": "TASK", "confidence": 0.8280245959758759}]}, {"text": "In this paper, we investigate the effect of discriminative reranking to semantic parsing.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 72, "end_pos": 88, "type": "TASK", "confidence": 0.7868520021438599}]}, {"text": "We examine if the features used in reranking syntactic parses can be adapted for semantic parsing, more concretely, for reranking the top SAPTs from the baseline model SCISSOR.", "labels": [], "entities": [{"text": "reranking syntactic parses", "start_pos": 35, "end_pos": 61, "type": "TASK", "confidence": 0.7411491473515829}, {"text": "semantic parsing", "start_pos": 81, "end_pos": 97, "type": "TASK", "confidence": 0.7084434330463409}]}, {"text": "The syntactic features introduced by for syntactic parsing are extended with similar semantic features, based on the coupling of syntax and semantics.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 41, "end_pos": 58, "type": "TASK", "confidence": 0.7286410629749298}]}, {"text": "We present experimental results on two corpora: an interpreter for coaching instructions in robotic soccer (CLANG) and a natural-language database interface (GeoQuery).", "labels": [], "entities": []}, {"text": "The best reranking model significantly improves F-measure on CLANG from 82.3% to 85.1% (15.8% relative error reduction), however, it fails to show improvements on GEOQUERY.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9970160722732544}, {"text": "relative error reduction", "start_pos": 94, "end_pos": 118, "type": "METRIC", "confidence": 0.7173486550649008}, {"text": "GEOQUERY", "start_pos": 163, "end_pos": 171, "type": "DATASET", "confidence": 0.9074963927268982}]}], "datasetContent": [{"text": "Two corpora of natural language sentences paired with MRs were used in the reranking experiments.", "labels": [], "entities": [{"text": "MRs", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.8882317543029785}]}, {"text": "For CLANG, 300 pieces of coaching advice were randomly selected from the log files of the 2003 RoboCup Coach Competition.", "labels": [], "entities": []}, {"text": "Each formal instruction was translated into English by one of four annotators).", "labels": [], "entities": []}, {"text": "The average length of an natural language sentence in this corpus is 22.52 words.", "labels": [], "entities": []}, {"text": "For GEOQUERY, 250 questions were collected by asking undergraduate students to generate English queries for the given database.", "labels": [], "entities": [{"text": "GEOQUERY", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.75928795337677}]}, {"text": "Queries were then manually translated into logical form).", "labels": [], "entities": []}, {"text": "The average length of a natural language sentence in this corpus is 6.87 words.", "labels": [], "entities": []}, {"text": "We adopted standard 10-fold cross validation for evaluation: 9/10 of the whole dataset was used for training (training set), and 1/10 for testing (test set).", "labels": [], "entities": []}, {"text": "To train a reranking model on a training set, a separate \"internal\" 10-fold cross validation over the training set was employed to generate n-best SAPTs for each training example using a baseline learner, where each training set was again separated into 10 folds with 9/10 for training the baseline learner, and 1/10 for producing the nbest SAPTs for training the reranker.", "labels": [], "entities": []}, {"text": "Reranking models trained in this way ensure that the n-best SAPTs for each training example are not generated by a baseline model that has already seen that example.", "labels": [], "entities": []}, {"text": "To test a reranking model on a test set, a baseline model trained on a whole training set was used to generate n-best SAPTs for each test example, and then the reranking model trained with the above method was used to choose a best SAPT from the candidate SAPTs.", "labels": [], "entities": []}, {"text": "The performance of semantic parsing was measured in terms of precision (the percentage of completed MRs that were correct), recall (the percentage of all sentences whose MRs were correctly generated) and F-measure (the harmonic mean of precision and recall).", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 19, "end_pos": 35, "type": "TASK", "confidence": 0.7314782440662384}, {"text": "precision", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9992440938949585}, {"text": "recall", "start_pos": 124, "end_pos": 130, "type": "METRIC", "confidence": 0.9992815852165222}, {"text": "F-measure", "start_pos": 204, "end_pos": 213, "type": "METRIC", "confidence": 0.9983469247817993}, {"text": "precision", "start_pos": 236, "end_pos": 245, "type": "METRIC", "confidence": 0.9934172630310059}, {"text": "recall", "start_pos": 250, "end_pos": 256, "type": "METRIC", "confidence": 0.9700071215629578}]}, {"text": "Since even a single mistake in an MR could totally change the meaning of an example (e.g. having OUR in an MR instead of OP-PONENT in CLANG), no partial credit was given for examples with partially-correct SAPTs.", "labels": [], "entities": [{"text": "OUR", "start_pos": 97, "end_pos": 100, "type": "METRIC", "confidence": 0.9962943196296692}, {"text": "OP-PONENT", "start_pos": 121, "end_pos": 130, "type": "METRIC", "confidence": 0.952364444732666}]}, {"text": "Averaged perceptron, which has been successfully applied to several tagging and parsing reranking tasks: The performance of the baseline model SCISSOR+ compared with SCISSOR (with the best result in bold), where P = precision, R = recall, and F = F-measure.: Oracle recalls on CLANG and GEOQUERY as a function of number n of n-best SAPTs.", "labels": [], "entities": [{"text": "tagging and parsing reranking", "start_pos": 68, "end_pos": 97, "type": "TASK", "confidence": 0.6908020079135895}, {"text": "precision", "start_pos": 216, "end_pos": 225, "type": "METRIC", "confidence": 0.9941816926002502}, {"text": "recall", "start_pos": 231, "end_pos": 237, "type": "METRIC", "confidence": 0.9494038820266724}, {"text": "F-measure.", "start_pos": 247, "end_pos": 257, "type": "METRIC", "confidence": 0.9681456089019775}, {"text": "GEOQUERY", "start_pos": 287, "end_pos": 295, "type": "DATASET", "confidence": 0.8710064888000488}]}, {"text": "To choose the correct SAPT of a training example required for training the averaged perceptron, we selected a SAPT that results in the correct MR; if multiple such SAPTs exist, the one with the highest baseline score was chosen.", "labels": [], "entities": [{"text": "MR", "start_pos": 143, "end_pos": 145, "type": "METRIC", "confidence": 0.9640029668807983}]}, {"text": "Since no partial credit was awarded in evaluation, a training example was discarded if it had no correct SAPT.", "labels": [], "entities": [{"text": "SAPT", "start_pos": 105, "end_pos": 109, "type": "METRIC", "confidence": 0.9172960519790649}]}, {"text": "Rerankers were trained on the 50-best SAPTs provided by SCISSOR, and the number of perceptron iterations over the training examples was limited to 10.", "labels": [], "entities": [{"text": "SCISSOR", "start_pos": 56, "end_pos": 63, "type": "DATASET", "confidence": 0.8754342794418335}]}, {"text": "Typically, in order to avoid over-fitting, reranking features are filtered by removing those occurring in less than some minimal number of training examples.", "labels": [], "entities": []}, {"text": "We only removed features that never occurred in the training data since experiments with higher cut-offs failed to show any improvements.", "labels": [], "entities": []}, {"text": "shows the results comparing the baseline learner SCISSOR using both the back-off parameters in Ge and Mooney (2005) (SCISSOR) and the revised parameters in Section 2.2 (SCISSOR+).", "labels": [], "entities": []}, {"text": "As we expected, SCISSOR+ has better recall and worse precision than SCISSOR on both corpora due to the additional levels of back-off.", "labels": [], "entities": [{"text": "recall", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.9995887875556946}, {"text": "precision", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.9993839263916016}]}, {"text": "SCISSOR+ is used as the baseline model for all reranking experiments in the next section.", "labels": [], "entities": []}, {"text": "gives oracle recalls for CLANG and GEOQUERY where an oracle picks the correct parse from the n-best SAPTs if any of them are correct.", "labels": [], "entities": [{"text": "GEOQUERY", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.5845227837562561}]}, {"text": "Results are shown for increasing values of n.", "labels": [], "entities": []}, {"text": "The trends for CLANG and GEOQUERY are different: small values of n show significant improvements for CLANG, while a larger n is needed to improve results for GEOQUERY.", "labels": [], "entities": [{"text": "GEOQUERY", "start_pos": 25, "end_pos": 33, "type": "DATASET", "confidence": 0.8263435363769531}, {"text": "GEOQUERY", "start_pos": 158, "end_pos": 166, "type": "DATASET", "confidence": 0.8803042769432068}]}], "tableCaptions": [{"text": " Table 2: The performance of the baseline model SCISSOR+ compared with SCISSOR (with the best result in  bold), where P = precision, R = recall, and F = F-measure.", "labels": [], "entities": [{"text": "precision", "start_pos": 122, "end_pos": 131, "type": "METRIC", "confidence": 0.9953691363334656}, {"text": "recall", "start_pos": 137, "end_pos": 143, "type": "METRIC", "confidence": 0.953669011592865}, {"text": "F", "start_pos": 149, "end_pos": 150, "type": "METRIC", "confidence": 0.968858003616333}, {"text": "F-measure", "start_pos": 153, "end_pos": 162, "type": "METRIC", "confidence": 0.9327812194824219}]}, {"text": " Table 3: Oracle recalls on CLANG and GEOQUERY as a function of number n of n-best SAPTs.", "labels": [], "entities": [{"text": "GEOQUERY", "start_pos": 38, "end_pos": 46, "type": "DATASET", "confidence": 0.6163510680198669}]}, {"text": " Table 4: Reranking results on CLANG and GEOQUERY using different feature sets derived directly from  SAPTs (with the best results in bold and relative error reduction in parentheses). The reranking model  SYN uses the syntactic feature set in Section 3.1, SEM 1 uses the semantic feature set in Section 3.2.1, and  SYN+SEM 1 uses both.", "labels": [], "entities": [{"text": "GEOQUERY", "start_pos": 41, "end_pos": 49, "type": "DATASET", "confidence": 0.896144688129425}, {"text": "relative error reduction", "start_pos": 143, "end_pos": 167, "type": "METRIC", "confidence": 0.685420423746109}]}, {"text": " Table 5: Reranking results on CLANG and GEOQUERY comparing semantic features derived directly from  SAPTs, and semantic features from trees with purely-syntactic nodes removed. The symbol SEM 1 and SEM 2  refer to the semantic feature sets in Section 3.2.1 and 3.2.1 respectively, and SYN refers to the syntactic  feature set in Section 3.1.", "labels": [], "entities": []}]}