{"title": [{"text": "Discriminative Pruning of Language Models for Chinese Word Segmentation", "labels": [], "entities": [{"text": "Chinese Word Segmentation", "start_pos": 46, "end_pos": 71, "type": "TASK", "confidence": 0.5994818806648254}]}], "abstractContent": [{"text": "This paper presents a discriminative pruning method of n-gram language model for Chinese word segmentation.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 81, "end_pos": 106, "type": "TASK", "confidence": 0.6339757939179739}]}, {"text": "To reduce the size of the language model that is used in a Chinese word segmenta-tion system, importance of each bigram is computed in terms of discriminative pruning criterion that is related to the performance loss caused by pruning the bi-gram.", "labels": [], "entities": []}, {"text": "Then we propose a step-by-step growing algorithm to build the language model of desired size.", "labels": [], "entities": []}, {"text": "Experimental results show that the discriminative pruning method leads to a much smaller model compared with the model pruned using the state-of-the-art method.", "labels": [], "entities": []}, {"text": "At the same Chinese word segmentation F-measure, the number of bigrams in the model can be reduced by up to 90%.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.5604853630065918}]}, {"text": "Correlation between language model perplexity and word segmentation performance is also discussed.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.7273649722337723}]}], "introductionContent": [{"text": "Chinese word segmentation is the initial stage of many Chinese language processing tasks, and has received a lot of attention in the literature ().", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.5832534929116567}, {"text": "Chinese language processing tasks", "start_pos": 55, "end_pos": 88, "type": "TASK", "confidence": 0.6986758783459663}]}, {"text": "In, an approach based on source-channel model for Chinese word segmentation was proposed.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 50, "end_pos": 75, "type": "TASK", "confidence": 0.6213831603527069}]}, {"text": "further developed it to a linear mixture model.", "labels": [], "entities": []}, {"text": "In these statistical models, language models are essential for word segmentation disambiguation.", "labels": [], "entities": [{"text": "word segmentation disambiguation", "start_pos": 63, "end_pos": 95, "type": "TASK", "confidence": 0.8470707337061564}]}, {"text": "However, an uncompressed language model is usually too large for practical use since all realistic applications have memory constraints.", "labels": [], "entities": []}, {"text": "Therefore, language model pruning techniques are used to produce smaller models.", "labels": [], "entities": []}, {"text": "Pruning a language model is to eliminate a number of parameters explicitly stored in it, according to some pruning criteria.", "labels": [], "entities": []}, {"text": "The goal of research for language model pruning is to find criteria or methods, using which the model size could be reduced effectively, while the performance loss is kept as small as possible.", "labels": [], "entities": [{"text": "language model pruning", "start_pos": 25, "end_pos": 47, "type": "TASK", "confidence": 0.6403517226378123}]}, {"text": "A few criteria have been presented for language model pruning, including count cut-off, weighted difference factor, KullbackLeibler distance, rank and entropy ().", "labels": [], "entities": [{"text": "weighted difference factor", "start_pos": 88, "end_pos": 114, "type": "METRIC", "confidence": 0.758724570274353}]}, {"text": "These criteria are general for language model pruning, and are not optimized according to the performance of language model in specific tasks.", "labels": [], "entities": []}, {"text": "In recent years, discriminative training has been introduced to natural language processing applications such as parsing), machine translation and language model building ().", "labels": [], "entities": [{"text": "parsing", "start_pos": 113, "end_pos": 120, "type": "TASK", "confidence": 0.9731069207191467}, {"text": "machine translation", "start_pos": 123, "end_pos": 142, "type": "TASK", "confidence": 0.8473661839962006}, {"text": "language model building", "start_pos": 147, "end_pos": 170, "type": "TASK", "confidence": 0.6118552883466085}]}, {"text": "To the best of our knowledge, it has not been applied to language model pruning.", "labels": [], "entities": []}, {"text": "In this paper, we propose a discriminative pruning method of n-gram language model for Chinese word segmentation.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 87, "end_pos": 112, "type": "TASK", "confidence": 0.6160879631837209}]}, {"text": "It differentiates from the previous pruning approaches in two respects.", "labels": [], "entities": []}, {"text": "First, the pruning criterion is based on performance variation of word segmentation.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 66, "end_pos": 83, "type": "TASK", "confidence": 0.723112091422081}]}, {"text": "Second, the model of desired size is achieved by adding valuable bigrams to abase model, instead of by pruning bigrams from an unpruned model.", "labels": [], "entities": []}, {"text": "We define a misclassification function that approximately represents the likelihood that a sentence will be incorrectly segmented.", "labels": [], "entities": []}, {"text": "The variation value of the misclassification function caused by adding a parameter to the base model is used as the criterion for model pruning.", "labels": [], "entities": []}, {"text": "We also suggest a step-by-step growing algorithm that can generate models of any reasonably desired size.", "labels": [], "entities": []}, {"text": "We take the pruning method based on Kullback-Leibler distance as the baseline.", "labels": [], "entities": []}, {"text": "Experimental results show that our method outperforms the baseline significantly with small model size.", "labels": [], "entities": []}, {"text": "With the F-Measure of 96.33%, number of bigrams decreases by up to 90%.", "labels": [], "entities": [{"text": "F-Measure", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.9975171089172363}, {"text": "number", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9953528642654419}]}, {"text": "In addition, by combining the discriminative pruning method with the baseline method, we obtain models that achieve better performance for any model size.", "labels": [], "entities": []}, {"text": "Correlation between language model perplexity and system performance is also discussed.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 briefly discusses the related work on language model pruning.", "labels": [], "entities": []}, {"text": "Section 3 proposes our discriminative pruning method for Chinese word segmentation.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 57, "end_pos": 82, "type": "TASK", "confidence": 0.6058093706766764}]}, {"text": "Section 4 describes the experimental settings and results.", "labels": [], "entities": []}, {"text": "Result analysis and discussions are also presented in this section.", "labels": [], "entities": [{"text": "Result analysis", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.723601445555687}]}, {"text": "We draw the conclusions in section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "The training corpus comes from People's daily 2000, containing about 25 million Chinese characters.", "labels": [], "entities": [{"text": "People's daily 2000", "start_pos": 31, "end_pos": 50, "type": "DATASET", "confidence": 0.9808560907840729}]}, {"text": "It is manually segmented into word sequences, according to the word segmentation specification of Peking University ( ).", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 63, "end_pos": 80, "type": "TASK", "confidence": 0.7096819877624512}, {"text": "Peking University", "start_pos": 98, "end_pos": 115, "type": "DATASET", "confidence": 0.9104444682598114}]}, {"text": "The testing text that is provided by Peking University comes from the second international Chinese word segmentation bakeoff organized by SIGHAN.", "labels": [], "entities": [{"text": "word segmentation bakeoff", "start_pos": 99, "end_pos": 124, "type": "TASK", "confidence": 0.7788762350877126}]}, {"text": "The testing text is apart of People's daily 2001, consisting of about 170K Chinese characters.", "labels": [], "entities": [{"text": "People's daily 2001", "start_pos": 29, "end_pos": 48, "type": "DATASET", "confidence": 0.9826855659484863}]}, {"text": "The vocabulary is automatically extracted from the training corpus, and the words occurring only once are removed.", "labels": [], "entities": []}, {"text": "Finally, about 67K words are included in the vocabulary.", "labels": [], "entities": []}, {"text": "The fullbigram model and the unigram model are trained by CMU language model toolkit.", "labels": [], "entities": []}, {"text": "Without any count cut-off, the full-bigram model contains about 2 million bigrams.", "labels": [], "entities": []}, {"text": "The word segmentation system is developed based on a source-channel model similar to that described in (.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.725346028804779}]}, {"text": "Viterbi algorithm is applied to find the best word segmentation path.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.7065424770116806}]}, {"text": "The language models builtin our experiments are evaluated by two metrics.", "labels": [], "entities": []}, {"text": "One is F-Measure of the word segmentation result; the other is language model perplexity.", "labels": [], "entities": [{"text": "F-Measure", "start_pos": 7, "end_pos": 16, "type": "METRIC", "confidence": 0.9882839918136597}, {"text": "word segmentation", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.6786612123250961}]}, {"text": "For F-Measure evaluation, we firstly segment the raw testing text using the model to be evaluated.", "labels": [], "entities": [{"text": "F-Measure evaluation", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.7096385508775711}]}, {"text": "Then, the segmented result is evaluated by comparing with the gold standard set.", "labels": [], "entities": [{"text": "gold standard set", "start_pos": 62, "end_pos": 79, "type": "DATASET", "confidence": 0.8940645058949789}]}, {"text": "The evaluation tool is also from the word segmentation bakeoff.", "labels": [], "entities": []}, {"text": "F-Measure is calculated as: 1.", "labels": [], "entities": [{"text": "F-Measure", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9902819395065308}]}, {"text": "Given step size s; 2.", "labels": [], "entities": []}, {"text": "Set the base model to be the unigram model; 3.", "labels": [], "entities": []}, {"text": "Segment corpus with full-bigram model; 4.", "labels": [], "entities": []}, {"text": "Segment corpus with base model; 5.", "labels": [], "entities": []}, {"text": "Compute \"importance\" of each bigram included in the full-bigram model but excluded from the base model; 6.", "labels": [], "entities": []}, {"text": "Sort the bigrams according to their \"importance\"; 7.", "labels": [], "entities": []}, {"text": "Add s bigrams with the biggest \"importance\" to the base model; 8.", "labels": [], "entities": []}, {"text": "Re-compute backoff coefficients in the base model; 9.", "labels": [], "entities": []}, {"text": "If the base model is still smaller than the desired size, go to step 4; otherwise, stop.", "labels": [], "entities": []}, {"text": "For perplexity evaluation, the language model to be evaluated is used to provide the bigram probabilities for each word in the testing text.", "labels": [], "entities": [{"text": "perplexity evaluation", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.8071214854717255}]}, {"text": "The perplexity is the mean logarithm probability as shown in equation:", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1, where the F-Measure is  96.33%. The last column shows the relative  model sizes with respect to the KLD pruned  model. It shows that with the F-Measure of  96.33%, number of bigrams decreases by up to  90%.", "labels": [], "entities": [{"text": "F-Measure", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9963585734367371}, {"text": "KLD pruned  model", "start_pos": 110, "end_pos": 127, "type": "DATASET", "confidence": 0.8109690149625143}, {"text": "F-Measure", "start_pos": 152, "end_pos": 161, "type": "METRIC", "confidence": 0.978008508682251}, {"text": "number", "start_pos": 174, "end_pos": 180, "type": "METRIC", "confidence": 0.976934015750885}]}]}