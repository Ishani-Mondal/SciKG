{"title": [{"text": "Guessing Parts-of-Speech of Unknown Words Using Global Information", "labels": [], "entities": [{"text": "Guessing Parts-of-Speech of Unknown Words", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.8951300263404847}]}], "abstractContent": [{"text": "In this paper, we present a method for guessing POS tags of unknown words using local and global information.", "labels": [], "entities": [{"text": "POS tags of unknown words", "start_pos": 48, "end_pos": 73, "type": "TASK", "confidence": 0.7182203888893127}]}, {"text": "Although many existing methods use only local information (i.e. limited window size or intra-sentential features), global information (extra-sentential features) provides valuable clues for predicting POS tags of unknown words.", "labels": [], "entities": [{"text": "predicting POS tags of unknown words", "start_pos": 190, "end_pos": 226, "type": "TASK", "confidence": 0.8302888373533884}]}, {"text": "We propose a probabilistic model for POS guessing of unknown words using global information as well as local information, and estimate its parameters using Gibbs sampling.", "labels": [], "entities": [{"text": "POS guessing of unknown words", "start_pos": 37, "end_pos": 66, "type": "TASK", "confidence": 0.9424768924713135}]}, {"text": "We also attempt to apply the model to semi-supervised learning, and conduct experiments on multiple corpora.", "labels": [], "entities": []}], "introductionContent": [{"text": "Part-of-speech (POS) tagging is a fundamental language analysis task.", "labels": [], "entities": [{"text": "Part-of-speech (POS) tagging", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.5963188171386719}, {"text": "language analysis task", "start_pos": 46, "end_pos": 68, "type": "TASK", "confidence": 0.7845751742521921}]}, {"text": "In POS tagging, we frequently encounter words that do not exist in training data.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 3, "end_pos": 14, "type": "TASK", "confidence": 0.8675510883331299}]}, {"text": "Such words are called unknown words.", "labels": [], "entities": []}, {"text": "They are usually handled by an exceptional process in POS tagging, because the tagging system does not have information about the words.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 54, "end_pos": 65, "type": "TASK", "confidence": 0.7372185587882996}]}, {"text": "Guessing the POS tags of such unknown words is a difficult task.", "labels": [], "entities": [{"text": "Guessing the POS tags", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8136007189750671}]}, {"text": "But it is an important issue both for conducting POS tagging accurately and for creating word dictionaries automatically or semiautomatically.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 49, "end_pos": 60, "type": "TASK", "confidence": 0.9315577447414398}]}, {"text": "There have been many studies on POS guessing of unknown words).", "labels": [], "entities": [{"text": "POS guessing of unknown words", "start_pos": 32, "end_pos": 61, "type": "TASK", "confidence": 0.9360866904258728}]}, {"text": "In most of these previous works, POS tags of unknown words were predicted using only local information, such as lexical forms and POS tags of surrounding words or word-internal features (e.g. suffixes and character types) of the unknown words.", "labels": [], "entities": []}, {"text": "However, this approach has limitations in available information.", "labels": [], "entities": []}, {"text": "For example, common nouns and proper nouns are sometimes difficult to distinguish with only the information of a single occurrence because their syntactic functions are almost identical.", "labels": [], "entities": []}, {"text": "In English, proper nouns are capitalized and there is generally little ambiguity between common nouns and proper nouns.", "labels": [], "entities": []}, {"text": "In Chinese and Japanese, no such convention exists and the problem of the ambiguity is serious.", "labels": [], "entities": []}, {"text": "However, if an unknown word with the same lexical form appears in another part with informative local features (e.g. titles of persons), this will give useful clues for guessing the part-of-speech of the ambiguous one, because unknown words with the same lexical form usually have the same part-of-speech.", "labels": [], "entities": []}, {"text": "For another example, there is a part-of-speech named sahen-noun (verbal noun) in Japanese.", "labels": [], "entities": []}, {"text": "Verbal nouns behave as common nouns, except that they are used as verbs when they are followed by a verb \"suru\"; e.g., a verbal noun \"dokusho\" means \"reading\" and \"dokusho-suru\" is a verb meaning to \"read books\".", "labels": [], "entities": []}, {"text": "It is difficult to distinguish a verbal noun from a common noun if it is used as a noun.", "labels": [], "entities": []}, {"text": "However, it will be easy if we know that the word is followed by \"suru\" in another part in the document.", "labels": [], "entities": []}, {"text": "This issue was mentioned by as a problem of possibility-based POS tags.", "labels": [], "entities": [{"text": "POS tags", "start_pos": 62, "end_pos": 70, "type": "TASK", "confidence": 0.7031965553760529}]}, {"text": "A possibility-based POS tag is a POS tag that represents all the possible properties of the word (e.g., a verbal noun is used as a noun or a verb), rather than a property of each instance of the word.", "labels": [], "entities": []}, {"text": "For example, a sahennoun is actually a noun that can be used as a verb when it is followed by \"suru\".", "labels": [], "entities": []}, {"text": "This property cannot be confirmed without observing real usage of the word appearing with \"suru\".", "labels": [], "entities": []}, {"text": "Such POS tags may not be identified with only local information of one instance, because the property that each instance has is only one among all the possible properties.", "labels": [], "entities": []}, {"text": "To cope with these issues, we propose a method that uses global information as well as local information for guessing the parts-of-speech of unknown words.", "labels": [], "entities": [{"text": "guessing the parts-of-speech of unknown words", "start_pos": 109, "end_pos": 154, "type": "TASK", "confidence": 0.8543997009595236}]}, {"text": "With this method, all the occurrences of the unknown words in a document 1 are taken into consideration at once, rather than that each occurrence of the words is processed separately.", "labels": [], "entities": []}, {"text": "Thus, the method models the whole document and finds a set of parts-of-speech by maximizing its conditional joint probability given the document, rather than independently maximizing the probability of each part-of-speech given each sentence.", "labels": [], "entities": []}, {"text": "Global information is known to be useful in other NLP tasks, especially in the named entity recognition task, and several studies successfully used global features (.", "labels": [], "entities": [{"text": "named entity recognition task", "start_pos": 79, "end_pos": 108, "type": "TASK", "confidence": 0.6938082128763199}]}, {"text": "One potential advantage of our method is its ability to incorporate unlabeled data.", "labels": [], "entities": []}, {"text": "Global features can be increased by simply adding unlabeled data into the test data.", "labels": [], "entities": []}, {"text": "Models in which the whole document is taken into consideration need a lot of computation compared to models with only local features.", "labels": [], "entities": []}, {"text": "They also cannot process input data one-by-one.", "labels": [], "entities": []}, {"text": "Instead, the entire document has to be read before processing.", "labels": [], "entities": []}, {"text": "We adopt Gibbs sampling in order to compute the models efficiently, and these models are suitable for offline use such as creating dictionaries from raw text where real-time processing is not necessary but high-accuracy is needed to reduce human labor required for revising automatically analyzed data.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows: Section 2 describes a method for POS guessing of unknown words which utilizes global information.", "labels": [], "entities": [{"text": "POS guessing of unknown words", "start_pos": 81, "end_pos": 110, "type": "TASK", "confidence": 0.9450146794319153}]}, {"text": "Section 3 shows experimental results on multiple corpora.", "labels": [], "entities": []}, {"text": "Section 4 discusses related work, and Section 5 gives conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "In the table, local, local+global and local+global w/ unlabeled indicate that the results were obtained using only local information, local and global information, and local and global information with the extra unlabeled data, respectively.", "labels": [], "entities": []}, {"text": "The results using only local information were obtained by choosing POS noun-proper noun-person name-family name +32 noun-proper noun-place name +28 noun-proper noun-organization name +17 noun-proper noun-person name-first name noun-proper noun-place name-country name NNU (unit-of-measurement noun) which maximize the probabilities of the local model: The table shows the accuracies, the numbers of errors, the p-values of McNemar's test against the results using only local information, and the numbers of non-unique unknown words in the test data.", "labels": [], "entities": [{"text": "noun-proper noun-person name-first name noun-proper noun-place name-country name NNU", "start_pos": 187, "end_pos": 271, "type": "TASK", "confidence": 0.6206953128178915}, {"text": "McNemar's test", "start_pos": 423, "end_pos": 437, "type": "DATASET", "confidence": 0.8154430190722147}]}, {"text": "On an Opteron 250 processor with 8GB of RAM, model parameter estimation and decoding without unlabeled data for the eight corpora took 117 minutes and 39 seconds in total, respectively.", "labels": [], "entities": [{"text": "Opteron 250 processor", "start_pos": 6, "end_pos": 27, "type": "DATASET", "confidence": 0.9272053837776184}, {"text": "model parameter estimation", "start_pos": 45, "end_pos": 71, "type": "TASK", "confidence": 0.6178235411643982}]}, {"text": "In the CTB, PFR, KUC, RWC and WSJ corpora, the accuracies were improved using global information (statistically significant at p < 0.05), compared to the accuracies obtained using only local information.", "labels": [], "entities": [{"text": "CTB", "start_pos": 7, "end_pos": 10, "type": "DATASET", "confidence": 0.9772522449493408}, {"text": "PFR", "start_pos": 12, "end_pos": 15, "type": "DATASET", "confidence": 0.8742615580558777}, {"text": "KUC", "start_pos": 17, "end_pos": 20, "type": "DATASET", "confidence": 0.7911892533302307}, {"text": "RWC", "start_pos": 22, "end_pos": 25, "type": "DATASET", "confidence": 0.7167858481407166}, {"text": "WSJ corpora", "start_pos": 30, "end_pos": 41, "type": "DATASET", "confidence": 0.8428598046302795}, {"text": "accuracies", "start_pos": 47, "end_pos": 57, "type": "METRIC", "confidence": 0.9941233992576599}]}, {"text": "The increases of the accuracies on the English corpora (the GEN and SUS corpora) were small.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 21, "end_pos": 31, "type": "METRIC", "confidence": 0.9977686405181885}, {"text": "English corpora", "start_pos": 39, "end_pos": 54, "type": "DATASET", "confidence": 0.9488232433795929}, {"text": "GEN and SUS corpora", "start_pos": 60, "end_pos": 79, "type": "DATASET", "confidence": 0.6904096379876137}]}, {"text": "shows the increased/decreased number of correctly tagged words using global information in the PFR, RWC and SUS corpora.", "labels": [], "entities": [{"text": "PFR", "start_pos": 95, "end_pos": 98, "type": "DATASET", "confidence": 0.9322607517242432}, {"text": "RWC and SUS corpora", "start_pos": 100, "end_pos": 119, "type": "DATASET", "confidence": 0.6220693662762642}]}, {"text": "In the PFR (Chinese) and RWC (Japanese) corpora, many proper nouns were correctly tagged using global information.", "labels": [], "entities": [{"text": "PFR (Chinese) and RWC (Japanese) corpora", "start_pos": 7, "end_pos": 47, "type": "DATASET", "confidence": 0.6135859996080398}]}, {"text": "In Chinese and Japanese, proper nouns are not capitalized, therefore proper nouns are difficult to distinguish from common nouns with only local information.", "labels": [], "entities": []}, {"text": "One reason that only the small increases were obtained with global information in the English corpora seems to be the low ambiguities of proper nouns.", "labels": [], "entities": []}, {"text": "Many verbal nouns in PFR and a few sahen-nouns (Japanese verbal nouns) in RWC, which suffer from the problem of possibility-based POS tags, were also correctly tagged using global information.", "labels": [], "entities": [{"text": "PFR", "start_pos": 21, "end_pos": 24, "type": "DATASET", "confidence": 0.8085936903953552}, {"text": "RWC", "start_pos": 74, "end_pos": 77, "type": "DATASET", "confidence": 0.918609619140625}]}, {"text": "When the unlabeled data was used, the number of nonunique words in the test data increased.", "labels": [], "entities": []}, {"text": "Compared with the case without the unlabeled data, the accu-    Since our method uses Gibbs sampling in the training and the testing phases, the results are affected by the sequences of random numbers used in the sampling.", "labels": [], "entities": [{"text": "accu-", "start_pos": 55, "end_pos": 60, "type": "METRIC", "confidence": 0.977058082818985}]}, {"text": "In order to investigate the influence, we conduct 10 trials with different sequences of pseudo random numbers.", "labels": [], "entities": []}, {"text": "We also conduct experiments using simulated annealing in decoding, as conducted by for information extraction.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 87, "end_pos": 109, "type": "TASK", "confidence": 0.8356728851795197}]}, {"text": "We increase inverse temperature \u03b2 in Equation (1) from \u03b2 = 1 to \u03b2 \u2248 \u221e with the linear cooling schedule.", "labels": [], "entities": [{"text": "inverse temperature \u03b2", "start_pos": 12, "end_pos": 33, "type": "METRIC", "confidence": 0.8497175176938375}, {"text": "Equation", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9748097062110901}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "The table shows the mean values and the standard deviations of the accuracies for the 10 trials, and Marginal and S.A. mean that decoding is conducted using Equation (11) and simulated annealing respectively.", "labels": [], "entities": [{"text": "Marginal", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9904278516769409}]}, {"text": "The variances caused by random numbers and the differences of the accuracies between Marginal and S.A. are relatively small.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 66, "end_pos": 76, "type": "METRIC", "confidence": 0.9916589856147766}, {"text": "Marginal", "start_pos": 85, "end_pos": 93, "type": "DATASET", "confidence": 0.7298588752746582}]}], "tableCaptions": [{"text": " Table 1: Statistical Information of Corpora", "labels": [], "entities": []}, {"text": " Table 5: Results of Multiple Trials and Compari- son to Simulated Annealing", "labels": [], "entities": [{"text": "Simulated Annealing", "start_pos": 57, "end_pos": 76, "type": "TASK", "confidence": 0.9427258670330048}]}]}