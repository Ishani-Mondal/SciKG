{"title": [{"text": "Incorporating speech recognition confidence into discriminative named entity recognition of speech data", "labels": [], "entities": [{"text": "Incorporating speech recognition", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8108550707499186}]}], "abstractContent": [{"text": "This paper proposes a named entity recognition (NER) method for speech recognition results that uses confidence on automatic speech recognition (ASR) as a feature.", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 22, "end_pos": 52, "type": "TASK", "confidence": 0.8003620306650797}, {"text": "speech recognition", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.8002804219722748}, {"text": "automatic speech recognition (ASR)", "start_pos": 115, "end_pos": 149, "type": "TASK", "confidence": 0.6918558230002722}]}, {"text": "The ASR confidence feature indicates whether each word has been correctly recognized.", "labels": [], "entities": [{"text": "ASR confidence feature", "start_pos": 4, "end_pos": 26, "type": "METRIC", "confidence": 0.9022802114486694}]}, {"text": "The NER model is trained using ASR results with named entity (NE) labels as well as the corresponding transcriptions with NE labels.", "labels": [], "entities": [{"text": "ASR", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.9054468274116516}]}, {"text": "In experiments using support vector machines (SVMs) and speech data from Japanese newspaper articles, the proposed method outperformed a simple application of text-based NER to ASR results in NER F-measure by improving precision.", "labels": [], "entities": [{"text": "ASR", "start_pos": 177, "end_pos": 180, "type": "TASK", "confidence": 0.8608090281486511}, {"text": "NER F-measure", "start_pos": 192, "end_pos": 205, "type": "TASK", "confidence": 0.5211162716150284}, {"text": "precision", "start_pos": 219, "end_pos": 228, "type": "METRIC", "confidence": 0.9981794357299805}]}, {"text": "These results show that the proposed method is effective in NER for noisy inputs.", "labels": [], "entities": [{"text": "NER", "start_pos": 60, "end_pos": 63, "type": "TASK", "confidence": 0.9870036244392395}]}], "introductionContent": [{"text": "As network bandwidths and storage capacities continue to grow, a large volume of speech data including broadcast news and PodCasts is becoming available.", "labels": [], "entities": []}, {"text": "These data are important information sources as well as such text data as newspaper articles and WWW pages.", "labels": [], "entities": []}, {"text": "Speech data as information sources are attracting a great deal of interest, such as DARPA's global autonomous language exploitation (GALE) program.", "labels": [], "entities": [{"text": "global autonomous language exploitation (GALE)", "start_pos": 92, "end_pos": 138, "type": "TASK", "confidence": 0.7160638911383492}]}, {"text": "We also aim to use them for information extraction, question answering, and indexing.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 28, "end_pos": 50, "type": "TASK", "confidence": 0.8428155481815338}, {"text": "question answering", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.916236937046051}, {"text": "indexing", "start_pos": 76, "end_pos": 84, "type": "TASK", "confidence": 0.9678353071212769}]}, {"text": "Named entity recognition (NER) is a key technique for IE and other natural language processing tasks.", "labels": [], "entities": [{"text": "Named entity recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7999407947063446}, {"text": "IE", "start_pos": 54, "end_pos": 56, "type": "TASK", "confidence": 0.9844982028007507}]}, {"text": "Named entities (NEs) are the proper expressions for things such as peoples' names, locations' names, and dates, and NER identifies those expressions and their categories.", "labels": [], "entities": []}, {"text": "Unlike text data, speech data introduce automatic speech recognition (ASR) error problems to NER.", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 40, "end_pos": 74, "type": "TASK", "confidence": 0.7604164183139801}]}, {"text": "Although improvements to ASR are needed, developing a robust NER for noisy word sequences is also important.", "labels": [], "entities": [{"text": "ASR", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.973868727684021}]}, {"text": "In this paper, we focus on the NER of ASR results and discuss the suppression of ASR error problems in NER.", "labels": [], "entities": [{"text": "NER", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.9793329834938049}, {"text": "ASR", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.6570233702659607}, {"text": "ASR", "start_pos": 81, "end_pos": 84, "type": "TASK", "confidence": 0.9027481079101562}, {"text": "NER", "start_pos": 103, "end_pos": 106, "type": "TASK", "confidence": 0.9048768281936646}]}, {"text": "Most previous studies of the NER of speech data used generative models such as hidden Markov models (HMMs)).", "labels": [], "entities": [{"text": "NER", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.9878342151641846}]}, {"text": "On the other hand, in text-based NER, better results are obtained using discriminative schemes such as maximum entropy (ME) models, support vector machines (SVMs) (), and conditional random fields (CRFs).", "labels": [], "entities": []}, {"text": "applied a text-level ME-based NER to ASR results.", "labels": [], "entities": [{"text": "NER", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.8320177793502808}, {"text": "ASR", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.9488242268562317}]}, {"text": "These models have an advantage in utilizing various features, such as part-of-speech information, character types, and surrounding words, which maybe overlapped, while overlapping features are hard to use in HMM-based models.", "labels": [], "entities": []}, {"text": "To deal with ASR error problems in NER, proposed an HMMbased NER method that explicitly models ASR errors using ASR confidence and rejects erroneous word hypotheses in the ASR results.", "labels": [], "entities": [{"text": "ASR", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.9859362840652466}, {"text": "NER", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.9334938526153564}, {"text": "ASR", "start_pos": 95, "end_pos": 98, "type": "TASK", "confidence": 0.9609196782112122}]}, {"text": "Such rejection is especially effective when ASR accuracy is relatively low because many misrecognized words maybe extracted as NEs, which would decrease NER precision.", "labels": [], "entities": [{"text": "ASR", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.9871892333030701}, {"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9017082452774048}, {"text": "precision", "start_pos": 157, "end_pos": 166, "type": "METRIC", "confidence": 0.9027290344238281}]}, {"text": "Motivated by these issues, we extended their approach to discriminative models and propose an NER method that deals with ASR errors as fea-tures.", "labels": [], "entities": [{"text": "ASR", "start_pos": 121, "end_pos": 124, "type": "TASK", "confidence": 0.973639726638794}]}, {"text": "We use NE-labeled ASR results for training to incorporate the features into the NER model as well as the corresponding transcriptions with NE labels.", "labels": [], "entities": []}, {"text": "In testing, ASR errors are identified by ASR confidence scores and are used for the NER.", "labels": [], "entities": [{"text": "ASR", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.9114781022071838}, {"text": "ASR confidence scores", "start_pos": 41, "end_pos": 62, "type": "METRIC", "confidence": 0.840666393438975}, {"text": "NER", "start_pos": 84, "end_pos": 87, "type": "TASK", "confidence": 0.6968743205070496}]}, {"text": "In experiments using SVM-based NER and speech data from Japanese newspaper articles, the proposed method increased the NER F-measure, especially in precision, compared to simply applying text-based NER to the ASR results.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 123, "end_pos": 132, "type": "METRIC", "confidence": 0.5637463331222534}, {"text": "precision", "start_pos": 148, "end_pos": 157, "type": "METRIC", "confidence": 0.9994983673095703}]}], "datasetContent": [{"text": "We conducted the following experiments related to the NER of speech data to investigate the performance of the proposed method.", "labels": [], "entities": [{"text": "NER", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.9588319659233093}]}, {"text": "Evaluation was based on an averaged NER Fmeasure, which is the harmonic mean of NER precision and recall: NER precision = # correctly recognized NEs # recognized NEs NER recall = # correctly recognized NEs # NEs in original text . A recognized NE was accepted as correct if and only if it appeared in the same position as its reference NE through alignment, in addition to having the correct NE surface and category, because the same NEs might appear more than once.", "labels": [], "entities": [{"text": "Fmeasure", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.6730095148086548}, {"text": "precision", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.959490954875946}, {"text": "recall", "start_pos": 98, "end_pos": 104, "type": "METRIC", "confidence": 0.9982820749282837}, {"text": "precision", "start_pos": 110, "end_pos": 119, "type": "METRIC", "confidence": 0.6218203902244568}, {"text": "recall", "start_pos": 170, "end_pos": 176, "type": "METRIC", "confidence": 0.9426710605621338}]}, {"text": "Comparisons of NE surfaces did not include differences in word segmentation because of the segmentation ambiguity in Japanese.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 58, "end_pos": 75, "type": "TASK", "confidence": 0.7069199830293655}]}, {"text": "Note that NER recall with ASR results could not exceed the rate of the remaining NEs after ASR (about 82%) because NEs containing ASR errors were always lost.", "labels": [], "entities": [{"text": "NER", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.9386603832244873}, {"text": "recall", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.9358639121055603}]}, {"text": "In addition, we also evaluated the NER performance in NER precision and recall with NERlevel rejection using the procedure in Section 3.4, by modifying the non-NE class scores using offset value to .", "labels": [], "entities": [{"text": "precision", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9393090009689331}, {"text": "recall", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.9978715181350708}]}], "tableCaptions": [{"text": " Table 4: NER results in averaged NER F-measure, precision, and recall without considering NER-level  rejection (t o = 0). ASR word accuracy was 79.45%, and 82.00% of NEs remained in ASR results.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.7290632724761963}, {"text": "precision", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9987467527389526}, {"text": "recall", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9993219375610352}, {"text": "ASR word", "start_pos": 123, "end_pos": 131, "type": "TASK", "confidence": 0.8032282590866089}, {"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.853922963142395}]}]}