{"title": [{"text": "Tree-to-String Alignment Template for Statistical Machine Translation", "labels": [], "entities": [{"text": "Tree-to-String Alignment Template", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7310232520103455}, {"text": "Statistical Machine Translation", "start_pos": 38, "end_pos": 69, "type": "TASK", "confidence": 0.832794984181722}]}], "abstractContent": [{"text": "We present a novel translation model based on tree-to-string alignment template (TAT) which describes the alignment between a source parse tree and a target string.", "labels": [], "entities": []}, {"text": "A TAT is capable of generating both terminals and non-terminals and performing reordering at both low and high levels.", "labels": [], "entities": []}, {"text": "The model is linguistically syntax-based because TATs are extracted automatically from word-aligned, source side parsed parallel texts.", "labels": [], "entities": []}, {"text": "To translate a source sentence, we first employ a parser to produce a source parse tree and then apply TATs to transform the tree into a target string.", "labels": [], "entities": [{"text": "TATs", "start_pos": 103, "end_pos": 107, "type": "METRIC", "confidence": 0.984012246131897}]}, {"text": "Our experiments show that the TAT-based model significantly outper-forms Pharaoh, a state-of-the-art decoder for phrase-based models.", "labels": [], "entities": [{"text": "TAT-based", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.8028982877731323}]}], "introductionContent": [{"text": "Phrase-based translation models (), which go beyond the original IBM translation models 1 by modeling translations of phrases rather than individual words, have been suggested to be the state-of-theart in statistical machine translation by empirical evaluations.", "labels": [], "entities": [{"text": "Phrase-based translation", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.6896612644195557}, {"text": "statistical machine translation", "start_pos": 205, "end_pos": 236, "type": "TASK", "confidence": 0.63492285211881}]}, {"text": "In phrase-based models, phrases are usually strings of adjacent words instead of syntactic constituents, excelling at capturing local reordering and performing translations that are localized to The mathematical notation we use in this paper is taken from that paper: a source string f J 1 = f1, . .", "labels": [], "entities": []}, {"text": ", fJ is to be translated into a target string e I 1 = e1, . .", "labels": [], "entities": []}, {"text": ", eI . Here, I is the length of the target string, and J is the length of the source string.", "labels": [], "entities": []}, {"text": "substrings that are common enough to be observed on training data.", "labels": [], "entities": []}, {"text": "However, a key limitation of phrase-based models is that they fail to model reordering at the phrase level robustly.", "labels": [], "entities": []}, {"text": "Typically, phrase reordering is modeled in terms of offset positions at the word level), making little or no direct use of syntactic information.", "labels": [], "entities": [{"text": "phrase reordering", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.7979477047920227}]}, {"text": "Recent research on statistical machine translation has lead to the development of syntax-based models.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 19, "end_pos": 50, "type": "TASK", "confidence": 0.6772614320119222}]}, {"text": "proposes Inversion Transduction Grammars, treating translation as a process of parallel parsing of the source and target language via a synchronized grammar.", "labels": [], "entities": [{"text": "Inversion Transduction Grammars", "start_pos": 9, "end_pos": 40, "type": "TASK", "confidence": 0.8518953522046407}]}, {"text": "represent each production in parallel dependency tree as a finite transducer.", "labels": [], "entities": []}, {"text": "formalizes machine translation problem as synchronous parsing based on multitext grammars.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.8067325949668884}]}, {"text": "describe training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers.", "labels": [], "entities": []}, {"text": "presents a hierarchical phrasebased model that uses hierarchical phrase pairs, which are formally productions of asynchronous context-free grammar.", "labels": [], "entities": []}, {"text": "propose a syntax-based translation model based on a probabilistic synchronous dependency insert grammar, aversion of synchronous grammars defined on dependency trees.", "labels": [], "entities": []}, {"text": "All these approaches, though different in formalism, make use of synchronous grammars or tree-based transduction rules to model both source and target languages.", "labels": [], "entities": []}, {"text": "Another class of approaches make use of syntactic information in the target language alone, treating the translation problem as a parsing problem.", "labels": [], "entities": []}, {"text": "use a parser in the target language to train probabilities on a set of operations that transform a target parse tree into a source string.", "labels": [], "entities": []}, {"text": "Paying more attention to source language analysis, employ a source language dependency parser, a target language word segmentation component, and an unsupervised word alignment component to learn treelet translations from parallel corpus.", "labels": [], "entities": [{"text": "source language analysis", "start_pos": 25, "end_pos": 49, "type": "TASK", "confidence": 0.6430306633313497}, {"text": "target language word segmentation", "start_pos": 97, "end_pos": 130, "type": "TASK", "confidence": 0.6928089335560799}, {"text": "word alignment", "start_pos": 162, "end_pos": 176, "type": "TASK", "confidence": 0.7128146290779114}]}, {"text": "In this paper, we propose a statistical translation model based on tree-to-string alignment template which describes the alignment between a source parse tree and a target string.", "labels": [], "entities": [{"text": "statistical translation", "start_pos": 28, "end_pos": 51, "type": "TASK", "confidence": 0.6054748147726059}]}, {"text": "A TAT is capable of generating both terminals and non-terminals and performing reordering at both low and high levels.", "labels": [], "entities": []}, {"text": "The model is linguistically syntax-based because TATs are extracted automatically from word-aligned, source side parsed parallel texts.", "labels": [], "entities": []}, {"text": "To translate a source sentence, we first employ a parser to produce a source parse tree and then apply TATs to transform the tree into a target string.", "labels": [], "entities": [{"text": "TATs", "start_pos": 103, "end_pos": 107, "type": "METRIC", "confidence": 0.984012246131897}]}, {"text": "One advantage of our model is that TATs can be automatically acquired to capture linguistically motivated reordering at both low (word) and high (phrase, clause) levels.", "labels": [], "entities": []}, {"text": "In addition, the training of TAT-based model is less computationally expensive than tree-to-tree models.", "labels": [], "entities": []}, {"text": "Similarly to (), the tree-to-string alignment templates discussed in this paper are actually transformation rules.", "labels": [], "entities": []}, {"text": "The major difference is that we model the syntax of the source language instead of the target side.", "labels": [], "entities": []}, {"text": "As a result, the task of our decoder is to find the best target string while Galley's is to seek the most likely target tree.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments were on Chinese-to-English translation.", "labels": [], "entities": [{"text": "Chinese-to-English translation", "start_pos": 24, "end_pos": 54, "type": "TASK", "confidence": 0.6727910041809082}]}, {"text": "The training corpus consists of 31, 149 sentence pairs with 843, 256 Chinese words and 0.2178 \u00b1 0.0080) to train a trigram model with modified Kneser-Ney smoothing) on the 31, 149 English sentences.", "labels": [], "entities": []}, {"text": "We selected 571 short sentences from the 2002 NIST MT Evaluation test set as our development corpus, and used the 2005 NIST MT Evaluation test set as our test corpus.", "labels": [], "entities": [{"text": "NIST MT Evaluation test set", "start_pos": 46, "end_pos": 73, "type": "DATASET", "confidence": 0.9236536502838135}, {"text": "NIST MT Evaluation test set", "start_pos": 119, "end_pos": 146, "type": "DATASET", "confidence": 0.9344581723213196}]}, {"text": "We evaluated the translation quality using the BLEU metric (), as calculated by mteval-v11b.pl with its default setting except that we used case-sensitive matching of n-grams.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.9968164563179016}]}], "tableCaptions": [{"text": " Table 3: Feature weights obtained by minimum error rate training on the development corpus", "labels": [], "entities": []}]}