{"title": [{"text": "Reranking and Self-Training for Parser Adaptation", "labels": [], "entities": [{"text": "Reranking", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.915652334690094}, {"text": "Parser Adaptation", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.8477605581283569}]}], "abstractContent": [{"text": "Statistical parsers trained and tested on the Penn Wall Street Journal (WSJ) treebank have shown vast improvements over the last 10 years.", "labels": [], "entities": [{"text": "Penn Wall Street Journal (WSJ) treebank", "start_pos": 46, "end_pos": 85, "type": "DATASET", "confidence": 0.9739562794566154}]}, {"text": "Much of this improvement, however, is based upon an ever-increasing number of features to be trained on (typi-cally) the WSJ treebank data.", "labels": [], "entities": [{"text": "WSJ treebank data", "start_pos": 121, "end_pos": 138, "type": "DATASET", "confidence": 0.980431854724884}]}, {"text": "This has led to concern that such parsers maybe too finely tuned to this corpus at the expense of portability to other genres.", "labels": [], "entities": []}, {"text": "The standard \"Charniak parser\" checks in at a labeled precision-recall f-measure of 89.7% on the Penn WSJ test set, but only 82.9% on the test set from the Brown treebank corpus.", "labels": [], "entities": [{"text": "precision-recall f-measure", "start_pos": 54, "end_pos": 80, "type": "METRIC", "confidence": 0.9645229876041412}, {"text": "Penn WSJ test set", "start_pos": 97, "end_pos": 114, "type": "DATASET", "confidence": 0.9682937115430832}, {"text": "Brown treebank corpus", "start_pos": 156, "end_pos": 177, "type": "DATASET", "confidence": 0.9828583796819051}]}, {"text": "This paper should allay these fears.", "labels": [], "entities": []}, {"text": "In particular , we show that the reranking parser described in Charniak and Johnson (2005) improves performance of the parser on Brown to 85.2%.", "labels": [], "entities": [{"text": "Brown", "start_pos": 129, "end_pos": 134, "type": "DATASET", "confidence": 0.96250981092453}]}, {"text": "Furthermore, use of the self-training techniques described in (Mc-Closky et al., 2006) raise this to 87.8% (an error reduction of 28%) again without any use of labeled Brown data.", "labels": [], "entities": []}, {"text": "This is remarkable since training the parser and reranker on labeled Brown data achieves only 88.4%.", "labels": [], "entities": []}], "introductionContent": [{"text": "Modern statistical parsers require treebanks to train their parameters, but their performance declines when one parses genres more distant from the training data's domain.", "labels": [], "entities": []}, {"text": "Furthermore, the treebanks required to train said parsers are expensive and difficult to produce.", "labels": [], "entities": []}, {"text": "Naturally, one of the goals of statistical parsing is to produce a broad-coverage parser which is relatively insensitive to textual domain.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.8240681290626526}]}, {"text": "But the lack of corpora has led to a situation where much of the current work on parsing is performed on a single domain using training data from that domain -the Wall Street Journal (WSJ) section of the Penn Treebank (.", "labels": [], "entities": [{"text": "parsing", "start_pos": 81, "end_pos": 88, "type": "TASK", "confidence": 0.9656673073768616}, {"text": "Wall Street Journal (WSJ) section of the Penn Treebank", "start_pos": 163, "end_pos": 217, "type": "DATASET", "confidence": 0.9281429160724987}]}, {"text": "Given the aforementioned costs, it is unlikely that many significant treebanks will be created for new genres.", "labels": [], "entities": []}, {"text": "Thus, parser adaptation attempts to leverage existing labeled data from one domain and create a parser capable of parsing a different domain.", "labels": [], "entities": [{"text": "parser adaptation", "start_pos": 6, "end_pos": 23, "type": "TASK", "confidence": 0.9508867263793945}]}, {"text": "Unfortunately, the state of the art in parser portability (i.e. using a parser trained on one domain to parse a different domain) is not good.", "labels": [], "entities": [{"text": "parser portability", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.8948827087879181}]}, {"text": "The \"Charniak parser\" has a labeled precision-recall f -measure of 89.7% on WSJ but a lowly 82.9% on the test set from the Brown corpus treebank.", "labels": [], "entities": [{"text": "precision-recall f -measure", "start_pos": 36, "end_pos": 63, "type": "METRIC", "confidence": 0.9747419655323029}, {"text": "WSJ", "start_pos": 76, "end_pos": 79, "type": "DATASET", "confidence": 0.9716979265213013}, {"text": "Brown corpus treebank", "start_pos": 123, "end_pos": 144, "type": "DATASET", "confidence": 0.981329341729482}]}, {"text": "Furthermore, the treebanked Brown data is mostly general non-fiction and much closer to WSJ than, e.g., medical corpora would be.", "labels": [], "entities": [{"text": "treebanked Brown data", "start_pos": 17, "end_pos": 38, "type": "DATASET", "confidence": 0.6529194116592407}, {"text": "WSJ", "start_pos": 88, "end_pos": 91, "type": "DATASET", "confidence": 0.8811652660369873}]}, {"text": "Thus, most work on parser adaptation resorts to using some labeled in-domain data to fortify the larger quantity of outof-domain data.", "labels": [], "entities": [{"text": "parser adaptation", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.9667413830757141}]}, {"text": "In this paper, we present some encouraging results on parser adaptation without any in-domain data.", "labels": [], "entities": [{"text": "parser adaptation", "start_pos": 54, "end_pos": 71, "type": "TASK", "confidence": 0.9576147198677063}]}, {"text": "(Though we also present results with indomain data as a reference point.)", "labels": [], "entities": []}, {"text": "In particular we note the effects of two comparatively recent techniques for parser improvement.", "labels": [], "entities": [{"text": "parser improvement", "start_pos": 77, "end_pos": 95, "type": "TASK", "confidence": 0.952010452747345}]}, {"text": "The first of these, parse-reranking) starts with a \"standard\" generative parser, but uses it to generate the n-best parses rather than a single parse.", "labels": [], "entities": []}, {"text": "Then a reranking phase uses more detailed features, features which would (mostly) be impossible to incorporate in the initial phase, to reorder the list and pick a possibly different best parse.", "labels": [], "entities": []}, {"text": "At first blush one might think that gathering even more fine-grained features from a WSJ treebank would not help adaptation.", "labels": [], "entities": [{"text": "WSJ treebank", "start_pos": 85, "end_pos": 97, "type": "DATASET", "confidence": 0.9078651666641235}, {"text": "adaptation", "start_pos": 113, "end_pos": 123, "type": "TASK", "confidence": 0.9554674029350281}]}, {"text": "However, we find that reranking improves the parsers performance from 82.9% to 85.2%.", "labels": [], "entities": []}, {"text": "The second technique is self-training -parsing unlabeled data and adding it to the training corpus.", "labels": [], "entities": []}, {"text": "Recent work, (), has shown that adding many millions of words of machine parsed and reranked LA Times articles does, in fact, improve performance of the parser on the closely related WSJ data.", "labels": [], "entities": [{"text": "LA Times articles", "start_pos": 93, "end_pos": 110, "type": "DATASET", "confidence": 0.8374355634053549}, {"text": "WSJ data", "start_pos": 183, "end_pos": 191, "type": "DATASET", "confidence": 0.9718827307224274}]}, {"text": "Here we show that it also helps the father-afield Brown data.", "labels": [], "entities": [{"text": "father-afield Brown data", "start_pos": 36, "end_pos": 60, "type": "DATASET", "confidence": 0.722843070824941}]}, {"text": "Adding it improves performance yet-again, this time from 85.2% to 87.8%, fora net error reduction of 28%.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 82, "end_pos": 97, "type": "METRIC", "confidence": 0.949446052312851}]}, {"text": "It is interesting to compare this to our results fora completely Brown trained system (i.e. one in which the first-phase parser is trained on just Brown training data, and the second-phase reranker is trained on Brown 50-best lists).", "labels": [], "entities": []}, {"text": "This system performs at a 88.4% level -only slightly higher than that achieved by our system with only WSJ data.", "labels": [], "entities": [{"text": "WSJ data", "start_pos": 103, "end_pos": 111, "type": "DATASET", "confidence": 0.8924231231212616}]}], "datasetContent": [{"text": "We use the reranking parser in our experiments.", "labels": [], "entities": []}, {"text": "Unless mentioned otherwise, we use the WSJ-trained reranker (as opposed to a BROWN-trained reranker).", "labels": [], "entities": [{"text": "WSJ-trained reranker", "start_pos": 39, "end_pos": 59, "type": "DATASET", "confidence": 0.8340081572532654}, {"text": "BROWN-trained", "start_pos": 77, "end_pos": 90, "type": "METRIC", "confidence": 0.8341167569160461}]}, {"text": "To evaluate, we report bracketing f -scores.", "labels": [], "entities": [{"text": "bracketing f -scores", "start_pos": 23, "end_pos": 43, "type": "METRIC", "confidence": 0.7195394784212112}]}, {"text": "Parser f -scores reported are for sentences up to 100 words long, while reranking parser f -scores are overall sentences.", "labels": [], "entities": [{"text": "Parser f -scores", "start_pos": 0, "end_pos": 16, "type": "METRIC", "confidence": 0.9632810354232788}, {"text": "reranking parser f -scores", "start_pos": 72, "end_pos": 98, "type": "METRIC", "confidence": 0.6771839439868927}]}, {"text": "For simplicity and ease of comparison, most of our evaluations are performed on the development section of BROWN.", "labels": [], "entities": [{"text": "BROWN", "start_pos": 107, "end_pos": 112, "type": "DATASET", "confidence": 0.7953727841377258}]}], "tableCaptions": [{"text": " Table 1: Gildea and Bacchiani results on WSJ and  Brown test corpora using different WSJ and Brown  training sets. Gildea evaluates on sentences of  length \u2264 40, Bacchiani on all sentences.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 42, "end_pos": 45, "type": "DATASET", "confidence": 0.9499185085296631}, {"text": "WSJ", "start_pos": 86, "end_pos": 89, "type": "DATASET", "confidence": 0.9278214573860168}]}, {"text": " Table 2: Effects of adding NANC sentences to WSJ  training data on parsing performance. f -scores  for the parser with and without the WSJ reranker  are shown when evaluating on BROWN develop- ment. For this experiment, we use the WSJ-trained  reranker.", "labels": [], "entities": [{"text": "WSJ  training data", "start_pos": 46, "end_pos": 64, "type": "DATASET", "confidence": 0.727785050868988}, {"text": "parsing", "start_pos": 68, "end_pos": 75, "type": "TASK", "confidence": 0.9606114029884338}, {"text": "f -scores", "start_pos": 89, "end_pos": 98, "type": "METRIC", "confidence": 0.947222113609314}, {"text": "BROWN", "start_pos": 179, "end_pos": 184, "type": "METRIC", "confidence": 0.833817720413208}, {"text": "WSJ-trained  reranker", "start_pos": 232, "end_pos": 253, "type": "DATASET", "confidence": 0.9119516313076019}]}, {"text": " Table 4: Parser and reranking parser performance  on the SWITCHBOARD development corpus. In  this case, WSJ+NANC is a model created from WSJ  and 1,750k sentences from NANC.", "labels": [], "entities": [{"text": "SWITCHBOARD development corpus", "start_pos": 58, "end_pos": 88, "type": "DATASET", "confidence": 0.650753895441691}]}, {"text": " Table 6: Oracle f -scores of top n parses pro- duced by baseline WSJ parser, a combined WSJ and  NANC parser, and a baseline BROWN parser.", "labels": [], "entities": []}, {"text": " Table 3: f -scores from various combinations of WSJ, NANC, and BROWN corpora on BROWN develop- ment. The reranking parser used the WSJ-trained reranker model. The BROWN parsing model is naturally  better than the WSJ model for this task, but combining the two training corpora results in a better model  (as in Gildea (2001)). Adding small amounts of NANC further improves the models.", "labels": [], "entities": [{"text": "BROWN parsing", "start_pos": 164, "end_pos": 177, "type": "TASK", "confidence": 0.5166140049695969}]}, {"text": " Table 5: Performance of various combinations of parser and reranker models when evaluated on BROWN  test. The WSJ+NANC parser with the WSJ reranker comes close to the BROWN-trained reranking parser.  The BROWN reranker provides only a small improvement over its WSJ counterpart, which is not statisti- cally significant.", "labels": [], "entities": [{"text": "BROWN  test", "start_pos": 94, "end_pos": 105, "type": "DATASET", "confidence": 0.6727319955825806}]}, {"text": " Table 7: Agreement between the WSJ+NANC  parser with the WSJ reranker and the BROWN  parser with the BROWN reranker. Complete match  is how often the two reranking parsers returned the  exact same parse.", "labels": [], "entities": [{"text": "WSJ+NANC  parser", "start_pos": 32, "end_pos": 48, "type": "TASK", "confidence": 0.6191923469305038}, {"text": "WSJ", "start_pos": 58, "end_pos": 61, "type": "DATASET", "confidence": 0.9499704837799072}, {"text": "Complete", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.9955428242683411}]}, {"text": " Table 9: The logistic model of BROWN/BROWN  f -score > WSJ+NANC/WSJ f -score identified by  model selection. The feature IN is the num- ber prepositions in the sentence, while ID identi- fies the Brown subcorpus that the sentence comes  from. Stars indicate significance level.", "labels": [], "entities": [{"text": "BROWN", "start_pos": 32, "end_pos": 37, "type": "METRIC", "confidence": 0.9295012950897217}, {"text": "BROWN  f -score", "start_pos": 38, "end_pos": 53, "type": "METRIC", "confidence": 0.8625976890325546}, {"text": "NANC/WSJ f -score", "start_pos": 60, "end_pos": 77, "type": "METRIC", "confidence": 0.7419760773579279}, {"text": "significance level", "start_pos": 259, "end_pos": 277, "type": "METRIC", "confidence": 0.929478794336319}]}]}