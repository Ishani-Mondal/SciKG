{"title": [{"text": "A Fast, Accurate Deterministic Parser for Chinese", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a novel classifier-based deter-ministic parser for Chinese constituency parsing.", "labels": [], "entities": [{"text": "Chinese constituency parsing", "start_pos": 62, "end_pos": 90, "type": "TASK", "confidence": 0.6146746178468069}]}, {"text": "Our parser computes parse trees from bottom up in one pass, and uses classifiers to make shift-reduce decisions.", "labels": [], "entities": []}, {"text": "Trained and evaluated on the standard training and test sets, our best model (us-ing stacked classifiers) runs in linear time and has labeled precision and recall above 88% using gold-standard part-of-speech tags, surpassing the best published results.", "labels": [], "entities": [{"text": "precision", "start_pos": 142, "end_pos": 151, "type": "METRIC", "confidence": 0.9721189737319946}, {"text": "recall", "start_pos": 156, "end_pos": 162, "type": "METRIC", "confidence": 0.9986888766288757}]}, {"text": "Our SVM parser is 2-13 times faster than state-of-the-art parsers, while producing more accurate results.", "labels": [], "entities": []}, {"text": "Our Maxent and DTree parsers run at speeds 40-270 times faster than state-of-the-art parsers, but with 5-6% losses inaccuracy.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "We performed experiments using the Penn Chinese Treebank.", "labels": [], "entities": [{"text": "Penn Chinese Treebank", "start_pos": 35, "end_pos": 56, "type": "DATASET", "confidence": 0.9693500200907389}]}, {"text": "Sections 001-270 (3484 sentences, 84,873 words) were used for training, 271-300 (348 sentences, 7980 words) for development, and 271-300 (348 sentences, 7980 words) for testing.", "labels": [], "entities": []}, {"text": "The whole dataset contains 99629 words, which is about 1/10 of the size of the English Penn Treebank.", "labels": [], "entities": [{"text": "English Penn Treebank", "start_pos": 79, "end_pos": 100, "type": "DATASET", "confidence": 0.9107428391774496}]}, {"text": "Standard corpus preparation steps were done prior to parsing, so that empty nodes were removed, and the resulting A over A unary rewrite nodes are collapsed.", "labels": [], "entities": []}, {"text": "Functional labels of the nonterminal nodes are also removed, but we did not relabel the punctuations, unlike in.", "labels": [], "entities": []}, {"text": "Bracket scoring was done by the EVALB program 2 , and preterminals were not counted as constituents.", "labels": [], "entities": [{"text": "Bracket", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9760513305664062}, {"text": "EVALB program 2", "start_pos": 32, "end_pos": 47, "type": "DATASET", "confidence": 0.9172625343004862}]}, {"text": "In all our experiments, we used labeled recall (LR), labeled precision (LP) and F1 score (harmonic mean of LR and LP) as our evaluation metrics.", "labels": [], "entities": [{"text": "recall (LR)", "start_pos": 40, "end_pos": 51, "type": "METRIC", "confidence": 0.9010372012853622}, {"text": "precision (LP)", "start_pos": 61, "end_pos": 75, "type": "METRIC", "confidence": 0.923390656709671}, {"text": "F1 score", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9921623766422272}]}, {"text": "For the DTree learner, we experimented with two different classification strategies.", "labels": [], "entities": [{"text": "DTree learner", "start_pos": 8, "end_pos": 21, "type": "DATASET", "confidence": 0.7941392362117767}]}, {"text": "In our first approach, the classification is done in a single stage (DTree1).", "labels": [], "entities": [{"text": "classification", "start_pos": 27, "end_pos": 41, "type": "TASK", "confidence": 0.9678776264190674}, {"text": "DTree1", "start_pos": 69, "end_pos": 75, "type": "DATASET", "confidence": 0.7436991333961487}]}, {"text": "The learner is trained fora multi-class classification problem where the class labels include shift and all possible reduce actions.", "labels": [], "entities": [{"text": "multi-class classification", "start_pos": 28, "end_pos": 54, "type": "TASK", "confidence": 0.6992064565420151}]}, {"text": "But this approach yielded a lot of parse failures (42 out of 350 sentences failed during parsing, and partial parse tree was returned).", "labels": [], "entities": [{"text": "parse", "start_pos": 35, "end_pos": 40, "type": "TASK", "confidence": 0.9531338214874268}]}, {"text": "These failures were mostly due to false shift actions in cases where the queue is empty.", "labels": [], "entities": []}, {"text": "To alleviate this problem, we broke the classification process down to two stages (DTree2).", "labels": [], "entities": [{"text": "classification process", "start_pos": 40, "end_pos": 62, "type": "TASK", "confidence": 0.8681474924087524}, {"text": "DTree2", "start_pos": 83, "end_pos": 89, "type": "DATASET", "confidence": 0.6709551811218262}]}, {"text": "A first stage classifier makes a binary decision on whether the action is shift or reduce.", "labels": [], "entities": []}, {"text": "If the output is reduce, a second-stage classifier decides which reduce action to take.", "labels": [], "entities": []}, {"text": "Results showed that breaking down the classification task into two stages increased overall accuracy, and the number of failures was reduced to 30.", "labels": [], "entities": [{"text": "classification task", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.8865135610103607}, {"text": "accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9988111257553101}]}, {"text": "Classifier ensemble by itself has been a fruitful research direction in machine learning in recent years.", "labels": [], "entities": []}, {"text": "The basic idea in classifier ensemble is that combining multiple classifiers can often give significantly better results than any single classifier alone.", "labels": [], "entities": [{"text": "classifier ensemble", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.8075381815433502}]}, {"text": "We experimented with three different classifier ensemble strategies: classifier stacking, meta-classifier, and simple voting.", "labels": [], "entities": [{"text": "classifier stacking", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.8105088472366333}]}, {"text": "Using the SVM classifier's results as a baseline, we tested these approaches on the development set.", "labels": [], "entities": []}, {"text": "In classifier stacking, we collect the outputs from Maxent, DTree and TiMBL, which are all trained on a separate dataset from the training set (section 400-650 of the Penn Chinese Treebank, smaller than the original training set).", "labels": [], "entities": [{"text": "classifier stacking", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.8306937217712402}, {"text": "Penn Chinese Treebank", "start_pos": 167, "end_pos": 188, "type": "DATASET", "confidence": 0.9701931277910868}]}, {"text": "We use their classification output as features, in addition to the original feature set, to train anew SVM model on the original training set.", "labels": [], "entities": []}, {"text": "We achieved LR of 90.3% and LP of 90.5% on the development set, a 3.4% and 2.6% improvement in LR and LP, respectively.", "labels": [], "entities": [{"text": "LR", "start_pos": 12, "end_pos": 14, "type": "METRIC", "confidence": 0.9997302889823914}, {"text": "LP", "start_pos": 28, "end_pos": 30, "type": "METRIC", "confidence": 0.9996227025985718}, {"text": "LR", "start_pos": 95, "end_pos": 97, "type": "METRIC", "confidence": 0.9937053322792053}, {"text": "LP", "start_pos": 102, "end_pos": 104, "type": "METRIC", "confidence": 0.9448124170303345}]}, {"text": "When tested on the test set, we gained 1% improvement in F1 when gold-standard POS tagging is used.", "labels": [], "entities": [{"text": "F1", "start_pos": 57, "end_pos": 59, "type": "METRIC", "confidence": 0.9995958209037781}, {"text": "POS tagging", "start_pos": 79, "end_pos": 90, "type": "TASK", "confidence": 0.5872552543878555}]}, {"text": "When tested with automatic tagging, we achieved a 0.5% improvement in F1.", "labels": [], "entities": [{"text": "F1", "start_pos": 70, "end_pos": 72, "type": "METRIC", "confidence": 0.9967094659805298}]}, {"text": "Using Bikel's significant tester with 10000 times random shuffle, the p-value for LR and LP are 0.008 and 0.457, respectively.", "labels": [], "entities": [{"text": "LR", "start_pos": 82, "end_pos": 84, "type": "METRIC", "confidence": 0.8896684050559998}, {"text": "LP", "start_pos": 89, "end_pos": 91, "type": "METRIC", "confidence": 0.8694924712181091}]}, {"text": "The increase in recall is statistically significant, and it shows classifier stacking can improve performance.", "labels": [], "entities": [{"text": "recall", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.9996993541717529}, {"text": "classifier stacking", "start_pos": 66, "end_pos": 85, "type": "TASK", "confidence": 0.817693680524826}]}, {"text": "On the other hand, we did not find metaclassification and simple voting very effective.", "labels": [], "entities": []}, {"text": "In simple voting, we make the classifiers to vote in each step for every parse action.", "labels": [], "entities": []}, {"text": "The F1 of simple voting method is downgraded by 5.9% relative to SVM model's F1.", "labels": [], "entities": [{"text": "F1", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.9994262456893921}, {"text": "SVM", "start_pos": 65, "end_pos": 68, "type": "DATASET", "confidence": 0.8592429757118225}]}, {"text": "By analyzing the interagreement among classifiers, we found that there were no cases where Maxent's top output and DTree's output were both correct and SVM's output was wrong.", "labels": [], "entities": []}, {"text": "Using the top output from Maxent and DTree directly does not seem to be complementary to SVM.", "labels": [], "entities": []}, {"text": "In the meta-classifier approach, we first collect the output from each classifier trained on sec-: Comparison with related work on the test set using automatically generated POS tion 1-210 (roughly 3/4 of the entire training set).", "labels": [], "entities": []}, {"text": "Then specifically for Maxent, we collected the top output as well as its associated probability estimate.", "labels": [], "entities": []}, {"text": "Then we used the outputs and probability estimate as features to train an SVM classifier that makes a decision on which classifier to pick.", "labels": [], "entities": []}, {"text": "Meta-classifier results did not change at all from our baseline.", "labels": [], "entities": []}, {"text": "In fact, the meta-classifier always picked SVM as its output.", "labels": [], "entities": []}, {"text": "This agrees with our observation for the simple voting case.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Comparison of different classifier mod- els' parsing accuracies on development set for sen- tences \u2264 40 words, with gold-standard POS", "labels": [], "entities": [{"text": "POS", "start_pos": 140, "end_pos": 143, "type": "TASK", "confidence": 0.30817967653274536}]}, {"text": " Table 3: Comparison with related work on the test set using automatically generated POS", "labels": [], "entities": []}, {"text": " Table 4. The implication of this result is two- fold. On one hand, it shows that if POS tagging  accuracy can be increased, our parser is likely to  benefit more than the other two models; on the  other hand, it also indicates that our deterministic  model is less resilient to POS errors. Further de- tailed analysis is called for, to study the extent to  which POS tagging errors affects the deterministic  parsing model.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 85, "end_pos": 96, "type": "TASK", "confidence": 0.7143402695655823}, {"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.8353598713874817}, {"text": "POS tagging", "start_pos": 364, "end_pos": 375, "type": "TASK", "confidence": 0.8099722564220428}]}, {"text": " Table 4: Comparison with related work on the test  set for sentence \u2264 40 words, using gold-standard  POS", "labels": [], "entities": [{"text": "POS", "start_pos": 102, "end_pos": 105, "type": "DATASET", "confidence": 0.39953407645225525}]}]}