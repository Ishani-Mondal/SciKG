{"title": [{"text": "Topic-Focused Multi-document Summarization Using an Approximate Oracle Score", "labels": [], "entities": [{"text": "Topic-Focused Multi-document Summarization", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.5447907745838165}]}], "abstractContent": [{"text": "We consider the problem of producing a multi-document summary given a collection of documents.", "labels": [], "entities": []}, {"text": "Since most successful methods of multi-document summa-rization are still largely extractive, in this paper, we explore just how well an ex-tractive method can perform.", "labels": [], "entities": []}, {"text": "We introduce an \"oracle\" score, based on the probability distribution of unigrams inhuman summaries.", "labels": [], "entities": []}, {"text": "We then demonstrate that with the oracle score, we can generate extracts which score, on average, better than the human summaries, when evaluated with ROUGE.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 151, "end_pos": 156, "type": "METRIC", "confidence": 0.9885804057121277}]}, {"text": "In addition, we introduce an approximation to the oracle score which produces a system with the best known performance for the 2005 Document Understanding Conference (DUC) evaluation.", "labels": [], "entities": [{"text": "Document Understanding Conference (DUC) evaluation", "start_pos": 132, "end_pos": 182, "type": "TASK", "confidence": 0.49326144797461374}]}], "introductionContent": [{"text": "We consider the problem of producing a multidocument summary given a collection of documents.", "labels": [], "entities": []}, {"text": "Most automatic methods of multidocument summarization are largely extractive.", "labels": [], "entities": [{"text": "multidocument summarization", "start_pos": 26, "end_pos": 53, "type": "TASK", "confidence": 0.649973064661026}]}, {"text": "This mimics the behavior of humans for single document summarization; reported that 79% of the sentences in a human-generated abstract were a \"direct match\" to a sentence in a document.", "labels": [], "entities": [{"text": "single document summarization", "start_pos": 39, "end_pos": 68, "type": "TASK", "confidence": 0.6065135796864828}]}, {"text": "In contrast, for multi-document summarization,) report that no more than 55% of the vocabulary contained in human-generated abstracts can be found in the given documents.", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 17, "end_pos": 45, "type": "TASK", "confidence": 0.6361896693706512}]}, {"text": "Furthermore, multiple human summaries on the same collection of documents often have little agreement.", "labels": [], "entities": []}, {"text": "For example, report that unigram overlap is around 40%.", "labels": [], "entities": [{"text": "overlap", "start_pos": 33, "end_pos": 40, "type": "METRIC", "confidence": 0.6338979005813599}]}, {"text": "() used a \"factoid\" agreement analysis of human summaries fora single document and concluded that a resulting consensus summary is stable only if 30-40 summaries are collected.", "labels": [], "entities": []}, {"text": "In light of the strong evidence that nearly half of the terms in human-generated multi-document abstracts are not from the original documents, and that agreement of vocabulary among human abstracts is only about 40%, we pose two coupled questions about the quality of summaries that can be attained by document extraction: 1.", "labels": [], "entities": [{"text": "agreement", "start_pos": 152, "end_pos": 161, "type": "METRIC", "confidence": 0.9807104468345642}, {"text": "document extraction", "start_pos": 302, "end_pos": 321, "type": "TASK", "confidence": 0.7454204261302948}]}, {"text": "Given the sets of unigrams used by four human summarizers, can we produce an extract summary that is statistically indistinguishable from the human abstracts when measured by current automatic evaluation methods such as ROUGE?", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 220, "end_pos": 225, "type": "METRIC", "confidence": 0.6553267240524292}]}, {"text": "2. If such unigram information can produce good summaries, can we replace this information by a statistical model and still produce good summaries?", "labels": [], "entities": [{"text": "summaries", "start_pos": 48, "end_pos": 57, "type": "TASK", "confidence": 0.975774884223938}, {"text": "summaries", "start_pos": 137, "end_pos": 146, "type": "TASK", "confidence": 0.9594362378120422}]}, {"text": "We will show that the answer to the first question is, indeed, yes and, in fact, the unigram set information gives rise to extract summaries that usually score better than the 4 human abstractors!", "labels": [], "entities": []}, {"text": "Secondly, we give a method to statistically approximate the set of unigrams and find it produces extracts of the DUC 05 data which outperform all known evaluated machine entries.", "labels": [], "entities": [{"text": "DUC 05 data", "start_pos": 113, "end_pos": 124, "type": "DATASET", "confidence": 0.8987137277921041}]}, {"text": "We conclude with experiments on the extent that redundancy removal improves extracts, as well as a method of moving beyond simple extracting by employing shallow parsing techniques to shorten the sentences prior to selection.", "labels": [], "entities": [{"text": "redundancy removal", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.7601080238819122}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: Average ROUGE 2 Scores for DUC06:  Humans A-I", "labels": [], "entities": [{"text": "Average ROUGE 2 Scores", "start_pos": 10, "end_pos": 32, "type": "METRIC", "confidence": 0.8476045280694962}, {"text": "DUC06", "start_pos": 37, "end_pos": 42, "type": "DATASET", "confidence": 0.5790181159973145}, {"text": "Humans A-I", "start_pos": 45, "end_pos": 55, "type": "METRIC", "confidence": 0.6838695704936981}]}, {"text": " Table 4: Average ROUGE 1 Scores with stop  words removed for DUC04, Task 2", "labels": [], "entities": [{"text": "Average ROUGE 1 Scores", "start_pos": 10, "end_pos": 32, "type": "METRIC", "confidence": 0.8508734256029129}]}]}