{"title": [{"text": "Segment-based Hidden Markov Models for Information Extraction", "labels": [], "entities": [{"text": "Segment-based Hidden Markov", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.854061484336853}, {"text": "Information Extraction", "start_pos": 39, "end_pos": 61, "type": "TASK", "confidence": 0.72211654484272}]}], "abstractContent": [{"text": "Hidden Markov models (HMMs) are powerful statistical models that have found successful applications in Information Extraction (IE).", "labels": [], "entities": [{"text": "Information Extraction (IE)", "start_pos": 103, "end_pos": 130, "type": "TASK", "confidence": 0.8666333675384521}]}, {"text": "In current approaches to applying HMMs to IE, an HMM is used to model text at the document level.", "labels": [], "entities": [{"text": "IE", "start_pos": 42, "end_pos": 44, "type": "TASK", "confidence": 0.9100014567375183}]}, {"text": "This modelling might cause undesired redundancy in extraction in the sense that more than one filler is identified and extracted.", "labels": [], "entities": []}, {"text": "We propose to use HMMs to model text at the segment level, in which the extraction process consists of two steps: a segment retrieval step followed by an extraction step.", "labels": [], "entities": []}, {"text": "In order to retrieve extraction-relevant segments from documents, we introduce a method to use HMMs to model and retrieve segments.", "labels": [], "entities": []}, {"text": "Our experimental results show that the resulting segment HMM IE system not only achieves near zero extraction redundancy, but also has better overall extraction performance than traditional document HMM IE systems.", "labels": [], "entities": [{"text": "document HMM IE", "start_pos": 190, "end_pos": 205, "type": "TASK", "confidence": 0.6557323038578033}]}], "introductionContent": [{"text": "A Hidden Markov Model (HMM) is a finite state automaton with stochastic state transitions and symbol emissions.", "labels": [], "entities": []}, {"text": "The automaton models a random process that can produce a sequence of symbols by starting from some state, transferring from one state to another state with a symbol being emitted at each state, until a final state is reached.", "labels": [], "entities": []}, {"text": "Formally, a hidden Markov model (HMM) is specified by a five-tuple, where S is a set of states; K is the alphabet of observation symbols; \u03a0 is the initial state distribution; A is the probability distribution of state transitions; and B is the probability distribution of symbol emissions.", "labels": [], "entities": []}, {"text": "When the structure of an HMM is determined, the complete model parameters can be represented as \u03bb = (A, B, \u03a0).", "labels": [], "entities": []}, {"text": "HMMs are particularly useful in modelling sequential data.", "labels": [], "entities": []}, {"text": "They have been applied in several areas within natural language processing (NLP), with one of the most successful efforts in speech recognition.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 47, "end_pos": 80, "type": "TASK", "confidence": 0.8063983122507731}, {"text": "speech recognition", "start_pos": 125, "end_pos": 143, "type": "TASK", "confidence": 0.828752189874649}]}, {"text": "HMMs have also been applied in information extraction.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 31, "end_pos": 53, "type": "TASK", "confidence": 0.9323235750198364}]}, {"text": "An early work of using HMMs for IE is) in which HMMs are trained to extract gene name-location facts from a collection of scientific abstracts.", "labels": [], "entities": [{"text": "IE", "start_pos": 32, "end_pos": 34, "type": "TASK", "confidence": 0.9911448359489441}]}, {"text": "Another related work is () which used HMMs as part of its modelling for the name finding problem in information extraction.", "labels": [], "entities": [{"text": "name finding problem", "start_pos": 76, "end_pos": 96, "type": "TASK", "confidence": 0.8657975395520529}, {"text": "information extraction", "start_pos": 100, "end_pos": 122, "type": "TASK", "confidence": 0.8631011247634888}]}, {"text": "A more recent work on applying HMMs to IE is (), in which a separate HMM is built for extracting fillers for each slot.", "labels": [], "entities": [{"text": "IE", "start_pos": 39, "end_pos": 41, "type": "TASK", "confidence": 0.8361607789993286}]}, {"text": "To train an HMM for extracting fillers fora specific slot, maximum likelihood estimation is used to determine the probabilities (i.e., the initial state probabilities, the state transition probabilities, and the symbol emission probabilities) associated with each HMM from labelled texts.", "labels": [], "entities": [{"text": "maximum likelihood estimation", "start_pos": 59, "end_pos": 88, "type": "METRIC", "confidence": 0.7563885450363159}]}, {"text": "One characteristic of current HMM-based IE systems is that an HMM models the entire document.", "labels": [], "entities": [{"text": "HMM-based IE", "start_pos": 30, "end_pos": 42, "type": "TASK", "confidence": 0.6289386451244354}]}, {"text": "Each document is viewed as along sequence of tokens (i.e., words, punctuation marks etc.), which is the observation generated from the given HMM.", "labels": [], "entities": []}, {"text": "The extraction is performed by finding the best state sequence for this observed long token sequence constituting the whole document, and the subsequences of tokens that pass through the target filler state are extracted as fillers.", "labels": [], "entities": []}, {"text": "We call such approaches to applying HMMs to IE at the document level as document-based HMM IE or document HMM IE for brevity.", "labels": [], "entities": [{"text": "IE", "start_pos": 44, "end_pos": 46, "type": "TASK", "confidence": 0.9144940376281738}]}, {"text": "In addition to HMMs, there are other Markovian sequence models that have been applied to IE.", "labels": [], "entities": [{"text": "IE", "start_pos": 89, "end_pos": 91, "type": "TASK", "confidence": 0.9880231618881226}]}, {"text": "Examples of these models include maximum entropy Markov models (), Bayesian information extraction network, and conditional random fields) ().", "labels": [], "entities": [{"text": "information extraction network", "start_pos": 76, "end_pos": 106, "type": "TASK", "confidence": 0.7335225939750671}]}, {"text": "In the IE systems using these models, extraction is performed by sequential tag labelling.", "labels": [], "entities": [{"text": "IE", "start_pos": 7, "end_pos": 9, "type": "TASK", "confidence": 0.9471385478973389}]}, {"text": "Similar to HMM IE, each document is considered to be a single steam of tokens in these IE models as well.", "labels": [], "entities": [{"text": "HMM IE", "start_pos": 11, "end_pos": 17, "type": "DATASET", "confidence": 0.8745477795600891}]}, {"text": "In this paper, we introduce the concept of extraction redundancy, and show that current document HMM IE systems often produce undesired redundant extractions.", "labels": [], "entities": [{"text": "extraction redundancy", "start_pos": 43, "end_pos": 64, "type": "TASK", "confidence": 0.8555590212345123}, {"text": "HMM IE", "start_pos": 97, "end_pos": 103, "type": "TASK", "confidence": 0.7474232912063599}]}, {"text": "In order to address this extraction redundancy issue, we propose a segmentbased two-step extraction approach in which a segment retrieval step is imposed before the extraction step.", "labels": [], "entities": [{"text": "segmentbased two-step extraction", "start_pos": 67, "end_pos": 99, "type": "TASK", "confidence": 0.6788041293621063}]}, {"text": "Our experimental results show that the resulting segment-based HMM IE system not only achieves near-zero extraction redundancy but also improves the overall extraction performance.", "labels": [], "entities": [{"text": "HMM IE", "start_pos": 63, "end_pos": 69, "type": "TASK", "confidence": 0.7245415449142456}]}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "In section 2, we describe our document HMM IE system in which the Simple Good-Turning (SGT) smoothing is applied for probability estimation.", "labels": [], "entities": [{"text": "HMM IE", "start_pos": 39, "end_pos": 45, "type": "TASK", "confidence": 0.6278856992721558}, {"text": "probability estimation", "start_pos": 117, "end_pos": 139, "type": "TASK", "confidence": 0.7911170423030853}]}, {"text": "We also evaluate our document HMM IE system, and compare it to the related work.", "labels": [], "entities": [{"text": "HMM IE", "start_pos": 30, "end_pos": 36, "type": "TASK", "confidence": 0.6234849393367767}]}, {"text": "In Section 3, we point out the extraction redundancy issue in a document HMM IE system.", "labels": [], "entities": [{"text": "HMM IE", "start_pos": 73, "end_pos": 79, "type": "TASK", "confidence": 0.5601087361574173}]}, {"text": "The definition of the extraction redundancy is introduced for better evaluation of an IE system with possible redundant extraction.", "labels": [], "entities": []}, {"text": "In order to address this extraction redundancy issue, we propose our segment-based HMM IE method in Section 4, in which a segment retrieval step is applied before the extraction is performed.", "labels": [], "entities": [{"text": "HMM IE", "start_pos": 83, "end_pos": 89, "type": "TASK", "confidence": 0.6774497777223587}]}, {"text": "Section 5 presents a segment retrieval algorithm by using HMMs to model and retrieve segments.", "labels": [], "entities": [{"text": "segment retrieval", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.803467720746994}]}, {"text": "We compare the performance between the segment HMM IE system and the document HMM IE system in Section 6.", "labels": [], "entities": [{"text": "document HMM IE system", "start_pos": 69, "end_pos": 91, "type": "DATASET", "confidence": 0.7983711361885071}]}, {"text": "Finally, conclusions are made and some future work is mentioned in Section 7. 2 Document-based HMM IE with the SGT smoothing", "labels": [], "entities": [{"text": "HMM IE", "start_pos": 95, "end_pos": 101, "type": "TASK", "confidence": 0.5962787866592407}, {"text": "SGT smoothing", "start_pos": 111, "end_pos": 124, "type": "TASK", "confidence": 0.69608274102211}]}], "datasetContent": [{"text": "We evaluated our document HMM IE system on the seminar announcements IE domain using tenfold cross validation evaluation.", "labels": [], "entities": [{"text": "HMM IE", "start_pos": 26, "end_pos": 32, "type": "TASK", "confidence": 0.4868104010820389}, {"text": "seminar announcements IE domain", "start_pos": 47, "end_pos": 78, "type": "DATASET", "confidence": 0.5984986573457718}]}, {"text": "The data set consists of 485 annotated seminar announcements, with the fillers for the following four slots specified for each seminar: location (the location of a seminar), speaker (the speaker of a seminar), stime (the starting time of a seminar) and etime (the ending time of a seminar).", "labels": [], "entities": []}, {"text": "In our HMM IE experiments, the structure parameters are set to system default values, i.e., 4 for both pre-context and postcontext size, and 4 for the number of parallel filler paths.", "labels": [], "entities": [{"text": "HMM IE", "start_pos": 7, "end_pos": 13, "type": "TASK", "confidence": 0.827966719865799}]}, {"text": "shows F1 scores (95% confidence intervals) of our Document HMM IE system (Doc HMM).", "labels": [], "entities": [{"text": "F1", "start_pos": 6, "end_pos": 8, "type": "METRIC", "confidence": 0.9995404481887817}, {"text": "Document HMM IE system (Doc HMM)", "start_pos": 50, "end_pos": 82, "type": "DATASET", "confidence": 0.819695308804512}]}, {"text": "The performance numbers from other HMM IE systems) are also listed in for comparison, where HMM None is their HMM IE system that uses absolute discounting but with no shrinkage, and HMM Global is the representative version of their HMM IE system with shrinkage.", "labels": [], "entities": [{"text": "HMM Global", "start_pos": 182, "end_pos": 192, "type": "DATASET", "confidence": 0.8122431635856628}]}, {"text": "By using the same structure parameters (i.e., the same context size) as in (), our Doc HMM system performs consistently better on all slots than their HMM IE system using absolute discounting.", "labels": [], "entities": []}, {"text": "Even compared to their much more complex version of HMM IE with shrinkage, our system has achieved comparable results on location, speaker and stime, but obtained significantly better performance on the etime slot.", "labels": [], "entities": []}, {"text": "It is noted that our smoothing method is much simpler to apply, and does not require any extra effort such as specifying shrinkage topology or any extra labelled data fora held-out set.) that how exactly a correct extraction for one document is defined in HMM IE evaluation.", "labels": [], "entities": [{"text": "HMM IE", "start_pos": 256, "end_pos": 262, "type": "TASK", "confidence": 0.5039425194263458}]}, {"text": "One way to define a correct extraction fora document is to require that at least one of the text segments that pass the filler states is the same as a labelled filler.", "labels": [], "entities": []}, {"text": "Alternatively, we can define the correctness by requiring that all the text segments that pass the filler states are same as the labelled fillers.", "labels": [], "entities": [{"text": "correctness", "start_pos": 33, "end_pos": 44, "type": "METRIC", "confidence": 0.9491297602653503}]}, {"text": "In this case, it is actually required an exact match between the HMM state sequence determined by the system and the originally labelled one for that document.", "labels": [], "entities": []}, {"text": "Very likely, the former correctness criterion was used in evaluating these document-based HMM IE systems.", "labels": [], "entities": [{"text": "correctness criterion", "start_pos": 24, "end_pos": 45, "type": "METRIC", "confidence": 0.9554892778396606}, {"text": "HMM IE", "start_pos": 90, "end_pos": 96, "type": "TASK", "confidence": 0.7240608930587769}]}, {"text": "We used the same criterion for evaluating our document HMM IE systems in Section 2.", "labels": [], "entities": [{"text": "HMM IE", "start_pos": 55, "end_pos": 61, "type": "TASK", "confidence": 0.8066647350788116}]}, {"text": "Although it might be reasonable to define that a document is correctly extracted if anyone of the identified fillers from the state sequence labelled by the system is a correct filler, certain issues exist when a document HMM IE system returns multiple extractions for the same slot for one document.", "labels": [], "entities": []}, {"text": "For example, it is possible that some of the fillers found by the system are not correct extractions.", "labels": [], "entities": []}, {"text": "In this situation, such document-wise extraction evaluation alone would not be sufficient to measure the performance of an HMM IE system.", "labels": [], "entities": [{"text": "document-wise extraction", "start_pos": 24, "end_pos": 48, "type": "TASK", "confidence": 0.6939064264297485}, {"text": "HMM IE", "start_pos": 123, "end_pos": 129, "type": "TASK", "confidence": 0.7003653943538666}]}, {"text": "Document HMM IE modelling does provide any guidelines for selecting one mostly likely filler from the ones identified by the state sequence matching over the whole document.", "labels": [], "entities": [{"text": "Document HMM IE", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.5903148750464121}]}, {"text": "For the template filling IE problem that is of our interest in this paper, the ideal extraction result is one slot filler per document.", "labels": [], "entities": [{"text": "template filling IE", "start_pos": 8, "end_pos": 27, "type": "TASK", "confidence": 0.898500939210256}]}, {"text": "Otherwise, some further postprocessing would be required to choose only one extraction, from the multiple fillers possibly extracted by a document HMM IE system, for filling in the slot template for that document.", "labels": [], "entities": []}, {"text": "Shown from, the segment retrieval results have achieved high recall especially with the least correctly filtered correctness criterion.", "labels": [], "entities": [{"text": "recall", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9995817542076111}]}, {"text": "In addition, the system has produced the retrieval results with relatively small redundancy which means most of the segments that are fed to the segment HMM extractor from the retrieval step are actually extraction-related segments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: F1 of Document HMM IE systems on seminar announcements", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9892339706420898}, {"text": "Document HMM IE", "start_pos": 16, "end_pos": 31, "type": "TASK", "confidence": 0.6962027748425802}]}, {"text": " Table 2: F1 / redundancy in document HMM IE  on SA domain  Slot  F1  R  location 0.8220 0.0543  speaker 0.7135 0.0952  stime  1.0000 0.1312  etime  0.9488 0.0630", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9992156028747559}, {"text": "SA domain  Slot  F1  R  location 0.8220 0.0543  speaker", "start_pos": 49, "end_pos": 104, "type": "METRIC", "confidence": 0.6917389763726128}]}, {"text": " Table 3: Segment retrieval results", "labels": [], "entities": [{"text": "Segment retrieval", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.9825106561183929}]}, {"text": " Table 4: F1 comparison on seminar announcements (document HMM IE vs. segment HMM IE)", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9980666041374207}]}]}