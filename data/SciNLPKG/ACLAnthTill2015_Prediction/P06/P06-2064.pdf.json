{"title": [{"text": "Interpreting Semantic Relations in Noun Compounds via Verb Semantics", "labels": [], "entities": []}], "abstractContent": [{"text": "We propose a novel method for automatically interpreting compound nouns based on a predefined set of semantic relations.", "labels": [], "entities": [{"text": "automatically interpreting compound nouns", "start_pos": 30, "end_pos": 71, "type": "TASK", "confidence": 0.7049630507826805}]}, {"text": "First we map verb tokens in sentential contexts to a fixed set of seed verbs using WordNet::Similarity and Moby's Thesaurus.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 83, "end_pos": 90, "type": "DATASET", "confidence": 0.9518148303031921}]}, {"text": "We then match the sentences with semantic relations based on the semantics of the seed verbs and grammatical roles of the head noun and modifier.", "labels": [], "entities": []}, {"text": "Based on the semantics of the matched sentences, we then build a classifier using TiMBL.", "labels": [], "entities": [{"text": "TiMBL", "start_pos": 82, "end_pos": 87, "type": "DATASET", "confidence": 0.8701481223106384}]}], "introductionContent": [{"text": "The interpretation of noun compounds (hereafter, NCs) such as apple pie or family car is a wellestablished sub-task of language understanding.", "labels": [], "entities": [{"text": "interpretation of noun compounds (hereafter, NCs) such as apple pie or family car", "start_pos": 4, "end_pos": 85, "type": "TASK", "confidence": 0.7770375646650791}, {"text": "language understanding", "start_pos": 119, "end_pos": 141, "type": "TASK", "confidence": 0.6925783604383469}]}, {"text": "Conventionally, the NC interpretation task is defined in terms of unearthing the underspecified semantic relation between the head noun and modifier(s), e.g. pie and apple respectively in the case of apple pie.", "labels": [], "entities": [{"text": "NC interpretation task", "start_pos": 20, "end_pos": 42, "type": "TASK", "confidence": 0.8892729679743449}]}, {"text": "NC interpretation has been studied in the context of applications including question-answering and machine translation (.", "labels": [], "entities": [{"text": "NC interpretation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9079284369945526}, {"text": "machine translation", "start_pos": 99, "end_pos": 118, "type": "TASK", "confidence": 0.8163117170333862}]}, {"text": "Recent work on the automatic/semiautomatic interpretation of NCs (e.g.,,, and) has made assumptions about the scope of semantic relations or restricted the domain of interpretation.", "labels": [], "entities": [{"text": "automatic/semiautomatic interpretation of NCs", "start_pos": 19, "end_pos": 64, "type": "TASK", "confidence": 0.7476554115613302}]}, {"text": "This makes it difficult to gauge the general-purpose utility of the different methods.", "labels": [], "entities": []}, {"text": "Our method avoids any such assumptions while outperforming previous methods.", "labels": [], "entities": []}, {"text": "In seminal work on NC interpretation, Finin (1980) tried to interpret NCs based on hand-coded rules.", "labels": [], "entities": [{"text": "NC interpretation", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.9638037085533142}]}, {"text": "attempted the automatic interpretation of NCs using hand-written rules, with the obvious cost of manual intervention.", "labels": [], "entities": [{"text": "automatic interpretation of NCs", "start_pos": 14, "end_pos": 45, "type": "TASK", "confidence": 0.7570893913507462}]}, {"text": "estimated the knowledge required to interpret NCs and claimed that performance was closely tied to the volume of data acquired.", "labels": [], "entities": [{"text": "interpret NCs", "start_pos": 36, "end_pos": 49, "type": "TASK", "confidence": 0.90082186460495}]}, {"text": "In more recent work, used a semi-automatic method for NC interpretation in a fixed domain.", "labels": [], "entities": [{"text": "NC interpretation", "start_pos": 54, "end_pos": 71, "type": "TASK", "confidence": 0.9455394446849823}]}, {"text": "developed a fully automatic method but focused on nominalizations, a proper subclass of NCs.", "labels": [], "entities": []}, {"text": "1 classified the nouns in medical texts by tagging hierarchical information using neural networks.", "labels": [], "entities": []}, {"text": "used the word senses of nouns based on the domain or range of interpretation of an NC, leading to questions of scalability and portability to novel domains/NC types.", "labels": [], "entities": []}, {"text": "proposed a simplistic general-purpose method based on the lexical similarity of unseen NCs with training instances.", "labels": [], "entities": []}, {"text": "The aim of this paper is to develop an automatic method for interpreting NCs based on semantic relations.", "labels": [], "entities": [{"text": "interpreting NCs based on semantic relations", "start_pos": 60, "end_pos": 104, "type": "TASK", "confidence": 0.8392684757709503}]}, {"text": "We interpret semantic relations relative to a fixed set of constructions involving the modifier and head noun and a set of seed verbs for each semantic relation: e.g. (the) family owns (a) car is taken as evidence for family car being an instance of the POSSESSOR relation.", "labels": [], "entities": []}, {"text": "We then attempt to map all instances of the modifier and head noun as the heads of NPs in a transitive sentential context onto our set of constructions via lexical similarity over the verb, to arrive at an interpretation: e.g. we would hope to predict that possess is sufficiently similar to own that (the) family possesses (a) car would be recognised as support-ing evidence for the POSSESSOR relation.", "labels": [], "entities": [{"text": "POSSESSOR", "start_pos": 384, "end_pos": 393, "type": "METRIC", "confidence": 0.6994542479515076}]}, {"text": "We use a supervised classifier to combine together the evidence contributed by individual sentential contexts of a given modifier-head noun combination, and arrive at a final interpretation fora given NC.", "labels": [], "entities": []}, {"text": "Mapping the actual verbs in sentences to appropriate seed verbs is obviously crucial to the success of our method.", "labels": [], "entities": []}, {"text": "This is particularly important as there is no guarantee that we will find large numbers of modifier-head noun pairings in the sorts of sentential contexts required by our method, nor that we will find attested instances based on the seed verbs.", "labels": [], "entities": []}, {"text": "Thus an error in mapping an attested verb to the seed verbs could result in a wrong interpretation or no classification at all.", "labels": [], "entities": []}, {"text": "In this paper, we experiment with the use of WordNet and word clusters (based on Moby's Thesaurus) in mapping attested verbs to the seed verbs.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 45, "end_pos": 52, "type": "DATASET", "confidence": 0.9286444783210754}]}, {"text": "We also make use of CoreLex in dealing with the semantic relation TIME and the RASP parser) to determine the dependency structure of corpus data.", "labels": [], "entities": []}, {"text": "The data source for our set of NCs is binary NCs (i.e. NCs with a single modifier) from the Wall Street Journal component of the Penn Treebank.", "labels": [], "entities": [{"text": "Wall Street Journal component of the Penn Treebank", "start_pos": 92, "end_pos": 142, "type": "DATASET", "confidence": 0.9468101263046265}]}, {"text": "We deliberately choose to ignore NCs with multiple modifiers on the grounds that: (a) 88.4% of NC types in the Wall Street Journal component of the Penn Treebank and 90.6% of NC types in the British National Corpus are binary; and (b) we expect to be able to interpret NCs with multiple modifiers by decomposing them into binary NCs.", "labels": [], "entities": [{"text": "Wall Street Journal component of the Penn Treebank", "start_pos": 111, "end_pos": 161, "type": "DATASET", "confidence": 0.9414569586515427}, {"text": "British National Corpus", "start_pos": 191, "end_pos": 214, "type": "DATASET", "confidence": 0.9684653878211975}]}, {"text": "Another simplifying assumption we make is to remove NCs incorporating proper nouns since: (a) the lexical resources we employ in this research do not contain them in large numbers; and (b) there is some doubt as to whether the set of semantic relations required to interpret NCs incorporating proper nouns is that same as that for common nouns.", "labels": [], "entities": []}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 takes a brief look at the semantics of NCs and the basic idea behind the work.", "labels": [], "entities": []}, {"text": "Section 3 details the set of NC semantic relations that is used in our research, Section 4 presents an extended discussion of our approach, Section 5 briefly explains the tools we use, Section 6.1 describes how we gather and process the data, Section 6.2 explains how we map the verbs to seed verbs, and Section 7 and Section 8 present the results and analysis of our approach.", "labels": [], "entities": []}, {"text": "Finally we conclude our work in Section 9.", "labels": [], "entities": [{"text": "Section 9", "start_pos": 32, "end_pos": 41, "type": "TASK", "confidence": 0.47500351071357727}]}], "datasetContent": [{"text": "We evaluated our method over both 17 semantic relations (without EQUATIVE and TIME) and the full 19 semantic relations, due to the low frequency and lack of verb-based constructional contexts for EQUATIVE and TIME, as indicated in.", "labels": [], "entities": []}, {"text": "Note that the test data set is the same for both sets of semantic relations, but that the training data in the case of 17 semantic relations will not contain any instances for the EQUATIVE and TIME relations, meaning that all such test instances will be misclassified.", "labels": [], "entities": []}, {"text": "The baseline for all verb mapping methods is a simple majority-class classifier, which leads to an accuracy of 42.4% for the TOPIC relation.", "labels": [], "entities": [{"text": "verb mapping", "start_pos": 21, "end_pos": 33, "type": "TASK", "confidence": 0.7222914844751358}, {"text": "accuracy", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9994264841079712}]}, {"text": "In evaluation, we use two different values for our method: Count and Weight.", "labels": [], "entities": [{"text": "Count", "start_pos": 59, "end_pos": 64, "type": "METRIC", "confidence": 0.9993746876716614}, {"text": "Weight", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.9951717257499695}]}, {"text": "Count is based on the raw number of corpus instances, while Weight employs the seed verb weight described in Section 6.1.: Results of combining the proposed method and with the method of Kim and Baldwin As noted above, we excluded all NCs for which we were unable to find at least 5 instances of the modifier and head noun in an appropriate sentential context.", "labels": [], "entities": [{"text": "Count", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9393007159233093}]}, {"text": "This exclusion reduced the original set of 2,166 NCs to only 453, meaning that the proposed method is unable to classify up to 80% of NCs.", "labels": [], "entities": []}, {"text": "For real-world applications, a method which is only able to arrive at a classification for 20% of instances is clearly of limited utility, and we need someway of expanding the coverage of the proposed method.", "labels": [], "entities": []}, {"text": "This is achieved by adapting the similarity method proposed by to our task, wherein we use lexical similarity to identify the nearest-neighbour NC fora given NC, and classify the given NC according to the classification for the nearest-neighbour.", "labels": [], "entities": []}, {"text": "The results for the combined method are presented in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Coverage of D and D/I-Synonyms", "labels": [], "entities": [{"text": "Coverage of D and D/I-Synonyms", "start_pos": 10, "end_pos": 40, "type": "TASK", "confidence": 0.7538356014660427}]}, {"text": " Table 2: Results with 17 relations and with 19 relations", "labels": [], "entities": []}, {"text": " Table 3: Results of combining the proposed method and with the method of Kim and Baldwin", "labels": [], "entities": []}, {"text": " Table 4: Results for the method of Kim and Baldwin (2005) over the test set used in this research", "labels": [], "entities": []}]}