{"title": [{"text": "A Composite Kernel to Extract Relations between Entities with both Flat and Structured Features", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper proposes a novel composite kernel for relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.9495349824428558}]}, {"text": "The composite kernel consists of two individual kernels: an entity kernel that allows for entity-related features and a convolution parse tree kernel that models syntactic information of relation examples.", "labels": [], "entities": []}, {"text": "The motivation of our method is to fully utilize the nice properties of kernel methods to explore diverse knowledge for relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 120, "end_pos": 139, "type": "TASK", "confidence": 0.8830141425132751}]}, {"text": "Our study illustrates that the composite kernel can effectively capture both flat and structured features without the need for extensive feature engineering, and can also easily scale to include more features.", "labels": [], "entities": []}, {"text": "Evaluation on the ACE corpus shows that our method outperforms the previous best-reported methods and significantly out-performs previous two dependency tree kernels for relation extraction.", "labels": [], "entities": [{"text": "ACE corpus", "start_pos": 18, "end_pos": 28, "type": "DATASET", "confidence": 0.9683921933174133}, {"text": "relation extraction", "start_pos": 170, "end_pos": 189, "type": "TASK", "confidence": 0.8205986022949219}]}], "introductionContent": [{"text": "The goal of relation extraction is to find various predefined semantic relations between pairs of entities in text.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.8148944675922394}]}, {"text": "The research on relation extraction has been promoted by the Message Understanding Conferences (MUCs) and Automatic Content Extraction (ACE) program.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.9601699113845825}, {"text": "Message Understanding Conferences (MUCs)", "start_pos": 61, "end_pos": 101, "type": "TASK", "confidence": 0.804090827703476}, {"text": "Automatic Content Extraction (ACE)", "start_pos": 106, "end_pos": 140, "type": "TASK", "confidence": 0.7475582857926687}]}, {"text": "According to the ACE Program, an entity is an objector set of objects in the world and a relation is an explicitly or implicitly stated relationship among entities.", "labels": [], "entities": []}, {"text": "For example, the sentence \"Bill Gates is chairman and chief software architect of Microsoft Corporation.\" conveys the ACE-style relation \"EMPLOYMENT.exec\" between the entities \"Bill Gates\" (PERSON.Name) and \"Microsoft Corporation\" (ORGANIZATION. Commercial).", "labels": [], "entities": []}, {"text": "In this paper, we address the problem of relation extraction using kernel methods ().", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.8480104804039001}]}, {"text": "Many feature-based learning algorithms involve only the dot-product between feature vectors.", "labels": [], "entities": []}, {"text": "Kernel methods can be regarded as a generalization of the feature-based methods by replacing the dot-product with a kernel function between two vectors, or even between two objects.", "labels": [], "entities": []}, {"text": "A kernel function is a similarity function satisfying the properties of being symmetric and positive-definite.", "labels": [], "entities": []}, {"text": "Recently, kernel methods are attracting more interests in the NLP study due to their ability of implicitly exploring huge amounts of structured features using the original representation of objects.", "labels": [], "entities": []}, {"text": "For example, the kernels for structured natural language data, such as parse tree kernel, string kernel () and graph kernel ( are example instances of the wellknown convolution kernels 1 in NLP.", "labels": [], "entities": []}, {"text": "In relation extraction, typical work on kernel methods includes:, and.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.9301094114780426}]}, {"text": "This paper presents a novel composite kernel to explore diverse knowledge for relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 78, "end_pos": 97, "type": "TASK", "confidence": 0.9643667638301849}]}, {"text": "The composite kernel consists of an entity kernel and a convolution parse tree kernel.", "labels": [], "entities": []}, {"text": "Our study demonstrates that the composite kernel is very effective for relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 71, "end_pos": 90, "type": "TASK", "confidence": 0.9313516914844513}]}, {"text": "It also shows without the need for extensive feature engineering the composite kernel cannot only capture most of the flat features used in the previous work but also exploit the useful syntactic structure features effectively.", "labels": [], "entities": []}, {"text": "An advantage of our method is that the composite kernel can easily cover more knowledge by introducing more kernels.", "labels": [], "entities": []}, {"text": "Evaluation on the ACE corpus shows that our method outperforms the previous bestreported methods and significantly outperforms the previous kernel methods due to its effective exploration of various syntactic features.", "labels": [], "entities": [{"text": "ACE corpus", "start_pos": 18, "end_pos": 28, "type": "DATASET", "confidence": 0.97093066573143}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we review the previous work.", "labels": [], "entities": []}, {"text": "Section 3 discusses our composite kernel.", "labels": [], "entities": []}, {"text": "Section 4 reports the experimental results and our observations.", "labels": [], "entities": []}, {"text": "Section 5 compares our method with the 1 Convolution kernels were proposed fora discrete structure by in the machine learning field.", "labels": [], "entities": []}, {"text": "This framework defines a kernel between input objects by applying convolution \"sub-kernels\" that are the kernels for the decompositions (parts) of the objects.", "labels": [], "entities": []}, {"text": "previous work from the viewpoint of feature exploration.", "labels": [], "entities": [{"text": "feature exploration", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.7678903937339783}]}, {"text": "We conclude our work and indicate the future work in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this subsection, we report the experiments of different kernel setups for different purposes.", "labels": [], "entities": []}, {"text": "(1) Tree Kernel only over Different Relation Instance Spaces: In order to better study the impact of the syntactic structure information in a parse tree on relation extraction, we remove the entity-related information from parse trees by replacing the entity-related phrase types (\"E1-PER\" and soon as shown in) with \"NP\".", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 156, "end_pos": 175, "type": "TASK", "confidence": 0.7705817520618439}]}, {"text": "compares the performance of 5 tree kernel setups on the ACE 2003 data using the tree structure information only.", "labels": [], "entities": [{"text": "ACE 2003 data", "start_pos": 56, "end_pos": 69, "type": "DATASET", "confidence": 0.9851152102152506}]}, {"text": "It shows that: \u2022 Overall the five different relation instance spaces are all somewhat effective for relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 100, "end_pos": 119, "type": "TASK", "confidence": 0.9317183792591095}]}, {"text": "This suggests that structured syntactic information has good predication power for relation extraction and the structured syntactic information can be well captured by the tree kernel.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 83, "end_pos": 102, "type": "TASK", "confidence": 0.8803962469100952}]}, {"text": "\u2022 MCT performs much worse than the others.", "labels": [], "entities": [{"text": "MCT", "start_pos": 2, "end_pos": 5, "type": "DATASET", "confidence": 0.5985963344573975}]}, {"text": "The reasons maybe that MCT includes too much left and right context information, which may introduce many noisy features and cause over-fitting (high precision and very low recall as shown in).", "labels": [], "entities": [{"text": "precision", "start_pos": 150, "end_pos": 159, "type": "METRIC", "confidence": 0.9981057643890381}, {"text": "recall", "start_pos": 173, "end_pos": 179, "type": "METRIC", "confidence": 0.9983313679695129}]}, {"text": "This suggests that only keeping the complete (not partial) production rules in MCT does harm performance.", "labels": [], "entities": []}, {"text": "\u2022 PT achieves the best performance.", "labels": [], "entities": [{"text": "PT", "start_pos": 2, "end_pos": 4, "type": "METRIC", "confidence": 0.9971206784248352}]}, {"text": "This means that only keeping the portion of a parse tree enclosed by the shortest path between entities can model relations better than all others.", "labels": [], "entities": []}, {"text": "This maybe due to that most significant information is with PT and including context information may introduce too much noise.", "labels": [], "entities": [{"text": "PT", "start_pos": 60, "end_pos": 62, "type": "METRIC", "confidence": 0.9872389435768127}]}, {"text": "Although context may include some useful information, it is still a problem to correctly utilize such useful information in the tree kernel for relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 144, "end_pos": 163, "type": "TASK", "confidence": 0.8878129720687866}]}, {"text": "\u2022 CPT performs a bit worse than PT.", "labels": [], "entities": [{"text": "CPT", "start_pos": 2, "end_pos": 5, "type": "TASK", "confidence": 0.6933903694152832}, {"text": "PT", "start_pos": 32, "end_pos": 34, "type": "METRIC", "confidence": 0.9989168643951416}]}, {"text": "In some cases (e.g. in sentence \"the merge of company A and company B\u2026.\", \"merge\" is a critical context word), the context information is helpful.", "labels": [], "entities": []}, {"text": "However, the effective scope of context is hard to determine given the complexity and variability of natural languages.", "labels": [], "entities": []}, {"text": "\u2022 The two flattened trees perform worse than the original trees.", "labels": [], "entities": []}, {"text": "This suggests that the single nonterminal nodes are useful for relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.9473309218883514}]}, {"text": "Evaluation on the ACE 2004 data also shows that PT achieves the best performance (72.5/56.7 /63.6 in P/R/F).", "labels": [], "entities": [{"text": "ACE 2004 data", "start_pos": 18, "end_pos": 31, "type": "DATASET", "confidence": 0.9817981719970703}, {"text": "PT", "start_pos": 48, "end_pos": 50, "type": "METRIC", "confidence": 0.9630438685417175}, {"text": "P/R/F)", "start_pos": 101, "end_pos": 107, "type": "METRIC", "confidence": 0.891440232594808}]}, {"text": "More evaluations with the entity type and order information incorporated into tree nodes (\"E1-PER\", \"E2-PER\" and \"E-GPE\" as shown in) also show that PT performs best with 76.1/62.6/68.7 in P/R/F on the 2003 data and 74.1/62.4/67.7 in P/R/F on the 2004 data..", "labels": [], "entities": [{"text": "PT", "start_pos": 149, "end_pos": 151, "type": "METRIC", "confidence": 0.9179600477218628}]}, {"text": "Performance comparison of different kernel setups over the ACE major types of both the 2003 data (the numbers in parentheses) and the 2004 data (the numbers outside parentheses) (2) Composite Kernels: compares the performance of different kernel setups on the ACE major types.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. five different tree kernel setups on the  ACE 2003 five major types using the parse  tree structure information only (regardless of  any entity-related information)", "labels": [], "entities": [{"text": "ACE 2003", "start_pos": 52, "end_pos": 60, "type": "DATASET", "confidence": 0.9698083102703094}]}, {"text": " Table 5. Error distribution of major types on  both the 2003 and 2004 data for the compos- ite kernel by polynomial expansion  (4) Error Analysis:", "labels": [], "entities": [{"text": "Error", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9865742325782776}]}]}