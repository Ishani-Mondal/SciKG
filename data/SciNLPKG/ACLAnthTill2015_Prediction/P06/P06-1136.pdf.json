{"title": [{"text": "Reranking Answers for Definitional QA Using Language Modeling", "labels": [], "entities": [{"text": "Reranking Answers", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9030283391475677}]}], "abstractContent": [{"text": "* Statistical ranking methods based on cen-troid vector (profile) extracted from external knowledge have become widely adopted in the top definitional QA systems in TREC 2003 and 2004.", "labels": [], "entities": [{"text": "TREC 2003", "start_pos": 165, "end_pos": 174, "type": "DATASET", "confidence": 0.8229574263095856}]}, {"text": "In these approaches, terms in the centroid vector are treated as a bag of words based on the independent assumption.", "labels": [], "entities": []}, {"text": "To relax this assumption , this paper proposes a novel language model-based answer reranking method to improve the existing bag-of-words model approach by considering the dependence of the words in the centroid vector.", "labels": [], "entities": []}, {"text": "Experiments have been conducted to evaluate the different dependence models.", "labels": [], "entities": []}, {"text": "The results on the TREC 2003 test set show that the reranking approach with biterm language model, significantly outperforms the one with the bag-of-words model and unigram language model by 14.9% and 12.5% respectively in F-Measure(5).", "labels": [], "entities": [{"text": "TREC 2003 test set", "start_pos": 19, "end_pos": 37, "type": "DATASET", "confidence": 0.9500223696231842}, {"text": "F-Measure", "start_pos": 223, "end_pos": 232, "type": "METRIC", "confidence": 0.8793420195579529}]}], "introductionContent": [{"text": "In recent years, QA systems in TREC (Text REtrieval Conference) have made remarkable progress).", "labels": [], "entities": [{"text": "TREC (Text REtrieval Conference)", "start_pos": 31, "end_pos": 63, "type": "DATASET", "confidence": 0.7596217691898346}]}, {"text": "The task of TREC QA before 2003 has mainly focused on the factoid questions, in which the answer to the question is a number, a person name, or an organization name, or the like.", "labels": [], "entities": [{"text": "TREC QA", "start_pos": 12, "end_pos": 19, "type": "TASK", "confidence": 0.47832798957824707}]}, {"text": "Questions like \"Who is Colin Powell?\" or \"What is mold?\" are definitional questions * This work was finished while the first author was visiting Microsoft Research Asia during as a component of the project of AskBill Chatbot led by Dr. Ming Zhou.", "labels": [], "entities": []}, {"text": "(. Statistics from 2,516 Frequently Asked Questions (FAQ) extracted from Internet FAQ Archives show that around 23.6% are definitional questions.", "labels": [], "entities": [{"text": "FAQ) extracted from Internet FAQ Archives", "start_pos": 53, "end_pos": 94, "type": "DATASET", "confidence": 0.6075057898248944}]}, {"text": "This indicates that definitional questions occur frequently and are important question types.", "labels": [], "entities": [{"text": "definitional questions", "start_pos": 20, "end_pos": 42, "type": "TASK", "confidence": 0.9136138260364532}]}, {"text": "TREC started the evaluation for definitional QA in 2003.", "labels": [], "entities": [{"text": "TREC", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7722655534744263}, {"text": "definitional QA", "start_pos": 32, "end_pos": 47, "type": "TASK", "confidence": 0.9018277525901794}]}, {"text": "The definitional QA systems in TREC are required to extract definitional nuggets/sentences that contain the highly descriptive information about the question target from a given large corpus.", "labels": [], "entities": [{"text": "definitional QA", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.9030075967311859}]}, {"text": "For definitional question, statistical ranking methods based on centroid vector (profile) extracted from external resources, such as the online encyclopedia, are widely adopted in the top systems in).", "labels": [], "entities": [{"text": "definitional", "start_pos": 4, "end_pos": 16, "type": "TASK", "confidence": 0.9726867079734802}]}, {"text": "In these systems, fora given question, a vector is formed consisting of the most frequent co-occurring terms with the question target as the question profile.", "labels": [], "entities": []}, {"text": "Candidate answers extracted from a given large corpus are ranked based on their similarity to the question profile.", "labels": [], "entities": []}, {"text": "The similarity is normally the TFIDF score in which both the candidate answer and the question profile are treated as a bag of words in the framework of Vector Space Model (VSM).", "labels": [], "entities": [{"text": "similarity", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9850375056266785}, {"text": "TFIDF score", "start_pos": 31, "end_pos": 42, "type": "METRIC", "confidence": 0.9664872884750366}]}, {"text": "VSM is based on an independence assumption, which assumes that terms in a vector are statistically independent from one another.", "labels": [], "entities": []}, {"text": "Although this assumption makes the development of retrieval models easier and the retrieval operation tractable, it does not hold in textual data.", "labels": [], "entities": []}, {"text": "For example, for question \"Who is Bill Gates?\" words \"born\" and \"1955\" in the candidate answer are not independent.", "labels": [], "entities": []}, {"text": "In this paper, we are interested in considering the term dependence to improve the answer reranking for definitional QA.", "labels": [], "entities": []}, {"text": "Specifically, the language model is utilized to capture the term dependence.", "labels": [], "entities": []}, {"text": "A language model is a probability distribution that captures the statistical regularities of natural language use.", "labels": [], "entities": []}, {"text": "Ina language model, key elements are the probabilities of word sequences, denoted as P(w 1 , w 2 , ..., w n ) or P (w 1,n ) for short.", "labels": [], "entities": []}, {"text": "Recently, language model has been successfully used for information retrieval (IR)).", "labels": [], "entities": [{"text": "information retrieval (IR))", "start_pos": 56, "end_pos": 83, "type": "TASK", "confidence": 0.8533203184604645}]}, {"text": "Our natural thinking is to apply language model to rank the candidate answers as it has been applied to rank search results in IR task.", "labels": [], "entities": [{"text": "IR task", "start_pos": 127, "end_pos": 134, "type": "TASK", "confidence": 0.9048401415348053}]}, {"text": "The basic idea of our research is that, given a definitional question q, an ordered centroid OC which is learned from the web and a language model LM(OC) which is trained with it.", "labels": [], "entities": []}, {"text": "Candidate answers can be ranked by probability estimated by LM(OC).", "labels": [], "entities": [{"text": "LM(OC)", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.8525300323963165}]}, {"text": "A series of experiments on standard TREC 2003 collection have been conducted to evaluate bigram and biterm language models.", "labels": [], "entities": [{"text": "TREC 2003 collection", "start_pos": 36, "end_pos": 56, "type": "DATASET", "confidence": 0.8969825903574625}]}, {"text": "Results show that both these two language models produce promising results by capturing the term dependence and biterm model achieves the best performance.", "labels": [], "entities": [{"text": "biterm", "start_pos": 112, "end_pos": 118, "type": "METRIC", "confidence": 0.939612627029419}]}, {"text": "Biterm language model interpolating with unigram model significantly improves the VSM and unigram model by 14.9% and 12.5% in F-Measure(5).", "labels": [], "entities": []}, {"text": "In the rest of this paper, Section 2 reviews related work.", "labels": [], "entities": []}, {"text": "Section 3 presents details of the proposed method.", "labels": [], "entities": []}, {"text": "Section 4 introduces the structure of our experimental system.", "labels": [], "entities": []}, {"text": "We show the experimental results in Section 5, and conclude the paper in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to get comparable evaluation, we apply our approach to TREC 2003 definitional QA task.", "labels": [], "entities": [{"text": "TREC 2003 definitional QA task", "start_pos": 64, "end_pos": 94, "type": "DATASET", "confidence": 0.8452356934547425}]}, {"text": "More details will be shown in the following sections.", "labels": [], "entities": []}, {"text": "We employ the dataset from the TREC 2003 QA task.", "labels": [], "entities": [{"text": "TREC 2003 QA task", "start_pos": 31, "end_pos": 48, "type": "DATASET", "confidence": 0.8683401346206665}]}, {"text": "It includes the AQUAINT corpus of more than 1 million news articles from the New York, Xinhua News Agency and 50 definitional question/answer pairs.", "labels": [], "entities": [{"text": "AQUAINT corpus", "start_pos": 16, "end_pos": 30, "type": "DATASET", "confidence": 0.7648150622844696}, {"text": "New York, Xinhua News Agency", "start_pos": 77, "end_pos": 105, "type": "DATASET", "confidence": 0.6392571479082108}]}, {"text": "In these 50 definitional questions, 30 are for people (e.g., Aaron Copland), 10 are for organizations (e.g., Friends of the Earth) and 10 are for other entities (e.g., Quasars).", "labels": [], "entities": []}, {"text": "We employ Lemur 6 to retrieve relevant documents from the AQUAINT corpus.", "labels": [], "entities": [{"text": "AQUAINT corpus", "start_pos": 58, "end_pos": 72, "type": "DATASET", "confidence": 0.9617235958576202}]}, {"text": "For each query, we return the top 500 documents.", "labels": [], "entities": []}, {"text": "We adopt the evaluation metrics used in the TREC definitional QA task).", "labels": [], "entities": [{"text": "TREC definitional QA task", "start_pos": 44, "end_pos": 69, "type": "TASK", "confidence": 0.7547493278980255}]}, {"text": "TREC provides a list of essential and acceptable nuggets for answering each question.", "labels": [], "entities": [{"text": "TREC", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7571413516998291}]}, {"text": "We use these nuggets to assess our approach.", "labels": [], "entities": []}, {"text": "During this progress, two human assessors examine how many essential and acceptable nuggets are covered in the returned answers.", "labels": [], "entities": []}, {"text": "Every question is scored using nugget recall (NR) and an approximation to nugget precision (NP) based on answer length.", "labels": [], "entities": [{"text": "recall", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.8106297850608826}, {"text": "nugget precision (NP)", "start_pos": 74, "end_pos": 95, "type": "METRIC", "confidence": 0.7810802638530732}]}, {"text": "The final score fora definition response is computed using F-Measure.", "labels": [], "entities": [{"text": "F-Measure", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9812745451927185}]}, {"text": "In TREC 2003, the \u03b2 parameter was set to 5 indicating that recall is 5 times as important as precision where allowance = 100 * (# essential + # acceptable nuggets returned) and length = # nonwhite space characters in strings returned.", "labels": [], "entities": [{"text": "TREC 2003", "start_pos": 3, "end_pos": 12, "type": "DATASET", "confidence": 0.7901521623134613}, {"text": "recall", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.999268114566803}, {"text": "precision", "start_pos": 93, "end_pos": 102, "type": "METRIC", "confidence": 0.9993895292282104}, {"text": "allowance", "start_pos": 109, "end_pos": 118, "type": "METRIC", "confidence": 0.9579129219055176}, {"text": "length", "start_pos": 177, "end_pos": 183, "type": "METRIC", "confidence": 0.9649491906166077}]}, {"text": "As the first evaluation, we assess the performance obtained by our language model method against the baseline system without query expansion (QE).", "labels": [], "entities": []}, {"text": "The evaluation results are shown in  From, it is easy to observe that the unigram, bigram and biterm-based approaches improve the F(5) by 6.3%, 16.9% and 18.3% against the baseline system respectively.", "labels": [], "entities": [{"text": "biterm-based", "start_pos": 94, "end_pos": 106, "type": "METRIC", "confidence": 0.9347840547561646}, {"text": "F(5)", "start_pos": 130, "end_pos": 134, "type": "METRIC", "confidence": 0.949807420372963}]}, {"text": "At the same time, the bigram and biterm improves the F(5) by 10.0% and 11.3% against the unigram respectively.", "labels": [], "entities": [{"text": "biterm", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.9966546297073364}, {"text": "F(5)", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.956581249833107}]}, {"text": "The unigram slightly outperform the baseline.", "labels": [], "entities": []}, {"text": "We also notice that the biterm model improves slightly over the bigram model since it ignores the order of term-occurrence.", "labels": [], "entities": [{"text": "biterm", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9267319440841675}]}, {"text": "This observation coincides with the experimental results of.", "labels": [], "entities": []}, {"text": "These results show that the bigram and biterm models outperform the VSM model and the unigram model dramatically.", "labels": [], "entities": [{"text": "VSM", "start_pos": 68, "end_pos": 71, "type": "DATASET", "confidence": 0.7021998763084412}]}, {"text": "It is a clear indication that the language model which takes into account the term dependence among centroid vector is an effective way to rerank answers.", "labels": [], "entities": []}, {"text": "As mentioned above, QE is involved in our system.", "labels": [], "entities": [{"text": "QE", "start_pos": 20, "end_pos": 22, "type": "METRIC", "confidence": 0.4752340018749237}]}, {"text": "In the second evaluation, we assess the performance obtained by the language model method against the baseline system with QE.", "labels": [], "entities": []}, {"text": "We list the evaluation results in  From, we observe that, with QE, the bigram and biterm still outperform the baseline system (VSM) significantly by 12.1% (p 8 =0.03) and 14.9% (p=0.004) in F(5).", "labels": [], "entities": [{"text": "QE", "start_pos": 63, "end_pos": 65, "type": "METRIC", "confidence": 0.7421197891235352}, {"text": "biterm", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.9728182554244995}, {"text": "F", "start_pos": 190, "end_pos": 191, "type": "METRIC", "confidence": 0.9853294491767883}]}, {"text": "Furthermore, the bigram and biterm perform significantly better than the unigram by 9.7% (p=0.07) and 12.5% (p=0.02) in F(5) respectively.", "labels": [], "entities": [{"text": "biterm", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.926949143409729}, {"text": "F", "start_pos": 120, "end_pos": 121, "type": "METRIC", "confidence": 0.9886024594306946}]}, {"text": "This indicates that the term dependence is effective in keeping improving the performance.", "labels": [], "entities": []}, {"text": "It is easy to observe that the baseline is close to the unigram model since both two systems are based on the independent assumption.", "labels": [], "entities": []}, {"text": "We also notice that the biterm model improves slightly over the bigram model.", "labels": [], "entities": [{"text": "biterm", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9546104669570923}]}, {"text": "At the same time, all of the four systems improve the performance against the corresponding system without QE.", "labels": [], "entities": [{"text": "QE", "start_pos": 107, "end_pos": 109, "type": "METRIC", "confidence": 0.8599420189857483}]}, {"text": "The main reason is that the qualities of the centroid vector can be enhanced with QE.", "labels": [], "entities": [{"text": "QE", "start_pos": 82, "end_pos": 84, "type": "METRIC", "confidence": 0.9379031658172607}]}, {"text": "We are interested in the performance comparison with or without QE for each system.", "labels": [], "entities": [{"text": "QE", "start_pos": 64, "end_pos": 66, "type": "METRIC", "confidence": 0.8917752504348755}]}, {"text": "Through comparison it is found that the baseline system relies on QE more heavily than our approach does.", "labels": [], "entities": []}, {"text": "With QE, the baseline system improves the performance by 6.9% and the language model approaches improve the performance by 2.8%, 2.6% and 3.9%, respectively.", "labels": [], "entities": []}, {"text": "8 T-Test has been performed.", "labels": [], "entities": [{"text": "T-Test", "start_pos": 2, "end_pos": 8, "type": "METRIC", "confidence": 0.995851993560791}]}, {"text": "F(5) performance comparison between the baseline model and the biterm model for each of 50 TREC questions is shown in.", "labels": [], "entities": [{"text": "biterm", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.8937809467315674}]}, {"text": "QE is used in both the baseline system and the biterm system.", "labels": [], "entities": [{"text": "QE", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.611316442489624}, {"text": "biterm", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.8921094536781311}]}, {"text": "We are also interested in the comparison with the systems in TREC 2003.", "labels": [], "entities": [{"text": "TREC 2003", "start_pos": 61, "end_pos": 70, "type": "DATASET", "confidence": 0.9265105724334717}]}, {"text": "The best F(5) score returned by our proposed approach is 0.531, which is close to the top 1 run in TREC 2003.", "labels": [], "entities": [{"text": "F(5) score", "start_pos": 9, "end_pos": 19, "type": "METRIC", "confidence": 0.9515610814094544}, {"text": "TREC 2003", "start_pos": 99, "end_pos": 108, "type": "DATASET", "confidence": 0.867466539144516}]}, {"text": "The F(5) score of the best system is 0.555, reported by BBN's system (.", "labels": [], "entities": [{"text": "F(5) score", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9537176012992858}, {"text": "BBN's system", "start_pos": 56, "end_pos": 68, "type": "DATASET", "confidence": 0.9238750338554382}]}, {"text": "In BBN's experiments, the centroid vector was learned from the human made external knowledge resources, such as encyclopedia and the web.", "labels": [], "entities": [{"text": "BBN", "start_pos": 3, "end_pos": 6, "type": "DATASET", "confidence": 0.8820580840110779}]}, {"text": "gives the comparison between our biterm model-based system with the BBN's run with different \u03b2 values.", "labels": [], "entities": [{"text": "BBN", "start_pos": 68, "end_pos": 71, "type": "DATASET", "confidence": 0.9471223950386047}]}], "tableCaptions": [{"text": " Table 1. Comparisons without QE.", "labels": [], "entities": [{"text": "QE", "start_pos": 30, "end_pos": 32, "type": "METRIC", "confidence": 0.88401859998703}]}, {"text": " Table 2. Comparisons with QE.", "labels": [], "entities": [{"text": "QE", "start_pos": 27, "end_pos": 29, "type": "METRIC", "confidence": 0.5717805027961731}]}, {"text": " Table 3. Comparison with BBN's run.", "labels": [], "entities": [{"text": "BBN's run", "start_pos": 26, "end_pos": 35, "type": "DATASET", "confidence": 0.9525947570800781}]}]}