{"title": [{"text": "Automatically Extracting Nominal Mentions of Events with a Bootstrapped Probabilistic Classifier *", "labels": [], "entities": [{"text": "Automatically Extracting Nominal Mentions of Events", "start_pos": 0, "end_pos": 51, "type": "TASK", "confidence": 0.8012261639038721}]}], "abstractContent": [{"text": "Most approaches to event extraction focus on mentions anchored in verbs.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 19, "end_pos": 35, "type": "TASK", "confidence": 0.765010416507721}]}, {"text": "However, many mentions of events surface as noun phrases.", "labels": [], "entities": []}, {"text": "Detecting them can increase the recall of event extraction and provide the foundation for detecting relations between events.", "labels": [], "entities": [{"text": "recall", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9973453879356384}, {"text": "event extraction", "start_pos": 42, "end_pos": 58, "type": "TASK", "confidence": 0.6966652423143387}]}, {"text": "This paper describes a weakly-supervised method for detecting nominal event mentions that combines techniques from word sense disambiguation (WSD) and lexical acquisition to create a classifier that labels noun phrases as denoting events or non-events.", "labels": [], "entities": [{"text": "detecting nominal event mentions", "start_pos": 52, "end_pos": 84, "type": "TASK", "confidence": 0.8131202161312103}, {"text": "word sense disambiguation (WSD)", "start_pos": 115, "end_pos": 146, "type": "TASK", "confidence": 0.7638996740182241}]}, {"text": "The classifier uses boot-strapped probabilistic generative models of the contexts of events and non-events.", "labels": [], "entities": []}, {"text": "The contexts are the lexically-anchored semantic dependency relations that the NPs appear in.", "labels": [], "entities": []}, {"text": "Our method dramatically improves with bootstrapping, and comfortably outperforms lexical lookup methods which are based on very much larger hand-crafted resources.", "labels": [], "entities": []}], "introductionContent": [{"text": "The goal of information extraction is to generate a set of abstract information objects that represent the entities, events, and relations of particular types mentioned in unstructured text.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 12, "end_pos": 34, "type": "TASK", "confidence": 0.7720791399478912}]}, {"text": "For example, in a judicial domain, relevant event types might be ARREST, CHARGING, TRIAL, etc.", "labels": [], "entities": [{"text": "ARREST", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.9977854490280151}, {"text": "CHARGING", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9590420126914978}, {"text": "TRIAL", "start_pos": 83, "end_pos": 88, "type": "METRIC", "confidence": 0.9648981094360352}]}, {"text": "Although event extraction techniques usually focus on extracting mentions textually anchored by verb phrases or clauses, e.g. (Aone and Ramos- * This work was supported in part by SBIR grant FA8750-05-C-0187 from the Air Force Research Laboratory (AFRL)/IFED.), many event mentions, especially subsequent mentions of events that are the primary topic of a document, are referred to with nominals.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 9, "end_pos": 25, "type": "TASK", "confidence": 0.732410803437233}, {"text": "extracting mentions textually anchored by verb phrases or clauses", "start_pos": 54, "end_pos": 119, "type": "TASK", "confidence": 0.7739066017998589}, {"text": "SBIR grant FA8750-05-C-0187", "start_pos": 180, "end_pos": 207, "type": "DATASET", "confidence": 0.5596208572387695}, {"text": "Air Force Research Laboratory (AFRL)/IFED.", "start_pos": 217, "end_pos": 259, "type": "TASK", "confidence": 0.4549527242779732}]}, {"text": "Because of this, detecting nominal event mentions, like those in (1), can increase the recall of event extraction systems, in particular for the most important events in a document.", "labels": [], "entities": [{"text": "recall", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.9982617497444153}, {"text": "event extraction", "start_pos": 97, "end_pos": 113, "type": "TASK", "confidence": 0.7075886279344559}]}, {"text": "(1) The slain journalist was a main organizer of the massive demonstrations that forced Syria to withdraw its troops from Lebanon last April, after Assad was widely accused of planning Hariri's assassination in a February car bombing that was similar to today's blast.", "labels": [], "entities": []}, {"text": "Detecting event nominals is also an important step in detecting relations between event mentions, as in the causal relation between the demonstrations and the withdrawal and the similarity relation between the bombing and the blast in (1).", "labels": [], "entities": [{"text": "Detecting event nominals", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8669751087824503}, {"text": "similarity", "start_pos": 178, "end_pos": 188, "type": "METRIC", "confidence": 0.9790046215057373}]}, {"text": "Finally, detecting nominal events can improve detection and coreference of non-named mentions of non-event entities (e.g. persons, locations, and organizations) by removing event nominals from consideration as mentions of entities.", "labels": [], "entities": [{"text": "detecting nominal events", "start_pos": 9, "end_pos": 33, "type": "TASK", "confidence": 0.86619104941686}, {"text": "coreference of non-named mentions of non-event entities (e.g. persons, locations, and organizations)", "start_pos": 60, "end_pos": 160, "type": "TASK", "confidence": 0.8240130543708801}]}, {"text": "Current extraction techniques for verballyanchored events rest on the assumption that most verb phrases denote eventualities.", "labels": [], "entities": []}, {"text": "A system to extract untyped event mentions can output all constituents headed by a non-auxiliary verb with a filter to remove instances of to be, to seem, etc.", "labels": [], "entities": []}, {"text": "A statistical or rule-based classifier designed to detect event mentions of specific types can then be applied to filter these remaining instances.", "labels": [], "entities": []}, {"text": "Noun phrases, in contrast, can be used to denote anything-eventualities, entities, abstractions, and only some are suitable for event-type filtering.", "labels": [], "entities": [{"text": "event-type filtering", "start_pos": 128, "end_pos": 148, "type": "TASK", "confidence": 0.7232752442359924}]}], "datasetContent": [{"text": "Experiments were performed to investigate the performance of our models, both when using original seed lists, and also when varying the content of the seed lists using a bootstrapping technique that relies on the probabilistic framework of the model.", "labels": [], "entities": []}, {"text": "A 1,000-instance subset of the 9,381 test data instances was used as a validation set; the remaining 8,381 were used as evaluation data, on which we report all results (with the exception of which is on the full test set).", "labels": [], "entities": []}, {"text": "EXP1: Results using original seed sets Probabilistic models for non-events and events were built from the full list of 295 non-event and 95 event seeds, respectively, as described above. correct.", "labels": [], "entities": [{"text": "EXP1", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7055301070213318}]}, {"text": "In the second row (FAIR), undecided answers (d = 0) are left out of the total, so the number of correct answers stays the same, but the percentage of correct answers increases.", "labels": [], "entities": [{"text": "FAIR", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9953712821006775}]}, {"text": "Scores are measured in terms of accuracy on the EVENT instances, accuracy on the NONEVENT instances, TOTAL accuracy across all instances, and the simple AVERAGE of accuracies on non-events and events (last column).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9991196990013123}, {"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9993940591812134}, {"text": "TOTAL", "start_pos": 101, "end_pos": 106, "type": "METRIC", "confidence": 0.9967970252037048}, {"text": "accuracy", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.6475261449813843}, {"text": "AVERAGE of accuracies", "start_pos": 153, "end_pos": 174, "type": "METRIC", "confidence": 0.8735599319140116}]}, {"text": "The AVERAGE score assumes that performance on non-events and events is equally important to us.", "labels": [], "entities": [{"text": "AVERAGE score", "start_pos": 4, "end_pos": 17, "type": "METRIC", "confidence": 0.9653001129627228}]}, {"text": "\u00bfFrom EXP1, we see that the behavior of a term across an entire corpus is a better source of information about whether a particular instance of that term refers to an event than its immediate context.", "labels": [], "entities": []}, {"text": "We can further infer that this is because the immediate context only provides definitive evidence for the models in 63.0% of cases; when the context model is not penalized for indecision, its accuracy improves considerably.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 192, "end_pos": 200, "type": "METRIC", "confidence": 0.9995176792144775}]}, {"text": "Nonetheless, in combination with the word model, immediate context does not appear to provide much additional information over only the word.", "labels": [], "entities": []}, {"text": "In other words, based only on a term's distribution in the past, one can make a reasonable prediction about how it will be used when it is seen again.", "labels": [], "entities": []}, {"text": "Consequently, it seems that a well-constructed, i.e. domain customized, lexicon can classify nearly as well as a method that also takes context into account.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: (EXP1, EXP3) Accuracies of classifiers in terms of correct classifications, % correct, and % attempted (if allowed to", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 23, "end_pos": 33, "type": "METRIC", "confidence": 0.9898864030838013}]}, {"text": " Table 2: (EXP2) Results on ACE event nominals: %correct", "labels": [], "entities": [{"text": "ACE event nominals", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.572347084681193}]}, {"text": " Table 3: Accuracy of several lexicons, showing number and", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.995843231678009}]}, {"text": " Table 3.  Lex 1 5,435 entries from NomLex (Macleod et  al., 1998), FrameNet (Baker et al., 1998), CELEX  (CEL, 1993), Timebank(Day et al., 2003).  Lex 2 13,659 entries from WordNet 2.0 hypernym  classes EVENT, ACT, PROCESS, COGNITIVE PRO- CESS, & COMMUNICATION combined with Lex 1.  Lex 3 Combination of pre-existing lexicons in the  information extraction application from WordNet,  Oxford Advanced Learner's Dictionary, etc.  As shown in", "labels": [], "entities": [{"text": "CELEX  (CEL, 1993)", "start_pos": 99, "end_pos": 117, "type": "DATASET", "confidence": 0.759745274980863}, {"text": "Timebank", "start_pos": 119, "end_pos": 127, "type": "DATASET", "confidence": 0.8611421585083008}, {"text": "ACT", "start_pos": 211, "end_pos": 214, "type": "METRIC", "confidence": 0.9499511122703552}, {"text": "information extraction", "start_pos": 335, "end_pos": 357, "type": "TASK", "confidence": 0.7241042852401733}, {"text": "WordNet", "start_pos": 375, "end_pos": 382, "type": "DATASET", "confidence": 0.9674549102783203}, {"text": "Oxford Advanced Learner's Dictionary", "start_pos": 385, "end_pos": 421, "type": "DATASET", "confidence": 0.9364665627479554}]}]}