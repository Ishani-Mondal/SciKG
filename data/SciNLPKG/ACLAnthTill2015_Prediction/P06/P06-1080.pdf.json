{"title": [{"text": "Self-Organizing \u00d2-gram Model for Automatic Word Spacing", "labels": [], "entities": [{"text": "Automatic Word Spacing", "start_pos": 33, "end_pos": 55, "type": "TASK", "confidence": 0.5896052420139313}]}], "abstractContent": [{"text": "An automatic word spacing is one of the important tasks in Korean language processing and information retrieval.", "labels": [], "entities": [{"text": "word spacing", "start_pos": 13, "end_pos": 25, "type": "TASK", "confidence": 0.6999315023422241}, {"text": "Korean language processing", "start_pos": 59, "end_pos": 85, "type": "TASK", "confidence": 0.6658633550008138}, {"text": "information retrieval", "start_pos": 90, "end_pos": 111, "type": "TASK", "confidence": 0.8315701484680176}]}, {"text": "Since there area number of confusing cases in word spacing of Korean, there are some mistakes in many texts including news articles.", "labels": [], "entities": [{"text": "word spacing of Korean", "start_pos": 46, "end_pos": 68, "type": "TASK", "confidence": 0.7857608422636986}]}, {"text": "This paper presents a high-accurate method for automatic word spacing based on self-organizing \u00d2-gram model.", "labels": [], "entities": [{"text": "automatic word spacing", "start_pos": 47, "end_pos": 69, "type": "TASK", "confidence": 0.6479418377081553}]}, {"text": "This method is basically a variant of \u00d2-gram model, but achieves high accuracy by automatically adapting context size.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9977601766586304}]}, {"text": "In order to find the optimal context size, the proposed method automatically increases the context size when the contex-tual distribution after increasing it dose not agree with that of the current context.", "labels": [], "entities": []}, {"text": "It also decreases the context size when the distribution of reduced context is similar to that of the current context.", "labels": [], "entities": []}, {"text": "This approach achieves high accuracy by considering higher dimensional data in case of necessity, and the increased computational cost are compensated by the reduced context size.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9975076913833618}]}, {"text": "The experimental results show that the self-organizing structure of \u00d2-gram model enhances the basic model.", "labels": [], "entities": []}], "introductionContent": [{"text": "Even though Korean widely uses Chinese characters, the ideograms, it has a word spacing model unlike Chinese and Japanese.", "labels": [], "entities": []}, {"text": "The word spacing of Korean, however, is not a simple task, though the basic rule for it is simple.", "labels": [], "entities": [{"text": "word spacing of Korean", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.7092960774898529}]}, {"text": "The basic rule asserts that all content words should be spaced.", "labels": [], "entities": []}, {"text": "However, there area number of exceptions due to various postpositions and endings.", "labels": [], "entities": []}, {"text": "For instance, it is difficult to distinguish some postpositions from incomplete nouns.", "labels": [], "entities": []}, {"text": "Such exceptions induce many mistakes of word spacing even in news articles.", "labels": [], "entities": [{"text": "word spacing", "start_pos": 40, "end_pos": 52, "type": "TASK", "confidence": 0.6950805187225342}]}, {"text": "The problem of the inaccurate word spacing is that they are fatal in language processing and information retrieval.", "labels": [], "entities": [{"text": "word spacing", "start_pos": 30, "end_pos": 42, "type": "TASK", "confidence": 0.7248951494693756}, {"text": "language processing", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.7366431057453156}, {"text": "information retrieval", "start_pos": 93, "end_pos": 114, "type": "TASK", "confidence": 0.7984065413475037}]}, {"text": "The incorrect word spacing would result in the incorrect morphological analysis.", "labels": [], "entities": []}, {"text": "For instance, let us consider a famous Korean sentence: \" \u00a7 \ud97b\udf59\ud97b\udf59 \u00c9\u00d8 \ud97b\udf59 \ud97b\udf59\ud97b\udf59 \u00f9 \ud97b\udf59\ud97b\udf59 \ud97b\udf59\ud97b\udf59 \ud97b\udf59 \u00d8\ud97b\udf59 \u00c9 \ud97b\udf59\u00b0 \ud97b\udf59 \ud97b\udf59\u00a2 \ud97b\udf59.", "labels": [], "entities": [{"text": "\ud97b\udf59\ud97b\udf59 \u00c9\u00d8 \ud97b\udf59 \ud97b\udf59\ud97b\udf59 \u00f9 \ud97b\udf59\ud97b\udf59 \ud97b\udf59\ud97b\udf59 \ud97b\udf59 \u00d8\ud97b\udf59 \u00c9", "start_pos": 60, "end_pos": 85, "type": "METRIC", "confidence": 0.6043737124313008}]}, {"text": "\" The true word spacing for this sentence is \" \u00a7 \ud97b\udf59\ud97b\udf59 \u00c9\u00d8 \ud97b\udf59 \ud97b\udf59#\ud97b\udf59 \u00f9 \ud97b\udf59 \ud97b\udf59 \ud97b\udf59#\ud97b\udf59 \ud97b\udf59 \u00d8\ud97b\udf59 \u00c9 \ud97b\udf59\u00b0 \ud97b\udf59 \ud97b\udf59\u00a2 \ud97b\udf59.", "labels": [], "entities": []}, {"text": "\" whose meaning is that my father entered the room.", "labels": [], "entities": []}, {"text": "If the sentence is written as \"\ud97b\udf59 \u00c9\u00c9 \u00c9\u00d8 \ud97b\udf59# \ud97b\udf59\ud97b\udf59 \u00f9 \ud97b\udf59\ud97b\udf59 \ud97b\udf59#\ud97b\udf59 \ud97b\udf59 \u00d8\ud97b\udf59 \u00c9 \ud97b\udf59\u00b0 \ud97b\udf59 \ud97b\udf59\u00a2 \ud97b\udf59.", "labels": [], "entities": [{"text": "\u00c9\u00c9 \u00c9\u00d8 \ud97b\udf59# \ud97b\udf59\ud97b\udf59 \u00f9 \ud97b\udf59\ud97b\udf59 \ud97b\udf59#\ud97b\udf59 \ud97b\udf59 \u00d8\ud97b\udf59 \u00c9 \ud97b\udf59\u00b0", "start_pos": 33, "end_pos": 63, "type": "METRIC", "confidence": 0.5583293636639913}]}, {"text": "\", it means that my father entered the bag, which is totally different from the original meaning.", "labels": [], "entities": []}, {"text": "That is, since the morphological analysis is the first-step inmost NLP applications, the sentences with incorrect word spacing must be corrected for their further processing.", "labels": [], "entities": []}, {"text": "In addition, the wrong word spacing would result in the incorrect index for terms in information retrieval.", "labels": [], "entities": []}, {"text": "Thus, correcting the sentences with incorrect word spacing is a critical task in Korean information processing.", "labels": [], "entities": [{"text": "Korean information processing", "start_pos": 81, "end_pos": 110, "type": "TASK", "confidence": 0.6942671438058218}]}, {"text": "One of the most simple and strong models for automatic word spacing is \u00d2-gram model.", "labels": [], "entities": [{"text": "automatic word spacing", "start_pos": 45, "end_pos": 67, "type": "TASK", "confidence": 0.5977248152097067}]}, {"text": "In spite of the advantages of the \u00d2-gram model, its problem should be also considered for achieving high performance.", "labels": [], "entities": []}, {"text": "The main problem of the model is that it is usually modeled with fixed window size, \u00d2. The small value for \u00d2 represents the narrow context in modeling, which results in poor performance in general.", "labels": [], "entities": []}, {"text": "However, it is also difficult to increase \u00d2 for better performance due to data sparseness.", "labels": [], "entities": [{"text": "\u00d2", "start_pos": 42, "end_pos": 43, "type": "METRIC", "confidence": 0.9974325299263}]}, {"text": "Since the corpus size is physically limited, it is highly possible that many \u00d2-grams which do not appear in the corpus exist in the real world.", "labels": [], "entities": []}, {"text": "The goal of this paper is to provide anew method for processing automatic word spacing with an \u00d2-gram model.", "labels": [], "entities": [{"text": "processing automatic word spacing", "start_pos": 53, "end_pos": 86, "type": "TASK", "confidence": 0.6395682841539383}]}, {"text": "The proposed method automatically adapts the window size \u00d2.", "labels": [], "entities": []}, {"text": "That is, this method begins with a bigram model, and it shrinks to an unigram model when data sparseness occurs.", "labels": [], "entities": []}, {"text": "It also grows up to a trigram, fourgram, and soon when it requires more specific information in determining word spacing.", "labels": [], "entities": []}, {"text": "Ina word, the proposed model organizes the windows size \u00d2 online, and achieves high accuracy by removing both data sparseness and information lack.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9988086223602295}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 surveys the previous work on automatic word spacing and the smoothing methods for \u00d2-gram models.", "labels": [], "entities": [{"text": "automatic word spacing", "start_pos": 39, "end_pos": 61, "type": "TASK", "confidence": 0.6068614621957144}]}, {"text": "Section 3 describes the general way to automatic word spacing by an \u00d2-gram model, and Section 4 proposes a self-organizing \u00d2-gram model to overcome some drawbacks of \u00d2-gram models.", "labels": [], "entities": [{"text": "automatic word spacing", "start_pos": 39, "end_pos": 61, "type": "TASK", "confidence": 0.6145691672960917}]}, {"text": "Section 5 presents the experimental results.", "labels": [], "entities": []}, {"text": "Finally, Section 6 draws conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate the performance of the proposed method, two well-known machine learning algorithms are compared together.", "labels": [], "entities": []}, {"text": "The tested machine learning algorithms are (i) decision tree and (ii) support vector machines.", "labels": [], "entities": []}, {"text": "We use C4.5 release 8 for decision tree induction and \u00cb \u00ce\u00c5 \u00d0\u00d0\u00d8 for support vector machines.", "labels": [], "entities": [{"text": "decision tree induction", "start_pos": 26, "end_pos": 49, "type": "TASK", "confidence": 0.8086228966712952}, {"text": "\u00ce\u00c5 \u00d0\u00d0\u00d8", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.9599454700946808}]}, {"text": "For all experiments with decision trees and support vector machines, the context size is set to two since the bigram shows the best performance in.  online, two operations of expanding and shrinking were proposed.", "labels": [], "entities": []}, {"text": "shows how much the number of errors is affected by their application order.", "labels": [], "entities": []}, {"text": "The number of errors made by expanding first is 108,831 while that by shrinking first is 114,343.", "labels": [], "entities": []}, {"text": "That is, if shrinking is applied ahead of expanding, 5,512 additional errors are made.", "labels": [], "entities": []}, {"text": "Thus, it is clear that expanding should be considered first.", "labels": [], "entities": []}, {"text": "The errors by expanding can be explained with two reasons: (i) the expression power of the model and (ii) data sparseness.", "labels": [], "entities": []}, {"text": "Since Korean is a partially-free word order language and the omission of words are very frequent, \u00d2-gram model that captures local information could not express the target task sufficiently.", "labels": [], "entities": []}, {"text": "In addition, the classconditional distribution after expanding could be very different from that before expanding due to data sparseness.", "labels": [], "entities": []}, {"text": "In such cases, the expanding should not be applied since the distribution after expanding is not trustworthy.", "labels": [], "entities": []}, {"text": "However, only the difference between two distributions is considered in the proposed method, and the errors could be made by data sparseness.", "labels": [], "entities": []}, {"text": "shows that the number of training instances does not matter in computing probabilities of \u00d2-grams.", "labels": [], "entities": []}, {"text": "Even though the accuracy increases slightly, the accuracy difference after 900,000 instances is not significant.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9996199607849121}, {"text": "accuracy difference", "start_pos": 49, "end_pos": 68, "type": "METRIC", "confidence": 0.9793915450572968}]}, {"text": "It implies that the errors made by the proposed method is not from the lack of training instance but from the lack of its expression power for the target task.", "labels": [], "entities": []}, {"text": "This result also complies with.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The experimental results of various meth- ods for automatic word spacing.", "labels": [], "entities": [{"text": "automatic word spacing", "start_pos": 60, "end_pos": 82, "type": "TASK", "confidence": 0.6311147809028625}]}, {"text": " Table 3: The effect of using both left and right  context.", "labels": [], "entities": []}, {"text": " Table 4: The effect of considering a tag sequence.", "labels": [], "entities": []}]}