{"title": [{"text": "Boosting Statistical Word Alignment Using Labeled and Unlabeled Data", "labels": [], "entities": [{"text": "Boosting Statistical Word Alignment", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.631814606487751}]}], "abstractContent": [{"text": "This paper proposes a semi-supervised boosting approach to improve statistical word alignment with limited labeled data and large amounts of unlabeled data.", "labels": [], "entities": [{"text": "statistical word alignment", "start_pos": 67, "end_pos": 93, "type": "TASK", "confidence": 0.6889409720897675}]}, {"text": "The proposed approach modifies the supervised boosting algorithm to a semi-supervised learning algorithm by incorporating the unlabeled data.", "labels": [], "entities": []}, {"text": "In this algorithm , we build a word aligner by using both the labeled data and the unlabeled data.", "labels": [], "entities": []}, {"text": "Then we build a pseudo reference set for the unlabeled data, and calculate the error rate of each word aligner using only the labeled data.", "labels": [], "entities": [{"text": "error rate", "start_pos": 79, "end_pos": 89, "type": "METRIC", "confidence": 0.9632907211780548}]}, {"text": "Based on this semi-supervised boosting algorithm, we investigate two boosting methods for word alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 90, "end_pos": 104, "type": "TASK", "confidence": 0.7854783236980438}]}, {"text": "In addition, we improve the word alignment results by combining the results of the two semi-supervised boosting methods.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 28, "end_pos": 42, "type": "TASK", "confidence": 0.7712762355804443}]}, {"text": "Experimental results on word alignment indicate that semi-supervised boosting achieves relative error reductions of 28.29% and 19.52% as compared with supervised boosting and unsupervised boosting, respectively.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 24, "end_pos": 38, "type": "TASK", "confidence": 0.817921906709671}, {"text": "relative error reductions", "start_pos": 87, "end_pos": 112, "type": "METRIC", "confidence": 0.761478066444397}]}], "introductionContent": [{"text": "Word alignment was first proposed as an intermediate result of statistical machine translation (.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7669537961483002}, {"text": "statistical machine translation", "start_pos": 63, "end_pos": 94, "type": "TASK", "confidence": 0.6187534133593241}]}, {"text": "In recent years, many researchers build alignment links with bilingual corpora).", "labels": [], "entities": []}, {"text": "These methods unsupervisedly train the alignment models with unlabeled data.", "labels": [], "entities": []}, {"text": "A question about word alignment is whether we can further improve the performances of the word aligners with available data and available alignment models.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 17, "end_pos": 31, "type": "TASK", "confidence": 0.8082989454269409}]}, {"text": "One possible solution is to use the boosting method, which is one of the ensemble methods).", "labels": [], "entities": []}, {"text": "The underlying idea of boosting is to combine simple \"rules\" to form an ensemble such that the performance of the single ensemble is improved.", "labels": [], "entities": []}, {"text": "The AdaBoost (Adaptive Boosting) algorithm by was developed for supervised learning.", "labels": [], "entities": []}, {"text": "When it is applied to word alignment, it should solve the problem of building a reference set for the unlabeled data.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 22, "end_pos": 36, "type": "TASK", "confidence": 0.797600656747818}]}, {"text": "developed an unsupervised AdaBoost algorithm by automatically building a pseudo reference set for the unlabeled data to improve alignment results.", "labels": [], "entities": []}, {"text": "In fact, large amounts of unlabeled data are available without difficulty, while labeled data is costly to obtain.", "labels": [], "entities": []}, {"text": "However, labeled data is valuable to improve performance of learners.", "labels": [], "entities": []}, {"text": "Consequently, semi-supervised learning, which combines both labeled and unlabeled data, has been applied to some NLP tasks such as word sense disambiguation), classification), clustering (), named entity classification, and parsing.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 131, "end_pos": 156, "type": "TASK", "confidence": 0.602663387854894}, {"text": "classification", "start_pos": 159, "end_pos": 173, "type": "TASK", "confidence": 0.925491452217102}, {"text": "named entity classification", "start_pos": 191, "end_pos": 218, "type": "TASK", "confidence": 0.6164696216583252}, {"text": "parsing", "start_pos": 224, "end_pos": 231, "type": "TASK", "confidence": 0.9617329835891724}]}, {"text": "In this paper, we propose a semi-supervised boosting method to improve statistical word alignment with both limited labeled data and large amounts of unlabeled data.", "labels": [], "entities": [{"text": "statistical word alignment", "start_pos": 71, "end_pos": 97, "type": "TASK", "confidence": 0.7012118399143219}]}, {"text": "The proposed approach modifies the supervised AdaBoost algorithm to a semi-supervised learning algorithm by incorporating the unlabeled data.", "labels": [], "entities": []}, {"text": "Therefore, it should address the following three problems.", "labels": [], "entities": []}, {"text": "The first is to build a word alignment model with both labeled and unlabeled data.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 24, "end_pos": 38, "type": "TASK", "confidence": 0.7515583336353302}]}, {"text": "In this paper, with the labeled data, we build a supervised model by directly estimating the parameters in the model instead of using the Expectation Maximization (EM) algorithm in.", "labels": [], "entities": [{"text": "Expectation Maximization (EM)", "start_pos": 138, "end_pos": 167, "type": "TASK", "confidence": 0.6938085556030273}]}, {"text": "With the unlabeled data, we build an unsupervised model by estimating the parameters with the EM algorithm.", "labels": [], "entities": []}, {"text": "Based on these two word alignment models, an interpolated model is built through linear interpolation.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 19, "end_pos": 33, "type": "TASK", "confidence": 0.700084313750267}]}, {"text": "This interpolated model is used as a learner in the semi-supervised AdaBoost algorithm.", "labels": [], "entities": []}, {"text": "The second is to build a reference set for the unlabeled data.", "labels": [], "entities": []}, {"text": "It is automatically built with a modified \"refined\" combination method as described in.", "labels": [], "entities": []}, {"text": "The third is to calculate the error rate on each round.", "labels": [], "entities": [{"text": "error rate", "start_pos": 30, "end_pos": 40, "type": "METRIC", "confidence": 0.9845513701438904}]}, {"text": "Although we build a reference set for the unlabeled data, it still contains alignment errors.", "labels": [], "entities": [{"text": "alignment", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.9332640767097473}]}, {"text": "Thus, we use the reference set of the labeled data instead of that of the entire training data to calculate the error rate on each round.", "labels": [], "entities": [{"text": "error rate", "start_pos": 112, "end_pos": 122, "type": "METRIC", "confidence": 0.970862627029419}]}, {"text": "With the interpolated model as a learner in the semi-supervised AdaBoost algorithm, we investigate two boosting methods in this paper to improve statistical word alignment.", "labels": [], "entities": [{"text": "statistical word alignment", "start_pos": 145, "end_pos": 171, "type": "TASK", "confidence": 0.6822419464588165}]}, {"text": "The first method uses the unlabeled data only in the interpolated model.", "labels": [], "entities": []}, {"text": "During training, it only changes the distribution of the labeled data.", "labels": [], "entities": []}, {"text": "The second method changes the distribution of both the labeled data and the unlabeled data during training.", "labels": [], "entities": []}, {"text": "Experimental results show that both of these two methods improve the performance of statistical word alignment.", "labels": [], "entities": [{"text": "statistical word alignment", "start_pos": 84, "end_pos": 110, "type": "TASK", "confidence": 0.6954061090946198}]}, {"text": "In addition, we combine the final results of the above two semi-supervised boosting methods.", "labels": [], "entities": []}, {"text": "Experimental results indicate that this combination outperforms the unsupervised boosting method as described in , achieving a relative error rate reduction of 19.52%.", "labels": [], "entities": [{"text": "error rate reduction", "start_pos": 136, "end_pos": 156, "type": "METRIC", "confidence": 0.9371284246444702}]}, {"text": "And it also achieves a reduction of 28.29% as compared with the supervised boosting method that only uses the labeled data.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 briefly introduces the statistical word alignment model.", "labels": [], "entities": [{"text": "statistical word alignment", "start_pos": 33, "end_pos": 59, "type": "TASK", "confidence": 0.6712858776251475}]}, {"text": "Section 3 describes parameter estimation method using the labeled data.", "labels": [], "entities": []}, {"text": "Section 4 presents our semi-supervised boosting method.", "labels": [], "entities": []}, {"text": "Section 5 reports the experimental results.", "labels": [], "entities": []}, {"text": "Finally, we conclude in section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "With the data in section 5.1, we get the word alignment results shown in table 2.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 41, "end_pos": 55, "type": "TASK", "confidence": 0.7290288805961609}]}, {"text": "For all of the methods in this table, we perform bi-directional (source to target and target to source) word alignment, and obtain two alignment results on the testing set.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 104, "end_pos": 118, "type": "TASK", "confidence": 0.7313689142465591}]}, {"text": "Based on the two results, we get the \"refined\" combination as described in.", "labels": [], "entities": []}, {"text": "Thus, the results in table 2 are those of the \"refined\" combination.", "labels": [], "entities": []}, {"text": "For EM training, we use the GIZA++ toolkit 4 . In this paper, we take English to Chinese word alignment as a case study.", "labels": [], "entities": [{"text": "EM training", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9316731989383698}, {"text": "GIZA++ toolkit", "start_pos": 28, "end_pos": 42, "type": "DATASET", "confidence": 0.8617244958877563}, {"text": "English to Chinese word alignment", "start_pos": 70, "end_pos": 103, "type": "TASK", "confidence": 0.5604409277439117}]}, {"text": "We use the same evaluation metrics as described in , which is similar to those in.", "labels": [], "entities": []}, {"text": "The difference lies in that  take all alignment links assure links.", "labels": [], "entities": []}, {"text": "Our methods that directly estimate parameters in IBM model 4 are better than that using the EM algorithm.", "labels": [], "entities": [{"text": "IBM model 4", "start_pos": 49, "end_pos": 60, "type": "DATASET", "confidence": 0.9062426288922628}]}, {"text": "\"Labeled+Direct\" is better than \"Labeled+EM\", achieving a relative error rate reduction of 22.97%.", "labels": [], "entities": [{"text": "error rate reduction", "start_pos": 67, "end_pos": 87, "type": "METRIC", "confidence": 0.9713036417961121}]}, {"text": "And \"Labeled+Direct+Boost\" is better than \"Labeled+EM+Boost\", achieving a relative error rate reduction of 22.98%.", "labels": [], "entities": [{"text": "error rate reduction", "start_pos": 83, "end_pos": 103, "type": "METRIC", "confidence": 0.9653372168540955}]}, {"text": "In addition, the two boosting methods perform better than their corresponding methods without If we use to represent the set of alignment links identified by the proposed method and to denote the reference alignment set, the meth-", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. The labeled  data is manually word aligned, including 156,421  alignment links.", "labels": [], "entities": []}, {"text": " Table 1. Statistics for Training Data", "labels": [], "entities": []}]}