{"title": [{"text": "Maximum Entropy Based Phrase Reordering Model for Statistical Machine Translation", "labels": [], "entities": [{"text": "Phrase Reordering", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.7199311852455139}, {"text": "Statistical Machine Translation", "start_pos": 50, "end_pos": 81, "type": "TASK", "confidence": 0.8360691865285238}]}], "abstractContent": [{"text": "We propose a novel reordering model for phrase-based statistical machine translation (SMT) that uses a maximum entropy (MaxEnt) model to predicate reorderings of neighbor blocks (phrase pairs).", "labels": [], "entities": [{"text": "phrase-based statistical machine translation (SMT)", "start_pos": 40, "end_pos": 90, "type": "TASK", "confidence": 0.7376678586006165}]}, {"text": "The model provides content-dependent, hierarchical phrasal reordering with generalization based on features automatically learned from a real-world bitext.", "labels": [], "entities": []}, {"text": "We present an algorithm to extract all reordering events of neighbor blocks from bilingual data.", "labels": [], "entities": []}, {"text": "In our experiments on Chinese-to-English translation, this MaxEnt-based reordering model obtains significant improvements in BLEU score on the NIST MT-05 and IWSLT-04 tasks.", "labels": [], "entities": [{"text": "Chinese-to-English translation", "start_pos": 22, "end_pos": 52, "type": "TASK", "confidence": 0.6195280849933624}, {"text": "BLEU score", "start_pos": 125, "end_pos": 135, "type": "METRIC", "confidence": 0.9823148250579834}, {"text": "NIST MT-05", "start_pos": 143, "end_pos": 153, "type": "DATASET", "confidence": 0.8224801123142242}, {"text": "IWSLT-04", "start_pos": 158, "end_pos": 166, "type": "DATASET", "confidence": 0.49804192781448364}]}], "introductionContent": [{"text": "Phrase reordering is of great importance for phrase-based SMT systems and becoming an active area of research recently.", "labels": [], "entities": [{"text": "Phrase reordering", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9410939812660217}, {"text": "SMT", "start_pos": 58, "end_pos": 61, "type": "TASK", "confidence": 0.7955189943313599}]}, {"text": "Compared with word-based SMT systems, phrase-based systems can easily address reorderings of words within phrases.", "labels": [], "entities": [{"text": "SMT", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.84406977891922}]}, {"text": "However, at the phrase level, reordering is still a computationally expensive problem just like reordering at the word level).", "labels": [], "entities": []}, {"text": "Many systems use very simple models to reorder phrases . One is distortion model () which penalizes translations according to their jump distance instead of their content.", "labels": [], "entities": []}, {"text": "For example, if N words are skipped, a penalty of N will be paid regardless of which words are reordered.", "labels": [], "entities": []}, {"text": "This model takes the risk of penalizing long distance jumps In this paper, we focus our discussions on phrases that are not necessarily aligned to syntactic constituent boundary.", "labels": [], "entities": []}, {"text": "which are common between two languages with very different orders.", "labels": [], "entities": []}, {"text": "Another simple model is flat reordering model) which is not content dependent either.", "labels": [], "entities": []}, {"text": "Flat model assigns constant probabilities for monotone order and non-monotone order.", "labels": [], "entities": []}, {"text": "The two probabilities can beset to prefer monotone or non-monotone orientations depending on the language pairs.", "labels": [], "entities": []}, {"text": "In view of content-independency of the distortion and flat reordering models, several researchers ( proposed a more powerful model called lexicalized reordering model that is phrase dependent.", "labels": [], "entities": []}, {"text": "Lexicalized reordering model learns local orientations (monotone or non-monotone) with probabilities for each bilingual phrase from training data.", "labels": [], "entities": []}, {"text": "During decoding, the model attempts to finding a Viterbi local orientation sequence.", "labels": [], "entities": []}, {"text": "Performance gains have been reported for systems with lexicalized reordering model.", "labels": [], "entities": []}, {"text": "However, since reorderings are related to concrete phrases, researchers have to design their systems carefully in order not to cause other problems, e.g. the data sparseness problem.", "labels": [], "entities": []}, {"text": "Another smart reordering model was proposed by.", "labels": [], "entities": []}, {"text": "In his approach, phrases are reorganized into hierarchical ones by reducing subphrases to variables.", "labels": [], "entities": []}, {"text": "This template-based scheme not only captures the reorderings of phrases, but also integrates some phrasal generalizations into the global model.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel solution for phrasal reordering.", "labels": [], "entities": [{"text": "phrasal reordering", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.7375262677669525}]}, {"text": "Here, under the ITG constraint), we need to consider just two kinds of reorderings, straight and inverted between two consecutive blocks.", "labels": [], "entities": [{"text": "ITG", "start_pos": 16, "end_pos": 19, "type": "DATASET", "confidence": 0.730991780757904}]}, {"text": "Therefore reordering can be modelled as a problem of classification with only two labels, straight and inverted.", "labels": [], "entities": []}, {"text": "In this paper, we build a maximum entropy based classification model as the reordering model.", "labels": [], "entities": []}, {"text": "Different from lexicalized reordering, we do not use the whole block as reordering evidence, but only features extracted from blocks.", "labels": [], "entities": []}, {"text": "It makes our model reorder any blocks, observed in training or not.", "labels": [], "entities": []}, {"text": "The whole maximum entropy based reordering model is embedded inside a log-linear phrase-based model of translation.", "labels": [], "entities": []}, {"text": "Following the Bracketing Transduction Grammar (BTG), we built a CKY-style decoder for our system, which makes it possible to reorder phrases hierarchically.", "labels": [], "entities": [{"text": "Bracketing Transduction Grammar (BTG)", "start_pos": 14, "end_pos": 51, "type": "TASK", "confidence": 0.7314464996258417}]}, {"text": "To create a maximum entropy based reordering model, the first step is learning reordering examples from training data, similar to the lexicalized reordering model.", "labels": [], "entities": []}, {"text": "But in our way, any evidences of reorderings will be extracted, not limited to reorderings of bilingual phrases of length less than a predefined number of words.", "labels": [], "entities": []}, {"text": "Secondly, features will be extracted from reordering examples according to feature templates.", "labels": [], "entities": []}, {"text": "Finally, a maximum entropy classifier will be trained on the features.", "labels": [], "entities": []}, {"text": "In this paper we describe our system and the MaxEnt-based reordering model with the associated algorithm.", "labels": [], "entities": []}, {"text": "We also present experiments that indicate that the MaxEnt-based reordering model improves translation significantly compared with other reordering approaches and a state-of-the-art distortion-based system).", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: BLEU-4 scores (%) with the 95% confi- dence intervals. Italic numbers refer to results for  which the difference to the best result (indicated in  bold) is not statistically significant.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9982993006706238}, {"text": "confi- dence intervals", "start_pos": 41, "end_pos": 63, "type": "METRIC", "confidence": 0.7284315824508667}]}]}