{"title": [{"text": "Integrated Morphological and Syntactic Disambiguation for Modern Hebrew", "labels": [], "entities": [{"text": "Morphological and Syntactic Disambiguation for Modern Hebrew", "start_pos": 11, "end_pos": 71, "type": "TASK", "confidence": 0.6069381790501731}]}], "abstractContent": [{"text": "Current parsing models are not immediately applicable for languages that exhibit strong interaction between morphology and syntax, e.g., Modern Hebrew (MH), Arabic and other Semitic languages.", "labels": [], "entities": []}, {"text": "This work represents a first attempt at model-ing morphological-syntactic interaction in a generative probabilistic framework to allow for MH parsing.", "labels": [], "entities": [{"text": "MH parsing", "start_pos": 139, "end_pos": 149, "type": "TASK", "confidence": 0.977088063955307}]}, {"text": "We show that morphological information selected in tandem with syntactic categories is instrumental for parsing Semitic languages.", "labels": [], "entities": [{"text": "parsing Semitic languages", "start_pos": 104, "end_pos": 129, "type": "TASK", "confidence": 0.9058163364728292}]}, {"text": "We further show that redundant morphological information helps syntactic disambiguation.", "labels": [], "entities": [{"text": "syntactic disambiguation", "start_pos": 63, "end_pos": 87, "type": "TASK", "confidence": 0.8729789853096008}]}], "introductionContent": [{"text": "Natural Language Processing is typically viewed as consisting of different layers, 1 each of which is handled separately.", "labels": [], "entities": [{"text": "Natural Language Processing", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6146362125873566}]}, {"text": "The structure of Semitic languages poses clear challenges to this traditional division of labor.", "labels": [], "entities": []}, {"text": "Specifically, Semitic languages demonstrate strong interaction between morphological and syntactic processing, which limits the applicability of standard tools for, e.g., parsing.", "labels": [], "entities": []}, {"text": "This work focuses on MH and explores the ways morphological and syntactic processing interact.", "labels": [], "entities": [{"text": "MH", "start_pos": 21, "end_pos": 23, "type": "TASK", "confidence": 0.975109875202179}]}, {"text": "Using a morphological analyzer, a part-ofspeech tagger, and a PCFG-based general-purpose parser, we segment and parse MH sentences based on a small, annotated corpus.", "labels": [], "entities": []}, {"text": "Our integrated model shows that percolating morphological ambiguity to the lowest level of non-terminals in the syntactic parse tree improves parsing accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 142, "end_pos": 149, "type": "TASK", "confidence": 0.9741150140762329}, {"text": "accuracy", "start_pos": 150, "end_pos": 158, "type": "METRIC", "confidence": 0.8992272615432739}]}, {"text": "Moreover, we show that morphological cues facilitate syntactic disambiguation.", "labels": [], "entities": [{"text": "syntactic disambiguation", "start_pos": 53, "end_pos": 77, "type": "TASK", "confidence": 0.7775216102600098}]}, {"text": "A particular contribution of this work is to demonstrate that MH statistical parsing is feasible.", "labels": [], "entities": [{"text": "MH statistical parsing", "start_pos": 62, "end_pos": 84, "type": "TASK", "confidence": 0.8795312841733297}]}, {"text": "Yet, the results obtained are not comparable to those of, e.g., state-of-theart models for English, due to remaining syntactic ambiguity and limited morphological treatment.", "labels": [], "entities": []}, {"text": "We conjecture that adequate morphological and syntactic processing of MH should be done in a unified framework, in which both levels can interact and share information in both directions.", "labels": [], "entities": [{"text": "syntactic processing of MH", "start_pos": 46, "end_pos": 72, "type": "TASK", "confidence": 0.6378084123134613}]}, {"text": "Section 2 presents linguistic data that demonstrate the strong interaction between morphology and syntax in MH, thus motivating our choice to treat both in the same framework.", "labels": [], "entities": []}, {"text": "Section 3 surveys previous work and demonstrates again the unavoidable interaction between the two.", "labels": [], "entities": []}, {"text": "Section 4.1 puts forward the formal setting of an integrated probabilistic language model, followed by the evaluation metrics defined for the integrated task in section 4.2.", "labels": [], "entities": []}, {"text": "Sections 4.3 and 4.4 then describe the experimental setup and preliminary results for our baseline implementation, and section 5 discusses more sophisticated models we intend to investigate.", "labels": [], "entities": []}], "datasetContent": [{"text": "The intertwined nature of morphology and syntax in MH poses additional challenges to standard parsing evaluation metrics.", "labels": [], "entities": [{"text": "MH", "start_pos": 51, "end_pos": 53, "type": "TASK", "confidence": 0.9545356035232544}]}, {"text": "First, note that we cannot use morphemes as the basic units for comparison, as the proposed segmentation need not coincide with the gold segmentation fora given sentence.", "labels": [], "entities": []}, {"text": "Since words are complex entities that Since concatenated particles (conjunctions et al.) appear in front of the stem, pronominal and inflectional affixes at the end of the stem, and derivational morphology inside the stem, there is typically a unique way to restore word boundaries.", "labels": [], "entities": []}, {"text": "can span across phrases (see, we cannot use them for comparison either.", "labels": [], "entities": []}, {"text": "We propose to redefine precision and recall by considering the spans of syntactic categories based on the (spacefree) sequences of characters to which they correspond.", "labels": [], "entities": [{"text": "precision", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9975320100784302}, {"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9873477220535278}]}, {"text": "Formally, we define syntactic constituents as \ud97b\udf59i, A, j\ud97b\udf59 where i, j mark the location of characters.", "labels": [], "entities": []}, {"text": "T = {{i, A, j\ud97b\udf59|A spans from i to j} and G = {{i, A, j\ud97b\udf59|A spans from i to j} represent the test/gold parses, respectively, and we calculate: 8  Our departure point for the syntactic analysis of MH is that the basic units for processing are not words, but morphological segments that are concatenated together to form words.", "labels": [], "entities": [{"text": "syntactic analysis of MH", "start_pos": 171, "end_pos": 195, "type": "TASK", "confidence": 0.5797361955046654}]}, {"text": "Therefore, we obtain a segment-based probabilistic grammar by training a Probabilistic Context Free Grammar (PCFG) on a segmented and annotated MH corpus).", "labels": [], "entities": [{"text": "MH corpus", "start_pos": 144, "end_pos": 153, "type": "DATASET", "confidence": 0.7733505368232727}]}, {"text": "Then, we use existing tools -i.e., a morphological analyzer), a part-of-speech tagger), and a general-purpose parser) -to find compatible morphological segmentations and syntactic analyses for unseen sentences.", "labels": [], "entities": []}, {"text": "The Data The data set we use is taken from the MH treebank which consists of 5001 sentences from the daily newspaper 'ha'aretz').", "labels": [], "entities": [{"text": "MH treebank", "start_pos": 47, "end_pos": 58, "type": "DATASET", "confidence": 0.982714831829071}]}, {"text": "We employ the syntactic categories and POS tag sets developed therein.", "labels": [], "entities": []}, {"text": "Our data set includes 3257 sentences of length greater than 1 and less than 21.", "labels": [], "entities": []}, {"text": "The number of segments per sentence is 60% higher than the number of words per sentence.", "labels": [], "entities": []}, {"text": "We conducted 8 experiments in which the data is split to training and test sets and apply cross-fold validation to obtain robust averages.", "labels": [], "entities": []}, {"text": "The Models Model I uses the morphological analyzer and the POS tagger to find the most probable segmentation fora given sentence.", "labels": [], "entities": []}, {"text": "This is done by providing the POS tagger with multiple morphological analyses per word and maximizing the sum \ud97b\udf59  The average number of words per sentence in the complete corpus is 17 while the average number of morphological segments per sentence is probable parse tree for the selected sequence of morphological segments.", "labels": [], "entities": []}, {"text": "Formally, this model is a first approximation of equation using a stepwise maximization instead of a joint one.", "labels": [], "entities": []}, {"text": "In Model II we percolate the morphological ambiguity further, to the lowest level of non-terminals in the syntactic trees.", "labels": [], "entities": []}, {"text": "Here we use the morphological analyzer and the POS tagger to find the most probable segmentation and POS tag assignment by maximizing the joint probability P (t n 1 , s n 1 |w m 1 ).", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 47, "end_pos": 57, "type": "TASK", "confidence": 0.5816827118396759}, {"text": "POS tag assignment", "start_pos": 101, "end_pos": 119, "type": "TASK", "confidence": 0.6646218299865723}]}, {"text": "Then, the parser is used to parse the tagged segments.", "labels": [], "entities": []}, {"text": "Formally, this model attempts to approximate equation.", "labels": [], "entities": []}, {"text": "(Note that here we couple a morphological and a syntactic decision, as we are looking to maximize P (t n 1 , s n 1 |w m 1 ) \u2248 P (t n 1 |s n 1 )P (s n 1 |w m 1 ) and constrain the space of trees to those that agree with the resulting analysis.)", "labels": [], "entities": []}, {"text": "In both models, smoothing the estimated probabilities is delegated to the relevant subcomponents.", "labels": [], "entities": []}, {"text": "Out of vocabulary (OOV) words are treated by the morphological analyzer, which proposes all possible segmentations assuming that the stem is a proper noun.", "labels": [], "entities": []}, {"text": "The Tri-gram model used for POS tagging is smoothed using Good-Turing discounting (see, section 6.1)), and the parser uses absolute discounting with various backoff strategies, section 4.4).", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 28, "end_pos": 39, "type": "TASK", "confidence": 0.879863828420639}]}, {"text": "The Tag-Sets To examine the usefulness of various morphological features shared with the parsing task, we alter the set of morphosyntactic categories to include more fine-grained morphological distinctions.", "labels": [], "entities": [{"text": "parsing task", "start_pos": 89, "end_pos": 101, "type": "TASK", "confidence": 0.910111665725708}]}, {"text": "We use three sets: Set A contains bare POS categories, Set B identifies also definite nouns marked for possession, and Set C adds the distinction between finite and non-finite verb forms.", "labels": [], "entities": []}, {"text": "Evaluation We use seven measures to evaluate our models' performance on the integrated task.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Evaluation Metrics, Models I and II", "labels": [], "entities": []}]}