{"title": [{"text": "Combining Association Measures for Collocation Extraction", "labels": [], "entities": [{"text": "Combining Association Measures", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.5907695790131887}, {"text": "Collocation Extraction", "start_pos": 35, "end_pos": 57, "type": "TASK", "confidence": 0.8562529385089874}]}], "abstractContent": [{"text": "We introduce the possibility of combining lexical association measures and present empirical results of several methods employed in automatic collocation extraction.", "labels": [], "entities": [{"text": "automatic collocation extraction", "start_pos": 132, "end_pos": 164, "type": "TASK", "confidence": 0.6451275746027628}]}, {"text": "First, we present a comprehensive summary overview of association measures and their performance on manually annotated data evaluated by precision-recall graphs and mean average precision.", "labels": [], "entities": [{"text": "precision-recall", "start_pos": 137, "end_pos": 153, "type": "METRIC", "confidence": 0.974049985408783}, {"text": "mean average precision", "start_pos": 165, "end_pos": 187, "type": "METRIC", "confidence": 0.7988961537679037}]}, {"text": "Second, we describe several classification methods for combining association measures , followed by their evaluation and comparison with individual measures.", "labels": [], "entities": []}, {"text": "Finally , we propose a feature selection algorithm significantly reducing the number of combined measures with only a small performance degradation.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.7560076713562012}]}], "introductionContent": [{"text": "Lexical association measures are mathematical formulas determining the strength of association between two or more words based on their occurrences and cooccurrences in a text corpus.", "labels": [], "entities": []}, {"text": "They have a wide spectrum of applications in the field of natural language processing and computational linguistics such as automatic collocation extraction), bilingual word alignment () or dependency parsing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 58, "end_pos": 85, "type": "TASK", "confidence": 0.6733123660087585}, {"text": "automatic collocation extraction", "start_pos": 124, "end_pos": 156, "type": "TASK", "confidence": 0.7123812635739645}, {"text": "bilingual word alignment", "start_pos": 159, "end_pos": 183, "type": "TASK", "confidence": 0.6411862671375275}, {"text": "dependency parsing", "start_pos": 190, "end_pos": 208, "type": "TASK", "confidence": 0.8752935528755188}]}, {"text": "A number of various association measures were introduced in the last decades.", "labels": [], "entities": []}, {"text": "An overview of the most widely used techniques is given e.g. in or.", "labels": [], "entities": []}, {"text": "Several researchers also attempted to compare existing methods and suggest different evaluation schemes, e.g and.", "labels": [], "entities": []}, {"text": "A comprehensive study of statistical aspects of word cooccurrences can be found in Evert or.", "labels": [], "entities": [{"text": "Evert", "start_pos": 83, "end_pos": 88, "type": "DATASET", "confidence": 0.8982577323913574}]}, {"text": "In this paper we present a novel approach to automatic collocation extraction based on combining multiple lexical association measures.", "labels": [], "entities": [{"text": "automatic collocation extraction", "start_pos": 45, "end_pos": 77, "type": "TASK", "confidence": 0.6191688477993011}]}, {"text": "We also address the issue of the evaluation of association measures by precision-recall graphs and mean average precision scores.", "labels": [], "entities": [{"text": "precision-recall", "start_pos": 71, "end_pos": 87, "type": "METRIC", "confidence": 0.946630597114563}, {"text": "mean average precision scores", "start_pos": 99, "end_pos": 128, "type": "METRIC", "confidence": 0.8316760808229446}]}, {"text": "Finally, we propose a stepwise feature selection algorithm that reduces the number of combined measures needed with respect to performance on held-out data.", "labels": [], "entities": []}, {"text": "The term collocation has both linguistic and lexicographic character.", "labels": [], "entities": []}, {"text": "It has various definitions but none of them is widely accepted.", "labels": [], "entities": []}, {"text": "We adopt the definition from who defines a collocational expression as \"a syntactic and semantic unit whose exact and unambiguous meaning or connotation cannot be derived directly from the meaning or connotation of its components\".", "labels": [], "entities": []}, {"text": "This notion of collocation is relatively wide and covers abroad range of lexical phenomena such as idioms, phrasal verbs, light verb compounds, technological expressions, proper names, and stock phrases.", "labels": [], "entities": []}, {"text": "Our motivation originates from machine translation: we want to capture all phenomena that may require special treatment in translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.76336470246315}]}, {"text": "Experiments presented in this paper were performed on Czech data and our attention was restricted to two-word (bigram) collocations -primarily for the limited scalability of some methods to higher-order n-grams and also for the reason that experiments with longer word expressions would require processing of much larger corpus to obtain enough evidence of the observed events.", "labels": [], "entities": [{"text": "Czech data", "start_pos": 54, "end_pos": 64, "type": "DATASET", "confidence": 0.8694859445095062}]}], "datasetContent": [{"text": "Collocation extraction can be viewed as classification into two categories.", "labels": [], "entities": [{"text": "Collocation extraction", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9491831362247467}]}, {"text": "By setting a threshold, any association measure becomes a binary classifier: bigrams with higher association scores fall into one class (collocations), the rest into the other class (non-collocations).", "labels": [], "entities": []}, {"text": "Performance of such classifiers can be measured for example by accuracy -fraction of correct predictions.", "labels": [], "entities": [{"text": "accuracy -fraction", "start_pos": 63, "end_pos": 81, "type": "METRIC", "confidence": 0.9772905906041464}]}, {"text": "However, the proportion of the two classes in our case is far from equal and we want to distinguish classifier performance between them.", "labels": [], "entities": []}, {"text": "In this case, several authors, e.g. Evert (2001), suggest using precision -fraction of positive predictions correct and recall -fraction of positives correctly predicted.", "labels": [], "entities": [{"text": "Evert", "start_pos": 36, "end_pos": 41, "type": "METRIC", "confidence": 0.9326404929161072}, {"text": "precision -fraction", "start_pos": 64, "end_pos": 83, "type": "METRIC", "confidence": 0.9555231332778931}, {"text": "recall -fraction", "start_pos": 120, "end_pos": 136, "type": "METRIC", "confidence": 0.9720621903737386}]}, {"text": "The higher the scores the better the classification is.", "labels": [], "entities": []}, {"text": "We performed the model reduction experiment on the neural network with five units in the hidden layer (the best performing combination method).", "labels": [], "entities": [{"text": "model reduction", "start_pos": 17, "end_pos": 32, "type": "TASK", "confidence": 0.7019727528095245}]}, {"text": "The similarity matrix for hierarchical clustering was computed on the held-out data and parameter d (number of initial predictors) was experimentally set to 60.", "labels": [], "entities": []}, {"text": "In each iteration of the algorithm, we used four data folds (out of the five used in previous experiments) for fitting the models and the held-out fold to measure the performance of these models and to select the variable to be removed.", "labels": [], "entities": []}, {"text": "The new model was cross-validated on the same five data-folds as in the previous experiments.", "labels": [], "entities": []}, {"text": "Precision-recall curves for some intermediate models are shown in.", "labels": [], "entities": [{"text": "Precision-recall", "start_pos": 0, "end_pos": 16, "type": "METRIC", "confidence": 0.9910510778427124}]}, {"text": "We can conclude that we were able to reduce the NNet model to about 17 predictors without statistically significant difference in performance.", "labels": [], "entities": [{"text": "NNet model", "start_pos": 48, "end_pos": 58, "type": "DATASET", "confidence": 0.863647997379303}]}, {"text": "The corresponding association measures are marked in and highlighted in.", "labels": [], "entities": []}, {"text": "They include measures from the entire range of individual mean average precision values.", "labels": [], "entities": [{"text": "precision", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.5197852849960327}]}], "tableCaptions": [{"text": " Table 2: Performance of methods combining all association  measures: average precision (AP) for fixed recall values and  mean average precision (MAP) on the narrower recall interval  with relative improvement in the last column (values in %).", "labels": [], "entities": [{"text": "average precision (AP)", "start_pos": 70, "end_pos": 92, "type": "METRIC", "confidence": 0.9056489944458008}, {"text": "recall", "start_pos": 103, "end_pos": 109, "type": "METRIC", "confidence": 0.8327811360359192}, {"text": "mean average precision (MAP)", "start_pos": 122, "end_pos": 150, "type": "METRIC", "confidence": 0.9439855714639028}, {"text": "recall interval", "start_pos": 167, "end_pos": 182, "type": "METRIC", "confidence": 0.9523336291313171}]}]}