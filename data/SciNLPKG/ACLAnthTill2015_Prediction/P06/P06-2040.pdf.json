{"title": [{"text": "Reduced n-gram models for English and Chinese corpora", "labels": [], "entities": []}], "abstractContent": [{"text": "Statistical language models should improve as the size of the n-grams increases from 3 to 5 or higher.", "labels": [], "entities": []}, {"text": "However, the number of parameters and calculations, and the storage requirement increase very rapidly if we attempt to store all possible combinations of n-grams.", "labels": [], "entities": []}, {"text": "To avoid these problems, the reduced n-grams' approach previously developed by O'Boyle (1993) can be applied.", "labels": [], "entities": []}, {"text": "A reduced n-gram language model can store an entire corpus's phrase-history length within feasible storage limits.", "labels": [], "entities": []}, {"text": "Another theoretical advantage of reduced n-grams is that they are closer to being semantically complete than traditional models, which include all n-grams.", "labels": [], "entities": []}, {"text": "In our experiments, the reduced n-gram Zipf curves are first presented, and compared with previously obtained conventional n-grams for both English and Chinese.", "labels": [], "entities": []}, {"text": "The reduced n-gram model is then applied to large English and Chinese corpora.", "labels": [], "entities": []}, {"text": "For English, we can reduce the model sizes, compared to 7-gram traditional model sizes, with factors of 14.6 fora 40-million-word corpus and 11.0 fora 500-million-word corpus while obtaining 5.8% and 4.2% improvements in perplexities.", "labels": [], "entities": []}, {"text": "For Chinese, we gain a 16.9% perplexity reductions and we reduce the model size by a factor larger than 11.2.", "labels": [], "entities": []}, {"text": "This paper is a step towards the modeling of English and Chinese using semantically complete phrases in an n-gram model.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": [{"text": " Table 1 combines all of their results.", "labels": [], "entities": []}, {"text": " Table 2. Most common WSJ reduced n-grams", "labels": [], "entities": []}, {"text": " Table 3. Reduced perplexities for English WSJ", "labels": [], "entities": [{"text": "WSJ", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.5867453813552856}]}, {"text": " Table 4. Reduced perplexities for English NANT", "labels": [], "entities": [{"text": "NANT", "start_pos": 43, "end_pos": 47, "type": "TASK", "confidence": 0.2542562484741211}]}, {"text": " Table 5. Reduced perplexities for Mandarin  News words", "labels": [], "entities": [{"text": "Mandarin  News", "start_pos": 35, "end_pos": 49, "type": "DATASET", "confidence": 0.8309578597545624}]}]}