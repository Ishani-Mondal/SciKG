{"title": [{"text": "Trimming CFG Parse Trees for Sentence Compression Using Machine Learning Approaches", "labels": [], "entities": [{"text": "Trimming CFG Parse Trees", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7621540725231171}, {"text": "Sentence Compression", "start_pos": 29, "end_pos": 49, "type": "TASK", "confidence": 0.8992843329906464}]}], "abstractContent": [{"text": "Sentence compression is a task of creating a short grammatical sentence by removing extraneous words or phrases from an original sentence while preserving its meaning.", "labels": [], "entities": [{"text": "Sentence compression is a task of creating a short grammatical sentence by removing extraneous words or phrases from an original sentence while preserving its meaning", "start_pos": 0, "end_pos": 166, "type": "Description", "confidence": 0.778438514471054}]}, {"text": "Existing methods learn statistics on trimming context-free grammar (CFG) rules.", "labels": [], "entities": [{"text": "trimming context-free grammar (CFG) rules", "start_pos": 37, "end_pos": 78, "type": "TASK", "confidence": 0.8790744968823024}]}, {"text": "However, these methods sometimes eliminate the original meaning by incorrectly removing important parts of sentences, because trimming probabilities only depend on parents' and daughters' non-terminals in applied CFG rules.", "labels": [], "entities": []}, {"text": "We apply a maximum entropy model to the above method.", "labels": [], "entities": []}, {"text": "Our method can easily include various features, for example, other parts of a parse tree or words the sentences contain.", "labels": [], "entities": []}, {"text": "We evaluated the method using manually compressed sentences and human judgments.", "labels": [], "entities": []}, {"text": "We found that our method produced more grammatical and informative compressed sentences than other methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "In most automatic summarization approaches, text is summarized by extracting sentences from a given document without modifying the sentences themselves.", "labels": [], "entities": [{"text": "summarization", "start_pos": 18, "end_pos": 31, "type": "TASK", "confidence": 0.74640291929245}]}, {"text": "Although these methods have been significantly improved to extract good sentences as summaries, they are not intended to shorten sentences; i.e., the output often has redundant words or phrases.", "labels": [], "entities": []}, {"text": "These methods cannot be used to make a shorter sentence from an input sentence or for other applications such as generating headline news ( or messages for the small screens of mobile devices.", "labels": [], "entities": []}, {"text": "We need to compress sentences to obtain short and useful summaries.", "labels": [], "entities": []}, {"text": "This task is called sentence compression.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 20, "end_pos": 40, "type": "TASK", "confidence": 0.8246352672576904}]}, {"text": "While several methods have been proposed for sentence compression), this paper focuses on Knight and Marcu's noisy-channel model and presents an extension of their method.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 45, "end_pos": 65, "type": "TASK", "confidence": 0.8187310099601746}]}, {"text": "They developed a probabilistic model for trimming a CFG parse tree of an input sentence.", "labels": [], "entities": [{"text": "trimming a CFG parse tree of an input sentence", "start_pos": 41, "end_pos": 87, "type": "TASK", "confidence": 0.7797653741306729}]}, {"text": "Their method drops words of input sentences but does not change their order or change the words.", "labels": [], "entities": []}, {"text": "They use a parallel corpus that contains pairs of original and compressed sentences.", "labels": [], "entities": []}, {"text": "The method makes CFG parse trees of both original and compressed sentences and learns trimming probabilities from these pairs.", "labels": [], "entities": [{"text": "CFG parse trees", "start_pos": 17, "end_pos": 32, "type": "TASK", "confidence": 0.6613495151201884}]}, {"text": "Although their method is concise and well-defined, its accuracy is still unsatisfactory.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9993763566017151}]}, {"text": "Their method has two problems.", "labels": [], "entities": []}, {"text": "One is that probabilities are calculated only from the frequencies of applied CFG rules, and other characteristics like whether the phrase includes negative words cannot be introduced.", "labels": [], "entities": []}, {"text": "The other problem is that the parse trees of original and compressed sentences sometimes do not correspond.", "labels": [], "entities": []}, {"text": "To solve the former problem, we apply a maximum entropy model to Knight and Marcu's model to introduce machine learning features that are defined not only for CFG rules but also for other characteristics in a parse tree, such as the depth from the root node or words it contains.", "labels": [], "entities": []}, {"text": "To solve the latter problem, we introduce a novel matching method, the bottom-up method, to learn complicated relations of two unmatched trees.", "labels": [], "entities": []}, {"text": "We evaluated each algorithm using the ZiffDavis corpus, which has long and short sentence pairs.", "labels": [], "entities": []}, {"text": "We compared our method with Knight and Marcu's method in terms of F -measures, bigram F -measures, BLEU scores and human judgments.", "labels": [], "entities": [{"text": "F -measures", "start_pos": 66, "end_pos": 77, "type": "METRIC", "confidence": 0.9859447280565897}, {"text": "bigram F -measures", "start_pos": 79, "end_pos": 97, "type": "METRIC", "confidence": 0.9041611701250076}, {"text": "BLEU scores", "start_pos": 99, "end_pos": 110, "type": "METRIC", "confidence": 0.9811608493328094}]}], "datasetContent": [{"text": "We evaluated each sentence compression method using word F -measures, bigram F -measures, and BLEU scores ().", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 18, "end_pos": 38, "type": "TASK", "confidence": 0.7351527214050293}, {"text": "word F -measures", "start_pos": 52, "end_pos": 68, "type": "METRIC", "confidence": 0.8297780901193619}, {"text": "bigram F -measures", "start_pos": 70, "end_pos": 88, "type": "METRIC", "confidence": 0.8790723234415054}, {"text": "BLEU", "start_pos": 94, "end_pos": 98, "type": "METRIC", "confidence": 0.9993287324905396}]}, {"text": "BLEU scores are usually used for evaluating machine translation quality.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9816856384277344}, {"text": "machine translation", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.8143637478351593}]}, {"text": "A BLEU score is defined as the weighted geometric average of n-gram precisions with length penalties.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 2, "end_pos": 12, "type": "METRIC", "confidence": 0.9755659401416779}]}, {"text": "We used from unigram to 4-gram precisions and uniform weights for the BLEU scores.", "labels": [], "entities": [{"text": "precisions", "start_pos": 31, "end_pos": 41, "type": "METRIC", "confidence": 0.9395687580108643}, {"text": "BLEU", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.9979566335678101}]}, {"text": "ROUGE) is a set of recall-based criteria that is mainly used for evaluating summarization tasks.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9172427654266357}, {"text": "recall-based", "start_pos": 19, "end_pos": 31, "type": "METRIC", "confidence": 0.992201566696167}, {"text": "summarization tasks", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.9242897033691406}]}, {"text": "ROUGE-N uses average N-gram recall, and ROUGE-1 is word recall.", "labels": [], "entities": [{"text": "ROUGE-N", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9527976512908936}, {"text": "recall", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.9301201701164246}, {"text": "ROUGE-1", "start_pos": 40, "end_pos": 47, "type": "METRIC", "confidence": 0.99616539478302}, {"text": "recall", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.5669465065002441}]}, {"text": "ROUGE-L uses the length of the longest common subsequence (LCS) of the original and summarized sentences.", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.5808108448982239}]}, {"text": "In our model, the length of the LCS is equal to the number of common words, and ROUGE-L is equal to the unigram F -measure because words are not rearranged.", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 80, "end_pos": 87, "type": "METRIC", "confidence": 0.9987006187438965}, {"text": "unigram F -measure", "start_pos": 104, "end_pos": 122, "type": "METRIC", "confidence": 0.8075308501720428}]}, {"text": "ROUGE-L and ROUGE-1 are supposed to be appropriate for the headline gener-ation task).", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9699848294258118}, {"text": "ROUGE-1", "start_pos": 12, "end_pos": 19, "type": "METRIC", "confidence": 0.9908118844032288}]}, {"text": "This is not our task, but it is the most similar task in his paper.", "labels": [], "entities": []}, {"text": "We also evaluated the methods using human judgments.", "labels": [], "entities": []}, {"text": "The evaluator is not the author but not a native English speaker.", "labels": [], "entities": []}, {"text": "The judgment used the same criteria as those in Knight and Marcu's methods.", "labels": [], "entities": []}, {"text": "In the first experiment, evaluators scored from 1 to 5 points the grammaticality of the compressed sentence.", "labels": [], "entities": []}, {"text": "In the second one, they scored from 1 to 5 points how well the compressed sentence contained the important words of the original one.", "labels": [], "entities": []}, {"text": "We used the parallel corpus used in Ref.).", "labels": [], "entities": []}, {"text": "This corpus consists of sentence pairs extracted automatically from the ZiffDavis corpus, a set of newspaper articles about computer products.", "labels": [], "entities": []}, {"text": "This corpus has 1087 sentence pairs.", "labels": [], "entities": []}, {"text": "Thirty-two of these sentences were used for the human judgments in Knight and Marcu's experiment, and the same sentences were used for our human judgments.", "labels": [], "entities": []}, {"text": "The rest of the sentences were randomly shuffled, and 527 sentence pairs were used as a training corpus, 263 pairs as a development corpus, and 264 pairs as a test corpus.", "labels": [], "entities": []}, {"text": "To parse these corpora, we used Charniak and Johnson's parser).", "labels": [], "entities": []}, {"text": "We experimented with/without goal sentence length for summaries.", "labels": [], "entities": [{"text": "summaries", "start_pos": 54, "end_pos": 63, "type": "TASK", "confidence": 0.9785943627357483}]}, {"text": "In the first experiment, the system was given only a sentence and no sentence length information.", "labels": [], "entities": []}, {"text": "The sentence compression problem without the length information is a general task, but evaluating it is difficult because the correct length of a summary is not generally defined even by humans.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.7812126278877258}]}, {"text": "The following example shows this.", "labels": [], "entities": []}, {"text": "Original:\"A font, on the other hand, is a subcategory of a typeface, such as Helvetica Bold or Helvetica Medium.\"", "labels": [], "entities": []}, {"text": "Human: \"A font is a subcategory of a typeface, such as Helvetica Bold.\"", "labels": [], "entities": []}, {"text": "System: \"A font is a subcategory of a typeface.\"", "labels": [], "entities": []}, {"text": "The \"such as\" phrase is removed in this system output, but it is not removed in the human summary.", "labels": [], "entities": []}, {"text": "Neither result is wrong, but in such situations, the evaluation score of the system decreases.", "labels": [], "entities": []}, {"text": "This is because the compression rate of each algorithm is different, and evaluation scores are affected by the lengths of system outputs.", "labels": [], "entities": []}, {"text": "For this reason, results with different lengths cannot be   compared easily.", "labels": [], "entities": []}, {"text": "We therefore examined the relations between the average compression ratios and evaluation scores for all methods by changing the system summary length with the different length parameter \u03b1 introduced in Section 3.1.", "labels": [], "entities": []}, {"text": "In the second experiment, the system was given a sentence and the length for the compressed sentence.", "labels": [], "entities": [{"text": "length", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.9894022941589355}]}, {"text": "We compressed each input sentence to the length of the sentence in its goal summary.", "labels": [], "entities": []}, {"text": "This sentence compression problem is easier than that in which the system can generate sentences of any length.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 5, "end_pos": 25, "type": "TASK", "confidence": 0.7396064698696136}]}, {"text": "We selected the highest-scored sentence from the sentences of length l.", "labels": [], "entities": []}, {"text": "Note that the recalls, precisions and F-measures have the same scores in this setting.", "labels": [], "entities": [{"text": "recalls", "start_pos": 14, "end_pos": 21, "type": "METRIC", "confidence": 0.9989375472068787}, {"text": "precisions", "start_pos": 23, "end_pos": 33, "type": "METRIC", "confidence": 0.9974043965339661}, {"text": "F-measures", "start_pos": 38, "end_pos": 48, "type": "METRIC", "confidence": 0.9969906806945801}]}, {"text": "The results of the experiment without the sentence length information are shown in, 5 and 6.", "labels": [], "entities": []}, {"text": "Noisy-channel indicates the results of the noisy-channel model, ME indicates the results of the maximum-entropy method, and ME + bottomup indicates the results of the maximum-entropy  method with the bottom-up method.", "labels": [], "entities": []}, {"text": "We used the length parameter, \u03b1, introduced in Section 3.1, and obtained a set of summaries with different average lengths.", "labels": [], "entities": []}, {"text": "We plotted the compression ratios and three scores in the figures.", "labels": [], "entities": [{"text": "compression ratios", "start_pos": 15, "end_pos": 33, "type": "METRIC", "confidence": 0.9794341027736664}]}, {"text": "In these figures, a compression ratio is the ratio of the total number of words in compressed sentences to the total number of words in the original sentences.", "labels": [], "entities": []}, {"text": "In these figures, our maximum entropy methods obtained higher scores than the noisy-channel model at all compression ratios.", "labels": [], "entities": []}, {"text": "The maximum entropy method with the bottom-up method obtain the highest scores on these three measures.", "labels": [], "entities": []}, {"text": "The results of the experiment with the sentence length information are shown in.", "labels": [], "entities": []}, {"text": "In this experiment, the scores of the maximum entropy methods were higher than the scores of the noisychannel model.", "labels": [], "entities": []}, {"text": "The maximum entropy method with the bottom-up method achieved the highest scores on each measure.", "labels": [], "entities": []}, {"text": "The results of the human judgments are shown in.", "labels": [], "entities": []}, {"text": "In this experiment, each length of output is same as the length of goal sentence.", "labels": [], "entities": []}, {"text": "The  maximum entropy with the bottom-up method obtained the highest scores of the three methods.", "labels": [], "entities": []}, {"text": "We did t-tests (5% significance).", "labels": [], "entities": [{"text": "significance", "start_pos": 19, "end_pos": 31, "type": "METRIC", "confidence": 0.9612022638320923}]}, {"text": "Between the noisychannel model and the maximum entropy with the bottom-up method, importance is significantly different but grammaticality is not.", "labels": [], "entities": []}, {"text": "Between the human and the maximum entropy with the bottomup method, grammaticality is significantly different but importance is not.", "labels": [], "entities": []}, {"text": "There are no significant differences between the noisy-channel model and the maximum entropy model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results of human judgments.", "labels": [], "entities": []}, {"text": " Table 4: Comparison with original results.", "labels": [], "entities": []}]}