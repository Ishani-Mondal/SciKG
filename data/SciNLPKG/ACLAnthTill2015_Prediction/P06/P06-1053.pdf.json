{"title": [{"text": "Integrating Syntactic Priming into an Incremental Probabilistic Parser, with an Application to Psycholinguistic Modeling", "labels": [], "entities": []}], "abstractContent": [{"text": "The psycholinguistic literature provides evidence for syntactic priming, i.e., the tendency to repeat structures.", "labels": [], "entities": []}, {"text": "This paper describes a method for incorporating priming into an incremental probabilis-tic parser.", "labels": [], "entities": []}, {"text": "Three models are compared, which involve priming of rules between sentences, within sentences, and within coordinate structures.", "labels": [], "entities": []}, {"text": "These models simulate the reading time advantage for parallel structures found inhuman data, and also yield a small increase in overall parsing accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 136, "end_pos": 143, "type": "TASK", "confidence": 0.9601554274559021}, {"text": "accuracy", "start_pos": 144, "end_pos": 152, "type": "METRIC", "confidence": 0.8774672746658325}]}], "introductionContent": [{"text": "Over the last two decades, the psycholinguistic literature has provided a wealth of experimental evidence for syntactic priming, i.e., the tendency to repeat syntactic structures (e.g.,.", "labels": [], "entities": [{"text": "syntactic priming", "start_pos": 110, "end_pos": 127, "type": "TASK", "confidence": 0.828898698091507}]}, {"text": "Most work on syntactic priming has been concerned with sentence production; however, recent studies also demonstrate a preference for structural repetition inhuman parsing.", "labels": [], "entities": [{"text": "syntactic priming", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.8041041493415833}, {"text": "sentence production", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.7196419984102249}, {"text": "structural repetition inhuman parsing", "start_pos": 134, "end_pos": 171, "type": "TASK", "confidence": 0.7492747902870178}]}, {"text": "This includes the so-called parallelism effect demonstrated by: speakers processes coordinated structures more quickly when the second conjunct repeats the syntactic structure of the first conjunct.", "labels": [], "entities": []}, {"text": "Two alternative accounts of the parallelism effect have been proposed.", "labels": [], "entities": []}, {"text": "argue that the effect is simply an instance of a pervasive syntactic priming mechanism inhuman parsing.", "labels": [], "entities": []}, {"text": "They provide evidence from a series of corpus studies which show that parallelism is not limited to co-ordination, but occurs in a wide range of syntactic structures, both within and between sentences, as predicted if a general priming mechanism is assumed.", "labels": [], "entities": []}, {"text": "(They also show this effect is stronger in coordinate structures, which could explain results.) propose an alternative account of the parallelism effect in terms of a copying mechanism.", "labels": [], "entities": []}, {"text": "Unlike priming, this mechanism is highly specialized and only applies to coordinate structures: if the second conjunct is encountered, then instead of building new structure, the language processor simply copies the structure of the first conjunct; this explains why a speedup is observed if the two conjuncts are parallel.", "labels": [], "entities": []}, {"text": "If the copying account is correct, then we would expect parallelism effects to be restricted to coordinate structures and not to apply in other contexts.", "labels": [], "entities": []}, {"text": "This paper presents a parsing model which implements both the priming mechanism and the copying mechanism, making it possible to compare their predictions on human reading time data.", "labels": [], "entities": []}, {"text": "Our model also simulates other important aspects of human parsing: (i) it is broad-coverage, i.e., it yields accurate parses for unrestricted input, and (ii) it processes sentences incrementally, i.e., on a word-by-word basis.", "labels": [], "entities": []}, {"text": "This general modeling framework builds on probabilistic accounts of human parsing as proposed by and.", "labels": [], "entities": []}, {"text": "A priming-based parser is also interesting from an engineering point of view.", "labels": [], "entities": []}, {"text": "To avoid sparse data problems, probabilistic parsing models make strong independence assumptions; in particular, they generally assume that sentences are independent of each other, in spite of corpus evidence for structural repetition between sentences.", "labels": [], "entities": []}, {"text": "We therefore expect a parsing model that includes structural repetition to provide a better fit with real corpus data, resulting in better parsing performance.", "labels": [], "entities": [{"text": "parsing", "start_pos": 22, "end_pos": 29, "type": "TASK", "confidence": 0.9718273878097534}]}, {"text": "A simple and principled approach to handling structure re-use would be to use adaptation probabilities for probabilistic grammar rules), analogous to cache probabilities used in caching language models.", "labels": [], "entities": []}, {"text": "This is the approach we will pursue in this paper.", "labels": [], "entities": []}, {"text": "present a corpus study that demonstrates the existence of parallelism in corpus data.", "labels": [], "entities": []}, {"text": "This is an important precondition for understanding the parallelism effect; however, they do not develop a parsing model that accounts for the effect, which means they are unable to evaluate their claims against experimental data.", "labels": [], "entities": []}, {"text": "The present paper overcomes this limitation.", "labels": [], "entities": []}, {"text": "In Section 2, we present a formalization of the priming and copying models of parallelism and integrate them into an incremental probabilistic parser.", "labels": [], "entities": []}, {"text": "In Section 3, we evaluate this parser against reading time data taken from parallelism experiments.", "labels": [], "entities": []}, {"text": "In Section 4, we test the engineering aspects of our model by demonstrating that a small increase in parsing accuracy can be obtained with a parallelism-based model.", "labels": [], "entities": [{"text": "parsing", "start_pos": 101, "end_pos": 108, "type": "TASK", "confidence": 0.9696112275123596}, {"text": "accuracy", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9331085085868835}]}, {"text": "Section 5 provides an analysis of the performance of our model, focusing on the role of the distance between prime and target.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we test our models by applying them to experimental reading time data.", "labels": [], "entities": []}, {"text": "reported a series of experiments that examined the parallelism preference in reading.", "labels": [], "entities": []}, {"text": "In one of their experiments, they monitored subjects' eye-movements while they read sentences like (1): (1) a.", "labels": [], "entities": []}, {"text": "Hilda noticed a strange man and a tall woman when she entered the house. b. Hilda noticed a man and a tall woman when she entered the house.", "labels": [], "entities": []}, {"text": "They found that total reading times were faster on the phrase tall woman in (1a), where the coordinated noun phrases are parallel in structure, compared within (1b), where they are not.", "labels": [], "entities": []}, {"text": "There are various approaches to modeling processing difficulty using a probabilistic approach.", "labels": [], "entities": []}, {"text": "One possibility is to use an incremental parser with abeam search or an n-best approach.", "labels": [], "entities": []}, {"text": "Processing difficulty is predicted at points in the input string where the current best parse is replaced by an alternative derivation).", "labels": [], "entities": []}, {"text": "An alternative is to keep track of all derivations, and predict difficulty at points where there is a large change in the shape of the probability distribution across adjacent parsing states.", "labels": [], "entities": []}, {"text": "A third approach is to calculate the forward probability of the sentence using a PCFG.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 81, "end_pos": 85, "type": "DATASET", "confidence": 0.9508574604988098}]}, {"text": "Low probabilities are then predicted to correspond to high processing difficulty.", "labels": [], "entities": []}, {"text": "A variant of this third approach is to assume that processing difficulty is correlated with the (log) probability of the best parse.", "labels": [], "entities": []}, {"text": "This final formulation is the one used for the experiments presented in this paper.", "labels": [], "entities": []}, {"text": "In the previous section, we were able to show that the Copy and Within models are able to account for human reading-time performance for parallel coordinate structures.", "labels": [], "entities": []}, {"text": "While this result alone is sufficient to claim success as a psycholinguistic model, it has been argued that more realistic psycholinguistic models ought to also exhibit high accuracy and broad-coverage, both crucial properties of the human parsing mechanism (e.g.,).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 174, "end_pos": 182, "type": "METRIC", "confidence": 0.9986672401428223}]}, {"text": "This should not be difficult: our starting point was a PCFG, which already has broad coverage behavior (albeit with only moderate accuracy).", "labels": [], "entities": [{"text": "PCFG", "start_pos": 55, "end_pos": 59, "type": "DATASET", "confidence": 0.9236498475074768}, {"text": "accuracy", "start_pos": 130, "end_pos": 138, "type": "METRIC", "confidence": 0.9984027743339539}]}, {"text": "However, in this section we explore what effects our modifications have to overall coverage, and, perhaps more interestingly, to parsing accuracy.", "labels": [], "entities": [{"text": "coverage", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.8307343125343323}, {"text": "parsing", "start_pos": 129, "end_pos": 136, "type": "TASK", "confidence": 0.9805450439453125}, {"text": "accuracy", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.9473599195480347}]}], "tableCaptions": [{"text": " Table 1: Mean log probability estimates for Frazier et al (2000) items", "labels": [], "entities": [{"text": "Mean log probability", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.8508986632029215}]}, {"text": " Table 2: Parsing results for the Within, Between, and Copy model compared to a PCFG baseline.", "labels": [], "entities": [{"text": "PCFG baseline", "start_pos": 80, "end_pos": 93, "type": "DATASET", "confidence": 0.9114373028278351}]}]}