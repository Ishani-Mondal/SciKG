{"title": [{"text": "Techniques to incorporate the benefits of a Hierarchy in a modified hidden Markov model", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper explores techniques to take advantage of the fundamental difference in structure between hidden Markov models (HMM) and hierarchical hidden Markov models (HHMM).", "labels": [], "entities": []}, {"text": "The HHMM structure allows repeated parts of the model to be merged together.", "labels": [], "entities": []}, {"text": "A merged model takes advantage of the recurring patterns within the hierarchy, and the clusters that exist in some sequences of observations, in order to increase the extraction accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 178, "end_pos": 186, "type": "METRIC", "confidence": 0.9866623282432556}]}, {"text": "This paper also presents anew technique for reconstructing grammar rules automatically.", "labels": [], "entities": []}, {"text": "This work builds on the idea of combining a phrase extraction method with HHMM to expose patterns within English text.", "labels": [], "entities": [{"text": "phrase extraction", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.7924031317234039}]}, {"text": "The reconstruction is then used to simplify the complex structure of an HHMM The models discussed here are evaluated by applying them to natural language tasks based on CoNLL-2004 1 and a sub-corpus of the Lancaster Treebank 2 .", "labels": [], "entities": [{"text": "CoNLL-2004 1", "start_pos": 169, "end_pos": 181, "type": "DATASET", "confidence": 0.8856450915336609}, {"text": "Lancaster Treebank 2", "start_pos": 206, "end_pos": 226, "type": "DATASET", "confidence": 0.9571962753931681}]}], "introductionContent": [{"text": "Hidden Markov models (HMMs) were introduced in the late 1960s, and are widely used as a probabilistic tool for modeling sequences of observations.", "labels": [], "entities": []}, {"text": "They have proven to be capable of assigning semantic labels to tokens over a wide variety of input types.", "labels": [], "entities": []}, {"text": "This is useful for text-related tasks that involve some uncertainty, including part-of-speech tagging), text segmentation (), named entity recognition () and information extraction tasks).", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 79, "end_pos": 101, "type": "TASK", "confidence": 0.7403117418289185}, {"text": "text segmentation", "start_pos": 104, "end_pos": 121, "type": "TASK", "confidence": 0.7611092031002045}, {"text": "named entity recognition", "start_pos": 126, "end_pos": 150, "type": "TASK", "confidence": 0.6126447121302286}, {"text": "information extraction tasks", "start_pos": 158, "end_pos": 186, "type": "TASK", "confidence": 0.848110576470693}]}, {"text": "However, most natural language processing tasks are dependent on discovering a hierarchical structure hidden within the source information.", "labels": [], "entities": []}, {"text": "An example would be predicting semantic roles from English sentences.", "labels": [], "entities": [{"text": "predicting semantic roles from English sentences", "start_pos": 20, "end_pos": 68, "type": "TASK", "confidence": 0.9019544819990793}]}, {"text": "HMMs are less capable of reliably modeling these tasks.", "labels": [], "entities": []}, {"text": "In contrast hierarchical hidden Markov models (HHMMs) are better at capturing the underlying hierarchy structure.", "labels": [], "entities": []}, {"text": "While there are several difficulties inherent in extracting information from the patterns hidden within natural language information, by discovering the hierarchical structure more accurate models can be built.", "labels": [], "entities": []}, {"text": "HHMMs were first proposed by Fine (1998) to resolve the complex multi-scale structures that pervade natural language, such as speech, handwriting (, and text.", "labels": [], "entities": []}, {"text": "Skounakis (2003) described the HHMM as multiple \"levels\" of HMM states, where lower levels represents each individual output symbol, and upper levels represents the combinations of lower level sequences.", "labels": [], "entities": []}, {"text": "Any HHMM can be converted to a HMM by creating a state for every possible observation, a process called \"flattening\".", "labels": [], "entities": []}, {"text": "Flattening is performed to simplify the model to a linear sequence of Markov states, thus decreasing processing time.", "labels": [], "entities": []}, {"text": "But as a result of this process the model no longer contains any hierarchical structure.", "labels": [], "entities": []}, {"text": "To reduce the models complexity while maintaining some hierarchical structure, our algorithm uses a \"partial flattening\" process.", "labels": [], "entities": []}, {"text": "In recent years, artificial intelligence re-searchers have made strenuous efforts to reproduce the human interpretation of language, whereby patterns in grammar can be recognised and simplified automatically.", "labels": [], "entities": []}, {"text": "describes a simple rule-based approach for learning by rewriting the bracketing rule-a method for presenting the structure of natural language textfor linguistic knowledge.", "labels": [], "entities": [{"text": "presenting the structure of natural language textfor linguistic knowledge", "start_pos": 98, "end_pos": 171, "type": "TASK", "confidence": 0.6439311106999716}]}, {"text": "Similarly, Krotov (1999) puts forward a method for eliminating redundant grammar rules by applying a compaction algorithm.", "labels": [], "entities": []}, {"text": "This work draws upon the lessons learned from these sources by automatically detecting situations in which the grammar structure can be reconstructed.", "labels": [], "entities": []}, {"text": "This is done by applying the phrase extraction method introduced by to rewrite the bracketing rule by calculating the dependency of each possible phrase.", "labels": [], "entities": [{"text": "phrase extraction", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.7819722294807434}]}, {"text": "The outcome of this restructuring is to reduce the complexity of the hierarchical structure and reduce the number of levels in the hierarchy.", "labels": [], "entities": []}, {"text": "This paper considers the tasks of identifying the syntactic structure of text chunking and grammar parsing with previously annotated text documents.", "labels": [], "entities": [{"text": "grammar parsing with previously annotated text documents", "start_pos": 91, "end_pos": 147, "type": "TASK", "confidence": 0.7910244294575283}]}, {"text": "It analyses the use of HHMMs-both before and after the application of improvement techniques-for these tasks, then compares the results with HMMs.", "labels": [], "entities": []}, {"text": "This paper is organised as follows: Section 2 describes the method for training HHMMs.", "labels": [], "entities": []}, {"text": "Section 3 describes the flattening process for reducing the depth of hierarchical structure for HHMMs.", "labels": [], "entities": []}, {"text": "Section 4 discusses the use of HHMMs for the text chunking task and the grammar parser.", "labels": [], "entities": [{"text": "text chunking task", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.821904718875885}]}, {"text": "The evaluation results of the HMM, the plain HHMM and the merged and partially flattened HHMM are presented in Section 5.", "labels": [], "entities": []}, {"text": "Finally, Section 6 discusses the results.", "labels": [], "entities": []}], "datasetContent": [{"text": "The first evaluation presents preliminary evidence that the merged hierarchical hidden Markov Model (MHHMM) is able to produce more accurate results either a plain HHMM or a HMM during the text chunking task.", "labels": [], "entities": [{"text": "text chunking task", "start_pos": 189, "end_pos": 207, "type": "TASK", "confidence": 0.8073898156483968}]}, {"text": "The results suggest that the partial flattening process is capable of improving model accuracy when the input data contains complex hierarchical structures.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.989388644695282}]}, {"text": "The evaluation involves analysing the results over two sets of data.", "labels": [], "entities": []}, {"text": "The first is a selection of data from    The results indicate that the MHHMM gains efficiency, in terms of computation cost, by merging repeated sub-models, resulting in fewer states in the model.", "labels": [], "entities": [{"text": "MHHMM", "start_pos": 71, "end_pos": 76, "type": "DATASET", "confidence": 0.7728437185287476}]}, {"text": "In contrast the HMM has lower efficiency as it is required to identify every single path, leading to more states within the model and higher computation cost.", "labels": [], "entities": []}, {"text": "The extra costs of constructing a HHMM, which will have the same number of production states as the HMM, make it the least efficient.", "labels": [], "entities": []}, {"text": "The second evaluation presents preliminary evidence that the partially flattened hierarchical hidden Markov model (PFHHMM) can assign propositions to language texts (grammar parsing) at least as accurately as the HMM.", "labels": [], "entities": []}, {"text": "This is assignment is a task that HHMMs are generally not well suited to. shows the F 1 -measures of identified semantic roles for each different model on the Lancaster Treebank data set.", "labels": [], "entities": [{"text": "F 1 -measures", "start_pos": 84, "end_pos": 97, "type": "METRIC", "confidence": 0.9766643643379211}, {"text": "Lancaster Treebank data set", "start_pos": 159, "end_pos": 186, "type": "DATASET", "confidence": 0.9743585884571075}]}, {"text": "The models used in this evaluation were trained with observation data from the Lancaster Treebank training set.", "labels": [], "entities": [{"text": "Lancaster Treebank training set", "start_pos": 79, "end_pos": 110, "type": "DATASET", "confidence": 0.9934738725423813}]}, {"text": "The training set and testing set are sub-divided from the corpus in proportions of 2 3 and 1 3 . The PFHHMMs had extra training conditions as follows: PFHHMM obs 2000 made use of the partial flattening process, with the high dependency parameter determined by considering the highest 2000 dependency values from observation sequences from the corpus.", "labels": [], "entities": [{"text": "PFHHMMs", "start_pos": 101, "end_pos": 108, "type": "DATASET", "confidence": 0.9009158611297607}]}, {"text": "PFHHMM state 150 again uses partial flattening, however this time the highest 150 dependency values from state sequences were utilized in discovering the high dependency threshold.", "labels": [], "entities": [{"text": "PFHHMM state 150", "start_pos": 0, "end_pos": 16, "type": "DATASET", "confidence": 0.8350014289220175}]}, {"text": "The n values of 2000 and 150 were determined to be the optimal values when applied to the training set.", "labels": [], "entities": []}, {"text": "The results show that applying the partial flattening process to a model using observation sequences to determine high dependency values reduces the complexity of the model's hierarchy and consequently improves the model's accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 223, "end_pos": 231, "type": "METRIC", "confidence": 0.996369481086731}]}, {"text": "The state dependency method is shown to be less favorable for this particular task, but the micro-average result is still comparable with the HMM's performance.", "labels": [], "entities": []}, {"text": "The results also show no significant re-  lationship between the occurance count of a state against the various models prediction accuracy.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Observation dependency values of part- of-speech tags", "labels": [], "entities": []}, {"text": " Table 2: F1-measure of top 5 states during grammar parsing", "labels": [], "entities": [{"text": "F1-measure", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9985213875770569}, {"text": "grammar parsing", "start_pos": 44, "end_pos": 59, "type": "TASK", "confidence": 0.7160290479660034}]}]}