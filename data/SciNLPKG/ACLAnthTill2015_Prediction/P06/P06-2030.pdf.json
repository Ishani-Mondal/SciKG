{"title": [{"text": "Using Bilingual Comparable Corpora and Semi-supervised Clustering for Topic Tracking", "labels": [], "entities": [{"text": "Topic Tracking", "start_pos": 70, "end_pos": 84, "type": "TASK", "confidence": 0.8481919467449188}]}], "abstractContent": [{"text": "We address the problem dealing with skewed data, and propose a method for estimating effective training stories for the topic tracking task.", "labels": [], "entities": [{"text": "topic tracking task", "start_pos": 120, "end_pos": 139, "type": "TASK", "confidence": 0.8343348304430643}]}, {"text": "For a small number of labelled positive stories, we extract story pairs which consist of positive and its associated stories from bilingual comparable corpora.", "labels": [], "entities": []}, {"text": "To overcome the problem of a large number of labelled negative stories, we classify them into some clusters.", "labels": [], "entities": []}, {"text": "This is done by using k-means with EM.", "labels": [], "entities": []}, {"text": "The results on the TDT corpora show the effectiveness of the method.", "labels": [], "entities": [{"text": "TDT corpora", "start_pos": 19, "end_pos": 30, "type": "DATASET", "confidence": 0.7676225006580353}]}], "introductionContent": [{"text": "With the exponential growth of information on the Internet, it is becoming increasingly difficult to find and organize relevant materials.", "labels": [], "entities": []}, {"text": "Topic Tracking defined by the TDT project is a research area to attack the problem.", "labels": [], "entities": [{"text": "Topic Tracking", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8417912423610687}, {"text": "TDT project", "start_pos": 30, "end_pos": 41, "type": "DATASET", "confidence": 0.8752736449241638}]}, {"text": "It starts from a few sample stories and finds all subsequent stories that discuss the target topic.", "labels": [], "entities": []}, {"text": "Here, a topic in the TDT context is something that happens at a specific place and time associated with some specific actions.", "labels": [], "entities": []}, {"text": "A wide range of statistical and ML techniques have been applied to topic tracking.", "labels": [], "entities": [{"text": "ML", "start_pos": 32, "end_pos": 34, "type": "TASK", "confidence": 0.9280358552932739}, {"text": "topic tracking", "start_pos": 67, "end_pos": 81, "type": "TASK", "confidence": 0.9253978133201599}]}, {"text": "The main task of these techniques is to tune the parameters or the threshold to produce optimal results.", "labels": [], "entities": []}, {"text": "However, parameter tuning is a tricky issue for tracking) because the number of initial positive training stories is very small (one to four), and topics are localized in space and time.", "labels": [], "entities": []}, {"text": "For example, 'Taipei Mayoral Elections' and 'U.S. Mid-term Elections' are topics, but 'Elections' is not a topic.", "labels": [], "entities": [{"text": "Taipei Mayoral Elections'", "start_pos": 14, "end_pos": 39, "type": "DATASET", "confidence": 0.7518182992935181}]}, {"text": "Therefore, the system needs to estimate whether or not the test stories are the same topic with few information about the topic.", "labels": [], "entities": []}, {"text": "Moreover, the training data is skewed data, i.e. there is a large number of labelled negative stories compared to positive ones.", "labels": [], "entities": []}, {"text": "The system thus needs to balance the amount of positive and negative training stories not to hamper the accuracy of estimation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9991139769554138}]}, {"text": "In this paper, we propose a method for estimating efficient training stories for topic tracking.", "labels": [], "entities": [{"text": "topic tracking", "start_pos": 81, "end_pos": 95, "type": "TASK", "confidence": 0.8730886876583099}]}, {"text": "For a small number of labelled positive stories, we use bilingual comparable corpora (TDT1-3 English and Japanese newspapers, Mainichi and Yomiuri Shimbun).", "labels": [], "entities": [{"text": "TDT1-3 English and Japanese newspapers", "start_pos": 86, "end_pos": 124, "type": "DATASET", "confidence": 0.881883704662323}, {"text": "Mainichi and Yomiuri Shimbun", "start_pos": 126, "end_pos": 154, "type": "DATASET", "confidence": 0.745275504887104}]}, {"text": "Our hypothesis using bilingual corpora is that many of the broadcasting station from one country report local events more frequently and in more detail than overseas' broadcasting stations, even if it is a world-wide famous ones.", "labels": [], "entities": []}, {"text": "Let us take a look at some topic from the TDT corpora.", "labels": [], "entities": [{"text": "TDT corpora", "start_pos": 42, "end_pos": 53, "type": "DATASET", "confidence": 0.9750141203403473}]}, {"text": "A topic, 'Kobe Japan quake' from the TDT1 is a world-wide famous one, and 89 stories are included in the TDT1.", "labels": [], "entities": [{"text": "Kobe Japan quake' from the TDT1", "start_pos": 10, "end_pos": 41, "type": "DATASET", "confidence": 0.8619000571114677}, {"text": "TDT1", "start_pos": 105, "end_pos": 109, "type": "DATASET", "confidence": 0.9817850589752197}]}, {"text": "However, Mainichi and Yomiuri Japanese newspapers have much more stories from the same period of time, i.e. 5,029 and 4,883 stories for each.", "labels": [], "entities": [{"text": "Mainichi and Yomiuri Japanese newspapers", "start_pos": 9, "end_pos": 49, "type": "DATASET", "confidence": 0.7904400944709777}]}, {"text": "These observations show that it is crucial to investigate the use of bilingual comparable corpora based on the NL techniques in terms of collecting more information about some specific topics.", "labels": [], "entities": []}, {"text": "We extract Japanese stories which are relevant to the positive English stories using English-Japanese bilingual corpora, together with the EDR bilingual dictionary.", "labels": [], "entities": [{"text": "EDR bilingual dictionary", "start_pos": 139, "end_pos": 163, "type": "DATASET", "confidence": 0.9353154897689819}]}, {"text": "The associated story is the result of alignment of a Japanese term association with an English term association.", "labels": [], "entities": []}, {"text": "For a large number of labelled negative stories, we classify them into some clusters using labelled positive stories.", "labels": [], "entities": []}, {"text": "We used a semisupervised clustering technique which combines labeled and unlabeled stories during clustering.", "labels": [], "entities": []}, {"text": "Our goal for semi-supervised clustering is to classify negative stories into clusters where each cluster is meaningful in terms of class distribution provided by one cluster of positive training stories.", "labels": [], "entities": []}, {"text": "We introduce k-means clustering that can be viewed as instances of the EM algorithm, and classify negative stories into clusters.", "labels": [], "entities": []}, {"text": "In general, the number of clusters k for the k-means algorithm is not given beforehand.", "labels": [], "entities": []}, {"text": "We thus use the Bayesian Information Criterion (BIC) as the splitting criterion, and select the proper number fork.", "labels": [], "entities": [{"text": "Bayesian Information Criterion (BIC)", "start_pos": 16, "end_pos": 52, "type": "METRIC", "confidence": 0.5636615852514902}]}], "datasetContent": [{"text": "The English data we used for extracting terms is Reuters'96 corpus(806,791 stories) including TDT1 and TDT3 corpora.", "labels": [], "entities": [{"text": "Reuters'96 corpus", "start_pos": 49, "end_pos": 66, "type": "DATASET", "confidence": 0.9803592562675476}, {"text": "TDT1", "start_pos": 94, "end_pos": 98, "type": "DATASET", "confidence": 0.9441950917243958}, {"text": "TDT3 corpora", "start_pos": 103, "end_pos": 115, "type": "DATASET", "confidence": 0.8368662297725677}]}, {"text": "The Japanese data was 1,874,947 stories from 14 years(from 1991 to 2004) Mainichi newspapers(1,499,936 stories), and 3 years) Yomiuri newspapers(375,011 stories).", "labels": [], "entities": [{"text": "Japanese data", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.7602787017822266}, {"text": "Mainichi newspapers", "start_pos": 73, "end_pos": 92, "type": "DATASET", "confidence": 0.9591176211833954}]}, {"text": "All Japanese stories were tagged by the morphological analysis Chasen.", "labels": [], "entities": [{"text": "Chasen", "start_pos": 63, "end_pos": 69, "type": "DATASET", "confidence": 0.6910135746002197}]}, {"text": "English stories were tagged by a part-of-speech tagger, and stop word removal.", "labels": [], "entities": [{"text": "stop word removal", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.6576448579629263}]}, {"text": "We applied n-gram model with Church-Gale smoothing to noun words, and selected terms whose probabilities are higher than a certain threshold . As a result, we obtained 338,554 Japanese and 130,397 English terms.", "labels": [], "entities": []}, {"text": "We used the EDR bilingual dictionary, and translated Japanese terms into English.", "labels": [], "entities": [{"text": "EDR bilingual dictionary", "start_pos": 12, "end_pos": 36, "type": "DATASET", "confidence": 0.9303116798400879}]}, {"text": "Some of the words had no translation.", "labels": [], "entities": []}, {"text": "For these, we estimated term correspondences.", "labels": [], "entities": []}, {"text": "Each story is represented as a vector of terms with tf \u00b7idf weights.", "labels": [], "entities": []}, {"text": "We calculated story similarities and extracted story pairs between positive and its associated stories 5 . Inthe tracking, we used the extracted terms together with all verbs, adjectives, and numbers, and represented each story as a vector of these with tf \u00b7idf weights.", "labels": [], "entities": []}, {"text": "We set the evaluation measures used in the TDT benchmark evaluations.", "labels": [], "entities": [{"text": "TDT", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.8517815470695496}]}, {"text": "'Miss' denotes Miss rate, which is the ratio of the stories that were judged as YES but were not evaluated as such for the run in question.", "labels": [], "entities": [{"text": "Miss'", "start_pos": 1, "end_pos": 6, "type": "METRIC", "confidence": 0.9974284768104553}, {"text": "Miss rate", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9607591331005096}, {"text": "YES", "start_pos": 80, "end_pos": 83, "type": "METRIC", "confidence": 0.980517566204071}]}, {"text": "'F/A' shows false alarm rate, which is the ratio of the stories judged as NO but were evaluated as YES.", "labels": [], "entities": [{"text": "F/A", "start_pos": 1, "end_pos": 4, "type": "METRIC", "confidence": 0.9528496066729227}, {"text": "false alarm rate", "start_pos": 12, "end_pos": 28, "type": "METRIC", "confidence": 0.9333270390828451}, {"text": "NO", "start_pos": 74, "end_pos": 76, "type": "METRIC", "confidence": 0.9969124794006348}, {"text": "YES", "start_pos": 99, "end_pos": 102, "type": "METRIC", "confidence": 0.9952985644340515}]}, {"text": "The DET curve plots misses and false alarms, and better performance is indicated by curves more to the lower left of the graph.", "labels": [], "entities": [{"text": "DET", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.7485585808753967}]}, {"text": "The detection cost function(C Det ) is defined by Eq.(8).", "labels": [], "entities": [{"text": "detection cost function(C Det )", "start_pos": 4, "end_pos": 35, "type": "METRIC", "confidence": 0.8201603208269391}]}, {"text": "C Miss , CF a , and PT arget are the costs of a missed detection, false alarm, and priori probability of finding a target, respectively.", "labels": [], "entities": [{"text": "C Miss", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.6792317628860474}, {"text": "CF a", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.9756737351417542}, {"text": "PT arget", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.8961077332496643}]}, {"text": "C Miss , CF a , and PT arget are usually set to 10, 1, and 0.02, respectively.", "labels": [], "entities": [{"text": "Miss", "start_pos": 2, "end_pos": 6, "type": "METRIC", "confidence": 0.5624117255210876}, {"text": "CF a", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.9365646541118622}, {"text": "PT arget", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.8728503882884979}]}, {"text": "The normalized cost function is defined by Eq., and lower cost scores indicate better performance.", "labels": [], "entities": [{"text": "Eq.", "start_pos": 43, "end_pos": 46, "type": "METRIC", "confidence": 0.6633803248405457}]}, {"text": "MIN denotes MIN(C Det ) Norm which is the value of (C Det ) Norm at the best possible threshold.", "labels": [], "entities": [{"text": "MIN", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.8252158761024475}, {"text": "MIN(C Det ) Norm", "start_pos": 12, "end_pos": 28, "type": "METRIC", "confidence": 0.6227501680453619}]}, {"text": "N t is the number of initial positive training stories.", "labels": [], "entities": []}, {"text": "We recall that we used subset of the topics defined by the TDT.", "labels": [], "entities": []}, {"text": "We thus implemented Allan's method) which is similar to our method, and compared the results.", "labels": [], "entities": []}, {"text": "It is based on a tracking query which is created from the top 10 most commonly occurring features in the N t stories, with weight equal to the number of times the term occurred in those stories multiplied by its incremental idf value.", "labels": [], "entities": []}, {"text": "They used a shallow tagger and selected all nouns, verbs, adjectives, and numbers.", "labels": [], "entities": []}, {"text": "We added the extracted terms to these part-of-speech words to make their results comparable with the results by our method.", "labels": [], "entities": []}, {"text": "'Baseline' in shows the best result with their method among varying threshold values of similarity between queries and test stories.", "labels": [], "entities": []}, {"text": "We can see that the performance of our method was competitive to the baseline at every N t value.", "labels": [], "entities": []}, {"text": "shows DET curves by both our method and Allan's method(baseline) for 23 topics from the TDT2 and 3.", "labels": [], "entities": [{"text": "DET", "start_pos": 6, "end_pos": 9, "type": "TASK", "confidence": 0.693452775478363}, {"text": "TDT2", "start_pos": 88, "end_pos": 92, "type": "DATASET", "confidence": 0.9589879512786865}]}, {"text": "illustrates the results for 3 topics from TDT2 and 3 which occurred in Japan.", "labels": [], "entities": [{"text": "TDT2", "start_pos": 42, "end_pos": 46, "type": "DATASET", "confidence": 0.9480581283569336}]}, {"text": "To make some comparison possible, only the N t = 4 is given for each.", "labels": [], "entities": []}, {"text": "show that we have an advantage using bilingual comparable corpora.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Performance of story pairs(24 topics)", "labels": [], "entities": []}]}