{"title": [{"text": "Training Conditional Random Fields with Multivariate Evaluation Measures", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper proposes a framework for training Conditional Random Fields (CRFs) to optimize multivariate evaluation measures , including non-linear measures such as F-score.", "labels": [], "entities": [{"text": "F-score", "start_pos": 163, "end_pos": 170, "type": "METRIC", "confidence": 0.827719509601593}]}, {"text": "Our proposed framework is derived from an error minimization approach that provides a simple solution for directly optimizing any evaluation measure.", "labels": [], "entities": [{"text": "error minimization", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.7273667603731155}]}, {"text": "Specifically focusing on sequential segmentation tasks, i.e. text chunking and named entity recognition, we introduce a loss function that closely reflects the target evaluation measure for these tasks, namely, segmentation F-score.", "labels": [], "entities": [{"text": "sequential segmentation tasks", "start_pos": 25, "end_pos": 54, "type": "TASK", "confidence": 0.7801075180371603}, {"text": "text chunking", "start_pos": 61, "end_pos": 74, "type": "TASK", "confidence": 0.7069816440343857}, {"text": "named entity recognition", "start_pos": 79, "end_pos": 103, "type": "TASK", "confidence": 0.6359576483567556}]}, {"text": "Our experiments show that our method performs better than standard CRF training.", "labels": [], "entities": [{"text": "CRF", "start_pos": 67, "end_pos": 70, "type": "TASK", "confidence": 0.9489101767539978}]}], "introductionContent": [{"text": "Conditional random fields (CRFs) area recently introduced formalism ( for representing a conditional model p(y|x), where both a set of inputs, x, and a set of outputs, y, display non-trivial interdependency.", "labels": [], "entities": []}, {"text": "CRFs are basically defined as a discriminative model of Markov random fields conditioned on inputs (observations) x.", "labels": [], "entities": []}, {"text": "Unlike generative models, CRFs model only the output y's distribution over x.", "labels": [], "entities": []}, {"text": "This allows CRFs to use flexible features such as complicated functions of multiple observations.", "labels": [], "entities": [{"text": "CRFs", "start_pos": 12, "end_pos": 16, "type": "TASK", "confidence": 0.9485525488853455}]}, {"text": "The modeling power of CRFs has been of great benefit in several applications, such as shallow parsing ( and information extraction.", "labels": [], "entities": [{"text": "shallow parsing", "start_pos": 86, "end_pos": 101, "type": "TASK", "confidence": 0.6933532953262329}, {"text": "information extraction", "start_pos": 108, "end_pos": 130, "type": "TASK", "confidence": 0.8876884281635284}]}, {"text": "Since the introduction of CRFs, intensive research has been undertaken to boost their effectiveness.", "labels": [], "entities": []}, {"text": "The first approach to estimating CRF parameters is the maximum likelihood (ML) criterion over conditional probability p(y|x) itself).", "labels": [], "entities": [{"text": "estimating CRF", "start_pos": 22, "end_pos": 36, "type": "TASK", "confidence": 0.7490785121917725}, {"text": "maximum likelihood (ML) criterion", "start_pos": 55, "end_pos": 88, "type": "METRIC", "confidence": 0.8616285075743993}]}, {"text": "The ML criterion, however, is prone to over-fitting the training data, especially since CRFs are often trained with a very large number of correlated features.", "labels": [], "entities": [{"text": "ML", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.8181307315826416}]}, {"text": "The maximum a posteriori (MAP) criterion over parameters, \u03bb, given x and y is the natural choice for reducing over-fitting.", "labels": [], "entities": [{"text": "maximum a posteriori (MAP) criterion", "start_pos": 4, "end_pos": 40, "type": "METRIC", "confidence": 0.8696321419307164}]}, {"text": "Moreover, the Bayes approach, which optimizes both MAP and the prior distribution of the parameters, has also been proposed ().", "labels": [], "entities": [{"text": "MAP", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.5768270492553711}]}, {"text": "Furthermore, large margin criteria have been employed to optimize the model parameters ().", "labels": [], "entities": []}, {"text": "These training criteria have yielded excellent results for various tasks.", "labels": [], "entities": []}, {"text": "However, real world tasks are evaluated by task-specific evaluation measures, including non-linear measures such as Fscore, while all of the above criteria achieve optimization based on the linear combination of average accuracies, or error rates, rather than a given task-specific evaluation measure.", "labels": [], "entities": [{"text": "Fscore", "start_pos": 116, "end_pos": 122, "type": "METRIC", "confidence": 0.9956188797950745}]}, {"text": "For example, sequential segmentation tasks (SSTs), such as text chunking and named entity recognition, are generally evaluated with the segmentation F-score.", "labels": [], "entities": [{"text": "sequential segmentation tasks (SSTs)", "start_pos": 13, "end_pos": 49, "type": "TASK", "confidence": 0.8811607857545217}, {"text": "text chunking", "start_pos": 59, "end_pos": 72, "type": "TASK", "confidence": 0.7654104232788086}, {"text": "named entity recognition", "start_pos": 77, "end_pos": 101, "type": "TASK", "confidence": 0.6002357403437296}, {"text": "F-score", "start_pos": 149, "end_pos": 156, "type": "METRIC", "confidence": 0.8690778613090515}]}, {"text": "This inconsistency between the objective function during training and the task evaluation measure might produce a suboptimal result.", "labels": [], "entities": []}, {"text": "In fact, to overcome this inconsistency, an SVM-based multivariate optimization method has recently been proposed).", "labels": [], "entities": [{"text": "SVM-based multivariate optimization", "start_pos": 44, "end_pos": 79, "type": "TASK", "confidence": 0.7234951059023539}]}, {"text": "Moreover, an F-score optimization method for logistic regression has also been proposed).", "labels": [], "entities": []}, {"text": "In the same spirit as the above studies, we first propose a generalization framework for CRF training that allows us to optimize directly not only the error rate, but also any evaluation measure.", "labels": [], "entities": [{"text": "CRF training", "start_pos": 89, "end_pos": 101, "type": "TASK", "confidence": 0.912897527217865}]}, {"text": "In other words, our framework can incorporate any evaluation measure of interest into the loss function and then optimize this loss function as the training objective function.", "labels": [], "entities": []}, {"text": "Our proposed framework is fundamentally derived from an approach to (smoothed) error rate minimization well known in the speech and pattern recognition community, namely the Minimum Classification Error (MCE) framework.", "labels": [], "entities": [{"text": "speech and pattern recognition", "start_pos": 121, "end_pos": 151, "type": "TASK", "confidence": 0.6146003156900406}, {"text": "Minimum Classification Error (MCE)", "start_pos": 174, "end_pos": 208, "type": "TASK", "confidence": 0.7156774351994196}]}, {"text": "The framework of MCE criterion training supports the theoretical background of our method.", "labels": [], "entities": [{"text": "MCE criterion training", "start_pos": 17, "end_pos": 39, "type": "TASK", "confidence": 0.7191449205080668}]}, {"text": "The approach proposed here subsumes the conventional ML/MAP criteria training of CRFs, as described in the following.", "labels": [], "entities": [{"text": "ML/MAP", "start_pos": 53, "end_pos": 59, "type": "TASK", "confidence": 0.7592780192693075}]}, {"text": "After describing the new framework, as an example of optimizing multivariate evaluation measures, we focus on SSTs and introduce a segmentation F-score loss function for CRFs.", "labels": [], "entities": [{"text": "SSTs", "start_pos": 110, "end_pos": 114, "type": "TASK", "confidence": 0.9660021662712097}, {"text": "F-score loss function", "start_pos": 144, "end_pos": 165, "type": "METRIC", "confidence": 0.9118993083635966}]}], "datasetContent": [{"text": "Thus far, we have discussed the error rate version of MCE.", "labels": [], "entities": [{"text": "error rate", "start_pos": 32, "end_pos": 42, "type": "METRIC", "confidence": 0.9580398499965668}]}, {"text": "Unlike ML/MAP, the framework of MCE criterion training allows the embedding of not only a linear combination of error rates, but also any evaluation measure, including non-linear measures.", "labels": [], "entities": [{"text": "ML/MAP", "start_pos": 7, "end_pos": 13, "type": "TASK", "confidence": 0.8120097915331522}, {"text": "MCE criterion training", "start_pos": 32, "end_pos": 54, "type": "TASK", "confidence": 0.8029323617617289}]}, {"text": "Several non-linear objective functions, such as F-score for text classification (, and BLEU-score and some other evaluation measures for statistical machine translation, have been introduced with reference to the framework of MCE criterion training.", "labels": [], "entities": [{"text": "F-score", "start_pos": 48, "end_pos": 55, "type": "METRIC", "confidence": 0.9901242852210999}, {"text": "text classification", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.8012231886386871}, {"text": "BLEU-score", "start_pos": 87, "end_pos": 97, "type": "METRIC", "confidence": 0.9984637498855591}, {"text": "statistical machine translation", "start_pos": 137, "end_pos": 168, "type": "TASK", "confidence": 0.6966381271680196}, {"text": "MCE criterion training", "start_pos": 226, "end_pos": 248, "type": "TASK", "confidence": 0.826139489809672}]}, {"text": "We used the same Chunking and 'English' NER task data used for the shared tasks of) and, respectively.", "labels": [], "entities": []}, {"text": "Chunking data was obtained from the Wall Street Journal (WSJ) corpus: sections 15-18 as training data, and section 20 as test data (2,012 sentences and 47,377 tokens), with 11 different chunk-tags, such as NP and VP plus the 'O' tag, which represents the outside of any target chunk (segment).", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ) corpus", "start_pos": 36, "end_pos": 68, "type": "DATASET", "confidence": 0.9387341908046177}]}, {"text": "The English NER data was taken from the Reuters Corpus2 1 . The data consists of 203,621, 51,362 and 46,435 tokens from 14,987, 3,466 and 3,684 sentences in training, development and test data, respectively, with four named entity tags, PERSON, LOCATION, ORGANIZATION and MISC, plus the 'O' tag.", "labels": [], "entities": [{"text": "English NER data", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.758668581644694}, {"text": "Reuters Corpus2 1", "start_pos": 40, "end_pos": 57, "type": "DATASET", "confidence": 0.9701511859893799}, {"text": "PERSON", "start_pos": 237, "end_pos": 243, "type": "METRIC", "confidence": 0.9833018183708191}, {"text": "LOCATION", "start_pos": 245, "end_pos": 253, "type": "METRIC", "confidence": 0.8625997304916382}, {"text": "ORGANIZATION", "start_pos": 255, "end_pos": 267, "type": "METRIC", "confidence": 0.9590451717376709}, {"text": "MISC", "start_pos": 272, "end_pos": 276, "type": "METRIC", "confidence": 0.9226511716842651}, {"text": "O", "start_pos": 288, "end_pos": 289, "type": "METRIC", "confidence": 0.9697641134262085}]}], "tableCaptions": [{"text": " Table 1: Performance of text chunking and named  entity recognition data", "labels": [], "entities": [{"text": "text chunking", "start_pos": 25, "end_pos": 38, "type": "TASK", "confidence": 0.7467149794101715}, {"text": "named  entity recognition", "start_pos": 43, "end_pos": 68, "type": "TASK", "confidence": 0.6658342281977335}]}, {"text": " Table 2: Performance when initial parameters are  derived from MAP", "labels": [], "entities": [{"text": "MAP", "start_pos": 64, "end_pos": 67, "type": "TASK", "confidence": 0.41050952672958374}]}]}