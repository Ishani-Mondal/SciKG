{"title": [{"text": "An Automatic Method for Summary Evaluation Using Multiple Evaluation Results by a Manual Method", "labels": [], "entities": [{"text": "Summary Evaluation", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.9779101312160492}]}], "abstractContent": [{"text": "To solve a problem\ud97b\udf59of how to evaluate computer-produced summaries, a number of automatic\ud97b\udf59and manual methods have been proposed.", "labels": [], "entities": []}, {"text": "Manual methods evaluate summaries correctly, because humans evaluate them, but are costly.", "labels": [], "entities": [{"text": "summaries", "start_pos": 24, "end_pos": 33, "type": "TASK", "confidence": 0.9785723686218262}]}, {"text": "On the other hand, automatic methods, which use evaluation tools or programs, are low cost, although these methods cannot evaluate summaries as accurately as manual methods.", "labels": [], "entities": []}, {"text": "In this paper, we investigate an automatic evaluation method that can reduce the errors of traditional automatic methods by using several evaluation results obtained manually.", "labels": [], "entities": []}, {"text": "We conducted some experiments using the data of the Text Summarization Challenge 2 (TSC-2).", "labels": [], "entities": [{"text": "Text Summarization Challenge 2 (TSC-2)", "start_pos": 52, "end_pos": 90, "type": "TASK", "confidence": 0.6411890728133065}]}, {"text": "A comparison with conventional automatic methods shows that our method outperforms other methods usually used.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recently, the evaluation of computer-produced summaries has\ud97b\udf59become recognized as one of the problem areas that must be addressed in the field of automatic summarization.", "labels": [], "entities": [{"text": "summaries", "start_pos": 46, "end_pos": 55, "type": "TASK", "confidence": 0.6471965909004211}, {"text": "summarization", "start_pos": 155, "end_pos": 168, "type": "TASK", "confidence": 0.7048954367637634}]}, {"text": "To solve this problem, a number of automatic\ud97b\udf59() and manual methods\ud97b\udf59 () have been proposed.", "labels": [], "entities": []}, {"text": "Manual methods evaluate summaries correctly, because humans evaluate them, but are costly.", "labels": [], "entities": [{"text": "summaries", "start_pos": 24, "end_pos": 33, "type": "TASK", "confidence": 0.9785724878311157}]}, {"text": "On the other hand, automatic methods, which use evaluation tools or programs, are low cost, although these methods cannot evaluate summaries as accurately as manual methods.", "labels": [], "entities": []}, {"text": "In this paper, we investigate an automatic method that can reduce the errors of traditional automatic methods by using several evaluation results obtained manually.", "labels": [], "entities": []}, {"text": "Unlike other automatic methods, our method estimates manual evaluation scores.", "labels": [], "entities": []}, {"text": "Therefore, our method makes it possible to compare anew system with other systems that have been evaluated manually.", "labels": [], "entities": []}, {"text": "There are two research studies related to our work (.", "labels": [], "entities": []}, {"text": "proposed an automatic evaluation method using multiple evaluation results from a manual method.", "labels": [], "entities": []}, {"text": "In the field of machine translation, proposed an automatic method that gives an evaluation result of a translation system as a score for the Test of English for International Communication (TOEIC).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.7920925617218018}, {"text": "International Communication (TOEIC)", "start_pos": 161, "end_pos": 196, "type": "TASK", "confidence": 0.6252876162528992}]}, {"text": "Although the effectiveness of both methods was confirmed experimentally, further discussion of four points, which we describe in Section 3, is necessary fora more accurate summary evaluation.\ud97b\udf59 In this paper, we address three of these points based on Kazawa's and Yasuda's methods.", "labels": [], "entities": []}, {"text": "We also investigate whether these methods can outperform other automatic methods.\ud97b\udf59 The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes related work.", "labels": [], "entities": []}, {"text": "Section 3 describes our method.", "labels": [], "entities": []}, {"text": "To investigate the effectiveness of our method, we conducted some examinations and Section 4 reports on these.", "labels": [], "entities": []}, {"text": "We present some conclusions in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We investigate an automatic method using multiple evaluation results by a manual method based on Kazawa's and Yasuda's method.", "labels": [], "entities": []}, {"text": "The procedure of our evaluation method is shown as follows; (Step 1) Prepare summaries and their evaluation results by a manual method (Step 2) Calculate the similarities between a summary to be evaluated and the pooled summaries (Step 3) Combine manual scores of pooled summaries in proportion to their similarities to the summary to be evaluated For each step, we need to discuss the following points.", "labels": [], "entities": []}, {"text": "indicates the number of discourse units 1 that appear in both x ij and x, and | x | represents the number of words in x.", "labels": [], "entities": []}, {"text": "However, there are many other measures that can be used to calculate the topical similarities between two documents (or passages).", "labels": [], "entities": []}, {"text": "As well as Yasuda's method does, using W H is another way to calculate similarities between a summary to be evaluated and pooled summaries indirectly.", "labels": [], "entities": []}, {"text": "tested DP matching (   To investigate the three points described in Section 3.2, we conducted the following four experiments.", "labels": [], "entities": [{"text": "DP matching", "start_pos": 7, "end_pos": 18, "type": "TASK", "confidence": 0.5393214523792267}]}, {"text": "\ud97b\udf59 Exp-1: We examined Points 2 and 3 based on Kazawa's method.", "labels": [], "entities": []}, {"text": "We tested threshold values from 0 to 1 at 0.005 intervals.", "labels": [], "entities": []}, {"text": "We also tested several similarity measures, such as cosine distance and 11 kinds of ROUGE.", "labels": [], "entities": [{"text": "cosine distance", "start_pos": 52, "end_pos": 67, "type": "METRIC", "confidence": 0.7479261159896851}, {"text": "ROUGE", "start_pos": 84, "end_pos": 89, "type": "METRIC", "confidence": 0.9920893907546997}]}, {"text": "\ud97b\udf59 Exp-2: In order to investigate whether the evaluation based on Kazawa's method can outperform other automatic methods, we compared the evaluation with other automatic methods.", "labels": [], "entities": []}, {"text": "In this experiment, we used the similarity measure, which obtain the best performance in Exp-1.", "labels": [], "entities": [{"text": "similarity measure", "start_pos": 32, "end_pos": 50, "type": "METRIC", "confidence": 0.9748241305351257}, {"text": "Exp-1", "start_pos": 89, "end_pos": 94, "type": "DATASET", "confidence": 0.8466594815254211}]}, {"text": "\ud97b\udf59 Exp-3: We also examined Point 2 based on Yasuda's method.", "labels": [], "entities": []}, {"text": "As a similarity measure, we tested cosine distance and 11 kinds of ROUGE.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 67, "end_pos": 72, "type": "METRIC", "confidence": 0.9931586384773254}]}, {"text": "Then, we examined Point 4 by comparing the result of Yasuda's method with that of Kazawa's.", "labels": [], "entities": []}, {"text": "\ud97b\udf59 Exp-4: In the same way as Exp-2, we compared the evaluation with other automatic methods, which we describe in the next section, to investigate whether the evaluation based on Yasuda's method can outperform other automatic methods.", "labels": [], "entities": [{"text": "\ud97b\udf59", "start_pos": 0, "end_pos": 1, "type": "DATASET", "confidence": 0.8758359551429749}, {"text": "Exp-4", "start_pos": 2, "end_pos": 7, "type": "DATASET", "confidence": 0.7134062051773071}]}, {"text": "In the following, we show the automatic evaluation methods used in our experiments.", "labels": [], "entities": []}, {"text": "This measure evaluates summaries by comparing their content words with those of the humanproduced extracts.", "labels": [], "entities": [{"text": "summaries", "start_pos": 23, "end_pos": 32, "type": "TASK", "confidence": 0.9597213864326477}]}, {"text": "The score of the contentbased measure is obtained by computing the similarity between the term vector using tf*idf weighting of a computer-produced summary and the term vector of a human-produced summary by cosine distance.", "labels": [], "entities": []}, {"text": "In the following, we elaborate on the evaluation methods for each experiment.", "labels": [], "entities": []}, {"text": "To address Points 2 and 3, we evaluated summaries by the method based on Kazawa's method using 12 measures, described in Section 4.4, as measures to calculate topical similarities between summaries, and compared these measures by Gap.", "labels": [], "entities": [{"text": "summaries", "start_pos": 40, "end_pos": 49, "type": "TASK", "confidence": 0.9662413597106934}]}, {"text": "The experimental results for summarization ratios of 40%\ud97b\udf59 and 20% are shown in Coverage value from 0.2 to 1.0 at 0.1 intervals.", "labels": [], "entities": [{"text": "summarization", "start_pos": 29, "end_pos": 42, "type": "METRIC", "confidence": 0.7435029745101929}, {"text": "Coverage", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9950205087661743}]}, {"text": "Average values of Gap for each measure are also shown in these tables.", "labels": [], "entities": [{"text": "Gap", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.9958718419075012}]}, {"text": "As can be seen from Tables 1\ud97b\udf59and 2, the larger the threshold value, the smaller the value of Gap.", "labels": [], "entities": [{"text": "Gap", "start_pos": 93, "end_pos": 96, "type": "METRIC", "confidence": 0.9810189604759216}]}, {"text": "From the result, we can conclude for Point 3 that more accurate evaluation is possible when we use similar pooled summaries (Point 2).", "labels": [], "entities": []}, {"text": "However, the number of summaries that can be evaluated by this method was limited when the threshold value was large.", "labels": [], "entities": []}, {"text": "Of the 12 measures, unigram-based methods, such as cosine distance and ROUGE-1, produced good results.", "labels": [], "entities": [{"text": "cosine distance", "start_pos": 51, "end_pos": 66, "type": "METRIC", "confidence": 0.741297572851181}, {"text": "ROUGE-1", "start_pos": 71, "end_pos": 78, "type": "METRIC", "confidence": 0.9924483299255371}]}, {"text": "However, there were no significant differences between measures except for when ROUGE-L was used.", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 80, "end_pos": 87, "type": "METRIC", "confidence": 0.9931377172470093}]}, {"text": "In Exp-1, cosine distance outperformed the other 11 measures.", "labels": [], "entities": [{"text": "Exp-1", "start_pos": 3, "end_pos": 8, "type": "DATASET", "confidence": 0.7744995951652527}, {"text": "cosine distance", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.7544368803501129}]}, {"text": "We therefore used cosine distance in Kazawa's method in Exp-2.", "labels": [], "entities": [{"text": "Exp-2", "start_pos": 56, "end_pos": 61, "type": "DATASET", "confidence": 0.915746808052063}]}, {"text": "We ranked summaries by Kazawa's method, ROUGE and cosine distance, calculated using Precision.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 40, "end_pos": 45, "type": "METRIC", "confidence": 0.9980564117431641}, {"text": "cosine distance", "start_pos": 50, "end_pos": 65, "type": "METRIC", "confidence": 0.7630985975265503}, {"text": "Precision", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.934249222278595}]}, {"text": "The results of the evaluation by Precision for summarization ratios of 40% and 20% are shown in, respectively.", "labels": [], "entities": [{"text": "Precision", "start_pos": 33, "end_pos": 42, "type": "DATASET", "confidence": 0.7567852735519409}, {"text": "summarization", "start_pos": 47, "end_pos": 60, "type": "METRIC", "confidence": 0.8629741072654724}]}, {"text": "We plotted the Precision value of Kazawa's method by changing the threshold value from 0 to 1 at 0.05 intervals.", "labels": [], "entities": [{"text": "Precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9985878467559814}]}, {"text": "We also plotted the Precision values of ROUGE-2 as dotted lines.", "labels": [], "entities": [{"text": "Precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9703494310379028}, {"text": "ROUGE-2", "start_pos": 40, "end_pos": 47, "type": "METRIC", "confidence": 0.9807393550872803}]}, {"text": "ROUGE-2 was superior to the other 11 measures in terms of Ranking.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9153079390525818}, {"text": "Ranking", "start_pos": 58, "end_pos": 65, "type": "METRIC", "confidence": 0.9945673942565918}]}, {"text": "The X and Y axes in show the threshold value of Kazawa's method and the Precision values, respectively.", "labels": [], "entities": [{"text": "Precision", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.9855440258979797}]}, {"text": "From the result shown in, we found that Kazawa's method outperformed ROUGE-2, when the threshold value was greater than 0.968.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 69, "end_pos": 76, "type": "METRIC", "confidence": 0.9962769150733948}]}, {"text": "The Coverage value of this point was 0.203.", "labels": [], "entities": [{"text": "Coverage value", "start_pos": 4, "end_pos": 18, "type": "METRIC", "confidence": 0.9709660112857819}]}, {"text": "In, the Precision curve of Kazawa's method crossed the dotted line at a threshold value of 0.890.", "labels": [], "entities": [{"text": "Precision curve", "start_pos": 8, "end_pos": 23, "type": "METRIC", "confidence": 0.9767222106456757}]}, {"text": "The Coverage value of this point was 0.405.", "labels": [], "entities": [{"text": "Coverage value", "start_pos": 4, "end_pos": 18, "type": "METRIC", "confidence": 0.9710983633995056}]}, {"text": "To improve these Coverage values, we need to prepare more summaries and their manual evaluation results, because the Coverage is critically dependent on the number and variety of pooled summaries.", "labels": [], "entities": []}, {"text": "This is exactly the first point in Section 3.1, which we do not address in this paper.", "labels": [], "entities": []}, {"text": "We will investigate this point as the next step in our future work..", "labels": [], "entities": []}, {"text": "When the ratio is 20%, ROUGE-SU4 is the best.", "labels": [], "entities": [{"text": "ROUGE-SU4", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9972285628318787}]}, {"text": "The N-gram and the skip-bigram are both useful when the summarization ratio is low.", "labels": [], "entities": [{"text": "summarization", "start_pos": 56, "end_pos": 69, "type": "TASK", "confidence": 0.9421764612197876}]}, {"text": "For Point 4, we compared the result by Yasuda's method with that of Kazawa's method (in).", "labels": [], "entities": []}, {"text": "Yasuda's method could accurately estimate manual scores.", "labels": [], "entities": []}, {"text": "In particular, the Gap values of 0.023 by ROUGE-2 and by ROUGE-3 are smaller than those produced by Kazawa's method with a threshold value of 0.9).", "labels": [], "entities": [{"text": "Gap", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.9951128363609314}, {"text": "ROUGE-2", "start_pos": 42, "end_pos": 49, "type": "METRIC", "confidence": 0.7066389918327332}, {"text": "ROUGE-3", "start_pos": 57, "end_pos": 64, "type": "METRIC", "confidence": 0.7267282605171204}]}, {"text": "This indicates that regression analysis used in Yasuda's method is superior to that used in Kazawa's method.", "labels": [], "entities": []}, {"text": "We used the TSC-2 data) in our examinations.", "labels": [], "entities": [{"text": "TSC-2 data", "start_pos": 12, "end_pos": 22, "type": "DATASET", "confidence": 0.8289589583873749}]}, {"text": "The data consisted of human-produced extracts (denoted as \"PART\"), human-produced abstracts (denoted as \"FREE\"), computer-produced summaries (eight systems and a baseline system using the lead method (denoted as \"LEAD\")) , and their evaluation results by two manual methods.", "labels": [], "entities": [{"text": "PART", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.9190202355384827}, {"text": "FREE", "start_pos": 105, "end_pos": 109, "type": "METRIC", "confidence": 0.9969189167022705}, {"text": "LEAD", "start_pos": 213, "end_pos": 217, "type": "METRIC", "confidence": 0.9422922730445862}]}, {"text": "All the summaries were derived from 30 newspaper articles, written in Japanese, and were extracted from the Mainichi newspaper database for the years 1998 and 1999.", "labels": [], "entities": [{"text": "Mainichi newspaper database", "start_pos": 108, "end_pos": 135, "type": "DATASET", "confidence": 0.9877467155456543}]}, {"text": "Two tasks were conducted in TSC-2, and we used the data from a single document summarization task.", "labels": [], "entities": [{"text": "TSC-2", "start_pos": 28, "end_pos": 33, "type": "DATASET", "confidence": 0.7165810465812683}, {"text": "document summarization task", "start_pos": 70, "end_pos": 97, "type": "TASK", "confidence": 0.7111795941988627}]}, {"text": "In this task, participants were asked to produce summaries in plain text in the ratios of 20% and 40%.", "labels": [], "entities": []}, {"text": "Summaries were evaluated using a ranking evaluation method and the revision method evaluation.", "labels": [], "entities": []}, {"text": "In our experiments, we used the results of evaluation from the revision method.", "labels": [], "entities": []}, {"text": "This method evaluates summaries by measuring the degree to which computer-produced summaries are revised.", "labels": [], "entities": [{"text": "summaries", "start_pos": 22, "end_pos": 31, "type": "TASK", "confidence": 0.9854843020439148}]}, {"text": "The judges read the original texts and revised the computer-produced summaries in terms of their content and readability.", "labels": [], "entities": []}, {"text": "The human revisions were made with only three editing operations (insertion, deletion, replacement).", "labels": [], "entities": []}, {"text": "The degree of the human revision, called the \"edit distance,\" is computed from the number of revised characters divided by the number of characters in the original summary.", "labels": [], "entities": [{"text": "edit distance", "start_pos": 46, "end_pos": 59, "type": "METRIC", "confidence": 0.8809457421302795}]}, {"text": "If the summary's quality was so low that a revision of more than half of the original summary was required, the judges stopped the revision and a score of 0.5 was given.", "labels": [], "entities": []}, {"text": "The effectiveness of evaluation by the revision method was confirmed in our previous work).", "labels": [], "entities": []}, {"text": "We compared evaluation by revision with ranking evaluation.", "labels": [], "entities": []}, {"text": "We also tested other automatic methods: content-based evaluation, BLEU () and ROUGE-1, and compared their results with that of evaluation by revision as reference.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.9980816841125488}, {"text": "ROUGE-1", "start_pos": 78, "end_pos": 85, "type": "METRIC", "confidence": 0.9868764877319336}]}, {"text": "As a result, we found that evaluation by revision is effective for recognizing slight differences between computer-produced summaries.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3. When the ratio is 20%,  ROUGE-SU4 is the best. The N-gram and the  skip-bigram are both useful when the  summarization ratio is low.  For Point 4, we compared the result by  Yasuda's method", "labels": [], "entities": [{"text": "ROUGE-SU4", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9973626732826233}]}, {"text": " Table 3 Gap between the manual method and  Yasuda's method", "labels": [], "entities": []}]}