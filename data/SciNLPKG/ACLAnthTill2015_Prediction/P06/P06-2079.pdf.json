{"title": [{"text": "Examining the Role of Linguistic Knowledge Sources in the Automatic Identification and Classification of Reviews", "labels": [], "entities": [{"text": "Automatic Identification and Classification", "start_pos": 58, "end_pos": 101, "type": "TASK", "confidence": 0.7685406133532524}]}], "abstractContent": [{"text": "This paper examines two problems in document-level sentiment analysis: (1) determining whether a given document is a review or not, and (2) classifying the polarity of a review as positive or negative.", "labels": [], "entities": [{"text": "document-level sentiment analysis", "start_pos": 36, "end_pos": 69, "type": "TASK", "confidence": 0.7574426432450613}]}, {"text": "We first demonstrate that review identification can be performed with high accuracy using only unigrams as features.", "labels": [], "entities": [{"text": "review identification", "start_pos": 26, "end_pos": 47, "type": "TASK", "confidence": 0.9342912435531616}, {"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9933710098266602}]}, {"text": "We then examine the role of four types of simple linguistic knowledge sources in a polarity classification system.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentiment analysis involves the identification of positive and negative opinions from a text segment.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9574384391307831}, {"text": "identification of positive and negative opinions from a text segment", "start_pos": 32, "end_pos": 100, "type": "TASK", "confidence": 0.7350942909717559}]}, {"text": "The task has recently received a lot of attention, with applications ranging from multiperspective question-answering (e.g.,) to opinion-oriented information extraction (e.g.,) and summarization (e.g.,).", "labels": [], "entities": [{"text": "opinion-oriented information extraction", "start_pos": 129, "end_pos": 168, "type": "TASK", "confidence": 0.6164798339207967}, {"text": "summarization", "start_pos": 181, "end_pos": 194, "type": "TASK", "confidence": 0.9890368580818176}]}, {"text": "Research in sentiment analysis has generally proceeded at three levels, aiming to identify and classify opinions from documents, sentences, and phrases.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.9463066160678864}]}, {"text": "This paper examines two problems in document-level sentiment analysis, focusing on analyzing a particular type of opinionated documents: reviews.", "labels": [], "entities": [{"text": "document-level sentiment analysis", "start_pos": 36, "end_pos": 69, "type": "TASK", "confidence": 0.730514278014501}]}, {"text": "The first problem, polarity classification, has the goal of determining a review's polarity -positive (\"thumbs up\") or negative (\"thumbs down\").", "labels": [], "entities": [{"text": "polarity classification", "start_pos": 19, "end_pos": 42, "type": "TASK", "confidence": 0.8738071620464325}]}, {"text": "Recent work has expanded the polarity classification task to additionally handle documents expressing a neutral sentiment.", "labels": [], "entities": [{"text": "polarity classification task", "start_pos": 29, "end_pos": 57, "type": "TASK", "confidence": 0.8691538174947103}]}, {"text": "Although studied fairly extensively, polarity classification remains a challenge to natural language processing systems.", "labels": [], "entities": [{"text": "polarity classification", "start_pos": 37, "end_pos": 60, "type": "TASK", "confidence": 0.8589984774589539}]}, {"text": "We will focus on an important linguistic aspect of polarity classification: examining the role of a variety of simple, yet under-investigated, linguistic knowledge sources in a learning-based polarity classification system.", "labels": [], "entities": [{"text": "polarity classification", "start_pos": 51, "end_pos": 74, "type": "TASK", "confidence": 0.8302222788333893}]}, {"text": "Specifically, we will show how to build a high-performing polarity classifier by exploiting information provided by (1) high order n-grams, (2) a lexicon composed of adjectives manually annotated with their polarity information (e.g., happy is annotated as positive and terrible as negative), (3) dependency relations derived from dependency parses, and (4) objective terms and phrases extracted from neutral documents.", "labels": [], "entities": []}, {"text": "As mentioned above, the majority of work on document-level sentiment analysis to date has focused on polarity classification, assuming as input a set of reviews to be classified.", "labels": [], "entities": [{"text": "document-level sentiment analysis", "start_pos": 44, "end_pos": 77, "type": "TASK", "confidence": 0.7004266778628031}, {"text": "polarity classification", "start_pos": 101, "end_pos": 124, "type": "TASK", "confidence": 0.7897173762321472}]}, {"text": "A relevant question is: what if we don't know that an input document is a review in the first place?", "labels": [], "entities": []}, {"text": "The second task we will examine in this paper -review identification -attempts to address this question.", "labels": [], "entities": [{"text": "review identification", "start_pos": 47, "end_pos": 68, "type": "TASK", "confidence": 0.7780525088310242}]}, {"text": "Specifically, review identification seeks to determine whether a given document is a review or not.", "labels": [], "entities": [{"text": "review identification", "start_pos": 14, "end_pos": 35, "type": "TASK", "confidence": 0.7450607120990753}]}, {"text": "We view both review identification and polarity classification as a classification task.", "labels": [], "entities": [{"text": "review identification", "start_pos": 13, "end_pos": 34, "type": "TASK", "confidence": 0.8299779891967773}, {"text": "polarity classification", "start_pos": 39, "end_pos": 62, "type": "TASK", "confidence": 0.698073074221611}]}, {"text": "For review identification, we train a classifier to distinguish movie reviews and movie-related nonreviews (e.g., movie ads, plot summaries) using only unigrams as features, obtaining an accuracy of over 99% via 10-fold cross-validation.", "labels": [], "entities": [{"text": "review identification", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.7060591280460358}, {"text": "accuracy", "start_pos": 187, "end_pos": 195, "type": "METRIC", "confidence": 0.9993476271629333}]}, {"text": "Similar experiments using documents from the book domain also yield an accuracy as high as 97%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9993909597396851}]}, {"text": "An analysis of the results reveals that the high accuracy can be attributed to the difference in the vocabulary employed in reviews and non-reviews: while reviews can be composed of a mixture of subjective and objective language, our non-review documents rarely contain subjective expressions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9986086487770081}]}, {"text": "Next, we learn our polarity classifier using positive and negative reviews taken from two movie review datasets, one assembled by and the other by ourselves.", "labels": [], "entities": []}, {"text": "The resulting classifier, when trained on a feature set derived from the four types of linguistic knowledge sources mentioned above, achieves a 10-fold cross-validation accuracy of 90.5% and 86.1% on Pang et al.'s dataset and ours, respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 169, "end_pos": 177, "type": "METRIC", "confidence": 0.9351714849472046}, {"text": "Pang et al.'s dataset", "start_pos": 200, "end_pos": 221, "type": "DATASET", "confidence": 0.843378891547521}]}, {"text": "To our knowledge, our result on Pang et al.'s dataset is one of the best reported to date.", "labels": [], "entities": [{"text": "Pang et al.'s dataset", "start_pos": 32, "end_pos": 53, "type": "DATASET", "confidence": 0.959540436665217}]}, {"text": "Perhaps more importantly, an analysis of these results show that the various types of features interact in an interesting manner, allowing us to draw conclusions that provide new insights into polarity classification.", "labels": [], "entities": [{"text": "polarity classification", "start_pos": 193, "end_pos": 216, "type": "TASK", "confidence": 0.8289118707180023}]}], "datasetContent": [{"text": "Like several previous work (e.g., Mullen and Collier (2004),,), we view polarity classification as a supervised learning task.", "labels": [], "entities": [{"text": "polarity classification", "start_pos": 72, "end_pos": 95, "type": "TASK", "confidence": 0.8136058151721954}]}, {"text": "As in review identification, we use SVM light with default parameter settings to train polarity classifiers 13 , reporting all results as 10-fold CV accuracy.", "labels": [], "entities": [{"text": "review identification", "start_pos": 6, "end_pos": 27, "type": "TASK", "confidence": 0.7377649247646332}, {"text": "accuracy", "start_pos": 149, "end_pos": 157, "type": "METRIC", "confidence": 0.5781879425048828}]}, {"text": "We evaluate our polarity classifiers on two movie review datasets, each of which consists of 1000 positive reviews and 1000 negative reviews.", "labels": [], "entities": [{"text": "movie review datasets", "start_pos": 44, "end_pos": 65, "type": "DATASET", "confidence": 0.6427759726842245}]}, {"text": "The first one, which we will refer to as Dataset A, is the Pang et al. polarity dataset (version 2.0).", "labels": [], "entities": [{"text": "Pang et al. polarity dataset", "start_pos": 59, "end_pos": 87, "type": "DATASET", "confidence": 0.9130476236343383}]}, {"text": "The second one (Dataset B) was created by us, with the sole purpose of providing additional experimental results.", "labels": [], "entities": []}, {"text": "Reviews in Dataset B were randomly chosen from Pang et al.'s pool of 27886 unprocessed movie reviews (see Section 3) that have either a positive or a negative rating.", "labels": [], "entities": []}, {"text": "We followed exactly Pang et al.'s guideline when determining whether a review is positive or negative.", "labels": [], "entities": []}, {"text": "14 Also, we took care to ensure that reviews included in Dataset B do not appear in Dataset A. We applied to these reviews the same four pre-processing steps that we did to the neutral reviews in the previous section.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Polarity classification accuracies.", "labels": [], "entities": [{"text": "Polarity classification", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.6614252030849457}]}]}