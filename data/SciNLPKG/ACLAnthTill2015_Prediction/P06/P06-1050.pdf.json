{"title": [{"text": "Learning Event Durations from Event Descriptions", "labels": [], "entities": [{"text": "Learning Event Durations from Event Descriptions", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.7081446299950281}]}], "abstractContent": [{"text": "We have constructed a corpus of news articles in which events are annotated for estimated bounds on their duration.", "labels": [], "entities": []}, {"text": "Here we describe a method for measuring inter -annotator agreement for these event duration distributions.", "labels": [], "entities": []}, {"text": "We then show that machine learning techniques applied to this data yield coarse-grained event duration information, considerably outper-forming a baseline and approaching human performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Consider the sentence from a news article: George W. Bush met with Vladimir Putin in Moscow.", "labels": [], "entities": []}, {"text": "How long was the meeting?", "labels": [], "entities": []}, {"text": "Our first reaction to this question might be that we have no idea.", "labels": [], "entities": []}, {"text": "But in fact we do have an idea.", "labels": [], "entities": []}, {"text": "We know the meeting was longer than 10 seconds and less than a year.", "labels": [], "entities": []}, {"text": "How much tighter can we get the bounds to be?", "labels": [], "entities": []}, {"text": "Most people would say the meeting lasted between an hour and three days.", "labels": [], "entities": []}, {"text": "There is much temporal information in text that has hitherto been largely unexploited, encoded in the descriptions of events and relying on our knowledge of the range of usual durations of types of events.", "labels": [], "entities": []}, {"text": "This paper describes one part of an exploration into how this information can be captured automatically.", "labels": [], "entities": []}, {"text": "Specifically, we have developed annotation guidelines to minimize discrepant judgments and annotated 58 articles, comprising 2288 events; we have developed a method for measuring inter-annotator agreement when the judgments are intervals on a scale; and we have shown that machine learning techniques applied to the annotated data considerably outperform a baseline and approach human performance.", "labels": [], "entities": []}, {"text": "This research is potentially very important in applications in which the time course of events is to be extracted from news.", "labels": [], "entities": []}, {"text": "For example, whether two events overlap or are in sequence often depends very much on their durations.", "labels": [], "entities": []}, {"text": "If a war started yesterday, we can be pretty sure it is still going on today.", "labels": [], "entities": []}, {"text": "If a hurricane started last year, we can be sure it is over by now.", "labels": [], "entities": []}, {"text": "The corpus that we have annotated currently contains all the 48 non-Wall-Street-Journal (non-WSJ) news articles (a total of 2132 event instances), as well as 10 WSJ articles (156 event instances), from the TimeBank corpus annotated in TimeML ().", "labels": [], "entities": [{"text": "TimeBank corpus annotated in TimeML", "start_pos": 206, "end_pos": 241, "type": "DATASET", "confidence": 0.9194958567619324}]}, {"text": "The non-WSJ articles (mainly political and disaster news) include both print and broadcast news that are from a variety of news sources, such as ABC, AP, and VOA.", "labels": [], "entities": [{"text": "AP", "start_pos": 150, "end_pos": 152, "type": "DATASET", "confidence": 0.6675770878791809}, {"text": "VOA", "start_pos": 158, "end_pos": 161, "type": "DATASET", "confidence": 0.9299563765525818}]}, {"text": "In the corpus, every event to be annotated was already identified in TimeBank.", "labels": [], "entities": [{"text": "TimeBank", "start_pos": 69, "end_pos": 77, "type": "DATASET", "confidence": 0.9668470621109009}]}, {"text": "Annotators were instructed to provide lower and upper bounds on the duration of the event, encompassing 80% of the possibilities, excluding anomalous cases, and taking the entire context of the article into account.", "labels": [], "entities": []}, {"text": "For example, here is the graphical output of the annotations (3 annotators) for the \"finished\" event (underlined) in the sentence After the victim, Linda Sanders, 35, had finished her cleaning and was waiting for her clothes to dry,...", "labels": [], "entities": []}, {"text": "This graph shows that the first annotator believes that the event lasts for minutes whereas the second annotator believes it could only last for several seconds.", "labels": [], "entities": []}, {"text": "The third annotates the event to range from a few seconds to a few minutes.", "labels": [], "entities": []}, {"text": "A logarithmic scale is used for the output because of the intuition that the difference between 1 second and 20 seconds is significant, while the difference between 1 year 1 second and 1 year 20 seconds is negligible.", "labels": [], "entities": []}, {"text": "A preliminary exercise in annotation revealed about a dozen classes of systematic discrepancies among annotators' judgments.", "labels": [], "entities": []}, {"text": "We thus developed guidelines to make annotators aware of these cases and to guide them in making the judgments.", "labels": [], "entities": []}, {"text": "For example, many occurrences of verbs and other event descriptors refer to multiple events, especially but not exclusively if the subject or object of the verb is plural.", "labels": [], "entities": []}, {"text": "In \"Iraq has destroyed its long-range missiles\", there is the time it takes to destroy one missile and the duration of the interval in which all the individual events are situated -the time it takes to destroy all its missiles.", "labels": [], "entities": []}, {"text": "Initially, there were wide discrepancies because some annotators would annotate one value, others the other.", "labels": [], "entities": []}, {"text": "Annotators are now instructed to make judgments on both values in this case.", "labels": [], "entities": []}, {"text": "The use of the annotation guidelines resulted in about 10% improvement in inter-annotator agreement (), measured as described in Section 2.", "labels": [], "entities": []}, {"text": "There is a residual of gross discrepancies in annotators' judgments that result from differences of opinion, for example, about how long a government policy is typically in effect.", "labels": [], "entities": []}, {"text": "But the number of these discrepancies was surprisingly small.", "labels": [], "entities": []}, {"text": "The method and guidelines for annotation are described in much greater detail in ().", "labels": [], "entities": []}, {"text": "In the current paper, we focus on how inter-annotator agreement is measured, in Section 2, and in Sections 3-5 on the machine learning experiments.", "labels": [], "entities": []}, {"text": "Because the annotated corpus is still fairly small, we cannot hope to learn to make fine-grained judgments of event durations that are currently annotated in the corpus, but as we demonstrate, it is possible to learn useful coarse-grained judgments.", "labels": [], "entities": []}, {"text": "Although there has been much work on temporal anchoring and event ordering in text), to our knowledge, there has been no serious published empirical effort to model and learn vague and implicit duration information in natural language, such as the typical durations of events, and to perform reasoning over this information.", "labels": [], "entities": [{"text": "temporal anchoring", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.7541960775852203}, {"text": "event ordering", "start_pos": 60, "end_pos": 74, "type": "TASK", "confidence": 0.7295794188976288}]}, {"text": "(Cyc apparently has some fuzzy duration information, although it is not generally available; discusses the issue for less than a page; there has been work in fuzzy logic on representing and reasoning with imprecise durations (, but these make no attempt to collect human judgments on such durations or learn to extract them automatically from texts.)", "labels": [], "entities": []}], "datasetContent": [{"text": "The distribution of the means of the annotated durations in is bimodal, dividing the events into those that take less than a day and those that take more than a day.", "labels": [], "entities": []}, {"text": "Thus, in our first machine learning experiment, we have tried to learn this coarse-grained event duration information as a binary classification task.", "labels": [], "entities": []}, {"text": "Three supervised learning algorithms were evaluated for our binary classification task, namely, Support Vector Machines (SVM), Na\u00efve Bayes (NB), and Decision Trees C4.5.", "labels": [], "entities": [{"text": "binary classification task", "start_pos": 60, "end_pos": 86, "type": "TASK", "confidence": 0.7990138928095499}]}, {"text": "The Weka) machine learning package was used for the implementation of these learning algorithms.", "labels": [], "entities": [{"text": "Weka) machine learning package", "start_pos": 4, "end_pos": 34, "type": "DATASET", "confidence": 0.935361897945404}]}, {"text": "Linear kernel is used for SVM in our experiments.", "labels": [], "entities": [{"text": "SVM", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.9106264710426331}]}, {"text": "Each event instance has a total of 18 feature values, as described in Section 3, for the event only condition, and 30 feature values for the local context condition when n = 2.", "labels": [], "entities": []}, {"text": "For SVM and C4.5, all features are converted into binary features (6665 and 12502 features).", "labels": [], "entities": []}, {"text": "10-fold cross validation was used to train the learning models, which were then tested on the unseen held-out test set, and the performance (including the precision, recall, and F-score", "labels": [], "entities": [{"text": "precision", "start_pos": 155, "end_pos": 164, "type": "METRIC", "confidence": 0.9997090697288513}, {"text": "recall", "start_pos": 166, "end_pos": 172, "type": "METRIC", "confidence": 0.997810423374176}, {"text": "F-score", "start_pos": 178, "end_pos": 185, "type": "METRIC", "confidence": 0.9975166320800781}]}], "tableCaptions": [{"text": " Table 5: Test Performance of Three Algorithms.", "labels": [], "entities": []}, {"text": " Table 7: Feature Evaluation with Different Feature Sets using SVM.", "labels": [], "entities": [{"text": "Feature Evaluation", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.852473258972168}]}, {"text": " Table 8: Test Performance on WSJ data.", "labels": [], "entities": [{"text": "WSJ data", "start_pos": 30, "end_pos": 38, "type": "DATASET", "confidence": 0.9273980259895325}]}, {"text": " Table 9: Inter-Annotator Agreement for Most  Likely Temporal Unit.", "labels": [], "entities": []}, {"text": " Table 10: Overall Test Precisions.", "labels": [], "entities": []}]}