{"title": [{"text": "A Comparison of Document, Sentence, and Term Event Spaces", "labels": [], "entities": []}], "abstractContent": [{"text": "The trend in information retrieval systems is from document to sub-document retrieval, such as sentences in a summari-zation system and words or phrases in question-answering system.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 13, "end_pos": 34, "type": "TASK", "confidence": 0.7033897638320923}]}, {"text": "Despite this trend, systems continue to model language at a document level using the inverse document frequency (IDF).", "labels": [], "entities": [{"text": "inverse document frequency (IDF)", "start_pos": 85, "end_pos": 117, "type": "METRIC", "confidence": 0.8456839670737585}]}, {"text": "In this paper, we compare and contrast IDF with inverse sentence frequency (ISF) and inverse term frequency (ITF).", "labels": [], "entities": [{"text": "inverse sentence frequency (ISF)", "start_pos": 48, "end_pos": 80, "type": "METRIC", "confidence": 0.787212535738945}, {"text": "inverse term frequency (ITF)", "start_pos": 85, "end_pos": 113, "type": "METRIC", "confidence": 0.8958834807078043}]}, {"text": "A direct comparison reveals that all language models are highly correlated; however, the average ISF and ITF values are 5.5 and 10.4 higher than IDF.", "labels": [], "entities": [{"text": "IDF", "start_pos": 145, "end_pos": 148, "type": "METRIC", "confidence": 0.995013415813446}]}, {"text": "All language models appeared to follow a power law distribution with a slope coefficient of 1.6 for documents and 1.7 for sentences and terms.", "labels": [], "entities": []}, {"text": "We conclude with an analysis of IDF stability with respect to random, journal, and section partitions of the 100,830 full-text scientific articles in our experimental corpus.", "labels": [], "entities": [{"text": "IDF stability", "start_pos": 32, "end_pos": 45, "type": "METRIC", "confidence": 0.6144667267799377}]}], "introductionContent": [{"text": "The vector based information retrieval model identifies relevant documents by comparing query terms with terms from a document corpus.", "labels": [], "entities": []}, {"text": "The most common corpus weighting scheme is the term frequency (TF) x inverse document frequency (IDF), where TF is the number of times a term appears in a document, and IDF reflects the distribution of terms within the corpus.", "labels": [], "entities": [{"text": "term frequency (TF) x inverse document frequency (IDF)", "start_pos": 47, "end_pos": 101, "type": "METRIC", "confidence": 0.8809494972229004}, {"text": "TF", "start_pos": 109, "end_pos": 111, "type": "METRIC", "confidence": 0.9662728905677795}, {"text": "IDF", "start_pos": 169, "end_pos": 172, "type": "METRIC", "confidence": 0.992328941822052}]}, {"text": "Ideally, the system should assign the highest weights to terms with the most discriminative power.", "labels": [], "entities": []}, {"text": "One component of the corpus weight is the language model used.", "labels": [], "entities": []}, {"text": "The most common language model is the Inverse Document Frequency (IDF), which considers the distribution of terms between documents (see equation).", "labels": [], "entities": []}, {"text": "IDF has played a central role in retrieval systems since it was first introduced more than thirty years ago.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our goal in this paper is to compare and contrast language models based on a document with those based on a sentence and term event spaces.", "labels": [], "entities": []}, {"text": "We considered several of the corpora from the Text Retrieval Conferences (TREC, trec.nist.gov); however, those collections were primarily news articles.", "labels": [], "entities": [{"text": "Text Retrieval Conferences (TREC", "start_pos": 46, "end_pos": 78, "type": "TASK", "confidence": 0.5867714643478393}]}, {"text": "One exception was the recently added genomics track, which considered full-text scientific articles, but did not provide relevance judgments at a sentence or term level.", "labels": [], "entities": []}, {"text": "We also considered the sentence level judgments from the novelty track and the phrase level judgments from the question-answering track, but those were news and web documents respectively and we had wanted to explore the event spaces in the context of scientific literature.", "labels": [], "entities": []}, {"text": "shows the corpus that we developed for these experiments.", "labels": [], "entities": []}, {"text": "The American Chemistry Society provided 103,262 full-text documents, which were published in 27 journals from 2000-2004 . We processed the headings, text, and tables using Java BreakIterator class to identify sentences and a Java implementation of the Porter Stemming algorithm to identify terms.", "labels": [], "entities": []}, {"text": "The inverted index was stored in an Oracle 10i database.", "labels": [], "entities": [{"text": "inverted index", "start_pos": 4, "end_pos": 18, "type": "METRIC", "confidence": 0.9335107803344727}]}, {"text": "We made the following comparisons between the document, sentence, and term event spaces.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2. Examples of IDF, ISF and ITF for terms with increasing IDF.", "labels": [], "entities": [{"text": "IDF", "start_pos": 22, "end_pos": 25, "type": "METRIC", "confidence": 0.8538063168525696}, {"text": "IDF", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.9912290573120117}]}]}