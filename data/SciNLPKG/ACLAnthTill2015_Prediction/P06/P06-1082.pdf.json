{"title": [{"text": "Word Alignment in English-Hindi Parallel Corpus Using Recency-Vector Approach: Some Studies", "labels": [], "entities": [{"text": "Word Alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.6953523606061935}]}], "abstractContent": [{"text": "Word alignment using recency-vector based approach has recently become popular.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7446224093437195}]}, {"text": "One major advantage of these techniques is that unlike other approaches they perform well even if the size of the parallel corpora is small.", "labels": [], "entities": []}, {"text": "This makes these algorithms worth-studying for languages where resources are scarce.", "labels": [], "entities": []}, {"text": "In this work we studied the performance of two very popular recency-vector based approaches, proposed in (Fung and McKeown, 1994) and (Somers, 1998), respectively, for word alignment in English-Hindi parallel corpus.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 168, "end_pos": 182, "type": "TASK", "confidence": 0.7860630452632904}]}, {"text": "But performance of the above algorithms was not found to be satisfactory.", "labels": [], "entities": []}, {"text": "However, subsequent addition of some new constraints improved the performance of the recency-vector based alignment technique significantly for the said corpus.", "labels": [], "entities": [{"text": "recency-vector based alignment", "start_pos": 85, "end_pos": 115, "type": "TASK", "confidence": 0.646850049495697}]}, {"text": "The present paper discusses the new version of the algorithm and its performance in detail.", "labels": [], "entities": []}], "introductionContent": [{"text": "Several approaches including statistical techniques (, lexical techniques and hybrid techniques (Ahrenberg et al.,), have been pursued to design schemes for word alignment which aims at establishing links between words of a source language and a target language in a parallel corpus.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 157, "end_pos": 171, "type": "TASK", "confidence": 0.767843097448349}]}, {"text": "All these schemes rely heavily on rich linguistic resources, either in the form of huge data of parallel texts or various language/grammar related tools, such as parser, tagger, morphological analyser etc.", "labels": [], "entities": []}, {"text": "Recency vector based approach has been proposed as an alternative strategy for word alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 79, "end_pos": 93, "type": "TASK", "confidence": 0.8139805793762207}]}, {"text": "Approaches based on recency vectors typically consider the positions of the word in the corresponding texts rather than sentence boundaries.", "labels": [], "entities": []}, {"text": "Two algorithms of this type can be found in and.", "labels": [], "entities": []}, {"text": "The algorithms first compute the position vector V w for the word win the text.", "labels": [], "entities": []}, {"text": "Typically, V w is of the form p 1 p 2 . .", "labels": [], "entities": []}, {"text": "pk , where the pi s indicate the positions of the word win a text T . A new vector R w , called the recency vector, is computed using the position vector V w , and is defined asp 2 \u2212p 1 , p 3 \u2212p 2 , . .", "labels": [], "entities": []}, {"text": ", pk \u2212p k\u22121 . In order to compute the alignment of a given word in the source language text, the recency vector of the word is compared with the recency vector of each target language word and the similarity between them is measured by computing a matching cost associated with the recency vectors using dynamic programming.", "labels": [], "entities": []}, {"text": "The target language word having the least cost is selected as the aligned word.", "labels": [], "entities": []}, {"text": "The results given in the above references show that the algorithms worked quite well in aligning words in parallel corpora of language pairs consisting of various European languages and Chinese, Japanese, taken pair-wise.", "labels": [], "entities": []}, {"text": "Precision of about 70% could be achieved using these algorithms.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9848629832267761}]}, {"text": "The major advantage of this approach is that it can work even on a relatively small dataset and it does not rely on rich language resources.", "labels": [], "entities": []}, {"text": "The above advantage motivated us to study the effectiveness of these algorithms for aligning words in English-Hindi parallel texts.", "labels": [], "entities": []}, {"text": "The corpus used for this work is described in.", "labels": [], "entities": []}, {"text": "It has been made manually from three different sources: children's storybooks, English to Hindi translation book material, and advertisements.", "labels": [], "entities": []}, {"text": "We shall call the three corpora as Storybook corpus, Sentence corpus and Advertisement corpus, respectively.", "labels": [], "entities": [{"text": "Storybook corpus", "start_pos": 35, "end_pos": 51, "type": "DATASET", "confidence": 0.8833721280097961}]}], "datasetContent": [{"text": "The results of the application of this algorithm have been very poor when applied on the three English to Hindi parallel corpora mentioned above without imposing any constraints.", "labels": [], "entities": []}, {"text": "We then experimented by varying the values of the parameters in the constraints in order to observe their effects on the accuracy of alignment.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.999203622341156}]}, {"text": "As was suggested in, we also observed that the Euclidean distance constraint is not very beneficial when the corpus size is small.", "labels": [], "entities": []}, {"text": "So this constraint has not been considered in our subsequent experiments.", "labels": [], "entities": []}, {"text": "Starting point constraint imposes a range within which the search for the matching word is restricted.", "labels": [], "entities": []}, {"text": "Although Fung and McKeown suggested the range to behalf of the length of the text, we felt that the optimum value of this range will vary from text to text depending on the type of corpus, length ratio of the two texts etc.", "labels": [], "entities": []}, {"text": "shows the results obtained on applying the DK vec algorithm on Sentence corpus for different lower values of range.", "labels": [], "entities": [{"text": "Sentence corpus", "start_pos": 63, "end_pos": 78, "type": "DATASET", "confidence": 0.7394093871116638}]}, {"text": "Similar results were obtained for the other two corpora.", "labels": [], "entities": []}, {"text": "The maximum increase observed in the F-score is around 0.062 for the Sentence corpus, 0.03 for the Story book corpus and 0.05 for the Advertisement corpus.", "labels": [], "entities": [{"text": "F-score", "start_pos": 37, "end_pos": 44, "type": "METRIC", "confidence": 0.9955832362174988}, {"text": "Sentence corpus", "start_pos": 69, "end_pos": 84, "type": "DATASET", "confidence": 0.8373557031154633}, {"text": "Story book corpus", "start_pos": 99, "end_pos": 116, "type": "DATASET", "confidence": 0.9229933619499207}, {"text": "Advertisement corpus", "start_pos": 134, "end_pos": 154, "type": "DATASET", "confidence": 0.7726947665214539}]}, {"text": "None of these improvements can be considered to be significant.", "labels": [], "entities": []}, {"text": "The algorithm provided by Somers works by first finding all the minimum score word pairs using dynamic programming, and then applying three filters Multiple Alignment Selection filter, Best Alignment Score Selection filter and Frequency Range constraint to the raw results to increase the accuracy of alignment.", "labels": [], "entities": [{"text": "Best Alignment Score Selection filter", "start_pos": 185, "end_pos": 222, "type": "METRIC", "confidence": 0.7569782137870789}, {"text": "Frequency Range constraint", "start_pos": 227, "end_pos": 253, "type": "METRIC", "confidence": 0.9279489517211914}, {"text": "accuracy", "start_pos": 289, "end_pos": 297, "type": "METRIC", "confidence": 0.9987996816635132}]}, {"text": "Somers has suggested that in such cases only the word pair that has the minimum alignment score should be considered.", "labels": [], "entities": [{"text": "alignment score", "start_pos": 80, "end_pos": 95, "type": "METRIC", "confidence": 0.9676178395748138}]}, {"text": "provides results (see column F-score old) when the raw output is passed through the MAS filters for the three corpora.", "labels": [], "entities": [{"text": "F-score", "start_pos": 29, "end_pos": 36, "type": "METRIC", "confidence": 0.9819013476371765}, {"text": "MAS", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.6145752668380737}]}, {"text": "Note that for all the three corpora a variety of frequency ranges have been considered, and we have observed that the results obtained are slightly better when the MAS filter has been used.", "labels": [], "entities": []}, {"text": "The best F-score is obtained when frequency range is high i.e. 100-150, 100-200.", "labels": [], "entities": [{"text": "F-score", "start_pos": 9, "end_pos": 16, "type": "METRIC", "confidence": 0.9988669157028198}]}, {"text": "But here the words are very few in number and are primarily pronoun, determiner or conjunction which are not significant from alignment perspective.", "labels": [], "entities": []}, {"text": "Also, it was observed that when medium frequency ranges, such as 30-50, are used the best result, in terms of precision, is around 20-28% for the three corpora.", "labels": [], "entities": [{"text": "precision", "start_pos": 110, "end_pos": 119, "type": "METRIC", "confidence": 0.9995415210723877}]}, {"text": "However, since the corpus size is small, here too the available and proposed aligned word pairs are very few (below 25).", "labels": [], "entities": []}, {"text": "Lower frequency ranges (viz. 2-20 and its sub-ranges) result in the highest number of aligned pairs.", "labels": [], "entities": []}, {"text": "We noticd that these aligned word pairs are typically verb, adjective, noun and adverb.", "labels": [], "entities": []}, {"text": "But here too the performance of the algorithm maybe considered to be unsatisfactory.", "labels": [], "entities": []}, {"text": "Although Somers has recommended words in the frequency ranges 10-30 to be considered for alignment, we have considered lower frequency words too in our experiments.", "labels": [], "entities": [{"text": "alignment", "start_pos": 89, "end_pos": 98, "type": "TASK", "confidence": 0.9742611646652222}]}, {"text": "This is because the corpus size being small we would otherwise have effectively overlooked many small-frequency words (e.g. noun, verb, adjective) that are significant from the alignment point of view.", "labels": [], "entities": []}, {"text": "Somers has further observed that if the Best Alignment Score Selection (BASS) filter is applied to yield the first few best results of alignment the overall quality of the result improves.", "labels": [], "entities": [{"text": "Best Alignment Score Selection (BASS) filter", "start_pos": 40, "end_pos": 84, "type": "METRIC", "confidence": 0.8537510074675083}]}, {"text": "shows the results of the experiments done for different alignment score cut-off without considering the Frequency Range constraint on the three corpora.", "labels": [], "entities": [{"text": "Frequency Range constraint", "start_pos": 104, "end_pos": 130, "type": "METRIC", "confidence": 0.957743505636851}]}, {"text": "However, it was observed that the performance of the algorithm reduced slightly on introducing this BASS filter.", "labels": [], "entities": [{"text": "BASS", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.9091535806655884}]}, {"text": "The above experiments suggest that the performance of the two algorithms is rather poor in the context of English-Hindi parallel texts as compared to other language pairs as shown by Fung and Somers.", "labels": [], "entities": []}, {"text": "In the following section we discuss the reasons for the low recall and precision values.", "labels": [], "entities": [{"text": "recall", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.999390721321106}, {"text": "precision", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.999220609664917}]}, {"text": "We have conducted experiments to determine the number of segments above and below the current segment that should be considered for searching the match of a word for each corpus.", "labels": [], "entities": []}, {"text": "In this respect we define i-segment constraint in which the search is restricted to the segments k \u2212 i to k + i of the target language corpus when the word under consideration is in the segment k of the source language corpus.", "labels": [], "entities": []}, {"text": "Evidently, the value of i depends on the accuracy of sentence alignment.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9992994070053101}, {"text": "sentence alignment", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.7093923836946487}]}, {"text": "suggests that the quality of alignment is different for the three corpora that we considered.", "labels": [], "entities": []}, {"text": "Due to the very high precision and recall for Sentence corpus we have restricted our search to the k th segment only, i.e. the value of i is 0.", "labels": [], "entities": [{"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9994235038757324}, {"text": "recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9995513558387756}, {"text": "Sentence corpus", "start_pos": 46, "end_pos": 61, "type": "DATASET", "confidence": 0.7207034528255463}]}, {"text": "However, since the results are not so good for the Storybook and Advertisement corpora we found after experimenting that the best results were obtained when i was 1.", "labels": [], "entities": []}, {"text": "During the experiments it was observed that as the number of segments was lowered or increased from the optimum segment the accuracy of alignment decreased continuously by around 10% for low frequency ranges for the three corpora and remained almost same for high frequency ranges.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9995504021644592}]}, {"text": "shows the results obtained when segment constraint is applied on the three corpora at optimum segment range for various frequency ranges.", "labels": [], "entities": []}, {"text": "A comparison between the F-score given by algorithm in) (the column Fscore old in the table) and the F-score obtained by applying the improved scheme (the column Fscore new in the table) indicate that the results have improved significantly for low frequency ranges.", "labels": [], "entities": [{"text": "F-score", "start_pos": 25, "end_pos": 32, "type": "METRIC", "confidence": 0.994668185710907}, {"text": "Fscore", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.9697969555854797}, {"text": "F-score", "start_pos": 101, "end_pos": 108, "type": "METRIC", "confidence": 0.9956550598144531}]}, {"text": "It is observed that the accuracy of alignment for almost 95% of the available words has increased significantly.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9995642304420471}]}, {"text": "This accounts for words within low frequency range of 2-40 for Sentence corpus, 2-30 for Storybook corpus, and 2-20 for Advertisement corpus.", "labels": [], "entities": [{"text": "Storybook corpus", "start_pos": 89, "end_pos": 105, "type": "DATASET", "confidence": 0.859816163778305}]}, {"text": "Also, most of the correct word pairs given by the modified approach are verbs, adjectives or nouns.", "labels": [], "entities": []}, {"text": "Also it was observed that as the noise in the corpus increased the results became poorer.", "labels": [], "entities": []}, {"text": "This accounts for the lowest F-score values for advertisement corpus.", "labels": [], "entities": [{"text": "F-score", "start_pos": 29, "end_pos": 36, "type": "METRIC", "confidence": 0.9985590577125549}]}, {"text": "The Sentence corpus, however, has been found to be the least noisy, and highest precision and recall values were obtained with this corpus.", "labels": [], "entities": [{"text": "Sentence corpus", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.8219323754310608}, {"text": "precision", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.9992315769195557}, {"text": "recall", "start_pos": 94, "end_pos": 100, "type": "METRIC", "confidence": 0.9980632662773132}]}, {"text": "Using Somers' second filter on each corpus for the optimum segment we found that the results at low scores were better as shown in.", "labels": [], "entities": []}, {"text": "The word pairs obtained after applying the modified approach can be used as anchor points for further alignment as well as for vocabulary extraction.", "labels": [], "entities": [{"text": "vocabulary extraction", "start_pos": 127, "end_pos": 148, "type": "TASK", "confidence": 0.7939510643482208}]}, {"text": "In case of the Sentence corpus, best result for anchor points for further alignment lies at the score cutoff 1000 where precision and recall are 86.88% and 80.35% respectively.", "labels": [], "entities": [{"text": "Sentence corpus", "start_pos": 15, "end_pos": 30, "type": "DATASET", "confidence": 0.7980869710445404}, {"text": "precision", "start_pos": 120, "end_pos": 129, "type": "METRIC", "confidence": 0.9995710253715515}, {"text": "recall", "start_pos": 134, "end_pos": 140, "type": "METRIC", "confidence": 0.9984649419784546}]}, {"text": "Hence F-score is 0.835 which is very high as compared to 0.173 obtained by Somers' approach and indicates an improvement of 382.65%.", "labels": [], "entities": [{"text": "F-score", "start_pos": 6, "end_pos": 13, "type": "METRIC", "confidence": 0.9997062087059021}]}, {"text": "Also, here the number of correct word pairs is 198, whereas the algorithms in and gave only 62 and 61 correct word pairs, respectively.", "labels": [], "entities": []}, {"text": "Hence the results are very useful for vocabulary extraction as well.", "labels": [], "entities": [{"text": "vocabulary extraction", "start_pos": 38, "end_pos": 59, "type": "TASK", "confidence": 0.8660130202770233}]}, {"text": "Similarly, and show significant improvements for the other two corpora.", "labels": [], "entities": []}, {"text": "At any score cut-off, the modified approach gives better results than the algorithms proposed in).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Details of English-Hindi Parallel Corpora", "labels": [], "entities": []}, {"text": " Table 2: Results of DK-vec Algorithm on Sentence Corpus for different range", "labels": [], "entities": [{"text": "Sentence Corpus", "start_pos": 41, "end_pos": 56, "type": "TASK", "confidence": 0.8866250813007355}]}, {"text": " Table 3: Comparison of experimental results with Segment Constraint on the three Engish-Hindi parallel  corpora", "labels": [], "entities": []}, {"text": " Table 4: Experimental root word parallel corpora of English -Hindi", "labels": [], "entities": []}, {"text": " Table 5: Results of Church and Gale Algorithm for Sentence level Alignment", "labels": [], "entities": [{"text": "Sentence level Alignment", "start_pos": 51, "end_pos": 75, "type": "TASK", "confidence": 0.9538173874219259}]}]}