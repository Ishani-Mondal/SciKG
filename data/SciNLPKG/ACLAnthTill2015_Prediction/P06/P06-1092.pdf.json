{"title": [{"text": "Phoneme-to-Text Transcription System with an Infinite Vocabulary", "labels": [], "entities": [{"text": "Phoneme-to-Text Transcription", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7664395272731781}]}], "abstractContent": [{"text": "The noisy channel model approach is successfully applied to various natural language processing tasks.", "labels": [], "entities": []}, {"text": "Currently the main research focus of this approach is adaptation methods, how to capture characteristics of words and expressions in a target domain given example sentences in that domain.", "labels": [], "entities": []}, {"text": "As a solution we describe a method enlarging the vocabulary of a language model to an almost infinite size and capturing their context information.", "labels": [], "entities": []}, {"text": "Especially the new method is suitable for languages in which words are not delimited by whitespace.", "labels": [], "entities": []}, {"text": "We applied our method to a phoneme-to-text transcription task in Japanese and reduced about 10% of the errors in the results of an existing method.", "labels": [], "entities": [{"text": "phoneme-to-text transcription task", "start_pos": 27, "end_pos": 61, "type": "TASK", "confidence": 0.7765036821365356}]}], "introductionContent": [{"text": "The noisy channel model approach is being successfully applied to various natural language processing (NLP) tasks, such as speech recognition, spelling correction (), machine translation (, etc.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 123, "end_pos": 141, "type": "TASK", "confidence": 0.7913536131381989}, {"text": "spelling correction", "start_pos": 143, "end_pos": 162, "type": "TASK", "confidence": 0.9112032353878021}, {"text": "machine translation", "start_pos": 167, "end_pos": 186, "type": "TASK", "confidence": 0.8355717957019806}]}, {"text": "In this approach an NLP system is composed of two modules: one is a taskdependent part (an acoustic model for speech recognition) which describes a relationship between an input signal sequence and a word, the other is a language model (LM) which measures the likelihood of a sequence of words as a sentence in the language.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 110, "end_pos": 128, "type": "TASK", "confidence": 0.752068042755127}]}, {"text": "Since the LM is a common part, its improvement augments the accuracies of all NLP systems based on a noisy channel model.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 60, "end_pos": 70, "type": "METRIC", "confidence": 0.9750297665596008}]}, {"text": "Recently the main research focus of LM is shifting to the adaptation method, how to capture the characteristics of words and expressions in a target domain.", "labels": [], "entities": []}, {"text": "The standard adaptation method is to prepare a corpus in the application domain, count the frequencies of words and word sequences, and manually annotate new words with their input signal sequences to be added to the vocabulary.", "labels": [], "entities": []}, {"text": "It is now easy to gather machine-readable sentences in various domains because of the ease of publication and access via the Web ().", "labels": [], "entities": []}, {"text": "In addition, traditional machinereadable forms of medical reports or business reports are also available.", "labels": [], "entities": []}, {"text": "When we need to develop an NLP system in various domains, there is a huge but unannotated corpus.", "labels": [], "entities": []}, {"text": "For languages, such as Japanese and Chinese, in which the words are not delimited by whitespace, one encounters a word identification problem before counting the frequencies of words and word sequences.", "labels": [], "entities": [{"text": "word identification", "start_pos": 114, "end_pos": 133, "type": "TASK", "confidence": 0.7277918010950089}]}, {"text": "To solve this problem one must have a good word segmenter in the domain of the corpus.", "labels": [], "entities": []}, {"text": "The only robust and reliable word segmenter in the domain is, however, a word segmenter based on the statistics of the lexicons in the domain!", "labels": [], "entities": [{"text": "word segmenter", "start_pos": 29, "end_pos": 43, "type": "TASK", "confidence": 0.7510626912117004}, {"text": "word segmenter", "start_pos": 73, "end_pos": 87, "type": "TASK", "confidence": 0.6951071172952652}]}, {"text": "Thus we are obliged to pay a high cost for the manual annotation of a corpus for each new subject domain.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel framework for building an NLP system based on a noisy channel model with an almost infinite vocabulary.", "labels": [], "entities": []}, {"text": "In our method, first we estimate the probability of a word boundary existing between two characters at each point of a raw corpus in the target domain.", "labels": [], "entities": []}, {"text": "Using these probabilities we regard the corpus as a stochastically segmented corpus (SSC).", "labels": [], "entities": []}, {"text": "We then estimate word \u00d2-gram probabilities from the SSC.", "labels": [], "entities": []}, {"text": "Then we build an NLP system, the phoneme-totext transcription system in this paper.", "labels": [], "entities": []}, {"text": "To describe the stochastic relationship between a character sequence and its phoneme sequence, we also propose a character-based unknown word model.", "labels": [], "entities": []}, {"text": "With this unknown word model and a word \u00d2-gram model estimated from the SSC, the vocabulary of our LM, a set of known words with their context information, is expanded from words in a small annotated corpus to an almost infinite size, including all substrings appearing in the large corpus in the target domain.", "labels": [], "entities": []}, {"text": "In experiments, we estimated LMs from a relatively small annotated corpus in the general domain and a large raw corpus in the target domain.", "labels": [], "entities": []}, {"text": "A phoneme-to-text transcription system based on our LM and unknown word model eliminated about 10% of the errors in the results of an existing method.", "labels": [], "entities": []}], "datasetContent": [{"text": "As an evaluation of our phoneme-to-text transcription system, we measured transcription accuracies of several systems on test corpora in two domains: one is a general domain in which we have a small annotated corpus with word boundary information and phoneme sequence for each word, and the other is a target domain in which only a large raw corpus is available.", "labels": [], "entities": []}, {"text": "As the transcription result, we took the word sequence of the highest probability.", "labels": [], "entities": []}, {"text": "In this section we show the results and evaluate our new framework.", "labels": [], "entities": []}, {"text": "The segmented corpus used in our experiments is composed of articles extracted from newspapers and example sentences in a dictionary of daily conversation.", "labels": [], "entities": []}, {"text": "Each sentence in the corpus is segmented into words and each word is annotated with a phoneme sequence.", "labels": [], "entities": []}, {"text": "The corpus was divided into ten parts.", "labels": [], "entities": []}, {"text": "The parameters of the model were estimated from nine of them (learning) and the model was tested on the remaining one (test).", "labels": [], "entities": []}, {"text": "Another corpus we used in the experiments is composed of daily business reports.", "labels": [], "entities": []}, {"text": "This corpus is not annotated with word boundary information nor phoneme sequence for each word.", "labels": [], "entities": []}, {"text": "For evaluation, we selected 1,000 sentences randomly and annotated them with the phoneme sequences to be used as a test set.", "labels": [], "entities": []}, {"text": "The rest was used for LM estimation (see).", "labels": [], "entities": [{"text": "LM estimation", "start_pos": 22, "end_pos": 35, "type": "TASK", "confidence": 0.8795748651027679}]}, {"text": "The criterion we used for transcription systems is precision and recall based on the number of characters in the longest common subsequence (LCS).", "labels": [], "entities": [{"text": "precision", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9993010759353638}, {"text": "recall", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.9994115829467773}]}, {"text": "Let AE \u00c7 \u00ca be the number of characters in the correct sentence, AE \u00cb \u00cb\u00cb be that in the output of a system, and AE \u00c4\u00c4\u00cb be that of the LCS of the correct sentence and the output of the system, so the recall is defined as AE \u00c4\u00c4\u00cb \ud97b\udf59AE \u00c7 \u00ca and the precision as AE \u00c4\u00c4\u00cb \ud97b\udf59AE \u00cb \u00cb\u00cb .", "labels": [], "entities": [{"text": "AE", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.9793710708618164}, {"text": "AE", "start_pos": 64, "end_pos": 66, "type": "METRIC", "confidence": 0.9894296526908875}, {"text": "AE \u00c4\u00c4\u00cb", "start_pos": 111, "end_pos": 117, "type": "METRIC", "confidence": 0.9715792536735535}, {"text": "recall", "start_pos": 198, "end_pos": 204, "type": "METRIC", "confidence": 0.9987918734550476}, {"text": "AE \u00c4\u00c4\u00cb \ud97b\udf59AE \u00c7 \u00ca", "start_pos": 219, "end_pos": 233, "type": "METRIC", "confidence": 0.8673308789730072}, {"text": "precision", "start_pos": 242, "end_pos": 251, "type": "METRIC", "confidence": 0.9993919134140015}, {"text": "AE \u00c4\u00c4\u00cb \ud97b\udf59AE \u00cb \u00cb\u00cb", "start_pos": 255, "end_pos": 270, "type": "METRIC", "confidence": 0.8716656764348348}]}], "tableCaptions": [{"text": " Table 1: Annotated corpus in general domain", "labels": [], "entities": []}, {"text": " Table 2: Raw corpus in the target domain", "labels": [], "entities": []}, {"text": " Table 3: Phoneme-to-text transcription accuracy.", "labels": [], "entities": [{"text": "Phoneme-to-text transcription", "start_pos": 10, "end_pos": 39, "type": "TASK", "confidence": 0.7987171113491058}, {"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9720901250839233}]}, {"text": " Table 4: Relationship between the raw corpus size  and the accuracies.", "labels": [], "entities": []}]}