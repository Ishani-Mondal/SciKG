{"title": [{"text": "Machine-Learning-Based Transformation of Passive Japanese Sentences into Active by Separating Training Data into Each Input Particle", "labels": [], "entities": [{"text": "Machine-Learning-Based Transformation of Passive Japanese Sentences", "start_pos": 0, "end_pos": 67, "type": "TASK", "confidence": 0.7611832221349081}]}], "abstractContent": [{"text": "We developed anew method of transforming Japanese case particles when transforming Japanese passive sentences into active sentences.", "labels": [], "entities": []}, {"text": "It separates training data into each input particle and uses machine learning for each particle.", "labels": [], "entities": []}, {"text": "We also used numerous rich features for learning.", "labels": [], "entities": []}, {"text": "Our method obtained a high rate of accuracy (94.30%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9993636012077332}]}, {"text": "In contrast, a method that did not separate training data for any input particles obtained a lower rate of accuracy (92.00%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.998174786567688}]}, {"text": "In addition, a method that did not have many rich features for learning used in a previous study (Mu-rata and Isahara, 2003) obtained a much lower accuracy rate (89.77%).", "labels": [], "entities": [{"text": "accuracy rate", "start_pos": 147, "end_pos": 160, "type": "METRIC", "confidence": 0.9859601557254791}]}, {"text": "We confirmed that these improvements were significant through a statistical test.", "labels": [], "entities": []}, {"text": "We also conducted experiments utilizing traditional methods using verb dictionaries and manually prepared heuristic rules and confirmed that our method obtained much higher accuracy rates than traditional methods.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 173, "end_pos": 181, "type": "METRIC", "confidence": 0.9981476068496704}]}], "introductionContent": [{"text": "This paper describes how passive Japanese sentences can be automatically transformed into active.", "labels": [], "entities": []}, {"text": "There is an example of a passive Japanese sentence in.", "labels": [], "entities": []}, {"text": "The Japanese suffix reta functions as an auxiliary verb indicating the passive voice.", "labels": [], "entities": []}, {"text": "There is a corresponding active-voice sentence in.", "labels": [], "entities": []}, {"text": "When the sentence in Figure 1 is transformed into an active sentence, (i) ni (by), which is a case postpositional particle with the meaning of \"by\", is changed into ga, which is a case postpositional particle indicating the subjective case, and (ii) ga (subject), which is a case postpositional particle indicating the subjective case, is changed into wo (object), which is a case postpositional particle indicating the objective case.", "labels": [], "entities": []}, {"text": "In this paper, we discuss the transformation of Japanese case particles (i.e., ni \u2192 ga) through machine learning.", "labels": [], "entities": []}, {"text": "The transformation of passive sentences into active is useful in many research areas including generation, knowledge extraction from databases written in natural languages, information extraction, and answering questions.", "labels": [], "entities": [{"text": "knowledge extraction from databases written in natural languages", "start_pos": 107, "end_pos": 171, "type": "TASK", "confidence": 0.854673657566309}, {"text": "information extraction", "start_pos": 173, "end_pos": 195, "type": "TASK", "confidence": 0.878608375787735}, {"text": "answering questions", "start_pos": 201, "end_pos": 220, "type": "TASK", "confidence": 0.8952183723449707}]}, {"text": "For example, when the answer is in the passive voice and the question is in the active voice, a question-answering system cannot match the answer with the question because the sentence structures are different and it is thus difficult to find the answer to the question.", "labels": [], "entities": []}, {"text": "Methods of transforming passive sentences into active are important in natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 71, "end_pos": 98, "type": "TASK", "confidence": 0.6471079190572103}]}, {"text": "The transformation of case particles in transforming passive sentences into active is not easy because particles depend on verbs and their use.", "labels": [], "entities": []}, {"text": "We developed anew method of transforming Japanese case particles when transforming passive Japanese sentences into active in this study.", "labels": [], "entities": []}, {"text": "Our method separates training data into each input particle and uses machine learning for each input particle.", "labels": [], "entities": []}, {"text": "We also used numerous rich features for learning.", "labels": [], "entities": []}, {"text": "Our experiments confirmed that our method was effective.", "labels": [], "entities": []}, {"text": "inu ni watashi ga kama-reta.", "labels": [], "entities": []}, {"text": "(dog) (by) (I) subjective-case postpositional particle (bite) passive voice (I was bitten by a dog.)", "labels": [], "entities": []}], "datasetContent": [{"text": "We used the corpus we constructed described in Section 2 as supervised data.", "labels": [], "entities": []}, {"text": "We divided the supervised data into closed and open data (Both the closed data and open data had 1788 items each.).", "labels": [], "entities": []}, {"text": "The distribution of target case particles in the data are listed in.", "labels": [], "entities": []}, {"text": "We used the closed data to determine features that were deleted in feature selection and used the open data as test data (data for evaluation).", "labels": [], "entities": []}, {"text": "We used 10-fold cross validation for the experiments on closed data and we used closed data as the training data for the experiments on open data.", "labels": [], "entities": []}, {"text": "The target case particles were determined by using the machine-learning method explained in Section 3.", "labels": [], "entities": []}, {"text": "When multiple target particles could have been answers in the training data, we used pairs of them as answers for machine learning.", "labels": [], "entities": []}, {"text": "The experimental results are listed in.", "labels": [], "entities": []}, {"text": "Baseline 1 outputs a source case particle as the target case particle.", "labels": [], "entities": []}, {"text": "Baseline 2 outputs the most frequent target case particle (wo (direct object)) in the closed data as the target case particle in every case.", "labels": [], "entities": []}, {"text": "Baseline 3 outputs the most frequent target case particle for each source target case particle in the closed data as the target case particle.", "labels": [], "entities": []}, {"text": "For example, ni (indirect object) is the most frequent target case particle when the source case particle is ni, as listed in.", "labels": [], "entities": []}, {"text": "Baseline 3 outputs ni when the source case particle is ni.", "labels": [], "entities": []}, {"text": "KNP indicates the results that the Japanese syntactic parser, KNP, output.", "labels": [], "entities": []}, {"text": "Kondo indicates the results that Kondo's method, (), output.", "labels": [], "entities": []}, {"text": "KNP and Kondo can only work when a target predicate is defined in the IPAL dictionary or the VDIC dictionary.", "labels": [], "entities": [{"text": "VDIC dictionary", "start_pos": 93, "end_pos": 108, "type": "DATASET", "confidence": 0.9158255755901337}]}, {"text": "Otherwise, KNP and Kondo output nothing.", "labels": [], "entities": [{"text": "KNP", "start_pos": 11, "end_pos": 14, "type": "DATASET", "confidence": 0.7113903760910034}, {"text": "Kondo", "start_pos": 19, "end_pos": 24, "type": "DATASET", "confidence": 0.6665615439414978}]}, {"text": "\"KNP/Kondo + Baseline X\" indicates the use of outputs by Baseline X when KNP/Kondo have output nothing.", "labels": [], "entities": []}, {"text": "KNP and Kondo are traditional methods using verb dictionaries and manually prepared heuristic rules.", "labels": [], "entities": []}, {"text": "These traditional methods were used in this study to compare them with ours.", "labels": [], "entities": []}, {"text": "\"Murata 2003\" indicates results using a method they developed in a previous study.", "labels": [], "entities": [{"text": "Murata 2003\"", "start_pos": 1, "end_pos": 13, "type": "DATASET", "confidence": 0.8396982153256735}]}, {"text": "This method uses F1, F2, F5, F6, F7, F10, and F13 as features and does not have training data for any source case particles.", "labels": [], "entities": [{"text": "F1", "start_pos": 17, "end_pos": 19, "type": "METRIC", "confidence": 0.9882256984710693}, {"text": "F10", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.9804770946502686}, {"text": "F13", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.9885391592979431}]}, {"text": "\"Division\" indicates separating training data into each source particle.", "labels": [], "entities": []}, {"text": "\"Nodivision\" indicates not separating training data for any source particles.", "labels": [], "entities": []}, {"text": "\"All features\" indicates the use of all features with no features being selected.", "labels": [], "entities": []}, {"text": "\"Feature selection\" indicates features are selected.", "labels": [], "entities": []}, {"text": "We did two kinds of evaluations: \"Eval.", "labels": [], "entities": [{"text": "Eval", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.888075053691864}]}, {"text": "A\" and \"Eval.", "labels": [], "entities": [{"text": "A", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.9235754013061523}, {"text": "Eval", "start_pos": 8, "end_pos": 12, "type": "METRIC", "confidence": 0.9842358827590942}]}, {"text": "There are some cases where multiple target case particles can be answers.", "labels": [], "entities": []}, {"text": "For example, ga and de can be answers.", "labels": [], "entities": []}, {"text": "We judged the result to be correct in \"Eval.", "labels": [], "entities": [{"text": "Eval", "start_pos": 39, "end_pos": 43, "type": "DATASET", "confidence": 0.6886522769927979}]}, {"text": "A\" when ga and de could be answers and the system output the pair of ga and de as answers.", "labels": [], "entities": []}, {"text": "We judged the result to be correct in \"Eval.", "labels": [], "entities": [{"text": "Eval", "start_pos": 39, "end_pos": 43, "type": "DATASET", "confidence": 0.6886522769927979}]}, {"text": "B\" when ga and de could be answers and the system output ga, de, or the pair of ga and de as answers.", "labels": [], "entities": [{"text": "B", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.9179843664169312}]}, {"text": "lists the results using all data.", "labels": [], "entities": []}, {"text": "lists the results where a target predicate is defined in the IPAL and VDIC dictionaries.", "labels": [], "entities": [{"text": "VDIC dictionaries", "start_pos": 70, "end_pos": 87, "type": "DATASET", "confidence": 0.8238766193389893}]}, {"text": "There were 551 items in the closed data and 539 in the open.", "labels": [], "entities": []}, {"text": "We found the following from the results.", "labels": [], "entities": []}, {"text": "Although selection of features obtained higher rates of accuracy than use of all features in the closed data, it did not obtain higher rates of accuracy in the open data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9986035227775574}, {"text": "accuracy", "start_pos": 144, "end_pos": 152, "type": "METRIC", "confidence": 0.99774169921875}]}, {"text": "This indicates that feature selection was not effective and we should have used all features in this study.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.7519124746322632}]}, {"text": "Our method using all features in the open data and separating training data into each source particle obtained the highest rate of accuracy (94.30% in Eval. B).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.9996345043182373}, {"text": "Eval. B", "start_pos": 151, "end_pos": 158, "type": "DATASET", "confidence": 0.7962486445903778}]}, {"text": "This indicates that our method is ef-  fective.", "labels": [], "entities": []}, {"text": "Our method that used all the features and did not separate training data for any source particles obtained an accuracy rate of 92.00% in Eval.", "labels": [], "entities": [{"text": "accuracy rate", "start_pos": 110, "end_pos": 123, "type": "METRIC", "confidence": 0.988300085067749}, {"text": "Eval", "start_pos": 137, "end_pos": 141, "type": "DATASET", "confidence": 0.9079940319061279}]}, {"text": "B. The technique of separating training data into each source particles made an improvement of 2.30%.", "labels": [], "entities": []}, {"text": "We confirmed that this improvement has a significance level of 0.01 by using a two-sided binomial test (two-sided sign test).", "labels": [], "entities": [{"text": "significance level", "start_pos": 41, "end_pos": 59, "type": "METRIC", "confidence": 0.9688760638237}]}, {"text": "This indicates that the technique of separating training data for all source particles is effective.", "labels": [], "entities": []}, {"text": "Murata 2003 who used only seven features and did not separate training data for any source particles obtained an accuracy rate of 89.77% with Eval.", "labels": [], "entities": [{"text": "Murata 2003", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.9372757971286774}, {"text": "accuracy rate", "start_pos": 113, "end_pos": 126, "type": "METRIC", "confidence": 0.9887897968292236}, {"text": "Eval", "start_pos": 142, "end_pos": 146, "type": "DATASET", "confidence": 0.953052818775177}]}, {"text": "B. The method (92.00%) of using all features (32) made an improvement of 2.23% against theirs.", "labels": [], "entities": []}, {"text": "We confirmed that this improvement had a significance level of 0.01 by using a two-sided binomial test (two-sided sign test).", "labels": [], "entities": [{"text": "significance level", "start_pos": 41, "end_pos": 59, "type": "METRIC", "confidence": 0.9748320281505585}]}, {"text": "This indicates that our increased features are effective.", "labels": [], "entities": []}, {"text": "KNP and Kondo obtained low accuracy rates (29.14% and 41.00% in Eval. B for the open data).", "labels": [], "entities": [{"text": "KNP", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.901360809803009}, {"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9994959831237793}, {"text": "Eval. B", "start_pos": 64, "end_pos": 71, "type": "DATASET", "confidence": 0.6480331718921661}]}, {"text": "We did the evaluation using data and proved that these methods could work well.", "labels": [], "entities": []}, {"text": "A target predicate in the data is defined in the IPAL and VDIC dictionaries.", "labels": [], "entities": [{"text": "IPAL", "start_pos": 49, "end_pos": 53, "type": "DATASET", "confidence": 0.9008338451385498}, {"text": "VDIC dictionaries", "start_pos": 58, "end_pos": 75, "type": "DATASET", "confidence": 0.8290071785449982}]}, {"text": "The results are listed in.", "labels": [], "entities": []}, {"text": "KNP and Kondo obtained relatively higher accuracy rates (76.07% and 78.85% in Eval. B for the open data).", "labels": [], "entities": [{"text": "KNP", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.900248110294342}, {"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9994555115699768}, {"text": "Eval.", "start_pos": 78, "end_pos": 83, "type": "DATASET", "confidence": 0.7806658148765564}]}, {"text": "However, they were lower than that for Baseline 3.", "labels": [], "entities": []}, {"text": "source particles and uses the most frequent target case particle.", "labels": [], "entities": []}, {"text": "Our method involves separating the training data into source particles and using machine learning for each particle.", "labels": [], "entities": []}, {"text": "The fact that Baseline 3 obtained a relatively high accuracy rate supports the effectiveness of our method separating the training data into source particles.", "labels": [], "entities": [{"text": "accuracy rate", "start_pos": 52, "end_pos": 65, "type": "METRIC", "confidence": 0.9838881492614746}]}, {"text": "We next conducted experiments where we confirmed which features were effective.", "labels": [], "entities": []}, {"text": "The results are listed in.", "labels": [], "entities": []}, {"text": "We can seethe accuracy rate for deleting features and the accuracy rate for using all features.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9994117021560669}, {"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9995096921920776}]}, {"text": "We can see that not using F25 greatly decreased the accuracy rate (about 2%).", "labels": [], "entities": [{"text": "F25", "start_pos": 26, "end_pos": 29, "type": "DATASET", "confidence": 0.5068259239196777}, {"text": "accuracy rate", "start_pos": 52, "end_pos": 65, "type": "METRIC", "confidence": 0.9703919589519501}]}, {"text": "This indicates that F25 is particularly effective.", "labels": [], "entities": [{"text": "F25", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.9213555455207825}]}, {"text": "F25 is the transformation rule Kondo used for P and N in his method.", "labels": [], "entities": [{"text": "F25", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9115237593650818}]}, {"text": "The transformation rules in Kondo's method were made precisely for ni (indirect object), which is particularly difficult to handle.", "labels": [], "entities": []}, {"text": "We could also see not using F7 decreased the accuracy rate (about 0.5%).", "labels": [], "entities": [{"text": "F7", "start_pos": 28, "end_pos": 30, "type": "DATASET", "confidence": 0.5042768716812134}, {"text": "accuracy rate", "start_pos": 45, "end_pos": 58, "type": "METRIC", "confidence": 0.977975457906723}]}, {"text": "F7 has the semantic features for N.", "labels": [], "entities": [{"text": "F7", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.8054367899894714}]}, {"text": "We found that the semantic features for N were also effective.", "labels": [], "entities": []}, {"text": "We finally did experiments changing the number of training data.", "labels": [], "entities": []}, {"text": "The results are plotted in.", "labels": [], "entities": []}, {"text": "We used our two methods of all features \"Division\" and \"Non-division\".", "labels": [], "entities": []}, {"text": "We only plotted the We plotted accuracy rates when 1, 1/2, 1/4, 1/8, and 1/16 of the training data were used.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9905112385749817}]}, {"text": "\"Division\", which separates training data for all source particles, obtained a high accuracy rate (88.36%) even when the number of training data was small.", "labels": [], "entities": [{"text": "accuracy rate", "start_pos": 84, "end_pos": 97, "type": "METRIC", "confidence": 0.9827568233013153}]}, {"text": "In contrast, \"Non-division\", which does not separate training data for any source particles, obtained a low accuracy rate (75.57%), when the number of training data was small.", "labels": [], "entities": [{"text": "accuracy rate", "start_pos": 108, "end_pos": 121, "type": "METRIC", "confidence": 0.9921615719795227}]}, {"text": "This indicates that our method of separating training data for all source particles is effective.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Occurrence rates for target case particles", "labels": [], "entities": [{"text": "Occurrence", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9730398654937744}]}, {"text": " Table 5: Experimental results on data that can use IPAL and VDIC dictionaries", "labels": [], "entities": []}]}