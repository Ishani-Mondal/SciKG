{"title": [{"text": "Improving English Subcategorization Acquisition with Diathesis Al- ternations as Heuristic Information", "labels": [], "entities": [{"text": "Improving English Subcategorization Acquisition", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.8565334975719452}]}], "abstractContent": [{"text": "Automatically acquired lexicons with subcategorization information have already proved accurate and useful enough for some purposes but their accuracy still shows room for improvement.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 142, "end_pos": 150, "type": "METRIC", "confidence": 0.9987987279891968}]}, {"text": "By means of diathesis alternation, this paper proposes anew filtering method, which improved the performance of Korhonen's acquisition system remarkably, with the precision increased to 91.18% and recall unchanged, making the acquired lexicon much more practical for further manual proofreading and other NLP uses.", "labels": [], "entities": [{"text": "precision", "start_pos": 163, "end_pos": 172, "type": "METRIC", "confidence": 0.9994388222694397}, {"text": "recall", "start_pos": 197, "end_pos": 203, "type": "METRIC", "confidence": 0.9994083642959595}]}], "introductionContent": [{"text": "Subcategorization is the process that further classifies a syntactic category into its subsets.", "labels": [], "entities": []}, {"text": "defines the function of strict subcategorization features as appointing a set of constraints that dominate the selection of verbs and other arguments in deep structure.", "labels": [], "entities": []}, {"text": "Large subcategorized verbal lexicons have proved to be crucially important for many tasks of natural language processing, such as probabilistic parsers) and verb classifications (Schulte im.", "labels": [], "entities": [{"text": "verb classifications", "start_pos": 157, "end_pos": 177, "type": "TASK", "confidence": 0.7349010109901428}]}, {"text": "Since Brent (1993) a considerable amount of research focusing on large-scaled automatic acquisition of subcategorization frames (SCF) has met with some success not only in English but also in many other languages, including German (Schulte im), Spanish, Czech), Portuguese), and Chinese ().", "labels": [], "entities": [{"text": "automatic acquisition of subcategorization frames (SCF)", "start_pos": 78, "end_pos": 133, "type": "TASK", "confidence": 0.775903657078743}]}, {"text": "The general objective of this research is to acquire from a given corpus the SCF types and numbers for predicate verbs.", "labels": [], "entities": []}, {"text": "Two typical steps during the process of automatic acquisition are hypothesis generation and selection.", "labels": [], "entities": [{"text": "automatic acquisition", "start_pos": 40, "end_pos": 61, "type": "TASK", "confidence": 0.639042004942894}, {"text": "hypothesis generation", "start_pos": 66, "end_pos": 87, "type": "TASK", "confidence": 0.81819087266922}]}, {"text": "Usually based on heuristic rules, the first step generates SCF hypotheses for involved verbs; and the second selects reliable ones via statistical methods, such as BHT (binomial hypothesis testing), LLR (log likelihood ratio) and MLE (maximum likelihood estimation).", "labels": [], "entities": [{"text": "BHT", "start_pos": 164, "end_pos": 167, "type": "METRIC", "confidence": 0.9945390820503235}, {"text": "MLE", "start_pos": 230, "end_pos": 233, "type": "METRIC", "confidence": 0.9831075668334961}]}, {"text": "This second step is also called statistical filtering and has been widely regarded as problematic.", "labels": [], "entities": [{"text": "statistical filtering", "start_pos": 32, "end_pos": 53, "type": "TASK", "confidence": 0.896731436252594}]}, {"text": "English researchers have proposed some methods adjusting the corpus hypothesis frequencies before or while filtering.", "labels": [], "entities": []}, {"text": "These methods are often called backoff techniques for SCF acquisition.", "labels": [], "entities": [{"text": "SCF acquisition", "start_pos": 54, "end_pos": 69, "type": "TASK", "confidence": 0.9844323992729187}]}, {"text": "Some of them represent a remarkable improvement in the acquisition performance, for example diathesis alternation and semantic motivation).", "labels": [], "entities": [{"text": "diathesis alternation", "start_pos": 92, "end_pos": 113, "type": "TASK", "confidence": 0.7169119119644165}]}, {"text": "For the convenience of comparison between performances of different SCF acquisition methods, we define absolute and relative recall in this paper.", "labels": [], "entities": [{"text": "SCF acquisition", "start_pos": 68, "end_pos": 83, "type": "TASK", "confidence": 0.9651705026626587}, {"text": "recall", "start_pos": 125, "end_pos": 131, "type": "METRIC", "confidence": 0.9361693263053894}]}, {"text": "By absolute recall, we mean the figure computed against the background of input corpus, while relative recall is against the set of generated hypotheses.", "labels": [], "entities": [{"text": "recall", "start_pos": 12, "end_pos": 18, "type": "METRIC", "confidence": 0.9011682271957397}, {"text": "recall", "start_pos": 103, "end_pos": 109, "type": "METRIC", "confidence": 0.7068200707435608}]}, {"text": "At present, automatically acquired verb lexicons with SCF information have already proved accurate and useful enough for some NLP purposes).", "labels": [], "entities": []}, {"text": "As for English, reported that semantically motivated SCF acquisition achieved a precision of 87.1%, an absolute recall of 71.2% and a relative recall of 85.27%, thus making the acquired lexicon much more accurate and useful.", "labels": [], "entities": [{"text": "SCF acquisition", "start_pos": 53, "end_pos": 68, "type": "TASK", "confidence": 0.8991403877735138}, {"text": "precision", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.9989878535270691}, {"text": "recall", "start_pos": 112, "end_pos": 118, "type": "METRIC", "confidence": 0.8227866888046265}, {"text": "recall", "start_pos": 143, "end_pos": 149, "type": "METRIC", "confidence": 0.8600993156433105}]}, {"text": "However, the accuracy still shows room for improvement, especially for those SCF hypotheses with low frequencies.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9996954202651978}]}, {"text": "Detailed analysis on the acquisition system and some resulting data shows that three main causes should account for the comparatively unsatisfactory performance: a. the imperfect hypothesis generator, b. the Zipfian distribution of syntactic patterns, c. the incomplete partition over SCF types of a given verb.", "labels": [], "entities": []}, {"text": "The first problem mainly comes from the inadequate parsing performance and noises existing in the corpus, while the other two problems are inherent to natural languages and should be solved in terms of acquisition techniques particularly during the process of hypothesis selection.", "labels": [], "entities": []}], "datasetContent": [{"text": "We implemented an acquisition experiment on Korhonen's evaluation resources with the abovementioned filtering method.", "labels": [], "entities": []}, {"text": "The diathesis alternations in use are also those provided by Korhonen, except that we used them in a two-way manner (scf i \u00a1 \u00fb \u00a1 \u00fa scf j ) instead of one-way (scf i \u2192scf j ), because the two involved SCF types are usually alternative pragmatic formats of the concerned verb, as shown in examples in Section 3 and 4.", "labels": [], "entities": []}, {"text": "In the experiment we empirically set \u03b8 1 = 0.2, which is ten times of Korhonen's threshold for her MLE filter; \u03b8 2 = 0.002, which is one tenth of Korhonen's.", "labels": [], "entities": []}, {"text": "Thus, in a token set of hypotheses no more than 1000, an SCF type scf i will be accepted if it occurs two times or more and has a diathesis alternative type scf j already accepted for the verb.", "labels": [], "entities": []}, {"text": "The gold standard was the manually analysed results by Korhonen.", "labels": [], "entities": []}, {"text": "Precision, recall and Fmeasure were calculated via expressions given in Section 2.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9956398010253906}, {"text": "recall", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.9994969367980957}, {"text": "Fmeasure", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9995669722557068}]}, {"text": "lists the performances of the baseline method of non-filtering (No_f), MLE filtering with \u03b8 = 0.02, and our filtering method on the evaluation corpus, and also gives the best results of Korhonen's method that is using extra semantic information (Kor) to make a comparison.", "labels": [], "entities": [{"text": "MLE filtering", "start_pos": 71, "end_pos": 84, "type": "TASK", "confidence": 0.658016100525856}]}, {"text": "Here, Ab_R is the absolute recall ratio, Re_R the relative recall ratio, Ab_F the absolute Fmeasure that is calculated from Precision and Ab_R, and Re_F the relative F-measure that is from Precision and Re_R.", "labels": [], "entities": [{"text": "Ab_R", "start_pos": 6, "end_pos": 10, "type": "METRIC", "confidence": 0.9742943247159322}, {"text": "recall ratio", "start_pos": 27, "end_pos": 39, "type": "METRIC", "confidence": 0.9592388868331909}, {"text": "Re_R", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.928301731745402}, {"text": "recall ratio", "start_pos": 59, "end_pos": 71, "type": "METRIC", "confidence": 0.9040239751338959}, {"text": "Ab_F", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.9731777310371399}, {"text": "Fmeasure", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.7463716268539429}]}, {"text": "The evaluation shows that our new filtering method improved the acquisition performance remarkably: a.", "labels": [], "entities": []}, {"text": "Compared with MLE, precision increased by 23.29%, recall ratio remained unchanged, absolute F-measure increased by 3.96, and relative F-measure increased by 13.72; b.", "labels": [], "entities": [{"text": "MLE", "start_pos": 14, "end_pos": 17, "type": "METRIC", "confidence": 0.6206265687942505}, {"text": "precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9995819926261902}, {"text": "recall ratio", "start_pos": 50, "end_pos": 62, "type": "METRIC", "confidence": 0.9501045048236847}, {"text": "F-measure", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.7911235094070435}, {"text": "F-measure", "start_pos": 134, "end_pos": 143, "type": "METRIC", "confidence": 0.9050597548484802}]}, {"text": "Compared with Korhonen's best results, precision, Re_R and Re_F also increased respectively 3 . Thus, the general performance of our filtering method makes the acquired lexicon much more practical for further manual proofreading and other NLP uses.", "labels": [], "entities": [{"text": "precision", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.99981290102005}, {"text": "Re_R", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.9482001264890035}, {"text": "Re_F", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.9062896370887756}]}, {"text": "What's more, the data shown in implies that there is little room left for improvement of the statistical filter, since the absolute recall ratio is only 2.1% lower than that of the nonfiltering method.", "labels": [], "entities": [{"text": "recall ratio", "start_pos": 132, "end_pos": 144, "type": "METRIC", "confidence": 0.9657438695430756}]}, {"text": "Whereas, detailed analysis of the evaluation corpus shows that the hypothesis generator accounts for about 95% of those unrecalled and wrongly recalled SCF types, which indicates, for the present time, more improvement efforts need to be made on the first step of subcategorization acquisition, i.e. hypothesis generation.", "labels": [], "entities": [{"text": "subcategorization acquisition", "start_pos": 264, "end_pos": 293, "type": "TASK", "confidence": 0.7450021803379059}, {"text": "hypothesis generation", "start_pos": 300, "end_pos": 321, "type": "TASK", "confidence": 0.7372422218322754}]}], "tableCaptions": []}