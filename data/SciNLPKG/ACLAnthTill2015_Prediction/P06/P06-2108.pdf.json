{"title": [{"text": "Using Word Support Model to Improve Chinese Input System", "labels": [], "entities": [{"text": "Improve Chinese Input", "start_pos": 28, "end_pos": 49, "type": "TASK", "confidence": 0.7092093229293823}]}], "abstractContent": [{"text": "This paper presents a word support model (WSM).", "labels": [], "entities": [{"text": "word support model (WSM)", "start_pos": 22, "end_pos": 46, "type": "TASK", "confidence": 0.6211366752783457}]}, {"text": "The WSM can effectively perform homophone selection and syllable-word segmentation to improve Chinese input systems.", "labels": [], "entities": [{"text": "homophone selection", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.719115599989891}, {"text": "syllable-word segmentation", "start_pos": 56, "end_pos": 82, "type": "TASK", "confidence": 0.7438625991344452}]}, {"text": "The experimental results show that: (1) the WSM is able to achieve tonal (sylla-bles input with four tones) and tone-less (syllables input without four tones) syllable-to-word (STW) accuracies of 99% and 92%, respectively, among the converted words; and (2) while applying the WSM as an adaptation processing , together with the Microsoft Input Method Editor 2003 (MSIME) and an optimized bigram model, the average tonal and toneless STW improvements are 37% and 35%, respectively .", "labels": [], "entities": [{"text": "syllable-to-word (STW) accuracies", "start_pos": 159, "end_pos": 192, "type": "METRIC", "confidence": 0.5754609704017639}]}], "introductionContent": [{"text": "According to;, the approaches of Chinese input methods (i.e. Chinese input systems) can be classified into two types: (1) keyboard based approach: including phonetic and pinyin based (, arbitrary codes based and structure scheme based; and (2) non-keyboard based approach: including optical character recognition (OCR), online handwriting () and speech recognition ().", "labels": [], "entities": [{"text": "optical character recognition (OCR)", "start_pos": 283, "end_pos": 318, "type": "TASK", "confidence": 0.7818810145060221}, {"text": "speech recognition", "start_pos": 346, "end_pos": 364, "type": "TASK", "confidence": 0.7623834013938904}]}, {"text": "Currently, the most popular Chinese input system is phonetic and pinyin based approach, because Chinese people are taught to write phonetic and pinyin syllables of each Chinese character in primary school.", "labels": [], "entities": []}, {"text": "In Chinese, each Chinese word can be a mono-syllabic word, such as \"\u9f20(mouse)\", a bisyllabic word, such as \"\u888b\u9f20(kangaroo)\", or a multi-syllabic word, such as \"\u7c73\uf934\u9f20(Mickey mouse).\"", "labels": [], "entities": []}, {"text": "The corresponding phonetic and pinyin syllables of each Chinese word is called syllable-words, such as \"dai4 shu3\" is the pinyin syllable-word of \"\u888b\u9f20(kangaroo).\"", "labels": [], "entities": []}, {"text": "According to our computation, the {minimum, maximum, average} words per each distinct mono-syllableword and poly-syllable-word (including bisyllable-word and multi-syllable-word) in the CKIP dictionary (Chinese Knowledge Information Processing Group, 1995) are {1, 28, 2.8} and {1, 7, 1.1}, respectively.", "labels": [], "entities": [{"text": "CKIP dictionary (Chinese Knowledge Information Processing Group, 1995)", "start_pos": 186, "end_pos": 256, "type": "DATASET", "confidence": 0.9348191185431047}]}, {"text": "The CKIP dictionary is one of most commonly-used Chinese dictionaries in the research field of Chinese natural language processing (NLP).", "labels": [], "entities": [{"text": "CKIP dictionary", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.9604628086090088}, {"text": "Chinese natural language processing (NLP)", "start_pos": 95, "end_pos": 136, "type": "TASK", "confidence": 0.7521650493144989}]}, {"text": "Since the size of problem space for syllable-to-word (STW) conversion is much less than that of syllable-tocharacter (STC) conversion, the most pinyinbased Chinese input systems; Microsoft Research Center in Beijing;) are addressed on STW conversion.", "labels": [], "entities": [{"text": "syllable-to-word (STW) conversion", "start_pos": 36, "end_pos": 69, "type": "TASK", "confidence": 0.7015987396240234}, {"text": "syllable-tocharacter (STC) conversion", "start_pos": 96, "end_pos": 133, "type": "TASK", "confidence": 0.709435224533081}, {"text": "Microsoft Research Center in Beijing", "start_pos": 179, "end_pos": 215, "type": "DATASET", "confidence": 0.8677139520645142}, {"text": "STW conversion", "start_pos": 235, "end_pos": 249, "type": "TASK", "confidence": 0.6991133391857147}]}, {"text": "On the other hand, STW conversion is the main task of Chinese Language Processing in typical Chinese speech recognition systems (.", "labels": [], "entities": [{"text": "STW conversion", "start_pos": 19, "end_pos": 33, "type": "TASK", "confidence": 0.8224579691886902}, {"text": "Chinese Language Processing", "start_pos": 54, "end_pos": 81, "type": "TASK", "confidence": 0.6218254168828329}, {"text": "Chinese speech recognition", "start_pos": 93, "end_pos": 119, "type": "TASK", "confidence": 0.6656797031561533}]}, {"text": "As per, homophone selection and syllableword segmentation are two critical problems in developing a Chinese input system.", "labels": [], "entities": [{"text": "homophone selection", "start_pos": 8, "end_pos": 27, "type": "TASK", "confidence": 0.7971419990062714}, {"text": "syllableword segmentation", "start_pos": 32, "end_pos": 57, "type": "TASK", "confidence": 0.7453851699829102}]}, {"text": "Incorrect homophone selection and syllable-word seg-mentation will directly influence the STW conversion accuracy.", "labels": [], "entities": [{"text": "STW conversion", "start_pos": 90, "end_pos": 104, "type": "TASK", "confidence": 0.7460013329982758}, {"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9491190910339355}]}, {"text": "Conventionally, there are two approaches to resolve the two critical problems: (1) linguistic approach: based on syntax parsing, semantic template matching and contextual information); and (2) statistical approach: based on the n-gram models where n is usually 2, i.e. bigram model.", "labels": [], "entities": [{"text": "syntax parsing", "start_pos": 113, "end_pos": 127, "type": "TASK", "confidence": 0.7493196129798889}, {"text": "semantic template matching", "start_pos": 129, "end_pos": 155, "type": "TASK", "confidence": 0.6635379989941915}]}, {"text": "From the studies, the linguistic approach requires considerable effort in designing effective syntax rules, semantic templates or contextual information, thus, it is more user-friendly than the statistical approach on understanding why such a system makes a mistake.", "labels": [], "entities": []}, {"text": "The statistical language model (SLM) used in the statistical approach requires less effort and has been widely adopted in commercial Chinese input systems.", "labels": [], "entities": []}, {"text": "In our previous work), a wordpair (WP) identifier was proposed and shown a simple and effective way to improve Chinese input systems by providing tonal and toneless STW accuracies of 98.5% and 90.7% on the identified poly-syllabic words, respectively.", "labels": [], "entities": [{"text": "STW accuracies", "start_pos": 165, "end_pos": 179, "type": "METRIC", "confidence": 0.862175852060318}]}, {"text": "In), we have shown that the WP identifier can be used to reduce the over weighting and corpus sparseness problems of bigram models and achieve better STW accuracy to improve Chinese input systems.", "labels": [], "entities": [{"text": "STW", "start_pos": 150, "end_pos": 153, "type": "METRIC", "confidence": 0.6431362628936768}, {"text": "accuracy", "start_pos": 154, "end_pos": 162, "type": "METRIC", "confidence": 0.6433133482933044}]}, {"text": "As per our computation, poly-syllabic words cover about 70% characters of Chinese sentences.", "labels": [], "entities": []}, {"text": "Since the identified character ratio of the WP identifier) is about 55%, there are still about 15% improving room left.", "labels": [], "entities": []}, {"text": "The objective of this study is to illustrate a word support model (WSM) that is able to improve our WP-identifier by achieving better identified character ratio and STW accuracy on the identified poly-syllabic words with the same word-pair database.", "labels": [], "entities": [{"text": "STW", "start_pos": 165, "end_pos": 168, "type": "METRIC", "confidence": 0.990013599395752}, {"text": "accuracy", "start_pos": 169, "end_pos": 177, "type": "METRIC", "confidence": 0.7246131896972656}]}, {"text": "We conduct STW experiments to show the tonal and toneless STW accuracies of a commercial input product (Microsoft Input Method, and an optimized bigram model, BiGram), can both be improved by our WSM and achieve better STW improvements than that of these systems with the WP identifier.", "labels": [], "entities": []}, {"text": "The remainder of this paper is arranged as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we present an auto wordpair (AUTO-WP) generation used to generate the WP database.", "labels": [], "entities": []}, {"text": "Then, we develop a word support model with the WP database to perform STW conversion on identifying words from the Chinese syllables.", "labels": [], "entities": [{"text": "WP database", "start_pos": 47, "end_pos": 58, "type": "DATASET", "confidence": 0.9575207233428955}, {"text": "STW conversion", "start_pos": 70, "end_pos": 84, "type": "TASK", "confidence": 0.6806676536798477}]}, {"text": "In Section 3, we report and analyze our STW experimental results.", "labels": [], "entities": [{"text": "STW experimental", "start_pos": 40, "end_pos": 56, "type": "DATASET", "confidence": 0.6254827678203583}]}, {"text": "Finally, in Section 4, we give our conclusions and suggest some future research directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate the STW performance of our WSM, we define the STW accuracy, identified character ratio (ICR) and STW improvement, by the following equations: STW accuracy = # of correct characters / # of total characters.", "labels": [], "entities": [{"text": "WSM", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.8470887541770935}, {"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.8933882117271423}, {"text": "identified character ratio (ICR)", "start_pos": 72, "end_pos": 104, "type": "METRIC", "confidence": 0.881408711274465}, {"text": "STW improvement", "start_pos": 109, "end_pos": 124, "type": "METRIC", "confidence": 0.9189932644367218}, {"text": "accuracy", "start_pos": 158, "end_pos": 166, "type": "METRIC", "confidence": 0.702505886554718}]}, {"text": "Identified character ratio (ICR) = # of characters of identified WP / # of total characters in testing sentences.", "labels": [], "entities": [{"text": "Identified character ratio (ICR)", "start_pos": 0, "end_pos": 32, "type": "METRIC", "confidence": 0.9485615889231364}]}, {"text": "STW improvement (I) (i.e. STW error reduction rate) = (accuracy of STW system with WPaccuracy of STW system)) / (1 -accuracy of STW system).", "labels": [], "entities": [{"text": "STW improvement (I)", "start_pos": 0, "end_pos": 19, "type": "METRIC", "confidence": 0.7578409790992737}, {"text": "STW error reduction rate", "start_pos": 26, "end_pos": 50, "type": "METRIC", "confidence": 0.710507869720459}, {"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9992604851722717}, {"text": "accuracy", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9820948243141174}]}, {"text": "Step # Results Step.1 sui1 ran2 fu3 shi2 jin4 shi4 sui4 yue4 xi1 xu1 Step.2 WP set (word-pair / word-pair frequency) = {\u96d6\u7136-\u8fd1\u8996/6 (key WP for WP identifier), \u4fef\uf973-\u76e1\u662f/4, \u96d6\u7136-\u6b72\u6708/4, \u96d6\u7136-\u76e1\u662f/3, \u4fef\uf973-\u550f\u5653/2, \u96d6\u7136-\u4fef\uf973/2, \u4fef\uf973-\u6b72\u6708/2, \u76e1\u662f-\u550f\u5653/2, \u76e1\u662f-\u6b72\u6708/2, \u96d6\u7136-\u550f\u5653/2, \u6b72\u6708-\u550f\u5653/2} Step.3 WSM set (word / WS degree) =  The purpose of this experiment is to demonstrate the tonal and toneless STW accuracies among the identified words by using the WSM with the system WP database.", "labels": [], "entities": [{"text": "word / WS degree)", "start_pos": 262, "end_pos": 279, "type": "METRIC", "confidence": 0.6859247386455536}, {"text": "WSM", "start_pos": 410, "end_pos": 413, "type": "DATASET", "confidence": 0.8715615272521973}]}, {"text": "The comparative system is the WP identifier. is the experimental results.", "labels": [], "entities": []}, {"text": "The WP database and system dictionary of the WP identifier is same with that of the WSM.", "labels": [], "entities": [{"text": "WP database", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.8414872884750366}, {"text": "WSM", "start_pos": 84, "end_pos": 87, "type": "DATASET", "confidence": 0.8867643475532532}]}], "tableCaptions": [{"text": " Table 4. The analysis results of the STW errors  from the Top 300 tonal and toneless STW con- versions of the BiGram with the WP identifier  and the WSM.", "labels": [], "entities": [{"text": "BiGram", "start_pos": 111, "end_pos": 117, "type": "DATASET", "confidence": 0.9127866625785828}, {"text": "WSM", "start_pos": 150, "end_pos": 153, "type": "DATASET", "confidence": 0.9189461469650269}]}]}