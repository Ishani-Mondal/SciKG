{"title": [{"text": "Robust PCFG-Based Generation using Automatically Acquired LFG Approximations", "labels": [], "entities": [{"text": "Robust PCFG-Based Generation", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6887881755828857}, {"text": "Automatically Acquired LFG Approximations", "start_pos": 35, "end_pos": 76, "type": "METRIC", "confidence": 0.7764907330274582}]}], "abstractContent": [{"text": "We present a novel PCFG-based architecture for robust probabilistic generation based on wide-coverage LFG approximations (Cahill et al., 2004) automatically extracted from treebanks, maximising the probability of a tree given an f-structure.", "labels": [], "entities": []}, {"text": "We evaluate our approach using string-based evaluation.", "labels": [], "entities": []}, {"text": "We currently achieve coverage of 95.26%, a BLEU score of 0.7227 and string accuracy of 0.7476 on the Penn-II WSJ Section 23 sentences of length \u226420.", "labels": [], "entities": [{"text": "coverage", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9984787106513977}, {"text": "BLEU score", "start_pos": 43, "end_pos": 53, "type": "METRIC", "confidence": 0.9832136631011963}, {"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.8618135452270508}, {"text": "Penn-II WSJ Section 23 sentences", "start_pos": 101, "end_pos": 133, "type": "DATASET", "confidence": 0.959634804725647}]}], "introductionContent": [{"text": "Wide coverage grammars automatically extracted from treebanks area corner-stone technology in state-of-the-art probabilistic parsing.", "labels": [], "entities": []}, {"text": "They achieve robustness and coverage at a fraction of the development cost of hand-crafted grammars.", "labels": [], "entities": [{"text": "coverage", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9695646166801453}]}, {"text": "It is surprising to note that to date, such grammars do not usually figure in the complementary operation to parsing -natural language surface realisation.", "labels": [], "entities": [{"text": "parsing -natural language surface realisation", "start_pos": 109, "end_pos": 154, "type": "TASK", "confidence": 0.8633895019690195}]}, {"text": "Research on statistical natural language surface realisation has taken three broad forms, differing in where statistical information is applied in the generation process., for example, uses n-gram word statistics to rank alternative output strings from symbolic hand-crafted generators to select paths in parse forest representations.", "labels": [], "entities": [{"text": "statistical natural language surface realisation", "start_pos": 12, "end_pos": 60, "type": "TASK", "confidence": 0.6398903250694274}]}, {"text": "use n-gram word sequence statistics in a TAG-based generation model to rank output strings and additional statistical and symbolic resources at intermediate generation stages.", "labels": [], "entities": []}, {"text": "uses maximum entropy models to drive generation with word bigram or dependency representations taking into account (unrealised) semantic features.", "labels": [], "entities": []}, {"text": "present a discriminative disambiguation model using a hand-crafted HPSG grammar for generation.", "labels": [], "entities": []}, {"text": "describes a method for building statistical generation models using an automatically created generation treebank for weather forecasts.", "labels": [], "entities": []}, {"text": "None of these probabilistic approaches to NLG uses a full treebank grammar to drive generation.", "labels": [], "entities": []}, {"text": "investigate the effect of training size on performance while using grammars automatically extracted from the Penn-II for generation.", "labels": [], "entities": [{"text": "Penn-II", "start_pos": 109, "end_pos": 116, "type": "DATASET", "confidence": 0.9836946725845337}]}, {"text": "Using an automatically extracted XTAG grammar, they achieve a string accuracy of 0.749 on their test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.8990665078163147}]}, {"text": "present probabilistic models fora chart generator using a HPSG grammar acquired from the Penn-II Treebank (the Enju HPSG).", "labels": [], "entities": [{"text": "Penn-II Treebank (the Enju HPSG)", "start_pos": 89, "end_pos": 121, "type": "DATASET", "confidence": 0.8249358279364449}]}, {"text": "They investigate discriminative disambiguation models following and their best model achieves coverage of 90.56% and a BLEU score of 0.7723 on Penn-II WSJ Section 23 sentences of length \u226420.", "labels": [], "entities": [{"text": "coverage", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9993675351142883}, {"text": "BLEU score", "start_pos": 119, "end_pos": 129, "type": "METRIC", "confidence": 0.9816223084926605}, {"text": "Penn-II WSJ Section 23 sentences", "start_pos": 143, "end_pos": 175, "type": "DATASET", "confidence": 0.942793893814087}]}, {"text": "In this paper we present a novel PCFG-based architecture for probabilistic generation based on wide-coverage, robust Lexical Functional Grammar (LFG) approximations automatically extracted from treebanks ( ).", "labels": [], "entities": []}, {"text": "In Section 2 we briefly describe LFG.", "labels": [], "entities": [{"text": "LFG", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.5581973791122437}]}, {"text": "Section 3 presents our generation architecture.", "labels": [], "entities": []}, {"text": "Section 4 presents evaluation results on the Penn-II WSJ Section 23 test set using string-based metrics.", "labels": [], "entities": [{"text": "Penn-II WSJ Section 23 test set", "start_pos": 45, "end_pos": 76, "type": "DATASET", "confidence": 0.9551594356695811}]}, {"text": "Section 5 compares our approach with alternative approaches in the literature.", "labels": [], "entities": []}, {"text": "Section 6 concludes and outlines further research.", "labels": [], "entities": []}], "datasetContent": [{"text": "We train our system on WSJ Sections 02-21 of the Penn-II Treebank and evaluate against the raw gives the number of training and test sentences for each sentence length.", "labels": [], "entities": [{"text": "WSJ Sections 02-21 of the Penn-II Treebank", "start_pos": 23, "end_pos": 65, "type": "DATASET", "confidence": 0.8996980360576085}]}, {"text": "In each case, we use the automatically generated f-structures from  from the original Section 23 treebank trees as f-structure input to our generation experiments.", "labels": [], "entities": [{"text": "Section 23 treebank trees", "start_pos": 86, "end_pos": 111, "type": "DATASET", "confidence": 0.9297828376293182}]}, {"text": "We automatically mark adjunct and coordination scope in the input f-structure.", "labels": [], "entities": []}, {"text": "Notice that these automatically generated f-structures are not \"perfect\", i.e. they are not guaranteed to be complete and coherent: a local f-structure may contain material that is not supposed to be there (incoherence) and/or maybe missing material that is supposed to be there (incompleteness).", "labels": [], "entities": []}, {"text": "The results presented below show that our method is robust with respect to the quality of the f-structure input and will always attempt to generate partial output rather than fail.", "labels": [], "entities": []}, {"text": "We consider this an important property as pristine generation input cannot always be guaranteed in realistic application scenarios, such as probabilistic transfer-based machine translation where generation input may contain a certain amount of noise.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 169, "end_pos": 188, "type": "TASK", "confidence": 0.7332745790481567}]}, {"text": "We evaluate the output of our generation system against the raw strings of Section 23 using the Simple String Accuracy and BLEU () evaluation metrics.", "labels": [], "entities": [{"text": "Simple String Accuracy", "start_pos": 96, "end_pos": 118, "type": "METRIC", "confidence": 0.43507686257362366}, {"text": "BLEU", "start_pos": 123, "end_pos": 127, "type": "METRIC", "confidence": 0.9909842014312744}]}, {"text": "Simple String Accuracy is based on the string edit distance between the output of the generation system and the gold standard sentence.", "labels": [], "entities": []}, {"text": "BLEU is the weighted average of n-gram precision against the gold standard sentences.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9889100193977356}, {"text": "precision", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9653394818305969}]}, {"text": "We also measure coverage as the percentage of input f-structures that generate a string.", "labels": [], "entities": [{"text": "coverage", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.983828067779541}]}, {"text": "For evaluation, we automatically expand all contracted words.", "labels": [], "entities": []}, {"text": "We only evaluate strings produced by the system (similar to).", "labels": [], "entities": []}, {"text": "We conduct a total of four experiments.", "labels": [], "entities": []}, {"text": "The parameters we investigate are lexical smoothing (Section 3.3) and partial output.", "labels": [], "entities": []}, {"text": "Partial output is a robustness feature for cases where a sub-fstructure component fails to generate a string and the system outputs a concatenation of the strings generated by the remaining components, rather than fail completely.: Generation +partial output -lexical smoothing Varying the length of the sentences included in the training data shows that results improve (both in terms of coverage and string quality) as the length of sentence included in the training data increases.", "labels": [], "entities": []}, {"text": "give the results for the experiments including lexical smoothing and varying partial output.", "labels": [], "entities": [{"text": "lexical smoothing", "start_pos": 47, "end_pos": 64, "type": "TASK", "confidence": 0.6752148270606995}]}, {"text": "(+partial, +smoothing) shows that training on sentences of all lengths and evaluating all strings (including partial outputs), our system achieves coverage of 98.05%, a BLEU score of 0.6651 and string accuracy of 0.6808.", "labels": [], "entities": [{"text": "coverage", "start_pos": 147, "end_pos": 155, "type": "METRIC", "confidence": 0.9965212345123291}, {"text": "BLEU score", "start_pos": 169, "end_pos": 179, "type": "METRIC", "confidence": 0.9811525046825409}, {"text": "accuracy", "start_pos": 201, "end_pos": 209, "type": "METRIC", "confidence": 0.976093053817749}]}, {"text": "Table 5 (-partial, +smoothing) shows that coverage drops to 89.49%, BLEU score increases to 0.6979 and string accuracy to 0.7012, when the system is trained on sentences of all lengths.", "labels": [], "entities": [{"text": "coverage", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9987170696258545}, {"text": "BLEU score", "start_pos": 68, "end_pos": 78, "type": "METRIC", "confidence": 0.9704072177410126}, {"text": "accuracy", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.9595847129821777}]}, {"text": "Similarly, for strings \u226420, coverage drops from 98.65% to 95.26%, BLEU increases from 0.7077 to 0.7227 and String Accuracy from 0.7373 to 0.7476.", "labels": [], "entities": [{"text": "coverage", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.998365581035614}, {"text": "BLEU", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.9992220401763916}, {"text": "Accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.788303554058075}]}, {"text": "Including partial output increases coverage (by more than 8.5 percentage points for all sentences) and hence robustness while slightly decreasing quality.", "labels": [], "entities": [{"text": "coverage", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9975173473358154}]}, {"text": "Tables 3 (+partial, +smoothing) and 4 (+partial, -smoothing) give results for the experiments including partial output but varying lexical smoothing.", "labels": [], "entities": []}, {"text": "With no lexical smoothing, the system (trained on all sentence lengths) produces strings for 90.11% of the input f-structures and achieves a BLEU score of 0.5590 and string accuracy of 0.6207.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 141, "end_pos": 151, "type": "METRIC", "confidence": 0.9829415678977966}, {"text": "accuracy", "start_pos": 173, "end_pos": 181, "type": "METRIC", "confidence": 0.9589943289756775}]}, {"text": "Switching off lexical smoothing has a negative effect on all evaluation metrics (coverage and quality), because many more strings produced are now partial (since for PRED values unseen during training, no lexical entries are added to the chart).", "labels": [], "entities": [{"text": "coverage", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9528132677078247}]}, {"text": "Comparing Tables 5 (-partial, +smoothing) and 6 (-partial, -smoothing), where the system does not produce any partial outputs and lexical smoothing is varied, shows that training on all sentence lengths, BLEU score increases from 0.6979 to 0.7147 and string accuracy increases from 0.7012 to 0.7192.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 204, "end_pos": 214, "type": "METRIC", "confidence": 0.9776598811149597}, {"text": "accuracy", "start_pos": 258, "end_pos": 266, "type": "METRIC", "confidence": 0.9726887941360474}]}, {"text": "At the same time, coverage drops dramatically from 89.49% to 47.60%.", "labels": [], "entities": [{"text": "coverage", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9967983365058899}]}, {"text": "Comparing shows that while partial output almost doubles coverage, this comes at a price of a severe drop in quality (BLEU score drops from 0.7147 to 0.5590).", "labels": [], "entities": [{"text": "coverage", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.993591845035553}, {"text": "BLEU score", "start_pos": 118, "end_pos": 128, "type": "METRIC", "confidence": 0.9823242127895355}]}, {"text": "On the other hand, comparing shows that lexical smoothing achieves a similar increase in coverage with only a very slight drop in quality.", "labels": [], "entities": [{"text": "coverage", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9894549250602722}]}, {"text": "Using hand-crafted grammar-based generation systems, it is possible to achieve very high results.", "labels": [], "entities": []}, {"text": "However, hand-crafted systems are expensive to construct and not easily ported to new domains or other languages.", "labels": [], "entities": []}, {"text": "Our methodology, on the other hand, is based on resources automatically acquired from treebanks and easily ported to new domains and languages, simply by retraining on suitable data.", "labels": [], "entities": []}, {"text": "Recent work on the automatic acquisition of multilingual LFG resources from treebanks for Chinese, German and Spanish ( ) has shown that given a suitable treebank, it is possible to automatically acquire high quality LFG resources in a very short space of time.", "labels": [], "entities": []}, {"text": "The generation architecture presented here is easily ported to those different languages and treebanks.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Number of training and test sentences per  sentence length", "labels": [], "entities": []}, {"text": " Table 3: Generation +partial output +lexical smoothing", "labels": [], "entities": []}, {"text": " Table 4: Generation +partial output -lexical smoothing", "labels": [], "entities": []}, {"text": " Table 5: Generation -partial output +lexical smoothing", "labels": [], "entities": []}, {"text": " Table 6: Generation -partial output -lexical smoothing", "labels": [], "entities": []}]}