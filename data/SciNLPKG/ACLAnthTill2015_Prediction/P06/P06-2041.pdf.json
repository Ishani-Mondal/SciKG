{"title": [{"text": "Discriminative Classifiers for Deterministic Dependency Parsing", "labels": [], "entities": [{"text": "Parsing", "start_pos": 56, "end_pos": 63, "type": "TASK", "confidence": 0.8001095056533813}]}], "abstractContent": [{"text": "Deterministic parsing guided by treebank-induced classifiers has emerged as a simple and efficient alternative to more complex models for data-driven parsing.", "labels": [], "entities": [{"text": "Deterministic parsing", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7662502825260162}]}, {"text": "We present a systematic comparison of memory-based learning (MBL) and support vector machines (SVM) for inducing classifiers for deterministic dependency parsing, using data from Chinese, English and Swedish, together with a variety of different feature models.", "labels": [], "entities": [{"text": "deterministic dependency parsing", "start_pos": 129, "end_pos": 161, "type": "TASK", "confidence": 0.6287541091442108}]}, {"text": "The comparison shows that SVM gives higher accuracy for richly articulated feature models across all languages, albeit with considerably longer training times.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9989644289016724}]}, {"text": "The results also confirm that classifier-based deterministic parsing can achieve parsing accuracy very close to the best results reported for more complex parsing models.", "labels": [], "entities": [{"text": "classifier-based deterministic parsing", "start_pos": 30, "end_pos": 68, "type": "TASK", "confidence": 0.5934560100237528}, {"text": "parsing", "start_pos": 81, "end_pos": 88, "type": "TASK", "confidence": 0.9689597487449646}, {"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9295763969421387}]}], "introductionContent": [{"text": "Mainstream approaches in statistical parsing are based on nondeterministic parsing techniques, usually employing some kind of dynamic programming, in combination with generative probabilistic models that provide an n-best ranking of the set of candidate analyses derived by the parser).", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.7457705140113831}]}, {"text": "These parsers can be enhanced by using a discriminative model, which reranks the analyses output by the parser).", "labels": [], "entities": []}, {"text": "Alternatively, discriminative models can be used to search the complete space of possible parses.", "labels": [], "entities": []}, {"text": "A radically different approach is to perform disambiguation deterministically, using a greedy parsing algorithm that approximates a globally optimal solution by making a sequence of locally optimal choices, guided by a classifier trained on gold standard derivations from a treebank.", "labels": [], "entities": [{"text": "disambiguation deterministically", "start_pos": 45, "end_pos": 77, "type": "TASK", "confidence": 0.9541036486625671}]}, {"text": "This methodology has emerged as an alternative to more complex models, especially in dependencybased parsing.", "labels": [], "entities": [{"text": "dependencybased parsing", "start_pos": 85, "end_pos": 108, "type": "TASK", "confidence": 0.68284672498703}]}, {"text": "It was first used for unlabeled dependency parsing by (for Japanese) and (for English).", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.7435726821422577}]}, {"text": "It was extended to labeled dependency parsing by  (for Swedish) and (for English).", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.7501112222671509}]}, {"text": "More recently, it has been applied with good results to lexicalized phrase structure parsing by.", "labels": [], "entities": [{"text": "lexicalized phrase structure parsing", "start_pos": 56, "end_pos": 92, "type": "TASK", "confidence": 0.6641843616962433}]}, {"text": "The machine learning methods used to induce classifiers for deterministic parsing are dominated by two approaches.", "labels": [], "entities": []}, {"text": "Support vector machines (SVM), which combine the maximum margin strategy introduced by with the use of kernel functions to map the original feature space to a higher-dimensional space, have been used by,, and, among others.", "labels": [], "entities": []}, {"text": "Memory-based learning (MBL), which is based on the idea that learning is the simple storage of experiences in memory and that solving anew problem is achieved by reusing solutions from similar previously solved problems (Daelemans and Van den Bosch, 2005), has been used primarily by ,, and Sagae and.", "labels": [], "entities": [{"text": "Memory-based learning (MBL)", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6627308487892151}]}, {"text": "Comparative studies of learning algorithms are relatively rare.", "labels": [], "entities": []}, {"text": "report that SVM outperforms MaxEnt models in Chinese dependency parsing, using the algorithms of and, while Sagae and find that SVM gives better performance than MBL in a constituency-based shift-reduce parser for English.", "labels": [], "entities": [{"text": "Chinese dependency parsing", "start_pos": 45, "end_pos": 71, "type": "TASK", "confidence": 0.5716839830080668}]}, {"text": "In this paper, we present a detailed comparison of SVM and MBL for dependency parsing using the deterministic algorithm of.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 67, "end_pos": 85, "type": "TASK", "confidence": 0.8433926105499268}]}, {"text": "The comparison is based on data from three different languages -Chinese, English, and Swedish -and on five different feature models of varying complexity, with a separate optimization of learning algorithm parameters for each combination of language and feature model.", "labels": [], "entities": []}, {"text": "The central importance of feature selection and parameter optimization in machine learning research has been shown very clearly in recent research (.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.7357524633407593}, {"text": "parameter optimization", "start_pos": 48, "end_pos": 70, "type": "TASK", "confidence": 0.7395981848239899}]}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents the parsing framework, including the deterministic parsing algorithm and the history-based feature models.", "labels": [], "entities": [{"text": "parsing", "start_pos": 23, "end_pos": 30, "type": "TASK", "confidence": 0.9836996793746948}, {"text": "deterministic parsing", "start_pos": 56, "end_pos": 77, "type": "TASK", "confidence": 0.6570574939250946}]}, {"text": "Section 3 discusses the two learning algorithms used in the experiments, and section 4 describes the experimental setup, including data sets, feature models, learning algorithm parameters, and evaluation metrics.", "labels": [], "entities": []}, {"text": "Experimental results are presented and discussed in section 5, and conclusions in section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe the experimental setup, including data sets, feature models, parameter optimization, and evaluation metrics.", "labels": [], "entities": []}, {"text": "Experimental results are presented in section 5.", "labels": [], "entities": []}, {"text": "The evaluation metrics used for parsing accuracy are the unlabeled attachment score AS U , which is the proportion of tokens that are assigned the correct head (regardless of dependency type), and the labeled attachment score ASL , which is the proportion of tokens that are assigned the correct head and the correct dependency type.", "labels": [], "entities": [{"text": "parsing", "start_pos": 32, "end_pos": 39, "type": "TASK", "confidence": 0.9835890531539917}, {"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.7757821083068848}, {"text": "unlabeled attachment score AS U", "start_pos": 57, "end_pos": 88, "type": "METRIC", "confidence": 0.7380032122135163}, {"text": "labeled attachment score ASL", "start_pos": 201, "end_pos": 229, "type": "METRIC", "confidence": 0.6276635676622391}]}, {"text": "We also consider the unlabeled exact match EM U , which is the proportion of sentences that are assigned a completely correct dependency graph without considering dependency type labels, and the labeled exact match EM L , which also takes dependency type labels into account.", "labels": [], "entities": [{"text": "exact match EM U", "start_pos": 31, "end_pos": 47, "type": "METRIC", "confidence": 0.7925625294446945}]}, {"text": "Attachment scores are presented as mean scores per token, and punctuation tokens are excluded from all counts.", "labels": [], "entities": []}, {"text": "For all experiments we have performed a McNemar test of significance at \u03b1 = 0.01 for differences between the two learning methods.", "labels": [], "entities": [{"text": "McNemar", "start_pos": 40, "end_pos": 47, "type": "METRIC", "confidence": 0.8694982528686523}, {"text": "significance", "start_pos": 56, "end_pos": 68, "type": "METRIC", "confidence": 0.8148128986358643}]}, {"text": "We also compare learning and parsing times, as measured on an AMD 64-bit processor running Linux.", "labels": [], "entities": [{"text": "parsing", "start_pos": 29, "end_pos": 36, "type": "TASK", "confidence": 0.9358507990837097}]}, {"text": "shows the parsing accuracy for the combination of three languages (Swedish, English and Chinese), two learning methods (MBL and SVM) and five feature models (\u03a6 1 -\u03a6 5 ), with algorithm parameters optimized as described in section 4.3.", "labels": [], "entities": [{"text": "parsing", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9440128803253174}, {"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9479753971099854}]}, {"text": "For each combination, we measure the attachment score (AS) and the exact match (EM).", "labels": [], "entities": [{"text": "attachment score (AS)", "start_pos": 37, "end_pos": 58, "type": "METRIC", "confidence": 0.9547730088233948}, {"text": "exact match (EM)", "start_pos": 67, "end_pos": 83, "type": "METRIC", "confidence": 0.9453494071960449}]}, {"text": "A significant improvement for one learning method over the other is marked by an asterisk (*).", "labels": [], "entities": []}, {"text": "Independently of language and learning method, the most complex feature model \u03a6 5 gives the highest accuracy across all metrics.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9990934133529663}]}, {"text": "Not surprisingly, the lowest accuracy is obtained with the simplest feature model \u03a6 1 . By and large, more complex feature models give higher accuracy, with one exception for Swedish and the feature models \u03a6 3 and \u03a6 4 . It is significant in this context that the Swedish data set is the smallest of the three (about 20% of the Chinese data set and about 10% of the English one).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9990443587303162}, {"text": "accuracy", "start_pos": 142, "end_pos": 150, "type": "METRIC", "confidence": 0.9988529682159424}, {"text": "Swedish data set", "start_pos": 263, "end_pos": 279, "type": "DATASET", "confidence": 0.8623679876327515}, {"text": "Chinese data set", "start_pos": 327, "end_pos": 343, "type": "DATASET", "confidence": 0.7982107301553091}]}], "tableCaptions": []}