{"title": [{"text": "The Effect of Corpus Size in Combining Supervised and Unsupervised Training for Disambiguation", "labels": [], "entities": [{"text": "Corpus Size", "start_pos": 14, "end_pos": 25, "type": "TASK", "confidence": 0.7551018297672272}, {"text": "Disambiguation", "start_pos": 80, "end_pos": 94, "type": "TASK", "confidence": 0.6938554644584656}]}], "abstractContent": [{"text": "We investigate the effect of corpus size in combining supervised and unsuper-vised learning for two types of attachment decisions: relative clause attachment and prepositional phrase attachment.", "labels": [], "entities": [{"text": "relative clause attachment", "start_pos": 131, "end_pos": 157, "type": "TASK", "confidence": 0.7106645901997884}, {"text": "prepositional phrase attachment", "start_pos": 162, "end_pos": 193, "type": "TASK", "confidence": 0.6284871200720469}]}, {"text": "The supervised component is Collins' parser, trained on the Wall Street Journal.", "labels": [], "entities": [{"text": "Wall Street Journal", "start_pos": 60, "end_pos": 79, "type": "DATASET", "confidence": 0.9574803709983826}]}, {"text": "The unsupervised component gathers lexical statistics from an unannotated corpus of newswire text.", "labels": [], "entities": []}, {"text": "We find that the combined system only improves the performance of the parser for small training sets.", "labels": [], "entities": []}, {"text": "Surprisingly , the size of the unannotated corpus has little effect due to the noisi-ness of the lexical statistics acquired by unsupervised learning.", "labels": [], "entities": []}], "introductionContent": [{"text": "The best performing systems for many tasks in natural language processing are based on supervised training on annotated corpora such as the Penn Treebank ( and the prepositional phrase data set first described in.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 46, "end_pos": 73, "type": "TASK", "confidence": 0.6640125513076782}, {"text": "Penn Treebank", "start_pos": 140, "end_pos": 153, "type": "DATASET", "confidence": 0.9947401583194733}]}, {"text": "However, the production of training sets is expensive.", "labels": [], "entities": []}, {"text": "They are not available for many domains and languages.", "labels": [], "entities": []}, {"text": "This motivates research on combining supervised with unsupervised learning since unannotated text is in ample supply for most domains in the major languages of the world.", "labels": [], "entities": []}, {"text": "The question arises how much annotated and unannotated data is necessary in combination learning strategies.", "labels": [], "entities": []}, {"text": "We investigate this question for two attachment ambiguity problems: relative clause (RC) attachment and prepositional phrase (PP) attachment.", "labels": [], "entities": [{"text": "relative clause (RC) attachment", "start_pos": 68, "end_pos": 99, "type": "TASK", "confidence": 0.5929210583368937}, {"text": "prepositional phrase (PP) attachment", "start_pos": 104, "end_pos": 140, "type": "TASK", "confidence": 0.6494869887828827}]}, {"text": "The supervised component is, trained on the Wall Street Journal.", "labels": [], "entities": [{"text": "Wall Street Journal", "start_pos": 44, "end_pos": 63, "type": "DATASET", "confidence": 0.8708351254463196}]}, {"text": "The unsupervised component gathers lexical statistics from an unannotated corpus of newswire text.", "labels": [], "entities": []}, {"text": "The sizes of both types of corpora, annotated and unannotated, are of interest.", "labels": [], "entities": []}, {"text": "We would expect that large annotated corpora (training sets) tend to make the additional information from unannotated corpora redundant.", "labels": [], "entities": []}, {"text": "This expectation is confirmed in our experiments.", "labels": [], "entities": []}, {"text": "For example, when using the maximum training set available for PP attachment, performance decreases when \"unannotated\" lexical statistics are added.", "labels": [], "entities": [{"text": "PP attachment", "start_pos": 63, "end_pos": 76, "type": "TASK", "confidence": 0.9395074844360352}]}, {"text": "For unannotated corpora, we would expect the opposite effect.", "labels": [], "entities": []}, {"text": "The larger the unannotated corpus, the better the combined system should perform.", "labels": [], "entities": []}, {"text": "While there is a general tendency to this effect, the improvements in our experiments reach a plateau quickly as the unlabeled corpus grows, especially for PP attachment.", "labels": [], "entities": [{"text": "PP attachment", "start_pos": 156, "end_pos": 169, "type": "TASK", "confidence": 0.8956470787525177}]}, {"text": "We attribute this result to the noisiness of the statistics collected from unlabeled corpora.", "labels": [], "entities": []}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "Sections 2, 3 and 4 describe data sets, methods and experiments.", "labels": [], "entities": []}, {"text": "Section 5 evaluates and discusses experimental results.", "labels": [], "entities": []}, {"text": "Section 6 compares our approach to prior work.", "labels": [], "entities": []}, {"text": "Section 7 states our conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "The Reuters corpus was parsed with minipar and all dependencies were extracted.", "labels": [], "entities": [{"text": "Reuters corpus", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.9527432024478912}]}, {"text": "Three inverted indexes were created, corresponding to 10%, 50% and 100% of the corpus.", "labels": [], "entities": []}, {"text": "1 Five parameter sets for the Collins parser were created by training it on the WSJ training sets in.", "labels": [], "entities": [{"text": "Collins", "start_pos": 30, "end_pos": 37, "type": "DATASET", "confidence": 0.9050653576850891}, {"text": "WSJ training sets", "start_pos": 80, "end_pos": 97, "type": "DATASET", "confidence": 0.9643032550811768}]}, {"text": "Sentences with attachment ambiguities in the WSJ corpus were parsed with minipar to generate Lucene queries.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 45, "end_pos": 55, "type": "DATASET", "confidence": 0.9479296803474426}]}, {"text": "(We chose this procedure to ensure compatibility of query and index formats.)", "labels": [], "entities": []}, {"text": "The Lucene queries were run on the three indexes.", "labels": [], "entities": [{"text": "Lucene", "start_pos": 4, "end_pos": 10, "type": "DATASET", "confidence": 0.9032669067382812}]}, {"text": "LBD disambiguation was then applied based on the statistics returned by the queries.", "labels": [], "entities": [{"text": "LBD disambiguation", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.5903368890285492}]}, {"text": "LBD results are applied to the output of the Collins parser by simply replacing all attachment decisions with LBD decisions.", "labels": [], "entities": [{"text": "Collins", "start_pos": 45, "end_pos": 52, "type": "DATASET", "confidence": 0.9149782061576843}]}, {"text": "Evaluation results are shown in: Experimental results.", "labels": [], "entities": []}, {"text": "Results for LBD (without Collins) are given in the first lines.", "labels": [], "entities": [{"text": "LBD", "start_pos": 12, "end_pos": 15, "type": "DATASET", "confidence": 0.7939556837081909}]}, {"text": "# is the size of the test set.", "labels": [], "entities": []}, {"text": "The baselines are 73.1% (RC) and 51.4% (PP).", "labels": [], "entities": [{"text": "RC", "start_pos": 25, "end_pos": 27, "type": "METRIC", "confidence": 0.9429851174354553}]}, {"text": "The combined method performs better for small training sets.", "labels": [], "entities": []}, {"text": "There is no significant difference between 10%, 50% and 100% for the combination method (p < 0.05).", "labels": [], "entities": []}, {"text": "unlabeled corpus of size 0, achieves a performance of 76.1%.", "labels": [], "entities": []}, {"text": "The bottom five lines of each table evaluate combinations of a parameter set trained on a subset of WSJ (0.05% -50%) and a particular size of the unlabeled corpus (100% -0%).", "labels": [], "entities": [{"text": "WSJ", "start_pos": 100, "end_pos": 103, "type": "DATASET", "confidence": 0.8456876873970032}]}, {"text": "In addition, the third column gives the performance of Collins' parser without LBD.", "labels": [], "entities": [{"text": "Collins'", "start_pos": 55, "end_pos": 63, "type": "DATASET", "confidence": 0.9030321836471558}, {"text": "LBD", "start_pos": 79, "end_pos": 82, "type": "METRIC", "confidence": 0.9306010603904724}]}, {"text": "Recall that test set size (second column) varies because we discard a test instance if Collins' parser does not recognize that there is an ambiguity (e.g., because of a parse failure).", "labels": [], "entities": []}, {"text": "As expected, performance increases as the size of the training set grows, e.g., from 58.0% to 82.8% for PP attachment.", "labels": [], "entities": [{"text": "PP attachment", "start_pos": 104, "end_pos": 117, "type": "TASK", "confidence": 0.8826179802417755}]}, {"text": "The combination of Collins and LBD is consistently better than Collins for RC attachment (not statistically significant due to the size of the data set).", "labels": [], "entities": [{"text": "Collins", "start_pos": 63, "end_pos": 70, "type": "METRIC", "confidence": 0.7013527154922485}, {"text": "RC attachment", "start_pos": 75, "end_pos": 88, "type": "TASK", "confidence": 0.8322884738445282}]}, {"text": "However, this is not the case for PP attachment.", "labels": [], "entities": [{"text": "PP attachment", "start_pos": 34, "end_pos": 47, "type": "TASK", "confidence": 0.9495286643505096}]}, {"text": "Due to the good performance of Collins' parser for even small training sets, the combination is only superior for the two smallest training sets (significant for the smallest set, p < 0.001).", "labels": [], "entities": []}, {"text": "The most surprising result of the experiments is the small difference between the three unlabeled corpora.", "labels": [], "entities": []}, {"text": "There is no clear pattern in the data for PP attachment and only a small effect for RC attachment: an increase between 1% and 2% when corpus size is increased from 10% to 100%.", "labels": [], "entities": [{"text": "PP attachment", "start_pos": 42, "end_pos": 55, "type": "TASK", "confidence": 0.9260507524013519}, {"text": "RC attachment", "start_pos": 84, "end_pos": 97, "type": "TASK", "confidence": 0.7764453887939453}]}, {"text": "We performed an analysis of a sample of incorrectly attached PPs to investigate why unlabeled corpus size has such a small effect.", "labels": [], "entities": []}, {"text": "We found that the noisiness of the statistics extracted from Reuters were often responsible for attachment errors.", "labels": [], "entities": [{"text": "Reuters", "start_pos": 61, "end_pos": 68, "type": "DATASET", "confidence": 0.8600715398788452}]}, {"text": "The noisiness is caused by our filtering strategy (ambiguous PPs are not used, resulting in undercounting), by the approximation of counts by Lucene (Lucene overcounts and undercounts as discussed in Section 3) and by minipar parse errors.", "labels": [], "entities": [{"text": "Lucene", "start_pos": 142, "end_pos": 148, "type": "DATASET", "confidence": 0.9080412983894348}]}, {"text": "Parse errors are particularly harmful in cases like the impact it would have on prospects, where, due to the extraction of the NP impact, minipar attaches the PP to the verb.", "labels": [], "entities": [{"text": "Parse", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9526047706604004}]}, {"text": "We did not filter out these more complex ambiguous cases.", "labels": [], "entities": []}, {"text": "Finally, the two corpora are from distinct sources and from distinct time periods (early nineties vs. mid-nineties).", "labels": [], "entities": []}, {"text": "Many topicand time-specific dependencies can only be mined from more similar corpora.", "labels": [], "entities": []}, {"text": "The experiments reveal interesting differences between PP and RC attachment.", "labels": [], "entities": []}, {"text": "The dependencies used in RC disambiguation rarely occur in an ambiguous context (e.g., most subject-verb dependencies can be reliably extracted).", "labels": [], "entities": [{"text": "RC disambiguation", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.9501744508743286}]}, {"text": "In contrast, a large proportion of the dependencies needed in PP disambiguation (verb-prep and noun-prep dependencies) do occur in ambiguous contexts.", "labels": [], "entities": [{"text": "PP disambiguation", "start_pos": 62, "end_pos": 79, "type": "TASK", "confidence": 0.9026299118995667}]}, {"text": "Another difference is that RC attachment is syntactically more complex.", "labels": [], "entities": [{"text": "RC attachment", "start_pos": 27, "end_pos": 40, "type": "TASK", "confidence": 0.9370069801807404}]}, {"text": "It interacts with agreement, passive and long-distance depen-dencies.", "labels": [], "entities": []}, {"text": "The algorithm proposed for RC applies grammatical constraints successfully.", "labels": [], "entities": [{"text": "RC", "start_pos": 27, "end_pos": 29, "type": "TASK", "confidence": 0.9630920886993408}]}, {"text": "A final difference is that the baseline for RC is much higher than for PP and therefore harder to beat.", "labels": [], "entities": []}, {"text": "An innovation of our disambiguation system is the use of a search engine, lucene, for serving up dependency statistics.", "labels": [], "entities": []}, {"text": "The advantage is that counts can be computed quickly and dynamically.", "labels": [], "entities": []}, {"text": "New text can be added on an ongoing basis to the index.", "labels": [], "entities": []}, {"text": "The updated dependency statistics are immediately available and can benefit disambiguation performance.", "labels": [], "entities": []}, {"text": "Such a system can adapt easily to new topics and changes overtime.", "labels": [], "entities": []}, {"text": "However, this architecture negatively affects accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9971171617507935}]}, {"text": "The unsupervised approach of) achieves almost 80% accuracy by using partial dependency statistics to disambiguate ambiguous sentences in the unlabeled corpus.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9989186525344849}]}, {"text": "Ambiguous sentences were excluded from our index to make index construction simple and efficient.", "labels": [], "entities": [{"text": "index construction", "start_pos": 57, "end_pos": 75, "type": "TASK", "confidence": 0.7634454369544983}]}, {"text": "Our larger corpus (about 6 times as large as Hindle et al.'s) did not compensate for our lower-quality statistics.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Corpora used for the experiments:  unlabeled Reuters (R) corpus for attachment  statistics, labeled Penn treebank (WSJ) for  training the Collins parser.", "labels": [], "entities": [{"text": "Reuters (R) corpus", "start_pos": 55, "end_pos": 73, "type": "DATASET", "confidence": 0.566881263256073}, {"text": "Penn treebank (WSJ)", "start_pos": 110, "end_pos": 129, "type": "DATASET", "confidence": 0.961698842048645}]}, {"text": " Table 2: RC and PP attachment ambigui- ties in the Penn Treebank. Number of in- stances with high attachment (highA), low at- tachment (lowA), verb attachment (verbA),  and noun attachment (nounA) according to  the gold standard.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 52, "end_pos": 65, "type": "DATASET", "confidence": 0.995824009180069}]}, {"text": " Table 3: Queries for computing high attach- ment (above) and low attachment (below) for  Example 1.", "labels": [], "entities": [{"text": "attachment", "start_pos": 66, "end_pos": 76, "type": "METRIC", "confidence": 0.8536097407341003}]}, {"text": " Table 4: Experimental results. Results for LBD (without Collins) are given in the first lines. #  is the size of the test set. The baselines are 73.1% (RC) and 51.4% (PP). The combined method  performs better for small training sets. There is no significant difference between 10%, 50% and  100% for the combination method (p < 0.05).", "labels": [], "entities": []}]}