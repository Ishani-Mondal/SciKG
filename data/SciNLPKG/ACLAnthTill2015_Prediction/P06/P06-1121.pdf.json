{"title": [{"text": "Scalable Inference and Training of Context-Rich Syntactic Translation Models", "labels": [], "entities": [{"text": "Syntactic Translation", "start_pos": 48, "end_pos": 69, "type": "TASK", "confidence": 0.6735327690839767}]}], "abstractContent": [{"text": "Statistical MT has made great progress in the last few years, but current translation models are weak on reordering and target language fluency.", "labels": [], "entities": [{"text": "Statistical MT", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.5332977473735809}]}, {"text": "Syntactic approaches seek to remedy these problems.", "labels": [], "entities": []}, {"text": "In this paper, we take the framework for acquiring multi-level syntactic translation rules of (Gal-ley et al., 2004) from aligned tree-string pairs, and present two main extensions of their approach: first, instead of merely computing a single derivation that minimally explains a sentence pair, we construct a large number of derivations that include contex-tually richer rules, and account for multiple interpretations of unaligned words.", "labels": [], "entities": [{"text": "multi-level syntactic translation", "start_pos": 51, "end_pos": 84, "type": "TASK", "confidence": 0.6237692336241404}]}, {"text": "Second, we propose probability estimates and a training procedure for weighting these rules.", "labels": [], "entities": []}, {"text": "We contrast different approaches on real examples, show that our estimates based on multiple derivations favor phrasal re-orderings that are linguistically better motivated, and establish that our larger rules provide a 3.63 BLEU point increase over minimal rules.", "labels": [], "entities": [{"text": "BLEU point increase", "start_pos": 225, "end_pos": 244, "type": "METRIC", "confidence": 0.9727953672409058}]}], "introductionContent": [{"text": "While syntactic approaches seek to remedy wordordering problems common to statistical machine translation (SMT) systems, many of the earlier models-particularly child re-ordering modelsfail to account for human translation behavior.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 74, "end_pos": 111, "type": "TASK", "confidence": 0.7781503796577454}]}, {"text": "alleviate this modeling problem and present a method for acquiring millions of syntactic transfer rules from bilingual corpora, which we review below.", "labels": [], "entities": []}, {"text": "Here, we make the following new contributions: (1) we show how to acquire larger rules that crucially condition on more syntactic context, and show how to compute multiple derivations for each training example, capturing both large and small rules, as well as multiple interpretations for unaligned words; (2) we develop probability models for these multilevel transfer rules, and give estimation methods for assigning probabilities to very large rule sets.", "labels": [], "entities": []}, {"text": "We contrast our work with (), highlight some severe limitations of probability estimates computed from single derivations, and demonstrate that it is critical to account for many derivations for each sentence pair.", "labels": [], "entities": []}, {"text": "We also use real examples to show that our probability models estimated from a large number of derivations favor phrasal re-orderings that are linguistically well motivated.", "labels": [], "entities": []}, {"text": "An empirical evaluation against a state-of-the-art SMT system similar to) indicates positive prospects.", "labels": [], "entities": [{"text": "SMT", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.9905010461807251}]}, {"text": "Finally, we show that our contextually richer rules provide a 3.63 BLEU point increase over those of ().", "labels": [], "entities": [{"text": "BLEU point increase", "start_pos": 67, "end_pos": 86, "type": "METRIC", "confidence": 0.9768469929695129}]}], "datasetContent": [{"text": "The task of our decoder is to find the most likely English tree \u03c0 that maximizes all models involved in Equation 2.", "labels": [], "entities": []}, {"text": "Since xRs rules can be converted to context-free productions by increasing the number of non-terminals, we implemented our decoder as a standard CKY parser with beam search.", "labels": [], "entities": []}, {"text": "Its rule binarization is described in ().", "labels": [], "entities": []}, {"text": "We compare our syntax-based system against an implementation of the alignment template (AlTemp) approach to MT, which is widely considered to represent the state of the art in the field.", "labels": [], "entities": [{"text": "MT", "start_pos": 108, "end_pos": 110, "type": "TASK", "confidence": 0.94527268409729}]}, {"text": "We registered both systems in the NIST 2005 evaluation; results are presented in.", "labels": [], "entities": [{"text": "NIST 2005 evaluation", "start_pos": 34, "end_pos": 54, "type": "DATASET", "confidence": 0.9686613281567892}]}, {"text": "With a difference of 6.4 BLEU points for both language pairs, we consider the results of our syntax-based system particularly promising, since these are the highest scores to date that we know of using linguistic syntactic transformations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9992139339447021}]}, {"text": "Also, on the one hand, our AlTemp system represents quite mature technology, and incorporates highly tuned model parameters.", "labels": [], "entities": []}, {"text": "On the other hand, our syntax decoder is still work in progress: only one model was used during search, i.e., the EM-trained root-normalized SBTM, and as yet no language model is incorporated in the search (whereas the search in the AlTemp system uses two phrase-based translation models and   12 other feature functions).", "labels": [], "entities": []}, {"text": "Furthermore, our decoder doesn't incorporate any syntax-based language model, and admittedly our ability to penalize ill-formed parse trees is still limited.", "labels": [], "entities": []}, {"text": "Finally, we evaluated our system on the NIST-02 test set with the three different rule sets (see).", "labels": [], "entities": [{"text": "NIST-02 test set", "start_pos": 40, "end_pos": 56, "type": "DATASET", "confidence": 0.9850637912750244}]}, {"text": "The performance with our largest rule set represents a 3.63 BLEU point increase (14.8% relative) compared to using only minimal rules, which indicates positive prospects for using even larger rules.", "labels": [], "entities": [{"text": "BLEU point", "start_pos": 60, "end_pos": 70, "type": "METRIC", "confidence": 0.9760628640651703}]}, {"text": "While our rule inference algorithm scales to higher thresholds, one important area of future work will be the improvement of our decoder, conjointly with analyses of the impact in terms of BLEU of contextually richer rules.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 189, "end_pos": 193, "type": "METRIC", "confidence": 0.9963076114654541}]}], "tableCaptions": [{"text": " Table 4: Translation probabilities promote linguistically motivated constituent re-orderings (for lhs1 and lhs2), and enable  non-constituent (lhs3) and non-contiguous (lhs4) phrasal translations.", "labels": [], "entities": []}, {"text": " Table 5: BLEU-4 scores for the 2005 NIST test set.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9992361068725586}, {"text": "NIST test set", "start_pos": 37, "end_pos": 50, "type": "DATASET", "confidence": 0.8760069807370504}]}, {"text": " Table 6: BLEU-4 scores for the 2002 NIST test set, with rules  of increasing sizes.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9992902278900146}, {"text": "NIST test set", "start_pos": 37, "end_pos": 50, "type": "DATASET", "confidence": 0.8781542976697286}]}]}