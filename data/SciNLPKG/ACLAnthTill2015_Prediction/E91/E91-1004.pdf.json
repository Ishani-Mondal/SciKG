{"title": [], "abstractContent": [], "introductionContent": [{"text": "All natural language grammars are alnbiguous.", "labels": [], "entities": []}, {"text": "Even tightly fitting natural language grammars are ambiguous in some ways.", "labels": [], "entities": []}, {"text": "Loosely fitting grammars, which are necessary for handling the variability and complexity of unrestricted text and speech, are worse.", "labels": [], "entities": []}, {"text": "Tim standard technique for dealing with this ambiguity, pruning \u00b0This work was p,~rtially supported by DARPA grant gra.nunars I)y hand, is painful, time-consuming, and usually arbitrary.", "labels": [], "entities": []}, {"text": "The solution which many people have proposed is to use stochastic models to grain statistical grammars automatically from a large corpus.", "labels": [], "entities": []}, {"text": "Attempts in applying statistical techniques to natura, I iangt, age parsi,lg have exhibited varying degrees of success.", "labels": [], "entities": []}, {"text": "These successful and unsuccessful attempts have suggested to us that: . Stochastic techniques combined with traditional linguistic theories can (and indeed must) provide a solull|on to the natural language understanding problem.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 189, "end_pos": 219, "type": "TASK", "confidence": 0.6827159523963928}]}, {"text": "* In order for stochastic techniques to be effective, they must be applied with restraint (poor estimates of context arc worse than none).", "labels": [], "entities": []}, {"text": "-Interactive, interleaved architectvres are preferable to pipeline architectures in NLU systems, because they use more of the available information in the decision-nmkiug process.", "labels": [], "entities": []}, {"text": "Wc have constructed a stoch~tic parser,/)earl, which is based on these ideas.", "labels": [], "entities": []}, {"text": "The development of the 7~earl parser is an effort to combine the statistical models developed recently into a single tool which incorporates all of these models into the decisiou-making component of a parser, While we have only attempted to incorporate a few simple statistical models into this parser, ~earl is structured in away which allows any nt, mber of syntactic, semantic, and ~other knowledge sources to contribute to parsing decisions.", "labels": [], "entities": []}, {"text": "The current implementation of \"Pearl uses ChurclFs part-of-speech assignment trigram model, a simple probabilistic unknown word model, and a conditional probability model for grammar rules based on part-of-speech trigrams and parent rules.", "labels": [], "entities": []}, {"text": "By combining multiple knowledge sources and using a chart-parsing framework, 7~earl attempts to handle a number of difficult problems.", "labels": [], "entities": [{"text": "earl", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.8689185380935669}]}, {"text": "7%arl has the capability to parse word lattices, an ability which is useful in recognizing idioms in text processing, as well as in speech processing.", "labels": [], "entities": [{"text": "parse word lattices", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.8395910660425822}]}, {"text": "The parser uses probabilistic training from a corpus to disambiguate between grammatically ac(-i:ptal)h', structures, such ;m determining i)repo -sitional l)hrase attachment and conjunction scope.", "labels": [], "entities": [{"text": "hrase attachment", "start_pos": 157, "end_pos": 173, "type": "TASK", "confidence": 0.7135304063558578}]}, {"text": "Finally, ?earl maintains a well-formed substring I,able within its chart to allow for partial parse retrieval.", "labels": [], "entities": [{"text": "parse retrieval", "start_pos": 94, "end_pos": 109, "type": "TASK", "confidence": 0.913177490234375}]}, {"text": "Partial parses are usefid botll for error-message generation aud for pro(-cssitlg lulgrattUllal,i('al or illCOllll)h;I,e .'~;l|-I,(~llCes.", "labels": [], "entities": []}, {"text": "ht i)reliluinary tests, ?earl has shown protnisillg resuits in ha,idling part-of-speech ~ussignnlent,, preposit, ional I)hrase ;d,l, achnlcnl., ait(I Ilnknowlt wor(I categoriza6on.", "labels": [], "entities": []}, {"text": "Trained on a corpus of 1100 sentences from the Voyager direction-linding system 2 and using the string gra,ulm~r from l,he I)UNDIT l,aug,,age IhM,.rsl.atJ(ling Sysl,cuh ?carl correcl, ly i)a.rse(I 35 out of/10 or 88% of scIitellces sele('tcd frolu Voyager sentcil(:~}.~ tier used in the traini,lg data.", "labels": [], "entities": [{"text": "UNDIT", "start_pos": 125, "end_pos": 130, "type": "METRIC", "confidence": 0.9552385807037354}]}, {"text": "We will describe the details of this exl)crimelfl, lal,cr.", "labels": [], "entities": [{"text": "crimelfl", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.6765362620353699}]}, {"text": "In this I)al)cr , wc will lirsl, explain our contribul, ion l,o the sl,ochastic ,nodels which are used in ?earl: a context-free granunar with context-sensitive condil, ional probal)ilities.", "labels": [], "entities": []}, {"text": "Then, we will describe the parser's architecture and the parsing algorithtn, l\"ina.lly, we will give the results of some exi)erinlents we performed using ?earl which explore its capabilities.", "labels": [], "entities": []}], "datasetContent": [{"text": "While we haw; ,rot yet done ~-xte,miw~' testing of all of the Cal)abilities of \"/)carl, we perforumd some simple tests to determine if its I~erformance is at least consistent with the premises ,port which it is based.", "labels": [], "entities": []}, {"text": "The I.cst s,'ntcnces used for this evaluation are not fi'om the \u00b0This is a.u Unl~,,blishcd result, reportedly due to Fujisaki a.t IBM .]apitll.", "labels": [], "entities": [{"text": "\u00b0", "start_pos": 64, "end_pos": 65, "type": "METRIC", "confidence": 0.968646764755249}]}, {"text": "l0 In fact, h~r certain grail|liiars, th(.'", "labels": [], "entities": []}, {"text": "fr(.~qllClicy I.~tl)les may not conw:rge at all, or they may converge to zero, with the g,','tmmar gc,tcrati,lg no pa.rscs for the entire corpus.", "labels": [], "entities": []}, {"text": "This is a worst-case sccl,ario whicl, we do oct a,lticipate halq~cning.", "labels": [], "entities": []}, {"text": "training data on which the parser was trained.", "labels": [], "entities": []}, {"text": "Using .p,'arl's cont(.'xt-free gr;unmar, i,h~.~e test sentences produced an average of 64 parses per sentence, with some sentences producing over 100 parses.", "labels": [], "entities": []}], "tableCaptions": []}