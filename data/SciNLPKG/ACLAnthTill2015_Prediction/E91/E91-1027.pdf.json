{"title": [{"text": "THE RECOGNITION CAPACITY OF LOCAL SYNTACTIC CONSTRAINTS", "labels": [], "entities": []}], "abstractContent": [{"text": "Givcn a grammar fora language, it is possible to create finite state mechanisms that approximate its recognition capacity.", "labels": [], "entities": []}, {"text": "These simple automata consider only short context information~ drawn from local syntactic constraints which the grammar hnposes.", "labels": [], "entities": []}, {"text": "While it is short of providing the strong generative capacity of the grammar, such an approximation is useful for removing most word tagging ambiguities, identifying many cases of iU-fonncd input, and assisting efficiently in othcr natural language processing tasks.", "labels": [], "entities": [{"text": "word tagging ambiguities", "start_pos": 128, "end_pos": 152, "type": "TASK", "confidence": 0.7713585197925568}, {"text": "othcr natural language processing", "start_pos": 226, "end_pos": 259, "type": "TASK", "confidence": 0.5279946252703667}]}, {"text": "Our basic approach to the acquisition and usage of local syntactic constraints was presented clse-whcre; in this papcr we present some formal and empiric-,d results pertaining to properties of the approximating automata.", "labels": [], "entities": []}], "introductionContent": [{"text": "Parsing is a process by which an input sentence is not only recognized as belonging to the language, but is also assigned a structure.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.9449262022972107}]}, {"text": "Aserwick/Wcinbcrg 84] commcnt, recognition per se (i.e. a weak generative capacity analysis) is not of much value fora theory of language understanding, but it can be useful \"as a diagnostic\".", "labels": [], "entities": [{"text": "language understanding", "start_pos": 129, "end_pos": 151, "type": "TASK", "confidence": 0.7214109897613525}]}, {"text": "We claim that if an cfficient recognition procedure is availat~le, it can be tnost valuable as a prc-parsing reducer of lcxical ambiguity (especially, as] points out, for detcnninistic parsers), and cvcn more useful in applications where full parsing is not absolutely requirede.g. identification of iU-formed inputs in a text critique program.", "labels": [], "entities": [{"text": "identification of iU-formed inputs in a text critique program", "start_pos": 282, "end_pos": 343, "type": "TASK", "confidence": 0.8458205262819926}]}, {"text": "Still weaker than recognition procedures are 'methods which approximate the recognition capacity.", "labels": [], "entities": []}, {"text": "This is the kind of methods that we discuss in this paper.", "labels": [], "entities": []}, {"text": "More specifically, we analyze the recognition capacity of automata based on local (short context) considerations.", "labels": [], "entities": []}, {"text": "In [Herz/ we prescnted our approach to the acquisition and usage of local syntactic constraints, focusing on its use for reduction of word-level ambiguity.", "labels": [], "entities": [{"text": "acquisition and usage of local syntactic constraints", "start_pos": 43, "end_pos": 95, "type": "TASK", "confidence": 0.7036676917757306}]}, {"text": "After briefly reviewing this method in section 2 below, we examine in more detail various characteristics of the approximating automata, and suggest several applications.", "labels": [], "entities": []}, {"text": "to assume that a preliminary morphological phase separated word-forms to basic sequences of tags, and then state syntactic rules in terms of standard word classes.", "labels": [], "entities": []}, {"text": "In any case, it is reasonable to assume that the tag image it .....", "labels": [], "entities": []}, {"text": "IM cannot be uniquely assigned.", "labels": [], "entities": []}, {"text": "Fven with a coarse tag set (e.g. parts of speech with no features) many words have more than one interpretation, thus giving rise to exponentially many tag images fora sentence.", "labels": [], "entities": [{"text": "Fven", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8554867506027222}]}, {"text": "3 Following [Karlsson 90], we use the term cohort to refer to the set of lcxicaUy valid readings of a given word.", "labels": [], "entities": []}, {"text": "We use the term path to refer to a sequence of M tags (M~ N) which is a tagimage corresponding to the words W,..., WN of a given sentence S.", "labels": [], "entities": []}, {"text": "This is motivated by a view of lexical mnbiguity as a graph problem: we try to reduce the number of tentative paths in ambiguous cases by removing arcs from the Sentence Graph (SG) -a directed graph with vertices for all tags in all cohorts of the words in the given sentence, and arcs connecting each tag to ~dl tags in the cohort which follows it.", "labels": [], "entities": []}, {"text": "The removal of arcs and the testing of paths for validity as complete sentence interpretations are done using local constraints.", "labels": [], "entities": []}, {"text": "A local constraint of length k on a given tag t is a rule allowing or disaUowing a sequence of k tags from being in its right (or left) neighborhood in any tag image of a sentence.", "labels": [], "entities": []}, {"text": "In our approach, the local constraints are extractcd from the grammar (and this is the major aspect distinguishing it from some other short context methods such as For technical convenience we add the symbol \"$ <\" at the beginning of tag images and \"> $~ at the etad.", "labels": [], "entities": []}, {"text": "Given a grammar G (wlfich for the time being we assume to bean unrestricted contextfree phrase structure grammar), with a:set T of terminal symbols (tag set), a set V of variables (non-terminals, among which S is the root vailable for derivations), and a set P of production rules of the form A --.", "labels": [], "entities": []}, {"text": "a, where A is in V and a is in (VUT)* , we define the Right Short Context of length k of a terminal t (tag): SCr (t,k) fort in T and fork = 0,1,2,3...", "labels": [], "entities": []}, {"text": "The l.eft Short Context of length k of a tag t relative to the grammar G is denoted by SCI (t,k) and defined in; a similar way.", "labels": [], "entities": []}], "datasetContent": [{"text": "Consider the following sentence, which is in the language gcncratcd by grammar G of section 4: (1) Thc channing princess kissed a frog.", "labels": [], "entities": []}, {"text": "The unique tag image corresponding to this sentence is: [ $ <, dot, adi, n, v, det, n, > $ ].", "labels": [], "entities": []}, {"text": "This result is not surprising, given the simple scntence and toy grammar.", "labels": [], "entities": []}, {"text": "(In general, a grammar with a small number of rules relative to the size of the tag set cannot produce too many valid short contexts).", "labels": [], "entities": []}, {"text": "It is therefore interesting to examine another example, where each word is associated with a cohort of several interpretations.", "labels": [], "entities": []}, {"text": "We borrow from [llcrz/Rimon 9.1]: (2) All old people like books about fish.", "labels": [], "entities": []}, {"text": "Assuming the word tagging shown in section 6, there are 256 (2 x 2 x 2 x 4 x 2 x 2 x 2) tentative tag hnages (paths) for this sentence and for each of its 5040 permutations.", "labels": [], "entities": [{"text": "word tagging", "start_pos": 13, "end_pos": 25, "type": "TASK", "confidence": 0.7051506042480469}]}, {"text": "This generates a very htrge number of rather random tag images.", "labels": [], "entities": []}, {"text": "Applying LCA(I), only a small number of hnages are rccogtfizcd as potentially valid.", "labels": [], "entities": [{"text": "rccogtfizcd", "start_pos": 51, "end_pos": 62, "type": "METRIC", "confidence": 0.9217888712882996}]}, {"text": "Among them are syntactically correct sentences such as: (2a) Fish like old books about all people.", "labels": [], "entities": []}, {"text": ",and only less than 0.1% sentences which are locally valid but globally incorrect, such as: (2b) * Old tish all about books like people.", "labels": [], "entities": []}, {"text": "(tagged as [$ <, n, v, n, prep, n, v, n, > $]).", "labels": [], "entities": []}, {"text": "These two examples do not suggest any kind of proof, but they well illustrate the recognition power of even the least powerful automaton in the {LeA(i)} family.", "labels": [], "entities": []}, {"text": "To get another point of view, one may consider the simple formal language L consisting of the strings {ar\"b m} for I < rn, which can be generated by a context-free grammar (} over T = {a, b}.", "labels": [], "entities": []}, {"text": "I.CA(I) based on (; will recognize all strings of the form (a'b ~} for 1 <j,k, but none of the very many other strings over T.", "labels": [], "entities": []}, {"text": "It can be shown that, given arbitrary strings of length n over T, the probability that LeA(I) will not reject strings not belonging to L is proportional to n/2\", a term which tends rapidly to 0.", "labels": [], "entities": []}, {"text": "This is the over-recognition margin.", "labels": [], "entities": []}], "tableCaptions": []}