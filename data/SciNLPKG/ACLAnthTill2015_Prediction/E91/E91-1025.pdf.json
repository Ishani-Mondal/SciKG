{"title": [{"text": "Parsing without lexicon: the MorP system", "labels": [], "entities": [{"text": "Parsing", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.9381433129310608}, {"text": "MorP", "start_pos": 29, "end_pos": 33, "type": "DATASET", "confidence": 0.8602730631828308}]}], "abstractContent": [{"text": "MorP is a system for automatic word class assignment on the basis of surface features.", "labels": [], "entities": [{"text": "automatic word class assignment", "start_pos": 21, "end_pos": 52, "type": "TASK", "confidence": 0.6060706004500389}]}, {"text": "It has a very small lexicon of form words (%o entries), and for the rest works entirely on morphological and configurational patterns.", "labels": [], "entities": []}, {"text": "This makes it robust and fast, and in spite of the (deliberate) restrictedness of the system, its performance reaches an average accuracy level above 91% when run on unrestricted Swedish text.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.9991846680641174}]}], "introductionContent": [], "datasetContent": [{"text": "In an evaluation of the MorP parser, two texts of which there exists a manual tagging were chosen and cut at the first sentence boundary after 1,000 words.", "labels": [], "entities": []}, {"text": "The texts were run through the MorP parser and the output was compared to the manual tagging of the texts.", "labels": [], "entities": []}, {"text": "MorP was run by a batch file that calls the programs sequentially and builds up a series of intermediate outputs from each program.", "labels": [], "entities": [{"text": "MorP", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.861879289150238}]}, {"text": "Neither the programs themselves nor this mode of running them has in anyway been optimized for time, e.g., unproportionally much time is spent on opening and dosing both rule files and text files.", "labels": [], "entities": []}, {"text": "To run a full parse on an AT/386 took 1 minute 5 seconds for one text (1,006 words), giving an average of 0.065 sec/word, and for the other text (1,004 words) it took I minute I second, average 0.061 sec/word.", "labels": [], "entities": [{"text": "AT/386", "start_pos": 26, "end_pos": 32, "type": "DATASET", "confidence": 0.9168245395024618}]}, {"text": "With 10,000 words, the average is 0.055 sec/word.", "labels": [], "entities": []}, {"text": "The larger amounts of text that can be run in batch, the shorter the relative processing time will be, and if file handling were carried out differently, time would decrease considerably.", "labels": [], "entities": []}, {"text": "The figures for runtime could thus be much improved in several ways in applications where speed was a desirable factor.", "labels": [], "entities": []}, {"text": "In evaluating the accuracy of the output, single tagged words have been directly compared to the corresponding words in the manually tagged texts.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9987229704856873}]}, {"text": "When complex phrases are built up, their internal analysis is successively removed when it has played its role and is of no more use in the process.", "labels": [], "entities": []}, {"text": "The tags of words in phrases are thus evaluated in the following way: If a word has had an unambiguous tag at an earlier stage of the process that has been removed when building up the phrase, that tag is counted.", "labels": [], "entities": []}, {"text": "(Earlier tags can be seen in the intermediate outputs.)", "labels": [], "entities": []}, {"text": "If a word has had no tag at all or an ambiguous one and then been incorporated into a phrase, it is regarded as having the word class that the incorporation presupposes it to have.", "labels": [], "entities": []}, {"text": "That tag is then compared to that of the manually tagged text.", "labels": [], "entities": []}, {"text": "The errors can be of three kinds: erroneous word class assignment, unsolved ambiguity, and no assignment at all, which is rather a special case of unsolved ambiguity, cf. below.", "labels": [], "entities": [{"text": "word class assignment", "start_pos": 44, "end_pos": 65, "type": "TASK", "confidence": 0.6158118943373362}]}, {"text": "The figures for the three kinds are given below.", "labels": [], "entities": []}, {"text": "These results are remarkably good, in spite of the fact that many other systems are reported to reach an accuracy of 96-97%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9995114803314209}]}, {"text": "() Those systems, however, all use \"heavier artillery\" than MorP, that has been deliberately restricted in accordance with the hypotheses presented above.", "labels": [], "entities": [{"text": "MorP", "start_pos": 60, "end_pos": 64, "type": "DATASET", "confidence": 0.7788127660751343}]}, {"text": "This restrictiveness concerns both the size of the lexicon and the ways of carrying out disambiguation.", "labels": [], "entities": []}, {"text": "It is always difficult to define criteria for the correctness of parses, and the MorP parser must be judged in relation to the restrictions and the limited claims setup for it.", "labels": [], "entities": []}, {"text": "All, or most, errors can of cause be avoided if all disturbing words are put in a lexicon, but now the trick was to get as far as possible with as little lexicon as If we look at the roles that different parts of the MorP parser play in the analysis, we see that the lexical rules (which are only 435 in number) cover 54% of the 2,010 running words of the texts.", "labels": [], "entities": []}, {"text": "The two texts differ somewhat on this point.", "labels": [], "entities": []}, {"text": "One of them (text 402) contains very many quantifiers which are found in the lexicon, and that text has 58% of its running words covered.", "labels": [], "entities": []}, {"text": "Text 303 has 50% coverage after the lexical rules, a figure that is more \"normal\" in comparison with my earlier experiences with the parser.", "labels": [], "entities": [{"text": "Text 303", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8453280329704285}, {"text": "coverage", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9802682399749756}]}, {"text": "As can be seen from the table, the higher proportion of words covered by lexicon in text 402 does not have an overall positive effect on the final result.", "labels": [], "entities": []}, {"text": "The fact that a word is covered by the lexical rules is by no means a guarantee that it is correctly identified, as the lexicon only assigns the most probable word class.", "labels": [], "entities": []}, {"text": "The first three subprograms of MorP work entirely on the level of single words.", "labels": [], "entities": []}, {"text": "After they have been run, disambiguation proper starts.", "labels": [], "entities": [{"text": "disambiguation", "start_pos": 26, "end_pos": 40, "type": "TASK", "confidence": 0.9797585010528564}]}, {"text": "The MorP output in this intermediate situation is that 75% of the running words are marked as being unambiguous (though some of them later have their tags changed), 11% are marked as two-ways ambiguous, and 14% are unmarked.", "labels": [], "entities": []}, {"text": "In practice, this means that the latter are four-ways ambiguous, as they can finally come out as nouns, verbs, or adjectives, or remain untagged.", "labels": [], "entities": []}, {"text": "The syntactic part of MorP, covered by four subprograms, performs both disambiguation and identification of previously unmarked words, which, as stated above, can be seen as a generalization of the disambiguation process.", "labels": [], "entities": [{"text": "identification of previously unmarked words", "start_pos": 90, "end_pos": 133, "type": "TASK", "confidence": 0.8167048335075379}]}, {"text": "This part is entirely based on linguistic patterns rather than statistical ones.", "labels": [], "entities": []}, {"text": "Of course, there is \"statistics\" in the disambiguation rules as well as in the lexical assignment of tags, in the sense that the entire system is an implementation of my own intuitions as a native speaker of Swedish, and such intuitions certainly comprise a feeling for what is more or less common in a language.", "labels": [], "entities": []}, {"text": "Still, MorP would certainly gain a lot if it were based on actual statistics on, e.g., the structure of noun phrases or the placement of adverbials.", "labels": [], "entities": []}, {"text": "The errors arising from the application of syntactic patterns in the parsing of the two texts however rarely seem to be due to occurrence of infrequent patterns, but more to erroneous disambiguation of the words that are fitted into the patterns.", "labels": [], "entities": []}, {"text": "Next, I will give a few examples from the texts of the kind of errors that will typically occur with a simplified System like MorP.", "labels": [], "entities": []}, {"text": "Errors can arise from the lexicon, from the morphological analysis, from the syntactic disambiguation, and from combinations of these.", "labels": [], "entities": []}, {"text": "In text 402, there is also a misspelling, the non-existent form utterst for the adverb ytterst 'ultimately'.", "labels": [], "entities": []}, {"text": "This is correctly treated as a regularly formed adverb, which shows some of the robustness of MorP.", "labels": [], "entities": []}, {"text": "We have only a few instances in these texts where a word has been erroneously marked by the lexicon.", "labels": [], "entities": []}, {"text": "Most notorious is the case with the word om that can either be a preposition, 'about', or a conjunction, 'if'.", "labels": [], "entities": []}, {"text": "It is marked as a preposition in the lexicon and a later rule retags it as a conjunction if it has not been amalgamated with a following noun phrase to form a prepositional phrase by the end of the processing.", "labels": [], "entities": []}, {"text": "Mostly, however, it is impossible to decide the interpretation of the word om from its close context, as if-clauses almost always start with a subject noun phrase.", "labels": [], "entities": []}, {"text": "In the two texts, om occurs 17 times, 9 times as a preposition and 8 times as a conjunction.", "labels": [], "entities": []}, {"text": "One of the conjunctions is correctly retagged by the just mentioned rule, while the others remain uncorrected.", "labels": [], "entities": []}, {"text": "Regrettably, one of the prepositions has also been retagged as a conjunction, as it is followed by a that-clause and not by a noun phrase.", "labels": [], "entities": []}, {"text": "Of the 7 erroneously marked conjunctions, 3 are sentence-initial, while no occurrence of the word as a preposition is sentence-initial.", "labels": [], "entities": []}, {"text": "A possible heuristic would then be to have a retagging rule for this position before the rules that build prepositional phrases apply.", "labels": [], "entities": []}, {"text": "A remarkable fact is that none of the conjunctions om is followed by a later s/I 'then'.", "labels": [], "entities": []}, {"text": "A long-range context check looking for 'if-then' expressions would thus add nothing to the results here.", "labels": [], "entities": []}, {"text": "The case with om is a good and typical example of a situation where more statistics would be of great advantage in improving and refining the rules, but where there will always be a rest class of insoluble cases and cases which are contrary to the rules.", "labels": [], "entities": []}, {"text": "Still, there are not many words: in the sample texts where the tagging done by lexicon is wrong.", "labels": [], "entities": []}, {"text": "This is remarkable, as the lexicon always assigns exactly one tag, not a set of tags, even if a word is ambiguous.", "labels": [], "entities": []}, {"text": "The morphological analysis carries out a very substantial task and, consequently, is a large source of errors.", "labels": [], "entities": [{"text": "morphological analysis", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.8651235699653625}]}, {"text": "One example is the noun bevis 'proof', which occurs several times in one of the texts.", "labels": [], "entities": []}, {"text": "It has a very prototypical verbal look, with the prefix be-, a monosyllabic stem seemingly ending in a vowel and followed by a passive -s, exactly like the verbs beses, beg/ts, betros, bebos, etc.", "labels": [], "entities": []}, {"text": "It is justa coincidence that the verb is bevisa, not bevi, and the noun is formed by a rare deletion rather that by adding a derivational ending.", "labels": [], "entities": []}, {"text": "A similar error is when the noun resultat 'result' is treated as a supine verb, as -at is a very common, very productive supine ending.", "labels": [], "entities": []}, {"text": "Disambiguation of course also adds many errors, as the patterns for those rules are less clear than the patterns for word structure, and as all errors, ambiguities and doubtful cases from earlier programs accumulate as the processing proceeds.", "labels": [], "entities": []}, {"text": "Often it is the ambiguousmarked words that are disambiguated wrongly or not at all.", "labels": [], "entities": []}, {"text": "In one of the texts there is for instance the alleged finite verb djungler 'jungles'.", "labels": [], "entities": []}, {"text": "A foregoing adverb has caused the ambiguous ending -er to be classified as signalling present tense verb rather than plural noun.", "labels": [], "entities": []}, {"text": "The remaining ambiguities also often belong to this class of words, but on the whole, it is surprising how few of the ambiguous-marked words that remain in the output.", "labels": [], "entities": []}, {"text": "The set of words that are still unmarked by the end of the process is comparatively large.", "labels": [], "entities": []}, {"text": "A possible heuristic might be to make them all nouns, as that is the largest open word class, and as most singular and many plural indefinite nouns have no clear morphological characteristics in Swedish.", "labels": [], "entities": []}, {"text": "A closer look at the unmarked words reveals that this is not such a good idea: of 69 unmarked words, 25 are nouns, 18 adjectives, and 18 verbs.", "labels": [], "entities": []}, {"text": "One is a numeral, one is a very rare preposition that is a homograph of a slightly more common noun, 2 are adverbs with homographs in other word classes, and 2 are the first part of conjoined compounds, comparable to expressions like 'pre-or postprocessing'.", "labels": [], "entities": []}, {"text": "The hyphenated first part gets no mark in these cases.", "labels": [], "entities": [{"text": "mark", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9608694314956665}]}, {"text": "They could be done away with by manual preprocessing, as also the not infrequent cases occurring in headlines, where syntactic structure is often too reduced to be of any help.", "labels": [], "entities": []}, {"text": "For the rest, a careful examination of their word structure and context seems promising, but more data is needed.", "labels": [], "entities": []}, {"text": "By this, I hope to have shown that parsing without lexicon is both possible and interesting, and can give insights about the structure of natural languages that can be of use also in less restricted systems.", "labels": [], "entities": []}], "tableCaptions": []}