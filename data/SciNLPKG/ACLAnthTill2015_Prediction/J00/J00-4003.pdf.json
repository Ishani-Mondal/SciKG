{"title": [{"text": "An Empirically Based System for Processing Definite Descriptions", "labels": [], "entities": []}], "abstractContent": [{"text": "We present an implemented system for processing definite descriptions in arbitrary domains.", "labels": [], "entities": []}, {"text": "The design of the system is based on the results of a corpus analysis previously reported, which highlighted the prevalence of discourse-new descriptions in newspaper corpora.", "labels": [], "entities": []}, {"text": "The annotated corpus was used to extensively evaluate the proposed techniques for matching definite descriptions with their antecedents, discourse segmentation, recognizing discourse-new descriptions, and suggesting anchors for bridging descriptions.", "labels": [], "entities": [{"text": "discourse segmentation", "start_pos": 137, "end_pos": 159, "type": "TASK", "confidence": 0.7223812937736511}]}], "introductionContent": [{"text": "Most models of definite description processing proposed in the literature tend to emphasise the anaphoric role of these elements.", "labels": [], "entities": [{"text": "definite description processing", "start_pos": 15, "end_pos": 46, "type": "TASK", "confidence": 0.6966491540273031}]}, {"text": "1 is perhaps the best formalization of this type of theory).", "labels": [], "entities": []}, {"text": "This approach is challenged by the results of experiments we reported previously (, in which subjects were asked to classify the uses of definite descriptions in Wall Street Journal articles according to schemes derived from proposals by and.", "labels": [], "entities": [{"text": "Wall Street Journal articles", "start_pos": 162, "end_pos": 190, "type": "DATASET", "confidence": 0.8689971268177032}]}, {"text": "The results of these experiments indicated that definite descriptions are not primarily anaphoric; about half of the time they are used to introduce anew entity in the discourse.", "labels": [], "entities": []}, {"text": "In this paper, we present an implemented system for processing definite descriptions based on the results of that earlier study.", "labels": [], "entities": []}, {"text": "In our system, techniques for recognizing discourse-new descriptions play a role as important as techniques for identifying the antecedent of anaphoric ones.", "labels": [], "entities": []}, {"text": "A central characteristic of the work described here is that we intended from the start to develop a system whose performance could be evaluated using the texts annotated in the experiments mentioned above.", "labels": [], "entities": []}, {"text": "Assessing the performance of an NLP system on a large number of examples is increasingly seen as a much more thorough evaluation of its performance than trying to come up with counterexamples; it is considered essential for language engineering applications.", "labels": [], "entities": []}, {"text": "These advantages are thought by many to offset some of the obvious disadvantages of this way of developing NLP theories--in particular, the fact that, given the current state of language processing technology, many hypotheses of interest cannot be tested yet (see below).", "labels": [], "entities": []}, {"text": "As a result, quantitative evaluation is now commonplace in areas of language engineering such as parsing, and quantitative evaluation techniques are being proposed for semantic interpretation as well, for example, at the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7), which also included evaluations of systems on the so-called coreference task, a subtask of which is the resolution of definite descriptions.", "labels": [], "entities": [{"text": "parsing", "start_pos": 97, "end_pos": 104, "type": "TASK", "confidence": 0.8437269926071167}, {"text": "semantic interpretation", "start_pos": 168, "end_pos": 191, "type": "TASK", "confidence": 0.731901615858078}, {"text": "MUC-6", "start_pos": 274, "end_pos": 279, "type": "DATASET", "confidence": 0.8401950001716614}, {"text": "MUC-7", "start_pos": 284, "end_pos": 289, "type": "DATASET", "confidence": 0.49561411142349243}, {"text": "resolution of definite descriptions", "start_pos": 396, "end_pos": 431, "type": "TASK", "confidence": 0.8224438279867172}]}, {"text": "The system we present was developed to be evaluated in a quantitative fashion, as well, but because of the problems concerning agreement between annotators observed in our previous study, we evaluated the system both by measuring precision/recall against a \"gold standard,\" as done in MUC, and by measuring agreement between the annotations produced by the system and those proposed by the annotators.", "labels": [], "entities": [{"text": "precision", "start_pos": 230, "end_pos": 239, "type": "METRIC", "confidence": 0.9985219836235046}, {"text": "recall", "start_pos": 240, "end_pos": 246, "type": "METRIC", "confidence": 0.8730137348175049}, {"text": "MUC", "start_pos": 285, "end_pos": 288, "type": "DATASET", "confidence": 0.8819766640663147}]}, {"text": "The decision to develop a system that could be quantitatively evaluated on a large number of examples resulted in an important constraint: we could not make use of inference mechanisms such as those assumed by traditional computational theories of definite description resolution (e.g.,.", "labels": [], "entities": [{"text": "definite description resolution", "start_pos": 248, "end_pos": 279, "type": "TASK", "confidence": 0.6759602328141531}]}, {"text": "Too many facts and axioms would have to be encoded by hand for theories of this type to be tested even on a medium-sized corpus.", "labels": [], "entities": []}, {"text": "Our system, therefore, is based on a shallow-processing approach more radical even than that attempted by the first advocate of this approach,, or by the systems that participated in the MUC evaluations), since we made no attempt to fine-tune the system to maximize performance on a particular domain.", "labels": [], "entities": [{"text": "MUC", "start_pos": 187, "end_pos": 190, "type": "DATASET", "confidence": 0.7205362915992737}]}, {"text": "The system relies only on structural information, on the information provided by preexisting lexical sources such as WordNet), on minimal amounts of general hand-coded information, or on information that could be acquired automatically from a corpus.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 117, "end_pos": 124, "type": "DATASET", "confidence": 0.9596518874168396}]}, {"text": "As a result, the system does not really have the resources to correctly resolve those definite descriptions whose interpretation does require complex reasoning (we grouped these in what we call the \"bridging\" class).", "labels": [], "entities": []}, {"text": "We nevertheless developed heuristic techniques for processing these types of definites as well, the idea being that these heuristics may provide a baseline against which the gains in performance due to the use of commonsense knowledge can be assessed more clearly.", "labels": [], "entities": []}, {"text": "The paper is organized as follows: We first summarize the results of our previous corpus study and then discuss the model of deftnite description processing that we adopted as a result of that work and the general architecture of the system (Section 3).", "labels": [], "entities": []}, {"text": "In Section 4 we discuss the heuristics that we developed for resolving anaphoric definite descriptions, recognizing discourse-new descriptions, and processing bridging descriptions, and, in Section 5, how the performance of these heuristics was evaluated using the annotated corpus.", "labels": [], "entities": []}, {"text": "Finally, we present the final configuration of the two versions of the system that we developed (Section 6), review other systems that perform similar tasks (Section 7), and present our conclusions and indicate future work (Section 8).", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we discuss the tests we ran to arrive at a final configuration of the system.", "labels": [], "entities": []}, {"text": "The performance of the heuristics discussed in Section 4 was evaluated by comparing the results of the system with the human annotation of the corpus produced during the experiments discussed in.", "labels": [], "entities": []}, {"text": "Several variants of our heuristics were tried using Corpus 1 as training data; after deciding upon an optimal version, our algorithms were evaluated using Corpus 2 as test data.", "labels": [], "entities": []}, {"text": "Because our proposals concerning bridging descriptions are much less developed than those concerning anaphoric descriptions and discourse-new descriptions, we ran separate evaluations of two versions of the system: Version 1, which does not attempt to resolve bridging descriptions, and Version 2, which does; we will point out below which version of the system is considered in each evaluation.", "labels": [], "entities": []}, {"text": "The fact that the annotators working on our corpus did not always agree either on the classification of a definite description or on its anchor raises the question of how to evaluate the performance of our system.", "labels": [], "entities": []}, {"text": "We tried two different approaches: evaluating the performance of the system by measuring its precision and recall against a standardized annotation based on majority voting (as done in MUC), and measuring the extent of the system's agreement with the rest of the annotators by means of the same metric used to measure agreement among the annotators themselves (the kappa statistic).", "labels": [], "entities": [{"text": "precision", "start_pos": 93, "end_pos": 102, "type": "METRIC", "confidence": 0.9993150234222412}, {"text": "recall", "start_pos": 107, "end_pos": 113, "type": "METRIC", "confidence": 0.998687207698822}, {"text": "MUC", "start_pos": 185, "end_pos": 188, "type": "DATASET", "confidence": 0.8668621778488159}]}, {"text": "We used the first form of evaluation to measure both the performance of the single heuristics and the performance of the system as a whole; the agreement measure was only used to measure the overall performance of the system.", "labels": [], "entities": []}, {"text": "We discuss each of these in turn.", "labels": [], "entities": []}, {"text": "as 5.1.1 Precision and Recall.", "labels": [], "entities": [{"text": "Precision", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.9858578443527222}, {"text": "Recall", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.9601175785064697}]}, {"text": "Recall and precision are measures commonly used in Information Retrieval to evaluate a system's performance.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9747568368911743}, {"text": "precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9930644631385803}, {"text": "Information Retrieval", "start_pos": 51, "end_pos": 72, "type": "TASK", "confidence": 0.8306243717670441}]}, {"text": "Recall is the percentage of correct answers reported by the system in relation to the number of cases indicated by the annotated corpus: R = number of correct responses number of cases whereas precision is the percentage of correctly reported results in relation to the total reported: p = number of correct responses number of responses These two measures maybe combined to form one measure of performance, the F measure, which is computed as follows: F -(W + 1)RP  Annotation.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9861845374107361}, {"text": "precision", "start_pos": 193, "end_pos": 202, "type": "METRIC", "confidence": 0.9993799924850464}, {"text": "F measure", "start_pos": 412, "end_pos": 421, "type": "METRIC", "confidence": 0.986364483833313}, {"text": "F -(W + 1)RP  Annotation", "start_pos": 453, "end_pos": 477, "type": "METRIC", "confidence": 0.9069982543587685}]}, {"text": "The precision and recall figures for the different variants of the system were obtained by comparing the classification produced by each version with a standardized annotation, extracted from the annotations produced by our human annotators by majority judgement: whenever at least two of the three coders agreed on a class, that class was chosen.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9995591044425964}, {"text": "recall", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.9993816614151001}]}, {"text": "Details of how the standard annotation was obtained are given in Vieira (1998).", "labels": [], "entities": []}, {"text": "The system's performance as a classifier was automatically evaluated against the standard annotation of the corpus as follows.", "labels": [], "entities": []}, {"text": "Each NP in a text is given an index: A house1\u00b06...", "labels": [], "entities": [{"text": "A house1\u00b06", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.934515655040741}]}, {"text": "When a text is annotated or processed, the coder or system associates each index of a definite description with a type of use; both the standard annotation and the system's output are represented as Prolog assertions. a. system: b.", "labels": [], "entities": []}, {"text": "As mentioned above, we implemented two versions of the system.", "labels": [], "entities": []}, {"text": "Version 1 only resolves direct anaphora and identifies discourse-new descriptions; Version 2 also deals with bridging descriptions.", "labels": [], "entities": []}, {"text": "Both versions of the system have at their core a decision tree in which the heuristics discussed in the previous sections are tried in a fixed order to classify a certain definite description and find its anchor.", "labels": [], "entities": []}, {"text": "Determining the optimal order of application of the heuristics in the decision tree is crucial to the performance of the system.", "labels": [], "entities": []}, {"text": "In both versions of the system we used a decision tree developed by hand on the basis of extensive evaluation; we also attempted to determine the order of application automatically, by means of decision tree learning algorithms.", "labels": [], "entities": []}, {"text": "In this section we first present the hand-crafted decision tree and the results obtained using this decision tree for Version 1 and Version 2; we then present the results concerning the agreement between system and annotators, and we briefly discuss the results obtained using the decision tree acquired automatically.", "labels": [], "entities": []}, {"text": "The performance of the learned decision tree was compared with that of the algorithm we arrived at by trial and error as follows: The first 14 texts of Corpus 1 (845 descriptions) were used as training data to generate the decision tree.", "labels": [], "entities": []}, {"text": "We then tested the learned algorithm over the other 6 texts of that corpus (195 instances of definite descriptions).", "labels": [], "entities": []}, {"text": "Two different tests were undertaken: first, we gave as input to the learning algorithm all cases classified as direct anaphora, discourse-new, or bridging, 818 in total (this test produces the decision tree presented in the previous section); in a second test, the algorithm was trained only with direct anaphora and discourse-new descriptions (639 descriptions); all cases classified as bridging, idiom, or doubt in the standard annotation were not given as input in the learning process.", "labels": [], "entities": []}, {"text": "This algorithm was then only able to classify descriptions as one of those two classes.", "labels": [], "entities": []}, {"text": "The resulting decision tree classifies descriptions with a same-head antecedent as anaphoric; all the rest as discourse-new.", "labels": [], "entities": []}, {"text": "Here we present the results evaluated all together, considering the system as a classifier only, i.e., without considering the tasks of anaphora resolution and of identification of discourse-new descriptions separately.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 136, "end_pos": 155, "type": "TASK", "confidence": 0.7426321506500244}]}, {"text": "The output produced by the learned algorithm is compared to the standard annotation.", "labels": [], "entities": []}, {"text": "Since the learned algorithm classifies all cases, the number of responses is equal to the number of cases, as a consequence, recall is the same as precision, and so is the F measure.", "labels": [], "entities": [{"text": "recall", "start_pos": 125, "end_pos": 131, "type": "METRIC", "confidence": 0.9994072914123535}, {"text": "precision", "start_pos": 147, "end_pos": 156, "type": "METRIC", "confidence": 0.9995062351226807}, {"text": "F measure", "start_pos": 172, "end_pos": 181, "type": "METRIC", "confidence": 0.9806374311447144}]}, {"text": "The tests over 6 texts with 195 definite descriptions gave the following results: \u2022 R = P = F = 69% when the algorithm was trained with three classes; \u2022 R = P = F = 75%, when training with two classes only.", "labels": [], "entities": [{"text": "R", "start_pos": 84, "end_pos": 85, "type": "METRIC", "confidence": 0.9764618277549744}, {"text": "F", "start_pos": 92, "end_pos": 93, "type": "METRIC", "confidence": 0.5847744941711426}, {"text": "F", "start_pos": 161, "end_pos": 162, "type": "METRIC", "confidence": 0.611338198184967}]}, {"text": "The best results were achieved by the algorithm trained for two classes only.", "labels": [], "entities": []}, {"text": "This is not surprising, especially considering how difficult it was for our subjects to distinguish between discourse-new and bridging descriptions.", "labels": [], "entities": []}, {"text": "The hand-crafted decision tree (Version 2) achieved 62% recall and 85% precision (F = 71.70%) on those same texts: i.e., a higher precision, but a lower F measure, due to a lower recall, since---unlike the learned algorithm--it does not classify all instances of definite descriptions.", "labels": [], "entities": [{"text": "recall", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.9991645812988281}, {"text": "precision", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.998437225818634}, {"text": "F", "start_pos": 82, "end_pos": 83, "type": "METRIC", "confidence": 0.986160397529602}, {"text": "precision", "start_pos": 130, "end_pos": 139, "type": "METRIC", "confidence": 0.9935659766197205}, {"text": "F", "start_pos": 153, "end_pos": 154, "type": "METRIC", "confidence": 0.996707022190094}, {"text": "recall", "start_pos": 179, "end_pos": 185, "type": "METRIC", "confidence": 0.9976451992988586}]}, {"text": "If, however, we take the class discourse-new as a default for all cases of definite descriptions not resolved by the system, recall, precision, and F value go to 77%, slightly higher than the rates achieved by the decision tree produced by ID3.", "labels": [], "entities": [{"text": "recall", "start_pos": 125, "end_pos": 131, "type": "METRIC", "confidence": 0.9997426867485046}, {"text": "precision", "start_pos": 133, "end_pos": 142, "type": "METRIC", "confidence": 0.9995152950286865}, {"text": "F value", "start_pos": 148, "end_pos": 155, "type": "METRIC", "confidence": 0.9808676540851593}]}, {"text": "As the learned decision tree has the search fora same-head antecedent as the first test, we modified our algorithm to work in the same way, and tested it again with the two corpora.", "labels": [], "entities": []}, {"text": "The results with this configuration were: \u2022 R = 0.75, P = 0.87, F = 0.80, for the training data (compared with R = 0.76, P = 0.88, F = 0.81) ; \u2022 R = 0.59, P = 0.83, F = 0.69, for the test data (compared with R = 0.62, P = 0.83, F = 0.71).", "labels": [], "entities": [{"text": "R", "start_pos": 44, "end_pos": 45, "type": "METRIC", "confidence": 0.9863693714141846}, {"text": "F", "start_pos": 64, "end_pos": 65, "type": "METRIC", "confidence": 0.994496762752533}, {"text": "F", "start_pos": 165, "end_pos": 166, "type": "METRIC", "confidence": 0.9690301418304443}, {"text": "F", "start_pos": 228, "end_pos": 229, "type": "METRIC", "confidence": 0.9483940601348877}]}, {"text": "In other words, the results were about the same, although a slightly better performance was obtained when the tests to identify discourse-new descriptions were tried first.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3  Distribution of prepositional phrases and relative clauses.", "labels": [], "entities": []}, {"text": " Table 7. This table shows that by combining  the recency and loose segmentation approaches to segmentation we obtain a better  trade-off between recall and precision than using each heuristic separately. The version  with higher F value in", "labels": [], "entities": [{"text": "recall", "start_pos": 146, "end_pos": 152, "type": "METRIC", "confidence": 0.998769223690033}, {"text": "precision", "start_pos": 157, "end_pos": 166, "type": "METRIC", "confidence": 0.9926108717918396}, {"text": "F", "start_pos": 230, "end_pos": 231, "type": "METRIC", "confidence": 0.9987015724182129}]}, {"text": " Table 16. It is interesting to  note that the semantic relations found in this automatic search were not always those  observed in our manual analysis.", "labels": [], "entities": []}, {"text": " Table 17  Evaluation of the encoding of semantic relations in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 63, "end_pos": 70, "type": "DATASET", "confidence": 0.9281374216079712}]}, {"text": " Table 20  Evaluation of the bridging heuristics all together.", "labels": [], "entities": []}, {"text": " Table 20. The table lists the number of acceptable anchors and the  number of false positives found by each heuristic. Note that the system sometimes  finds anchors that are not those identified manually, but are nevertheless acceptable.  We found fewer bridging relations than the number we observed in the corpus  analysis", "labels": [], "entities": []}]}