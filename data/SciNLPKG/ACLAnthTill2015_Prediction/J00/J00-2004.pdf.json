{"title": [{"text": "Models of Translational Equivalence among Words", "labels": [], "entities": [{"text": "Translational Equivalence among Words", "start_pos": 10, "end_pos": 47, "type": "TASK", "confidence": 0.9397585541009903}]}], "abstractContent": [{"text": "Parallel texts (bitexts) have properties that distinguish them from other kinds of parallel data.", "labels": [], "entities": []}, {"text": "First, most words translate to only one other word.", "labels": [], "entities": []}, {"text": "Second, bitext correspondence is typically only partial-many words in each text have no clear equivalent in the other text.", "labels": [], "entities": []}, {"text": "This article presents methods for biasing statistical translation models to reflect these properties.", "labels": [], "entities": [{"text": "biasing statistical translation", "start_pos": 34, "end_pos": 65, "type": "TASK", "confidence": 0.6979095538457235}]}, {"text": "Evaluation with respect to independent human judgments has confirmed that translation models biased in this fashion are significantly more accurate than a baseline knowledge-free model.", "labels": [], "entities": []}, {"text": "This article also shows how a statistical translation model can take advantage of preexisting knowledge that might be available about particular language pairs.", "labels": [], "entities": [{"text": "statistical translation", "start_pos": 30, "end_pos": 53, "type": "TASK", "confidence": 0.57305708527565}]}, {"text": "Even the simplest kinds of language-specific knowledge, such as the distinction between content words and function words, are shown to reliably boost translation model performance on some tasks.", "labels": [], "entities": [{"text": "translation model", "start_pos": 150, "end_pos": 167, "type": "TASK", "confidence": 0.8913248479366302}]}, {"text": "Statistical models that reflect knowledge about the model domain combine the best of both the rationalist and empiricist paradigms.", "labels": [], "entities": []}], "introductionContent": [{"text": "The idea of a computer system for translating from one language to another is almost as old as the idea of computer systems.", "labels": [], "entities": []}, {"text": "Warren Weaver wrote about mechanical translation as early as 1949.", "labels": [], "entities": [{"text": "mechanical translation", "start_pos": 26, "end_pos": 48, "type": "TASK", "confidence": 0.7641366720199585}]}, {"text": "More recently, suggested that it maybe possible to construct machine translation systems automatically.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.7468747198581696}]}, {"text": "Instead of codifying the human translation process from introspection, Brown and his colleagues proposed machine learning techniques to induce models of the process from examples of its input and output.", "labels": [], "entities": []}, {"text": "The proposal generated much excitement, because it held the promise of automating a task that forty years of research have proven very labor-intensive and error-prone.", "labels": [], "entities": []}, {"text": "Yet very few other researchers have taken up the cause, partly because approach was quite a departure from the paradigm in vogue at the time.", "labels": [], "entities": []}, {"text": "Formally, built statistical models of translational equivalence (or translation models 1, for short).", "labels": [], "entities": [{"text": "translational equivalence", "start_pos": 38, "end_pos": 63, "type": "TASK", "confidence": 0.9283213317394257}]}, {"text": "In the context of computational linguistics, translational equivalence is a relation that holds between two expressions with the same meaning, where the two expressions are in different languages.", "labels": [], "entities": [{"text": "translational equivalence", "start_pos": 45, "end_pos": 70, "type": "TASK", "confidence": 0.9385240077972412}]}, {"text": "Empirical estimation of statistical translation models is typically based on parallel texts or bitexts--pairs of texts that are translations of each other.", "labels": [], "entities": [{"text": "Empirical estimation of statistical translation", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.6226467847824096}]}, {"text": "As with all statistical models, the best translation models are those whose parameters correspond best with the sources of variance in the data.", "labels": [], "entities": []}, {"text": "Probabilistic translation models whose parameters reflect universal properties of translational equivalence and/or existing knowledge about particular languages and language pairs benefit from the best of both the empiricist and rationalist traditions.", "labels": [], "entities": []}, {"text": "This article presents three such models, along with methods for efficiently estimating their parameters.", "labels": [], "entities": []}, {"text": "Each new method is designed to account for an additional universal property of translational equivalence in bitexts: . .", "labels": [], "entities": []}, {"text": "Most word tokens translate to only one word token.", "labels": [], "entities": []}, {"text": "I approximate this tendency with a one-to-one assumption.", "labels": [], "entities": []}, {"text": "Most text segments are not translated word-for-word.", "labels": [], "entities": []}, {"text": "I build an explicit noise model.", "labels": [], "entities": []}, {"text": "Different linguistic objects have statistically different behavior in translation.", "labels": [], "entities": []}, {"text": "I show away to condition translation models on different word classes to help account for the variety.", "labels": [], "entities": []}, {"text": "Quantitative evaluation with respect to independent human judgments has shown that each of these three estimation biases significantly improves translation model accuracy over a baseline knowledge-free model.", "labels": [], "entities": [{"text": "translation model", "start_pos": 144, "end_pos": 161, "type": "TASK", "confidence": 0.8969106078147888}, {"text": "accuracy", "start_pos": 162, "end_pos": 170, "type": "METRIC", "confidence": 0.8588376045227051}]}, {"text": "However, these biases will not produce the best possible translation models by themselves.", "labels": [], "entities": []}, {"text": "Anyone attempting to build an optimal translation model should infuse it with all available knowledge sources, including syntactic, dictionary, and cognate information.", "labels": [], "entities": []}, {"text": "My goal here is only to demonstrate the value of some previously unused kinds of information that are always available for translation modeling, and to show how these information sources can be integrated with others.", "labels": [], "entities": [{"text": "translation modeling", "start_pos": 123, "end_pos": 143, "type": "TASK", "confidence": 0.9816906154155731}]}, {"text": "A review of some previously published translation models follows an introduction to translation model taxonomy.", "labels": [], "entities": []}, {"text": "The core of the article is a presentation of the model estimation biases described above.", "labels": [], "entities": []}, {"text": "The last section reports the results of experiments designed to evaluate these innovations.", "labels": [], "entities": []}, {"text": "Throughout this article, I shall use CA\u00a3\u00a3/~GT4A/~C letters to denote entire text corpora and other sets of sets, CAPITAL letters to denote collections, including sequences and bags, and italics for scalar variables.", "labels": [], "entities": []}, {"text": "I shall also distinguish between types and tokens by using bold font for the former and plain font for the latter.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section compares translation model estimation methods A, B, and C to each other and to Brown et al.'s (1993b) Model 1.", "labels": [], "entities": [{"text": "translation model estimation", "start_pos": 22, "end_pos": 50, "type": "TASK", "confidence": 0.928754190603892}]}, {"text": "To reiterate, Model 1 is based on co-occurrence information only; Method A is based on the one-to-one assumption; Method B adds the \"one sense per collocation\" hypothesis to Method A; Method C conditions the auxiliary parameters of Method B on various word classes.", "labels": [], "entities": []}, {"text": "Whereas Methods A and B and Model 1 were fully specified in Section 4.3.1 and Section 5, the latter section described a variety of features on which Method C might classify links.", "labels": [], "entities": []}, {"text": "For the purposes of the experiments described in this article, Method C employed the simple classification in for both languages in the bitext.", "labels": [], "entities": []}, {"text": "All classification was performed by table lookup; no context-aware part-of-speech tagger was used.", "labels": [], "entities": []}, {"text": "In particular, words that were ambiguous between open classes and closed classes were always deemed to be in the closed class.", "labels": [], "entities": []}, {"text": "The only language-specific knowledge involved in this classification Word classes used by Method C for the experiments described in this article.", "labels": [], "entities": []}, {"text": "Link classes were constructed by taking the cross-product of the word classes.", "labels": [], "entities": []}, {"text": "End-Of-Sentence punctuation End-Of-Phrase punctuation, such as commas and colons Subordinate Clause Markers, such as \" and ( Symbols, such as ~ and * the NULL word, in a class by itself Content words: nouns, adjectives, adverbs, non-auxiliary verbs all other words, i.e., function words method is the list of function words in class F.", "labels": [], "entities": [{"text": "End-Of-Sentence punctuation End-Of-Phrase punctuation", "start_pos": 0, "end_pos": 53, "type": "METRIC", "confidence": 0.7115699201822281}]}, {"text": "Certainly, more sophisticated word classification methods could produce better models, but even the simple classification in should suffice to demonstrate the method's potential.", "labels": [], "entities": [{"text": "word classification", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.764944463968277}]}, {"text": "Until now, translation models have been evaluated either subjectively (e.g. White and O'Connell 1993) or using relative metrics, such as perplexity with respect to other models ().", "labels": [], "entities": [{"text": "translation", "start_pos": 11, "end_pos": 22, "type": "TASK", "confidence": 0.9721936583518982}]}, {"text": "Objective and more accurate tests can be carried out using a \"gold standard.\"", "labels": [], "entities": []}, {"text": "I hired bilingual annotators to link roughly 16,000 corresponding words between on-line versions of the Bible in French and English.", "labels": [], "entities": []}, {"text": "This bitext was selected to facilitate widespread use and standardization (see for details).", "labels": [], "entities": [{"text": "standardization", "start_pos": 58, "end_pos": 73, "type": "TASK", "confidence": 0.9058413505554199}]}, {"text": "The entire Bible bitext comprised 29,614 verse pairs, of which 250 verse pairs were hand-linked using a specially developed annotation tool.", "labels": [], "entities": [{"text": "Bible bitext", "start_pos": 11, "end_pos": 23, "type": "DATASET", "confidence": 0.8825506865978241}]}, {"text": "The annotation style guide (Melamed 1998b) was based on the intuitions of the annotators, so it was not biased towards any particular translation model.", "labels": [], "entities": []}, {"text": "The annotation was replicated five times by seven different annotators.", "labels": [], "entities": []}, {"text": "Each of the four methods was used to estimate a word-to-word translation model from the 29,614 verse pairs in the Bible bitext.", "labels": [], "entities": [{"text": "word-to-word translation", "start_pos": 48, "end_pos": 72, "type": "TASK", "confidence": 0.6941682398319244}, {"text": "Bible bitext", "start_pos": 114, "end_pos": 126, "type": "DATASET", "confidence": 0.929139107465744}]}, {"text": "All methods were deemed to have converged when less than .0001 of the translational probability distribution changed from one iteration to the next.", "labels": [], "entities": []}, {"text": "The links assigned by each of methods A, B, and C in the last iteration were normalized into joint probability distributions using Equation 19.", "labels": [], "entities": []}, {"text": "I shall refer to these joint distributions as Model A, Model B, and Model C, respectively.", "labels": [], "entities": []}, {"text": "Each of the joint probability distributions was further normalized into two conditional probability distributions, one in each direction.", "labels": [], "entities": []}, {"text": "Since Model 1 is inherently directional, its conditional probability distributions were estimated separately in each direction, instead of being derived from a joint distribution.", "labels": [], "entities": []}, {"text": "The four models' predictions were compared to the gold standard annotations.", "labels": [], "entities": []}, {"text": "Each model guessed one translation (either stochastically or deterministically, depending on the task) for each word on one side of the gold standard bitext.", "labels": [], "entities": []}, {"text": "Therefore, precision = recall here, and I shall refer to the results simply as \"percent correct.\"", "labels": [], "entities": [{"text": "precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9997417330741882}, {"text": "recall", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.9996817111968994}]}, {"text": "The accuracy of each model was averaged over the two directions of translation: English to French and French to English.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9996401071548462}]}, {"text": "The five-fold replication of annotations in the test data enabled computation of the statistical significance of the differences in model accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 138, "end_pos": 146, "type": "METRIC", "confidence": 0.9692183136940002}]}, {"text": "The statistical significance of all results in this section was measured at the c~ --.05 level, using the Wilcoxon signed ranks test.", "labels": [], "entities": []}, {"text": "Although the models were evaluated on part of the same bitext on which they were trained, the evaluations were with respect to the translational equivalence relation hidden in this bitext, not with respect to any of the bitext's visible features.", "labels": [], "entities": []}, {"text": "Such testing on training data is standard practice for unsupervised learning algorithms, where the objective is to compare several methods.", "labels": [], "entities": []}, {"text": "Of course, performance would degrade on previously unseen data.", "labels": [], "entities": []}, {"text": "In addition to the different translation models, there were two other independent variables in the experiment: method of translation and whether function words were included.", "labels": [], "entities": []}, {"text": "Some applications, such as query translation for CLIR, don't care about function words.", "labels": [], "entities": [{"text": "query translation", "start_pos": 27, "end_pos": 44, "type": "TASK", "confidence": 0.8418004512786865}]}, {"text": "To get a sense of the relative effectiveness of the different translation model estimation methods when function words are taken out of the equation, I removed from the gold standard all link tokens where one or both of the linked words were closed-class words.", "labels": [], "entities": [{"text": "translation model estimation", "start_pos": 62, "end_pos": 90, "type": "TASK", "confidence": 0.8724863529205322}]}, {"text": "Then, I removed all closed-class words (including nonalphabetic symbols) from the models and renormalized the conditional probabilities.", "labels": [], "entities": []}, {"text": "The method of translation was either single-best or whole distribution.", "labels": [], "entities": [{"text": "translation", "start_pos": 14, "end_pos": 25, "type": "TASK", "confidence": 0.9854710102081299}]}, {"text": "Singlebest translation is the kind that somebody might use to get the gist of a foreignlanguage document.", "labels": [], "entities": [{"text": "Singlebest translation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.5917602330446243}]}, {"text": "The input to the task was one side of the gold standard bitext.", "labels": [], "entities": []}, {"text": "The output was the model's single best guess about the translation of each word in the input, together with the input word.", "labels": [], "entities": []}, {"text": "In other words, each model produced link tokens consisting of input words and their translations.", "labels": [], "entities": []}, {"text": "For some applications, it is insufficient to guess only the single most likely translation of each word in the input.", "labels": [], "entities": []}, {"text": "The model is expected to output the whole distribution of possible translations for each input word.", "labels": [], "entities": []}, {"text": "This distribution is then combined with other distributions that are relevant to the application.", "labels": [], "entities": []}, {"text": "For example, for cross-language information retrieval, the translational distribution can be combined with the distribution of term frequencies.", "labels": [], "entities": [{"text": "cross-language information retrieval", "start_pos": 17, "end_pos": 53, "type": "TASK", "confidence": 0.7330012520154318}]}, {"text": "For statistical machine translation, the translational distribution can be decoded with a source language model ().", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 4, "end_pos": 35, "type": "TASK", "confidence": 0.6743143498897552}]}, {"text": "To predict how the different models might perform in such applications, the whole distribution task was to generate a whole set of links from each input word, weighted according to the probability assigned by the model to each of the input word's translations.", "labels": [], "entities": []}, {"text": "Each model was tested on this task with and without function words.", "labels": [], "entities": []}, {"text": "The mean results are plotted in Figures 4 and 5 with 95% confidence intervals.", "labels": [], "entities": []}, {"text": "All four graphs in these figures are on the same scale to facilitate comparison.", "labels": [], "entities": []}, {"text": "On both tasks involving the entire vocabulary, each of the biases presented in this article improves the efficiency of modeling the available training data.", "labels": [], "entities": []}, {"text": "When closed-class words were ignored, Model 1 performed better than Method A, because open-class words are more likely to violate the one-to-one assumption.", "labels": [], "entities": []}, {"text": "However, the explicit noise model in Methods B and C boosted their scores significantly higher than Model 1 and Method A. Method B was better than Method C at choosing the single best open-class links, and the situation was reversed for the whole distribution of open-class links.", "labels": [], "entities": []}, {"text": "However, the differences in performance between these two methods were tiny on the open-class tasks, because they left only two classes for Method C to distinguish: content words and NULLS.", "labels": [], "entities": []}, {"text": "Most of the scores on the whole distribution task were lower than their counterparts on the single-best translation task, because it is more difficult for any statistical method to correctly model the less common translations.", "labels": [], "entities": []}, {"text": "The \"best\" translations are usually the most common.", "labels": [], "entities": []}, {"text": "To study how the benefits of the various biases vary with training corpus size, I evaluated Models A, B, C, and 1 on the whole distribution translation task, after training them on three different-size subsets of the Bible bitext.", "labels": [], "entities": [{"text": "distribution translation task", "start_pos": 127, "end_pos": 156, "type": "TASK", "confidence": 0.8177750110626221}, {"text": "Bible bitext", "start_pos": 217, "end_pos": 229, "type": "DATASET", "confidence": 0.918766975402832}]}, {"text": "The first subset consisted of only the 250 verse pairs in the gold standard.", "labels": [], "entities": []}, {"text": "The second subset included these 250 plus another random sample of 2,250 fora total of 2,500, an order of magnitude larger than the first subset.", "labels": [], "entities": []}, {"text": "The third subset contained all 29,614 verse pairs in the Bible bitext, roughly an order of magnitude larger than the second subset.", "labels": [], "entities": [{"text": "Bible bitext", "start_pos": 57, "end_pos": 69, "type": "DATASET", "confidence": 0.9449759125709534}]}, {"text": "All models were compared to the five gold standard annotations, and the scores were  An important application of statistical translation models is to help lexicographers compile bilingual dictionaries.", "labels": [], "entities": []}, {"text": "Dictionaries are written to answer the question, \"What are the possible translations of X?\"", "labels": [], "entities": []}, {"text": "This is a question about link types, rather than about link tokens.", "labels": [], "entities": []}, {"text": "Evaluation by link type is a thorny issue.", "labels": [], "entities": []}, {"text": "Human judges often disagree about the degree to which context should play a role in judgments of translational equivalence.", "labels": [], "entities": [{"text": "translational equivalence", "start_pos": 97, "end_pos": 122, "type": "TASK", "confidence": 0.8111675977706909}]}, {"text": "lay judges would not consider instituer a correct French translation of appoint.", "labels": [], "entities": []}, {"text": "In actual translations, however, when the object of the verb is commission, task force, panel, etc., English appoint is usually translated into French as instituer.", "labels": [], "entities": []}, {"text": "To account for this kind of context-dependent translational equivalence, link types must be evaluated with respect to the bitext whence they were induced.", "labels": [], "entities": []}, {"text": "I performed a post hoc evaluation of the link types produced by an earlier version of Method B (Melamed 1996b).", "labels": [], "entities": []}, {"text": "The bitext used for this evaluation was the same aligned Hansards bitext used by, except that I used only 300,000 aligned segment pairs to save time.", "labels": [], "entities": [{"text": "Hansards bitext", "start_pos": 57, "end_pos": 72, "type": "DATASET", "confidence": 0.8683026134967804}]}, {"text": "The bitext was automatically pretokenized to delimit punctuation, English possessive pronouns, and French elisions.", "labels": [], "entities": []}, {"text": "Morphological variants in both halves of the bitext were stemmed to a canonical form.", "labels": [], "entities": []}, {"text": "The link types assigned by the converged model were sorted by the scores in Equation 36.", "labels": [], "entities": [{"text": "Equation", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.8095657825469971}]}, {"text": "shows the distribution of these scores on a log scale.", "labels": [], "entities": []}, {"text": "The log scale helps to illustrate the plateaus in the curve.", "labels": [], "entities": []}, {"text": "The longest plateau represents the set of word pairs that were linked once out of one co-occurrence (1/1) in the bitext.", "labels": [], "entities": []}, {"text": "All these word pairs were equally likely to be correct.", "labels": [], "entities": []}, {"text": "The second-longest plateau resulted from word pairs that were linked twice out of two co-occurrences (2/2) and the third longest plateau is from word pairs that were linked three times out of three co-occurrences (3/3).", "labels": [], "entities": []}, {"text": "As usual, the entries with higher scores were more likely to be correct.", "labels": [], "entities": []}, {"text": "By discarding entries with lower scores, coverage could be traded for accuracy.", "labels": [], "entities": [{"text": "coverage", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9969336986541748}, {"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9982060194015503}]}, {"text": "This trade-off was measured at three points, representing cutoffs at the end of each of the three longest plateaus.", "labels": [], "entities": []}, {"text": "The traditional method of measuring coverage requires knowledge of the correct link types, which is impossible to determine without a gold standard.", "labels": [], "entities": [{"text": "measuring coverage", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.5690740644931793}]}, {"text": "An approximate coverage measure can be based on the number of different words in the corpus.", "labels": [], "entities": []}, {"text": "For lexicons extracted from corpora, perfect coverage implies at least one entry containing each word in the corpus.", "labels": [], "entities": []}, {"text": "One-sided variants, which consider only source words, have also been used (.", "labels": [], "entities": []}, {"text": "shows both the marginal (one-sided) and the combined coverage at each of the three cutoff points.", "labels": [], "entities": [{"text": "coverage", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.961822509765625}]}, {"text": "It also shows the absolute number of (non-NULL) entries in each of the three lexicons.", "labels": [], "entities": []}, {"text": "Of course, the size of automatically induced lexicons depends on the size of the training bitext.", "labels": [], "entities": []}, {"text": "shows that, given a sufficiently large bitext, the method can automatically construct translation lexicons with as many entries as published bilingual dictionaries.", "labels": [], "entities": []}, {"text": "The next task was to measure accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9913500547409058}]}, {"text": "It would have taken too long to evaluate every lexicon entry manually.", "labels": [], "entities": []}, {"text": "Instead, I took five random samples (with replacement) of 100 entries each from each of the three lexicons.", "labels": [], "entities": []}, {"text": "Each of the samples was first compared to a translation lexicon extracted from a machine-readable bilingual dictionary ().", "labels": [], "entities": []}, {"text": "All the entries in the sample that appeared in the dictionary were assumed to be correct.", "labels": [], "entities": []}, {"text": "I checked the remaining entries in all the samples by hand.", "labels": [], "entities": []}, {"text": "To account for context-dependent translational equivalence, I evaluated the accuracy of the translation lexicons in the context of the bitext whence they were extracted, using a simple bilingual concordancer.", "labels": [], "entities": [{"text": "translational equivalence", "start_pos": 33, "end_pos": 58, "type": "TASK", "confidence": 0.7486358284950256}, {"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9985648989677429}]}, {"text": "A lexicon entry (u,v) was considered correct if u and v ever appeared as direct translations of each other in an aligned segment pair.", "labels": [], "entities": []}, {"text": "That is, a link type was considered correct if any of its tokens were correct.", "labels": [], "entities": []}, {"text": "Direct translations come in different flavors.", "labels": [], "entities": []}, {"text": "Most entries that I checked by hand were of the plain vanilla variety that you might find in a bilingual dictionary (entry type V).", "labels": [], "entities": []}, {"text": "However, a significant munber of words translated into a different part of speech (entry type P).", "labels": [], "entities": []}, {"text": "For instance, in the entry (protection, prot6g6), the English word is a noun but the French word is an adjective.", "labels": [], "entities": []}, {"text": "This entry appeared because to have protection is often translated as ~tre prot~g~ ('to be protected') in the bitext.", "labels": [], "entities": []}, {"text": "The entry will never occur in a bilingual dictionary, but users of translation lexicons, be they human or machine, will want to know that translations often happen this way.", "labels": [], "entities": []}, {"text": "The evaluation of translation models at the word type level is complicated by the possibility of phrasal translations, such as imm~diatement ~-~ right away.", "labels": [], "entities": []}, {"text": "All the methods being evaluated here produce models of translational equivalence between individual words only.", "labels": [], "entities": [{"text": "translational equivalence between individual words", "start_pos": 55, "end_pos": 105, "type": "TASK", "confidence": 0.8967358946800232}]}, {"text": "How can we decide whether a single-word translation \"matches\" a phrasal translation?", "labels": [], "entities": []}, {"text": "The answer lies in the observation that corpus-based lexicography usually involves a lexicographer.", "labels": [], "entities": []}, {"text": "Bilingual lexicographers can work with bilingual concordancing software that can point them to instances of any link type induced from a bitext and display these instances sorted by their contexts (e.g..", "labels": [], "entities": []}, {"text": "Given an incomplete link type, the lexicographer can usually reconstruct the complete link type from the contexts in the concordance.", "labels": [], "entities": []}, {"text": "For example, if the model proposes an equivalence between immddiatement and right, a bilingual concordance can show the lexicographer that the model was really trying to capture the equivalence between imm#diatement and right away or between imm#diatement and right now.", "labels": [], "entities": []}, {"text": "I counted incomplete entries in a third category (entry type I).", "labels": [], "entities": []}, {"text": "Whether links in this category should be considered correct depends on the application.", "labels": [], "entities": []}, {"text": "shows the distribution of correct lexicon entries among the types V, P and I. graphs the accuracy of the method against coverage, with 95% confidence intervals.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9997606873512268}, {"text": "coverage", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.9927957653999329}]}, {"text": "The upper curve represents accuracy when incomplete links are considered correct, and the lower when they are considered incorrect.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9995715022087097}]}, {"text": "On the former metric, the method can generate translation lexicons with accuracy and coverage both exceeding 90%, as well as dictionary-size translation lexicons that are over 99% correct.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9991422891616821}, {"text": "coverage", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9510800838470459}]}], "tableCaptions": [{"text": " Table 6  Distribution of different types of correct lexicon entries at varying levels of  coverage (mean + standard deviation).", "labels": [], "entities": []}]}