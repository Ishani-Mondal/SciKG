{"title": [{"text": "The Rhetorical Parsing of Unrestricted Texts: A Surface-based Approach", "labels": [], "entities": [{"text": "Rhetorical Parsing of Unrestricted Texts", "start_pos": 4, "end_pos": 44, "type": "TASK", "confidence": 0.8307405948638916}]}], "abstractContent": [{"text": "Coherent texts are not just simple sequences of clauses and sentences, but rather complex artifacts that have highly elaborate rhetorical structure.", "labels": [], "entities": []}, {"text": "This paper explores the extent to which well-formed rhetorical structures can be automatically derived by means of surface-form-based algorithms.", "labels": [], "entities": []}, {"text": "These algorithms identify discourse usages of cue phrases and break sentences into clauses, hypothesize rhetorical relations that hold among textual units, and produce valid rhetorical structure trees for unrestricted natural language texts.", "labels": [], "entities": []}, {"text": "The algorithms are empirically grounded in a corpus analysis of cue phrases and rely on a first-order formalization of rhetorical structure trees.", "labels": [], "entities": []}, {"text": "The algorithms are evaluated both intrinsically and extrinsically.", "labels": [], "entities": []}, {"text": "The intrinsic evaluation assesses the resemblance between automatically and manually constructed rhetorical structure trees.", "labels": [], "entities": []}, {"text": "The extrinsic evaluation shows that automatically derived rhetorical structures can be successfully exploited in the context of text summarization.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 128, "end_pos": 146, "type": "TASK", "confidence": 0.6991915702819824}]}], "introductionContent": [], "datasetContent": [{"text": "There are two ways to evaluate the correctness of the discourse trees that an automatic process builds.", "labels": [], "entities": []}, {"text": "One is to compare the automatically derived trees with trees that have been built manually.", "labels": [], "entities": []}, {"text": "The other is to evaluate the impact that they have on the accuracy of other natural language processing tasks, such as anaphora resolution, intention recognition, or text summarization.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9975510239601135}, {"text": "anaphora resolution", "start_pos": 119, "end_pos": 138, "type": "TASK", "confidence": 0.7015830278396606}, {"text": "intention recognition", "start_pos": 140, "end_pos": 161, "type": "TASK", "confidence": 0.6871023923158646}, {"text": "text summarization", "start_pos": 166, "end_pos": 184, "type": "TASK", "confidence": 0.7357884645462036}]}, {"text": "The rhetorical parser presented here was evaluated by following both of these avenues.", "labels": [], "entities": [{"text": "rhetorical parser", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.8577535450458527}]}], "tableCaptions": [{"text": " Table 4  Evaluation of the marker identification procedure.", "labels": [], "entities": [{"text": "marker identification", "start_pos": 28, "end_pos": 49, "type": "TASK", "confidence": 0.942707896232605}]}, {"text": " Table 5  Evaluation of the clause-like unit boundary identification procedure.", "labels": [], "entities": [{"text": "clause-like unit boundary identification", "start_pos": 28, "end_pos": 68, "type": "TASK", "confidence": 0.6201053708791733}]}, {"text": " Table 9  Distribution of the most frequently used 15  relations.", "labels": [], "entities": []}]}