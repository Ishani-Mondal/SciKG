{"title": [{"text": "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", "labels": [], "entities": [{"text": "Dialogue Act Modeling", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.5615734855333964}, {"text": "Automatic Tagging and Recognition of Conversational Speech", "start_pos": 26, "end_pos": 84, "type": "TASK", "confidence": 0.773699266569955}]}], "abstractContent": [{"text": "We describe a statistical approach for modeling dialogue acts in conversational speech, i.e., speech-act-like units such as STATEMENT, QUESTION, BACKCHANNEL, AGREEMENT, DISAGREEMENT , and APOLOGY.", "labels": [], "entities": [{"text": "STATEMENT", "start_pos": 124, "end_pos": 133, "type": "METRIC", "confidence": 0.8201728463172913}, {"text": "BACKCHANNEL", "start_pos": 145, "end_pos": 156, "type": "METRIC", "confidence": 0.9250710606575012}, {"text": "AGREEMENT", "start_pos": 158, "end_pos": 167, "type": "METRIC", "confidence": 0.9626538753509521}]}, {"text": "Our model detects and predicts dialogue acts based on lexical, colloca-tional, and prosodic cues, as well as on the discourse coherence of the dialogue act sequence.", "labels": [], "entities": []}, {"text": "The dialogue model is based on treating the discourse structure of a conversation as a hidden Markov model and the individual dialogue acts as observations emanating from the model states.", "labels": [], "entities": []}, {"text": "Constraints on the likely sequence of dialogue acts are modeled via a dialogue act n-gram.", "labels": [], "entities": []}, {"text": "The statistical dialogue grammar is combined with word n-grams, decision trees, and neural networks modeling the idiosyncratic lexical and prosodic manifestations of each dialogue act.", "labels": [], "entities": []}, {"text": "We develop a probabilistic integration of speech recognition with dialogue modeling, to improve both speech recognition and dialogue act classification accuracy.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.7788547277450562}, {"text": "dialogue modeling", "start_pos": 66, "end_pos": 83, "type": "TASK", "confidence": 0.8300759494304657}, {"text": "speech recognition", "start_pos": 101, "end_pos": 119, "type": "TASK", "confidence": 0.7765977084636688}, {"text": "dialogue act classification", "start_pos": 124, "end_pos": 151, "type": "TASK", "confidence": 0.698073148727417}, {"text": "accuracy", "start_pos": 152, "end_pos": 160, "type": "METRIC", "confidence": 0.8765292167663574}]}, {"text": "Models are trained and evaluated using a large hand-labeled database of 1,155 conversations from the Switchboard corpus of spontaneous human-to-human telephone speech.", "labels": [], "entities": [{"text": "Switchboard corpus of spontaneous human-to-human telephone speech", "start_pos": 101, "end_pos": 166, "type": "DATASET", "confidence": 0.9281232953071594}]}, {"text": "We achieved good dialogue act labeling accuracy (65% based on errorful, automatically recognized words and prosody, and 71% based on word transcripts, compared to a chance baseline accuracy of 35% and human accuracy of 84%) and a small reduction in word recognition error.", "labels": [], "entities": [{"text": "dialogue act labeling", "start_pos": 17, "end_pos": 38, "type": "TASK", "confidence": 0.6685860951741537}, {"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9412588477134705}, {"text": "accuracy", "start_pos": 181, "end_pos": 189, "type": "METRIC", "confidence": 0.5183103084564209}, {"text": "accuracy", "start_pos": 207, "end_pos": 215, "type": "METRIC", "confidence": 0.730101466178894}, {"text": "word recognition", "start_pos": 249, "end_pos": 265, "type": "TASK", "confidence": 0.7813295125961304}]}], "introductionContent": [{"text": "The ability to model and automatically detect discourse structure is an important step toward understanding spontaneous dialogue.", "labels": [], "entities": []}, {"text": "While there is hardly consensus on exactly how discourse structure should be described, some agreement exists that a useful first level of analysis involves the identification of dialogue acts (DAs).", "labels": [], "entities": [{"text": "identification of dialogue acts (DAs)", "start_pos": 161, "end_pos": 198, "type": "TASK", "confidence": 0.8441462091037205}]}, {"text": "A DA represents the meaning of an utterance at the level of illocutionary force).", "labels": [], "entities": []}, {"text": "Thus, a DA is approximately the equivalent of the speech act of, the conversational game move of, or the adjacency pair part of and.", "labels": [], "entities": []}, {"text": "shows a sample of the kind of discourse structure in which we are interested.", "labels": [], "entities": []}, {"text": "Each utterance is assigned a unique DA label (shown in column 2), drawn from a well-defined set (shown in).", "labels": [], "entities": []}, {"text": "Thus, DAs can bethought of as a tag set that classifies utterances according to a combination of pragmatic, semantic, and syntactic criteria.", "labels": [], "entities": []}, {"text": "The computational community has usually defined these DA categories so as to be relevant to a particular application, although efforts are underway to develop DA labeling systems that are domain-independent, such as the Discourse Resource Initiative's DAMSL architecture.", "labels": [], "entities": []}, {"text": "While not constituting dialogue understanding in any deep sense, DA tagging seems clearly useful to a range of applications.", "labels": [], "entities": [{"text": "dialogue understanding", "start_pos": 23, "end_pos": 45, "type": "TASK", "confidence": 0.7434496283531189}, {"text": "DA tagging", "start_pos": 65, "end_pos": 75, "type": "TASK", "confidence": 0.9545888602733612}]}, {"text": "For example, a meeting summarizer needs to keep track of who said what to whom, and a conversational agent needs to know whether it was asked a question or ordered to do something.", "labels": [], "entities": []}, {"text": "In related work DAs are used as a first processing step to infer dialogue games), a slightly higher level unit that comprises a small number of DAs.", "labels": [], "entities": []}, {"text": "Interactional dominance) might be measured more accurately using DA distributions than with simpler techniques, and could serve as an indicator of the type or genre of discourse at hand.", "labels": [], "entities": []}, {"text": "In all these cases, DA labels would enrich the available input for higher-level processing of the spoken words.", "labels": [], "entities": []}, {"text": "Another important role of DA information could be feedback to lower-level processing.", "labels": [], "entities": [{"text": "DA information", "start_pos": 26, "end_pos": 40, "type": "TASK", "confidence": 0.9093043804168701}]}, {"text": "For example, a speech recognizer could be constrained by expectations of likely DAs in a given context, constraining the potential recognition hypotheses so as to improve accuracy.", "labels": [], "entities": [{"text": "speech recognizer", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.696540042757988}, {"text": "accuracy", "start_pos": 171, "end_pos": 179, "type": "METRIC", "confidence": 0.9960165619850159}]}, {"text": "The 42 dialogue act labels.", "labels": [], "entities": []}, {"text": "DA frequencies are given as percentages of the total number of utterances in the overall corpus.", "labels": [], "entities": []}], "datasetContent": [{"text": "We tested both the mixture-of-posteriors and the mixture-of-LMs approaches on our Switchboard test set of 19 conversations.", "labels": [], "entities": [{"text": "Switchboard test set of 19 conversations", "start_pos": 82, "end_pos": 122, "type": "DATASET", "confidence": 0.9224255283673605}]}, {"text": "Instead of decoding the data from scratch using the modified models, we manipulated n-best lists consisting of up to 2,500 best hypotheses for each utterance.", "labels": [], "entities": []}, {"text": "This approach is also convenient since both approaches require access to the full word string for hypothesis scoring; the overall model is no longer Markovian, and is therefore inconvenient to use in the first decoding stage, or even in lattice rescoring.", "labels": [], "entities": [{"text": "hypothesis scoring", "start_pos": 98, "end_pos": 116, "type": "TASK", "confidence": 0.7624044418334961}]}, {"text": "The baseline for our experiments was obtained with a standard backoff trigram language model estimated from all available training data.", "labels": [], "entities": []}, {"text": "The DA-specific language models were trained on word transcripts of all the training utterances of a given type, and then smoothed further by interpolating them with the baseline LM.", "labels": [], "entities": []}, {"text": "Each DAspecific LM used its own interpolation weight, obtained by minimizing the perplexity of the interpolated model on held-out DA-specific training data.", "labels": [], "entities": []}, {"text": "Note that this smoothing step is helpful when using the DA-specific LMs for word recognition, but not for DA classification, since it renders the DA-specific LMs less discriminative.", "labels": [], "entities": [{"text": "word recognition", "start_pos": 76, "end_pos": 92, "type": "TASK", "confidence": 0.8394696414470673}, {"text": "DA classification", "start_pos": 106, "end_pos": 123, "type": "TASK", "confidence": 0.927094578742981}]}, {"text": "9 summarizes both the word error rates achieved with the various models and the perplexities of the corresponding LMs used in the rescoring (note that perplexity is not meaningful in the mixture-of-posteriors approach).", "labels": [], "entities": [{"text": "word error rates", "start_pos": 22, "end_pos": 38, "type": "METRIC", "confidence": 0.6514265239238739}]}, {"text": "For comparison, we also included two additional models: the 'q-best LM\" refers to always using the DAspecific LM corresponding to the most probable DA type for each utterance.", "labels": [], "entities": []}, {"text": "It is thus an approximation to both mixture approaches where only the top DA is considered.", "labels": [], "entities": []}, {"text": "Second, we included an \"oracle LM,\" i.e., always using the LM that corresponds to the hand-labeled DA for each utterance.", "labels": [], "entities": []}, {"text": "The purpose of this experiment was to give us an upper bound on the effectiveness of the mixture approaches, by assuming perfect DA recognition.", "labels": [], "entities": [{"text": "DA recognition", "start_pos": 129, "end_pos": 143, "type": "TASK", "confidence": 0.8905811309814453}]}, {"text": "It was somewhat disappointing that the word error rate (WER) improvement in the oracle experiment was small (2.2% relative), even though statistically highly significant (p < .0001, one-tailed, according to a Sign test on matched utterance pairs).", "labels": [], "entities": [{"text": "word error rate (WER)", "start_pos": 39, "end_pos": 60, "type": "METRIC", "confidence": 0.8512234091758728}]}], "tableCaptions": [{"text": " Table 6  DA classification accuracies (in %) from transcribed and recognized  words (chance = 35%).", "labels": [], "entities": [{"text": "DA classification", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.8117562532424927}]}, {"text": " Table 7  DA classification using prosodic  decision trees (chance = 35%).", "labels": [], "entities": [{"text": "DA classification", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.8813453316688538}, {"text": "chance", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.9752048850059509}]}, {"text": " Table 11  Switchboard word recognition error rates and  LM perplexities.", "labels": [], "entities": [{"text": "word recognition error rates", "start_pos": 23, "end_pos": 51, "type": "TASK", "confidence": 0.7076791599392891}, {"text": "LM perplexities", "start_pos": 57, "end_pos": 72, "type": "METRIC", "confidence": 0.8312084972858429}]}]}