{"title": [{"text": "A Compression-based Algorithm for Chinese Word Segmentation", "labels": [], "entities": [{"text": "Chinese Word Segmentation", "start_pos": 34, "end_pos": 59, "type": "TASK", "confidence": 0.5855936308701833}]}], "abstractContent": [{"text": "Chinese is written without using spaces or other word delimiters.", "labels": [], "entities": []}, {"text": "Although a text maybe thought of as a corresponding sequence of words, there is considerable ambiguity in the placement of boundaries.", "labels": [], "entities": []}, {"text": "Interpreting a text as a sequence of words is beneficial for some information retrieval and storage tasks:for example,full-text search, word-based compression, and keyphrase extraction.", "labels": [], "entities": [{"text": "Interpreting a text as a sequence of words", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.8102995306253433}, {"text": "information retrieval and storage", "start_pos": 66, "end_pos": 99, "type": "TASK", "confidence": 0.7448351606726646}, {"text": "word-based compression", "start_pos": 136, "end_pos": 158, "type": "TASK", "confidence": 0.7630078792572021}, {"text": "keyphrase extraction", "start_pos": 164, "end_pos": 184, "type": "TASK", "confidence": 0.7550221979618073}]}, {"text": "We describe a scheme that infers appropriate positions for word boundaries using an adaptive language model that is standard in text compression.", "labels": [], "entities": [{"text": "text compression", "start_pos": 128, "end_pos": 144, "type": "TASK", "confidence": 0.7045037001371384}]}, {"text": "It is trained on a corpus of presegmented text, and when applied to new text, interpolates word boundaries so as to maximize the compression obtained.", "labels": [], "entities": []}, {"text": "This simple and general method performs well with respect to specialized schemes for Chinese language segmentation.", "labels": [], "entities": [{"text": "Chinese language segmentation", "start_pos": 85, "end_pos": 114, "type": "TASK", "confidence": 0.626116156578064}]}], "introductionContent": [{"text": "Languages such as Chinese and Japanese are written without using any spaces or other word delimiters (except for punctuation marks)--indeed, the Western notion of a word boundary is literally alien).", "labels": [], "entities": []}, {"text": "Nevertheless, words are present in these languages, and Chinese words often comprise several characters, typically two, three, or four--five-character words also exist, but they are rare.", "labels": [], "entities": []}, {"text": "Many characters can standalone as words in themselves, while on other occasions the same character is the first or second character of a two-character word, and on still others it participates as a component of a three-or four-character word.", "labels": [], "entities": []}, {"text": "This phenomenon causes obvious ambiguities in word segmentation.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.7322482913732529}]}, {"text": "Readers unfamiliar with Chinese can gain an appreciation of the problem of multiple interpretations from, which shows two alternative interpretations of the same Chinese character sequence.", "labels": [], "entities": []}, {"text": "The text is a joke that relies on the ambiguity of phrasing.", "labels": [], "entities": []}, {"text": "Once upon a time, the story goes, a man set out on along journey.", "labels": [], "entities": []}, {"text": "Before he could return home the rainy season began, and he had to take shelter at a friend's house.", "labels": [], "entities": []}, {"text": "But he overstayed his welcome, and one day his friend wrote him a note: the first line in.", "labels": [], "entities": []}, {"text": "The intended interpretation is shown in the second line, which means \"It is raining, the god would like the guest to stay.", "labels": [], "entities": []}, {"text": "Although the god wants you to stay, I do not!\"", "labels": [], "entities": []}, {"text": "On seeing the note, the visitor took the hint and prepared to leave.", "labels": [], "entities": []}, {"text": "As a joke he amended the note with the punctuation shown in the third line, which leaves three sentences whose meaning is totally different--\"The rainy day, the staying day.", "labels": [], "entities": []}, {"text": "Would you like me to stay?", "labels": [], "entities": []}, {"text": "This example relies on ambiguity of phrasing, but the same kind of problem can arise with word segmentation.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 90, "end_pos": 107, "type": "TASK", "confidence": 0.7233514934778214}]}, {"text": "shows a more prosaic example.", "labels": [], "entities": []}, {"text": "For the ordinary sentence of the first line, there are two different interpretations depending on the context of the sentence: \"I like New Zealand flowers\" and \"I like fresh broccoli\" respectively.", "labels": [], "entities": []}, {"text": "The fact that machine-readable Chinese text is invariably stored in unsegmented form causes difficulty in applications that use the word as the basic unit.", "labels": [], "entities": []}, {"text": "For example, search engines index documents by storing a list of the words they contain, and allow the user to retrieve all documents that contain a specified combination of query terms.", "labels": [], "entities": []}, {"text": "This presupposes that the documents are segmented into words.", "labels": [], "entities": []}, {"text": "Failure to do so, and treating every character as a word in itself, greatly decreases the precision of retrieval since large numbers of extraneous documents are returned that contain characters, but not words, from the query.", "labels": [], "entities": [{"text": "precision", "start_pos": 90, "end_pos": 99, "type": "METRIC", "confidence": 0.9994887113571167}]}, {"text": "illustrates what happens when each character in a query is treated as a single-character word.", "labels": [], "entities": []}, {"text": "The intended query is \"physics\" or \"physicist.\"", "labels": [], "entities": []}, {"text": "The first character returns documents about such things as \"evidence,\" \"products,\" \"body, ....", "labels": [], "entities": []}, {"text": "image,\" \"prices\"; while the second returns documents about \"theory, ....", "labels": [], "entities": []}, {"text": "Thus many documents that are completely irrelevant to the query will be returned, causing the precision of information retrieval to decrease greatly.", "labels": [], "entities": [{"text": "precision", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.9995214939117432}]}, {"text": "Similar problems occur in word-based compression, speech recognition, and soon.", "labels": [], "entities": [{"text": "word-based compression", "start_pos": 26, "end_pos": 48, "type": "TASK", "confidence": 0.7709119617938995}, {"text": "speech recognition", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.7753893435001373}]}, {"text": "It is true that most search engines allow the user to search for multiword phrases by enclosing them in quotation marks, and this facility could be used to search for multicharacter words in Chinese.", "labels": [], "entities": []}, {"text": "This, however, runs the risk of retrieving irrelevant documents in which the same characters occur in sequence but with a different intended segmentation.", "labels": [], "entities": []}, {"text": "More importantly, it imposes on the user an artificial requirement to perform manual segmentation on each full-text query.", "labels": [], "entities": []}, {"text": "Word segmentation is an important prerequisite for such applications.", "labels": [], "entities": [{"text": "Word segmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6704068779945374}]}, {"text": "However, it is a difficult and ill-defined task.", "labels": [], "entities": []}, {"text": "According to and, experiments show that only about 75% agreement between native speakers is to be expected on the \"correct\" segmentation, and the figure reduces as more people become involved.", "labels": [], "entities": []}, {"text": "This paper describes a general scheme for segmenting text by inferring the position of word boundaries, thus supplying a necessary preprocessing step for applications like those mentioned above.", "labels": [], "entities": []}, {"text": "Unlike other approaches, which involve a dictionary of legal words and are therefore language-specific, it works by using a corpus of alreadysegmented text for training and thus can easily be retargeted for any language for which a suitable corpus of segmented material is available.", "labels": [], "entities": []}, {"text": "To infer word boundaries, a general adaptive text compression technique is used that predicts upcoming characters on the basis of their preceding context.", "labels": [], "entities": [{"text": "adaptive text compression", "start_pos": 36, "end_pos": 61, "type": "TASK", "confidence": 0.7572932442029318}]}, {"text": "Spaces are inserted into positions where their presence enables the text to be compressed more effectively.", "labels": [], "entities": []}, {"text": "This approach means that we can capitalize on existing research in text compression to create good models for word segmentation.", "labels": [], "entities": [{"text": "text compression", "start_pos": 67, "end_pos": 83, "type": "TASK", "confidence": 0.7439507246017456}, {"text": "word segmentation", "start_pos": 110, "end_pos": 127, "type": "TASK", "confidence": 0.7462394535541534}]}, {"text": "To build a segmenter fora new language, the only resource required is a corpus of segmented text to train the compression model.", "labels": [], "entities": []}, {"text": "The structure of this paper is as follows: The next section reviews previous work on the Chinese segmentation problem.", "labels": [], "entities": [{"text": "Chinese segmentation problem", "start_pos": 89, "end_pos": 117, "type": "TASK", "confidence": 0.79334823290507}]}, {"text": "Then we explain the operation of the adaptive text compression technique that will be used to predict word boundaries.", "labels": [], "entities": [{"text": "adaptive text compression", "start_pos": 37, "end_pos": 62, "type": "TASK", "confidence": 0.6989699999491373}, {"text": "predict word boundaries", "start_pos": 94, "end_pos": 117, "type": "TASK", "confidence": 0.7498621543248495}]}, {"text": "Next we show how space insertion can be viewed as a problem of hidden Markov modeling, and how higher-order models, such as the ones used in text compression, can be employed in this way.", "labels": [], "entities": [{"text": "space insertion", "start_pos": 17, "end_pos": 32, "type": "TASK", "confidence": 0.7960470914840698}, {"text": "text compression", "start_pos": 141, "end_pos": 157, "type": "TASK", "confidence": 0.7549552619457245}]}, {"text": "The following section describes several experiments designed to evaluate the success of the new word segmenter.", "labels": [], "entities": [{"text": "word segmenter", "start_pos": 96, "end_pos": 110, "type": "TASK", "confidence": 0.7596811056137085}]}, {"text": "Finally we discuss the application of language segmentation in digital libraries.", "labels": [], "entities": [{"text": "language segmentation", "start_pos": 38, "end_pos": 59, "type": "TASK", "confidence": 0.7438843846321106}]}, {"text": "Our system for segmenting Chinese text is available on the World Wide Web at http://www.nzdl.org/cgi-bin/congb.", "labels": [], "entities": [{"text": "segmenting Chinese text", "start_pos": 15, "end_pos": 38, "type": "TASK", "confidence": 0.8834560513496399}]}, {"text": "It takes GB-encoded input text, which can be cut from a Chinese document and pasted into the input window.", "labels": [], "entities": []}, {"text": "1 Once the segmenter has been invoked, the result is rewritten into the same window.", "labels": [], "entities": []}], "datasetContent": [{"text": "Before describing experiments to assess the success of the new word segmentation method, we first discuss measures that are used to evaluate the accuracy of automatic segmentation.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 63, "end_pos": 80, "type": "TASK", "confidence": 0.7093416601419449}, {"text": "accuracy", "start_pos": 145, "end_pos": 153, "type": "METRIC", "confidence": 0.9972383975982666}]}, {"text": "We then examine the application of the new segmentation method to English text, and show how it achieves results that significantly outperform the state of the art.", "labels": [], "entities": []}, {"text": "Next we describe application to a manually segmented corpus of Chinese text; again, excellent results are achieved.", "labels": [], "entities": []}, {"text": "Ina further experiment where we apply a model generated from the corpus to anew, independent, test file, performance deteriorates considerably--as one might expect.", "labels": [], "entities": []}, {"text": "We then apply the method to a different corpus, Teahan, Wen, McNab, and Witten Chinese Word Segmentation and investigate how well the model transfers from one corpus to another.", "labels": [], "entities": [{"text": "Chinese Word Segmentation", "start_pos": 79, "end_pos": 104, "type": "TASK", "confidence": 0.5781160493691763}]}, {"text": "We end with a discussion of how the results vary with the order of the compression model used to drive the segmenter.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3  Distribution of word length in  the corpus.", "labels": [], "entities": []}, {"text": " Table 4  Results for five 500-word segments from the Chinese corpus  (manually checked figures in parentheses).", "labels": [], "entities": [{"text": "Chinese corpus", "start_pos": 54, "end_pos": 68, "type": "DATASET", "confidence": 0.7319842278957367}]}, {"text": " Table 5  Error rate (mean and sd) for 1,000-word files from PH  and Rocling corpora.", "labels": [], "entities": [{"text": "Error rate", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9893550872802734}]}, {"text": " Table 6  Error rate (mean and sd) for different amounts of training data.", "labels": [], "entities": [{"text": "Error rate", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9927312731742859}]}]}